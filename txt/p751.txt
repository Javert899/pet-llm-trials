wanna improve process mining results?
it's high time we consider data quality issues seriously
r.p. jagadeesh chandra bose, ronny s. mans and wil m.p. van der aalst
department of mathematics and computer science, eindhoven university of
technology, p.o. box 513, nl-5600 mb, eindhoven, the netherlands.
{j.c.b.rantham.prabhakara,r.s.mans,w.m.p.v.d.aalst}@tue.nl
abstract. the growing interest in process mining is fueled by the grow-
ing availability of event data. process mining techniques use event logs
to automatically discover process models, check conformance, identify
bottlenecks and deviations, suggest improvements, and predict process-
ing times. lion's share of process mining research has been devoted to
analysis techniques. however, the quality of the events logs used as in-
put is critical for the success of any process mining eort. in this paper,
we identify ten categories of data quality issues, e.g., problems related to
timestamps in event logs, event granularity, ambiguity of activity names,
and mashed logging are described in detail. the systematic identication
and analysis of these problems calls for a consolidated eort from the pro-
cess mining community. five real-life event logs are analyzed to illustrate
the omnipresence data quality issues. we hope that these ndings will
encourage systematic logging approaches (to prevent data quality prob-
lems) and repair techniques (to alleviate data quality problems).
key words: process mining, data quality, event log, preprocessing,
data cleansing, outliers
1 introduction
htodo: abstract has been
added. various parts have
been rewritten. in other parts
only minor changes. ibusiness processes leave trails in a variety of data sources (e.g., audit trails,
databases, and transaction logs). process mining is a relatively young research
discipline aimed at discovering, monitoring and improving real processes by
extracting knowledge from event logs readily available in today's information
systems [1]. remarkable success stories have been reported on the applica-
bility of process mining based on event logs from real-life workow manage-
ment/information systems. in recent years, the scope of process mining broad-
ened from the analysis of workow logs to the analysis event data recorded by
physical devices, web services, erp systems, and transportation systems. pro-
cess mining has been applied to the logs of high-tech systems (e.g., medical
devices such as x-ray machines and ct scanners), copiers and printers, mission-
critical defense systems. the insights obtained through process mining are used
to optimize business processes and improve customer service. organizations ex-
pect process mining to produce accurate insights regarding their processes while
depicting only the desired traits and removing all irrelevant details . in addition,they expect the results to be comprehensible and context-sensitive .
while the success stories reported on using process mining are certainly con-
vincing, it is not easy to reproduce these best practices in many settings due to
the quality of event logs and the nature of processes. for example, contemporary
process discovery approaches have problems in dealing with ne-grained event
logs and less structured processes . the resulting spaghetti-like process models are
often hard to comprehend [1].
we have applied process mining techniques in over 100 organizations. these
practical experiences revealed that real-life logs are often far from ideal and their
quality leaves much to be desired. most real-life logs tend to be ne-granular ,
heterogeneous ,voluminous ,incomplete , and noisy . some of the more advanced
process discovery techniques try to address these problems. however, as the say-
ing \garbage in { garbage out" suggests, more attention should be paid to the
quality of event logs before applying process mining algorithms. the strongest
contributions addressing the `business process intelligence challenge' event logs
illustrate the signicance of log preprocessing [2{4].
the process mining manifesto [6] also stresses the need for high-quality event
logs. the manifesto lists ve maturity levels ranging from one star ( ?) to ve
stars ( ? ? ? ? ? ). at the lowest maturity level, event logs are of poor quality,
i.e., recorded events may not correspond to reality and events may be missing.
a typical example is an event log where events are recorded manually. at the
highest maturity level, event logs are of excellent quality (i.e., trustworthy and
complete) and events are well-dened. in this case, the events (and all of their
attributes) are recorded automatically and have clear semantics. for example,
the events may refer to a commonly agreed upon ontology.
in this paper, drawing from our experiences, we elicit a list of common data
quality issues that we encounter in event logs and their impact on process min-
ing. we describe ten categories of data quality problems. for example, there is
a category describing three timestamp related issues: (1) course granular times-
tamps (e.g., logs having just date information such that ordering of events on the
same day is unknown), (2) mixed granular timestamps (i.e., event logs that have
timestamps with dierent levels of precision, e.g., milliseconds, seconds, min-
utes, days), and (3) incorrect timestamps (e.g., events referring to dates that do
not exist or where timestamps and ordering information are conicting). inter-
estingly these problems appear in all application domains. for example, when
looking at hospital data we often encounter the problem that only dates are
recorded. when looking at x-ray machines, the events are recorded with mil-
lisecond precision, but due to buering and distributed clocks these timestamps
may be incorrect. in this paper we systematically identify the dierent problems
and suggest approaches to address them. we also evaluate several real-life event
logs to demonstrate that the classication can be used to identify problems.
2the rest of the paper is organized as follows. section 2 elicits a list of com-
mon data quality issues grouped in ten categories. in section 3, we analyze ve
real-life logs and evaluate data quality problems using the classication pre-
sented in section 2. related work is presented in section 4. finally, conclusions
are presented in section 5. htodo: the paper has two
weaknesses. first of all, the
paper is about event logs, but
a rigorous definition is miss-
ing. terminology is changing
constantly: trace, case, pro-
cess instance, etc. second, the
paper only points out prob-
lems and not solutions. only
hints are given for solutions.
i would also have put more
emphasis on prevention: how
to get level 5 logs. given the
space problems i would not
know how to address these
issues. i2 categories of process mining data quality problems
htodo: discussion: why are
the categories not orthogonal?
using a formal definition of
the log, one could do this i
think. see my visio file to
start the discussion iin this section, we present a comprehensive overview of data quality problems
related to event logs to be used for process mining. we identify ten broad (not
necessarily orthogonal) classes of data quality issues. each potential quality issue
is described in detail using a standard format that includes: a description of the
problem and the way it manifests in a log, examples, and its impact on the
application of process mining.
2.1 event granularity
description. event logs are often ne-grained and too detailed for most stake-
holders. the granularity at which events are logged varies widely (across do-
mains/applications) without considering the desired levels of analysis. events in
an event log are often at very dierent levels of granularity. this problem is due
to the lack of good standards and guidelines for logging. analysts and end users
often prefer higher levels of abstraction without being confronted with low level
events stored in raw event logs. one of the major challenges in process mining
is to bridge the gap between the higher level conceptual view of the process and
the low level event logs.
example. fine-granularity is more pronounced in event logs of high-tech sys-
tems and in event logs of information systems where events typically correspond
to automated statements in software supporting the information system. one
can also see such phenomena in event logs of healthcare processes, e.g., one can
see events related to ne-grained tests performed at a laboratory in conjunction
with coarse-grained surgical procedures.
eect. process mining techniques have diculties in dealing with ne-granular
event logs. for example, the discovered process models are often spaghetti-like
and hard to comprehend. for a log with jajevent classes (activities), a at
process model can be viewed as a graph containing jajnodes with edges corre-
sponding to the causality dened by the execution behavior in the log. graphs
become quickly overwhelming and unsuitable for human perception and cogni-
tive systems even if there are more than a few dozens of nodes [7]. this problem
is compounded if the graph is dense (which is often the case in unstructured pro-
cesses) thereby compromising the comprehensibility of models. several attempts
have been reported in the literature on grouping events to create higher levels
of abstraction ranging from the use of semantic ontologies [8,9] to the grouping
3of events based on correlating activities [10, 11] or the use of common patterns
of execution manifested in the log [12, 13]. however, most of these techniques
are tedious (e.g., semantic ontologies), only partly automated (e.g., abstractions
based on patterns), lack domain signicance (e.g., correlation of activities), or
result in discarding relevant information (e.g., abstraction).
2.2 case heterogeneity
description. many of today's processes are designed to be exible . this results
in event logs containing a heterogeneous mix of usage scenarios with diverse and
unstructured behaviors. although it is desirable to also record the scenario cho-
sen for a particular case, it is infeasible to dene all possible variants. another
source of heterogeneity stems from operational processes that change over time
to adapt to changing circumstances, e.g., new legislation, extreme variations in
supply and demand, seasonal eects, etc.
example. there is a growing interest in analyzing event logs of high-tech sys-
tems such as x-ray machines, wafer scanners, and copiers and printers. these
systems are complex large scale systems supporting a wide range of functionality.
for example, medical systems support medical procedures that have hundreds
of potential variations. these variations create heterogeneity in event logs.
eect. process mining techniques have problems when dealing with heterogene-
ity in event logs. for example, process discovery algorithms produce spaghetti-
like incomprehensible process models. moreover, users would be interested in
learning any variations in process behavior and have their insights on the pro-
cess put in perspective to those variations. analyzing the whole event log in
the presence of heterogeneity fails to achieve this objective. trace clustering has
been shown to be an eective way of dealing with such heterogeneity [14{19].
the basic idea of trace clustering is to partition an event log into homogenous
subsets of cases. analyzing homogenous subsets of cases is expected to improve
the comprehensibility of process mining results. in spite of its success, trace clus-
tering still remains a subjective technique. a desired goal would be to introduce
some objectivity in partitioning the log into homogenous cases.
2.3 voluminous data
description. today, we see an unprecedented growth of data from a wide vari-
ety of sources and systems across many domains and applications. for example,
high-tech systems such as medical systems and wafer scanners produce large
amounts of data, because they typically capture very low-level events such as
the events executed by the system components, application level events, net-
work/communication events, and sensor readings (indicating status of compo-
nents etc.). each atomic event in these environments has a short life-time and
hundreds of events can be triggered within a short time span (even within a
second).
example. boeing jet engines can produce 20 terabytes (tb) of operational in-
formation per hour. in just one atlantic crossing, a four-engine jumbo jet can
4generate 640 terabytes of data [20]. such voluminous data is emanating from
many areas such as banking, insurance, nance, retail, healthcare, and telecom-
munications. for example, walmart is logging one million customer transactions
per hour and feeding information into databases estimated at 2 :5 petabytes in
size [20], a global customer relationship management (crm) company is han-
dling around 10 million calls a day with 10{12 customer interaction events as-
sociated with each call [21]. the term \big data" has emerged to refer to the
spectacular growth of digitally recorded data [22].
eect. process mining has become all the more relevant in this era of \big data"
than ever before. the complexity of data demands powerful tools to mine use-
ful information and discover hidden knowledge. contemporary process mining
techniques/tools are unable to cope with massive event logs. there is a need
for research in both the algorithmic as well as deployment aspects of process
mining. for example, we should move towards developing ecient, scalable, and
distributed algorithms in process mining.
2.4 timestamp related issues
for the application of process mining, correct and precise knowledge about the
time at which events occurred is important. however, in practice we often nd
timestamp-related problems. we distinguish three types of problems.
{coarse granular timestamps
description. this corresponds to the scenario where a coarse level of gran-
ularity is used for the timestamps of events. this implies that the ordering
of events within the log may not conform to the actual ordering in which the
events occurred in reality. for example, the ordering of multiple events on the
same day may be lost due to events having only date information.
example. fig. 1 exemplies a too coarse granularity of timestamps. for a
trace, events \b" and \c" both have \05-12-2011" as the timestamp. it is not
clear whether event \b" occurred before \c" or the other way around. for
event \a" having timestamp \03-12-2011" it is clear that it occurred before
both \b" and \c". likewise, event \d" occurred after both \b" and \c" as
it has the timestamp \06-12-2011".
eect. process mining algorithms for discovering the control-ow assume
that all events within the log are totally ordered. as multiple events may exist
within a trace with the same timestamp, process mining algorithms may have
problems with identifying the correct control-ow. in particular, discovered
control-ow models tend to have a substantial amount of activities which oc-
cur in parallel. furthermore, event logs with coarse granular timestamps are
incapacitated for certain types of process mining analysis such as performance
analysis.
{mixed granular timestamps
description. the level of granularity of timestamps does not need to be the
same across all events in an event log, i.e. there are pairs of events for which
the level of granularity of their timestamps is dierent (e.g., seconds versus
5a b c d
time=
03-12-2011time=
05-12-2011time=
05-12-2011time=
06-12-2011?fig. 1. coarse granularity of timestamps: for events \b" and \c", it is not clear in
which order they occurred as both occurred on the same day.
days).
example. fig. 2 exemplies a mixed granularity of timestamps. for a trace,
there is an event \b" with timestamp \05-12-2011" and an event \c" with
timestamp \05-12-2012 17:59". it is not clear whether event \b" occurred
before \c" or the other way around.
eect. the eect of event logs with mixed granular timestamps is similar
to that of coarse granular event logs. for events having a timestamp with
a coarse level of granularity, the precise ordering may not be clear in case of
events which have a similar but more ne-grained timestamp. as such, process
mining algorithms have problems discovering the correct control-ow for these
events, e.g., sequential activities are modeled in parallel.
a b c d
time=
03-12-2011time=
05-12-2011time=
05-12-2011
17:59time=
06-12-2011?
fig. 2. mixed granularity of timestamps: for events \b" and \c" it is not clear in
which order they occurred as they both occurred on the same day. furthermore, event
\c" has a more ne-grained timestamp than that of event \b".
{incorrect timestamps
description. this corresponds to the scenario where the recorded timestamp
of (some or all) events in the log does not correspond to the real time at which
the event has occurred.
example. fig. 3(a) exemplies an incorrect recording of the timestamp of
an event. event \a" has been automatically registered at time \03-12-2011
13:45". however, the timestamp of the event recorded in the log is \03-12-2011
13:59". another example is that of a timestamp recorded as february 29 in a
non-leap year.
eect. process mining algorithms discover the control-ow based on behav-
6ior observed in the log. however, due to incorrect timestamps, the discovered
control-ow relations may be unreliable or even incorrect. moreover, in ap-
plications such as the discovery of signature patterns for diagnostic purposes
(e.g., fraudulent claims, fault diagnosis, etc.) [23], there is a danger of reversal
ofcause and eect phenomena due to incorrect timestamps. fig. 3(b) depicts
an example of such a scenario. although the events \a" and \b" occur in that
particular order in reality, they are logged as \b" and \a" thereby plausibly
leading to incorrect inference that \b" causes \a".
a
logged time(a)=
03-12-2011 13:59real time(a)=
03-12-2011 13:45
(a)
a b
logged time(a)=
03-12-2011 13:59real time(a)=
03-12-2011 13:45
logged time(b)=
03-12-2011 13:50real time(b)=
03-12-2011 13:50 (b)
fig. 3. incorrect timestamps: (a) event \a" occurred in reality at \03-12-2011 13:45",
but the timestamp of the event recorded in the log is \03-12-2011 13:59", (b) as a result
one can wrongly infer that \b" causes \a" where in reality \a" occurred before \b".
2.5 missing data
within a log, dierent kinds of information can be missing. we distinguish three
dierent types of missing information.
{missing attribute values
description. this corresponds to the scenario where in the event log cer-
tain attributes are missing or there are attributes that have no value. such
attributes can either belong to a trace (e.g. the identier of the case or the
name of the customer etc.) or an event (e.g. the name of the task to which
the event refers to or the timestamp of the event etc.).
example. fig. 4 exemplies missing data for both an event and a trace. for
the event, no information has been given about the associated task and the
time at which it occurred. for the trace, an unique identier is missing.
eect. event logs with missing attributes/values hinder the application of
certain process mining algorithms. for example, control-ow discovery algo-
rithms are impacted by missing timestamps or task information whereas tech-
niques that analyze the organizational-perspective are aected if information
is missing about the actor/resource that handled an event. in order to deal
with missing values, aected events or traces may be removed from the event
log. this may have as counter eect that too little information is remaining
in the log in order to obtain reliable results.
7atrace level
attributes
event level
attributesname = ?
originator = jane
time = ?identiﬁer = ?
name = john
age = 64fig. 4. missing values: for event \a" the name of the associated task and the time
at which the event occurred are missing. for the trace no unique identier has been
given.
{missing events (anywhere in a trace)
description. this corresponds to the scenario where some events may be
missing anywhere within the trace although they occurred in reality.
example. fig. 5 exemplies a missing event in a trace. in reality, events \a",
\b", and \c", have occurred. however, only events \a" and \c" have been
logged (event \b" has not been recorded for the trace).
eect. as events may be missing, there may be problems with the results
that are produced by a process mining algorithm. in particular, relations may
be inferred which hardly or even do not exist in reality.
a b c
fig. 5. missing events (anywhere in a trace): events \a", \b", and \c" have occurred
in reality. only events \a" and \b" are included in the log whereas event \b" is not
included.
{partial / incomplete traces
description. this is a special case of missing events where the prex and/or
sux events corresponding to a trace is missing although they occurred in
reality. this is more prevalent in scenarios where the event data for analysis
is considered over a dened time interval (e.g., between jan'12 and jun'12).
the initial events (prex) of cases that have started before jan'12 would have
been omitted due to the manner of event data selection. likewise cases that
have started between jan'12 and jun'12 but not yet completed by jun'12 are
incomplete and have their suxes missing.
example. fig. 6 exemplies a partial/incomplete trace. in reality, events \a",
\b", \c", \d", \e", and \f" have occurred. however, only events \c" and
\d" have been logged whereas the prex with events \a" and \b" and the
8sux with events \e" and \f" have not been recorded for the trace.
eect. for some algorithms there may be problems with the produced result
as dierent relations may be inferred. in particular, dierent relations may be
inferred for the start or end of a process. however, there are algorithms which
can deal with noise (e.g. the fuzzy miner [11] and the heuristics miner [1]).
alternatively, partial / incomplete traces may be ltered from the log. this
may have as counter eect that too less information is remaining in the log in
order to obtain reliable results.
a b c d e f
fig. 6. partial / incomplete traces: events \a", \b", \c", \d", \e", and \f" have
occurred in reality. only events \c" and \d" are included in the log whereas the prex
with events \a" and \b" and the sux with events \e" and \f" are not included.
2.6 ambiguity between events
{duplicate events
description. this corresponds to the scenario where multiple events have
the same activity name. these events may have the same connotation as, for
example, they occur in a row. alternatively, the events may have dierent con-
notations. for example, the activity corresponding to send acknowledgment
may mean dierently depending on the context in which it manifests.
example. fig. 7 exemplies duplicate events within a trace. one occurrence
of event \a" is immediately followed by another occurrence of event \a". the
two events either belong to the same instance of task \a" or they belong to
separate instances of task \a". the separate instances of task \a" may have
the same connotation or a dierent one.
eect. process mining algorithms have diculty in identifying the notion of
duplicate tasks and thereby produce results that are inaccurate. for exam-
ple, in process discovery, duplicate tasks are represented with a single node
resulting in a large fan-in/fan-out. in case of duplicate events which have the
same connotation, a simple lter may suce in order to aggregate the multiple
events into one.
{overlapping activity executions
description. this corresponds to the scenario where an instance of an activ-
ity is started and before it is completed another instance of the same activity
is started.
example. fig. 8 exemplies overlapping instances of execution of an activ-
ity. first, an instance of task \a" is started for which event \a (start)" is
recorded. afterwards, another instance of task \a" is started leading to the
recording of second event \a (start)". next, both instances of task \a" are
9a a
a a a?fig. 7. duplicate events: event \a" occurs two times in succession. as a consequence,
both events may belong to the same instance of task \a" or they may each belong to
separate instances of task \a".
completed represented by two recordings of event \a (complete)". as a con-
sequence, there is an ambiguity in associating the complete events with their
corresponding start events.
eect. as there is an ambiguity in associating the complete events with their
corresponding start events, process mining algorithms have problems with de-
ciding when a certain instance of an activity is completed. for instance, when
calculating the performance of a process, faulty statistics may be obtained for
the duration of an activity.
a a a a
start start complete complete??
??
fig. 8. overlapping instances of activity executions: two executions of activity \a" are
overlapping. as a result, a sequence of events \a (start)", \a (start)", \a (complete)",
and \a (complete)" are recorded. for the last two complete events it is not clear to
which execution of activity \a (start)" they correspond to.
2.7 process flexibility and concept drifts
often business processes are executed in a dynamic environment which means
that they are subject to a wide range of variations. as a consequence, process
exibility is needed in which a process is able to deal with both foreseen and
unforeseen changes. process changes manifest latently in event logs. analyzing
such changes is of the utmost importance to get an accurate insight on process
executions at any instant of time. based on the duration for which a change
is active, one can classify changes into momentary and permanent . momentary
changes are short-lived and aect only a very few cases while permanent changes
are persistent and stay for a while [24].
{evolutionary change
description. this refers to the scenario where during the period of time in
10which the process has been logged, the process has undergone a persistent
change. as a consequence, for a group of traces subsequent to the point of
change there are substantial dierences in comparison with earlier performed
traces with regard to the activities performed, the ordering of activities the
data involved and/or the people performing the activities.
example. fig. 9 depicts an example manifestation of an evolutionary change.
for a given process, events \a", \b", and \c" are always recorded sequen-
tially as part of the normal process ow. due to an evolutionary change, the
activities \b" and \c" are replaced (substituted) with the activities \d" and
\e" respectively. thus events \d" and \e" are recorded sequentially instead
of the events \b" and \c" in the traces subsequent to the point of change.
eect. current process mining algorithms assume processes to be in a steady
state. in case a log which contains multiple variants of a process is analyzed an
overly complex model is obtained. moreover, for the discovered model there is
no information on the process variants that existed during the logged period.
recent eorts in process mining have attempted at addressing this notion of
concept drifts [25{28]. several techniques have been proposed to detect and
deal with changes (if any) in an event log.
related data problems. case heterogeneity
timea b c
a b c
a d e
a d eprocess
changes
fig. 9. evolutionary change: due to an evolutionary change the events \d" and \e"
are recorded as part of the normal process ow instead of the events \b" and \c".
{momentary change
description. momentary changes are short-lived and aect only a very few
cases. for the process instances that are aected, one can perceive dierences
in comparison with other traces with regard to the activities performed, the
ordering of activities, data involved and/or the people performing the activi-
ties.
example. fig. 10 exemplies a momentary change. for a trace, tasks \a"
and \c" have been performed and for which events \a" and \c" have been
recorded. however, due to an exception, task \b" needed to be performed in
between tasks \a" and \c". as a consequence, event \b" has been recorded in
between events \a" and \c" which does not correspond to the normal process
ow.
11eect. momentary changes manifest as exceptional executions or outliers in
an event log. as such, process mining algorithms need to be able to distin-
guish common and frequent behavior from any exceptional behavior. some
examples are the heuristics miner [1] and the fuzzy miner [11] which can deal
with noise in a log. for any algorithm lacking the aforementioned capabilities,
the produced model is typically spaghetti-like and hard to understand as both
high and low frequent behavior is visualized. alternatively, one can also con-
sider techniques for outlier detection as a means of preprocessing the log and
ltering/removing them [29,30].
related data problem. outliers
a b c
exception
fig. 10. momentary change: as part of the normal process ow, the events \a"
and \c" are recorded after each other. however, due to an exception, event \b" has
occurred in between events \a" and \c".
2.8 noisy data/outliers
description. event logs may often contain outliers, i.e., rare, exceptional, or
anomalous execution behavior. outliers are also referred to as noise. there is no
clear denition of what constitutes an outlier/noise in a generic sense. it largely
varies based on the event log and processes that we are dealing with and the
contexts of analysis. there could be several sources of such outlier manifestation
in event logs ranging from issues in logging to more serious cases of fraudu-
lent behavior and/or non-compliance. outliers can be manifested in any of the
perspectives, e.g., control-ow resulting in inappropriate sequence of events, re-
sources resulting in non-privileged execution of activities, etc.
example. it could be the case that in an event log there are certain process
instances whose lengths deviate a lot from the average trace length in the log.
such traces constitute some rare executional scenarios. as another example, in
the nancial institute log provided for the 2012 bpi challenge [31], three loan
applications are approved by an automated resource.
eect. most process mining techniques are misled by the presence of outliers,
which impacts the goodness of the mined results. for example, outliers can
lead to infer incorrect control-ows between activities, which often manifest as
spaghetti-like processes. unfortunately, very limited work [29,30] has been done
in dealing with outliers in process mining. the basic challenge still remains in
being able to dene what constitutes an outlier and to be able to detect and
properly deal with them.
122.9 mashed processes
description. experiences from applying process discovery techniques on real-
life logs revealed that viewing processes as a at single monolithic entity is prob-
lematic. in reality, processes are designed and executed in a modular way with
a collection of (potentially concurrent) interacting entities. the overall process
can be seen as a mashup of its constituent sub-processes.
example. the event log provided for the bpi challenge 2012 [31] corresponds
to a process that is a merger of three intertwined sub-processes in a large dutch
financial institute. such extreme concurrency is also most often seen in health-
care processes. for example, during resuscitation of emergency patients in a
hospital, several physicians and nurses will be performing activities simultane-
ously.
eect. dealing with concurrency remains one of the core challenges of process
mining [1]. for example, process discovery algorithms yield either a ower model
or a spaghetti-model on event logs exhibiting extreme concurrency. if the associ-
ation between activities and the entities (sub-processes) that they correspond to
is known, one can think of mining individual sub-processes in isolation. however,
such an analysis might not be entirely interesting as an organization would be
more interested in nding intricacies in how these entities interact.
2.10 scoping
description. in several domains, the denition of what constitutes a case is not
explicit. the denition of a case can have dierent connotations based on the
contexts and purpose of analysis. domain knowledge is often needed in dening
such an appropriate scope.
example. when analyzing x-ray machine event data, completely dierent as-
pects need to be considered when it comes to gaining insights on (i) the real
usage of the system and (ii) recurring problems and system diagnosis, the for-
mer requiring the analysis of commands/functions invoked on the system while
the latter needing error and warning events. as another example, when analyzing
the process of an entire hospital, one would be interested in a high-level process
ow between the various departments in the hospital. however, when analyzing
a particular department workow, one would be interested in more ner level
details on the activities performed within the department.
eect. not choosing an appropriate scope of the event log creates several chal-
lenges for process mining and leads to inaccurate results or capturing results at
insucient level of detail. for example, ignoring critical events required for the
context of analysis will lead to not being able to uncover the required results (e.g.,
unable to discover the causes of failure) while considering more than required
will add to the complexity of analysis in addition to generating non-interesting
results (e.g., an incomprehensible spaghetti-like model).
133 evaluation of event logs
in this section, we analyze the issues highlighted in section 2 against several real-
life logs. the objective of this analysis is to provide an insight into the manner
and extent to which the identied data quality problems actually appear in real-
life data that is used for process mining. due to space constraints, we depict and
discuss the results of only ve real-life logs. table 1 summarizes the classes of
data quality issues manifested in these logs. htodo: ordering in table
does not match text. also
check consistency or names of
issues!! i
table 1. evaluation of process mining data quality problems on a selection of real-life
logs.
data quality problem philips
health-
carebpi
challenge
2011 (amc)bpi
challenge
2012catharina
hospitalcoselog
coarse granular times-
tampsx
mixed granular times-
tampsx
incorrect timestamps x x
missing values x x
missing events x x
missing events (anywhere
in a trace)x x
partial / incomplete
tracesx x x
duplicate tasks x x x x
overlapping activity exe-
cutionsx x
evolutionary change x
momentary change x
fine-granular events x x x x
mashed processes x x x
case heterogeneity x x x x
voluminous data x
noisy data / outliers x x x x x
scoping x x x
3.1 x-ray machine event logs of philips healthcare
philips healthcare has enabled the monitoring of its medical equipment (x-ray
machines, mr machines, ct scanners, etc.) across the globe. each of these sys-
tems records event logs capturing the operational events during its usage. philips
is interested in analyzing these event logs to understand the needs of their cus-
tomers, identify typical use case scenarios (to test their systems under realistic
circumstances), diagnose problems, service systems remotely, detect system de-
terioration, and learn from recurring problems. event logs from philips health-
care are huge and tend to be ne-granular, heterogeneous, and voluminous. we
observe the following data quality issues in these event logs:
{voluminous data: each cardio-vascular (cv) x-ray machine of philips
healthcare logs around 350 kb of event data in compressed format (5 mb in
14uncompressed format) every day. currently philips has event logs from 2500
systems installed across the globe. this implies that philips stores around 875
mb of compressed data with (comprising of millions of events) every day.
{fine-granular events: the event logs generated by x-ray machines are a result
ofwrite statements inserted in the software supporting the system. these
event logs capture very ne-grained events such as the commands executed on
the system and errors/warnings triggered by various components within the
system. a typical event log contains hundreds of event classes (activities). an
analyst is typically not interested in such low-level details and expects insights
at high-levels of abstraction when analyzing these event logs.
{case heterogeneity: x-ray machines are typically designed to be quite exible
in their operation. this results in event logs containing a heterogeneous mix of
usage scenarios with more diverse and less structured behavior. for example,
physicians could perform several dierent types of cardiac surgery procedures
(e.g., bypass surgery, stent procedure, angioplasty, etc.) on a patient using
a cardio-vascular x-ray machine. moreover, dierent physicians might apply
a particular medical procedure in dierent ways. all these lead to a huge
diversity in event logs.
{incorrect timestamps: the ordering of events are not always be reliable. an
x-ray machine has dozens of components with each component having a local
clock and a local buer. there could be a mismatch between the times when
an event is actually triggered and when an event is recorded (an event is rst
queued in the internal buer of a component before it is logged). furthermore,
there are scenarios where the various components in an x-ray machine are not
synchronized on clock.
{scoping: x-ray machine event logs also suer from a lack of proper scope .
all events that happened on/within the system on a given day is recorded as
a log. dierent aspects of the log need to be analyzed for dierent purposes.
for example, in order to analyze how a eld service engineer works on a
machine during fault diagnosis, we need to consider events logged only by
the engineer. philips records several attributes for each event which makes
it possible for dening and selecting an appropriate scope. use of domain
knowledge is essential in this process.
{noisy data/outliers: several components in an x-ray machine might deteri-
orate over time exhibiting a compromised functionality. the system is main-
tained both pro-actively and reactively. the event data during periods of sys-
tem malfunction manifest as a completely dierent behavior when compared
to a fully functional system. this creates several noisy/outlier data in the
event logs.
{duplicate tasks: when a physician operates a particular command on an x-ray
machine, the connotation largely depends on the context of its execution. for
example, a fluoroscopy procedure selection can have dierent interpretations
based on the surgical procedure applied on a patient. such duplicity of tasks
need to be properly disambiguated based on their relevant contexts.
153.2 the 2011 bpi challenge event log
we now discuss some of the data quality issues identied in another real-life
log, provided for the 2011 bpi challenge, from a large dutch academic hospital
[32]. the event log contains 1143 cases and 150 ;291 events distributed over 624
activities related to the activities pertaining to the treatment procedures that
are administered on patients in the hospital. several data issues highlighted in
the paper manifest in the log.
{case heterogeneity: the event log contains a heterogeneous mix of patients di-
agnosed for cancer (at dierent stages of malignancy) pertaining to the cervix,
vulva, uterus, and ovary. analyzing the event log in its entirety generates an
incomprehensible spaghetti-like model [4].
{coarse-granular timestamps: the event log also suers from several times-
tamp related problems. timestamps are recorded at the granularity of a day
for each event. this creates a loss of information on the exact timing and
ordering of events as executed in the process.
{mixed granular events: the activities in the log exhibit mixed granularity.
for example, there are coarse grained activities such as the administrative
tasks and ne-grained activities such as a particular lab test.
{missing values: the event log contains several events with missing informa-
tion. for example, 16 events do not contain the laboratory/department infor-
mation in which a medical test pertaining to the event is performed.
{scoping: the event log also suers from the scoping issue. each trace in the log
contains activities performed in dierent departments (often concurrently) and
at dierent instances of time over multiple visits to the hospital. appropriate
scope of the trace/log is to be considered based on the context of analysis,
e.g., if a particular department is interested in analyzing its process, then only
a subset of events pertaining to that department needs to be considered.
{duplicate tasks: there are a few duplicate tasks in the event log, e.g., geb.
antistoen tegen erys - dir.coombs and geb. antistoen tegen erys - dir. coomb
(one is specied as singular and the other is plural), natrium vlamfotometrisch
- spoed and natrium - vlamfotometrisch - spoed (the dierence between the two
is a hyphen), etc.
{noisy data/outliers: the event log also exhibits noisy/outlier behavior such
as the skipping of certain events. for more examples on some outlier behavior,
the reader is referred to [4].
3.3 the 2012 bpi challenge event log
our next log is the one provided for the 2012 bpi challenge pertaining to
the handling of loan/overdraft applications in a dutch nancial institute [31].
the event log contains 13 ;087 cases and 262 ;200 events distributed over 36
activities having timestamps in the period from 1-oct-2011 to 14-mar-2012.
the overall loan application process can be summarized as follows: a submitted
loan/overdraft application is subjected to some automatic checks. the applica-
tion can be declined if it does not pass any checks. often additional information
16is obtained by contacting the customer by phone. oers are sent to eligible ap-
plicants and their responses are assessed. applicants are contacted further for
incomplete/missing information. the application is subsequently subjected to a
nal assessment upon which the application is either approved and activated, de-
clined, or cancelled. the data quality issues encountered in the log are illustrated
in table 1 and explained below:
{mashed processes: the global process is dened over three sub-processes, viz.,
application, oer, and contact customers running concurrently, which man-
ifests as an extreme concurrency problem in the event log. this creates a
spaghetti-like process model from most discovery techniques.
{partial/incomplete traces: certain applications that have started towards
the end of the recorded time period are yet to be completed leading to par-
tial/incomplete traces (overall there are 399 cases that are incomplete).
{missing events: the event log contains several missing events. for example,
there are 1042 traces where an association between the start of an activity
and a completion of an activity is missed.
{overlapping activity executions: the event log contains one trace where we
see an overlapping execution of start and complete events for two instances of
execution of an activity.
{missing values: the event log contains missing information for some attributes
for several events. for example, there are 18009 events across 3528 traces that
have missing resource information, i.e., 6 :86% of events and 26 :96% of the
traces have partially missing resource information.
{case heterogeneity: the event log contains a heterogeneous mix of cases.
one can dene several classes of cases in the event log. for examples, cases
pertaining to applications that have been approved, declined, and cancelled,
cases that have been declined after making an oer, cases that have been
suspected for fraud, etc.
{noisy data/outliers: the event log contains several outlier traces. for exam-
ple, there are some traces where certain activities are executed even after an
application is cancelled or declined. there are three cases where a loan has
been approved by an automated resource.
3.4 catharina hospital event log
the next event log that will be discussed is describing the various activities
that take place in the intensive care unit (icu) of the catharina hospital.
it contains records mainly related to patients, their complications, diagnosis,
investigations, measurements, and characteristics of patients clinical admission.
the data belongs to 1308 patients for which 739 complications, 11484 treatments,
3498 examinations, 17775 medication administrations, and 21819 measurements
have been recorded. each action performed has been entered manually into the
system. for the log the following data quality problems apply.
17{mixed granular timestamps: for some specic class of treatments it is only
recorded on which day they have taken place whereas for all the other actions
it is recorded in terms of milliseconds when they have taken place.
{incorrect timestamps: due to manual recording of actions within the sys-
tem, typically a user records at the same time that a series of actions have
been completed. as a result, these actions have been completed in the same
millisecond whereas in reality they have been completed at dierent times.
{missing events: also, due to manual recording of actions, it is not recorded in
a structured way when actions have been scheduled, started, and completed.
as a result, for many actions one or more of these events are missing. in
particular, for 67% of the actions, no complete event has been registered.
{partial / incomplete traces: the log only contains only data for the year 2006.
therefore, for some patients the actions performed at the start or the end of
the stay at the icu are missing.
{overlapping activity executions: for 9% of the actions for which both the start
and the end have been recorded it is seen that there is an overlap between
multiple actions.
{duplicate events: within the event log there are several examples of the du-
plicate recording of an action. for example, one character of a foley catheter
has been written 906 times with a capital (\catheter a demeure") and 185
times without a capital (\cathether a demeure").
{momentary change: for patients admitted at an icu it can be imagined that
many dierent execution behaviors are possible. for example, for the group of
412 patients which received care after the heart surgery, we discovered the care
process via the heuristics miner which can deal with noise. for the discovery,
we only focused on the treatments and the \complete" event type, but still
the obtained model was spaghetti-like as it contained 67 nodes and more than
100 arcs.
{extreme concurrency: for around 20% of the patients, one or more compli-
cations are registered. when a complication occurs, typically multiple actions
are required in order to properly deal with the situation.
{case heterogeneity: the event log contains a heterogeneous mix of cases.
this is made clear by the fact that for the 1308 patients in the log there in
total 16 dierent main diagnoses (e.g. aftercare heartsurgery). for these main
diagnoses there exist in total 218 dierent indications for why a patient is
admitted to the icu (e.g. a bypass surgery). here, a patient may even have
multiple indications.
{fine-granular events: for a patient it is recorded which ne-grained lab tests
have been performed whereas it is also recorded which less ne-grained exam-
inations have been performed.
{noisy data / outliers: the event log contains multiple outlier trace. for ex-
ample, for the 412 patients which received care after the heart surgery the
majority of the patients has between 15 and 70 events. here, there are two
patients which have more than 300 events.
18{scoping: closer inspection of the log reveals that actions exist which are per-
formed on a daily basis for patients (e.g. \basic care" or \medium care")
whereas also patient-specic actions are performed (e.g. \cardioversion" or
\place swan ganz catheter"). for each situation, dierent aspects need to be
considered and dierent insights are obtained.
3.5 event log from a large dutch municipality
our last real-life log corresponds to one of the processes in a large dutch munici-
pality. the process pertains to the procedure for granting permission for projects
like the construction, alteration or use of a house or building, etc., and involves
some submittal requirements, followed by legal remedies procedure and enforce-
ment. the event log contains 434 cases and 14562 events distributed across 206
activities for the period between aug 26, 2010 and jun 09, 2012. several of the
data quality issues can be observed even in this log.
{fine-granular events: the events in this log are too ne-grained. this is ev-
ident from a large number (206) of distinct activities. analysts are mostly
interested in high-level abstract view of the process without being bothered
about the low-level activities.
{mixed-granular events: the event log also exhibits events of mixed granularity,
i.e., the event log emanates from a hierarchical process and we see events from
all levels of hierarchy in the same event log.
{evolutionary change: the process corresponding to this event log has under-
gone three (evolutionary) changes during the time period of the log and this
is manifested as concept drift [25].
{partial/incomplete traces: the event log contains 197 cases that are still
running (incomplete).
{missing values: the event log contains several attributes that give additional
information on the context of each event. however, these attributes are not
always present in all the events. in other words, there are several events where
we see missing information.
{noisy data/outliers: the event log also contains several cases with excep-
tional execution behavior. these are often manifested as a missing (skipped)
execution or extraneous execution of some activities.
4 related work
it is increasingly understood that data in data sources is often \dirty" and
therefore needs to be \cleansed" [33]. in total, there exist ve general taxonomies
which focus on classifying data quality problems [34]. although each of these ap-
proaches construct and sub-divide their taxonomies quite dierently [33,35{38],
they arrive at very similar ndings [33]. alternatively, as data quality problems
for time-oriented data have distinct characteristics, a taxonomy of dirty time-
oriented data is provided in [34]. although some of the problems within the
19previous taxonomies are very similar to process mining data quality problems,
the taxonomies are not specically geared towards the process mining domain.
so far, process mining has been applied in many organizations. in litera-
ture, several scholarly publications can be found in which an application of pro-
cess mining has been described. several publications mention the need of event
log preprocessing as data quality problems exist. for example, the healthcare
domain is a prime example of a domain where event logs suer from various
data quality problems. in [39, 40] the gynecological oncology healthcare pro-
cess within an university hospital has been analyzed; in [41] several processes
within an emergency department have been investigated; in [42] all computer
tomography (ct), magnetic resonance imaging (mri), ultrasound, and x-
ray appointments within a radiology workow have been analyzed; in [43] the
activities that are performed for patients during hospitalization for breast can-
cer treatment are investigated; in [44] the journey through multiple wards has
been discovered for inpatients; and nally, in [45], the workow of a laparoscopic
surgery has been analyzed. in total, these publications indicate problems such as
\missing values", \missing events", \duplicate events", \evolutionary change",
\momentary change", \ne-granular events", \case heterogeneity", \noisy data
/ outliers", and \scoping". in particular, \case heterogeneity" is mentioned as
a main problem. this is due to the fact that healthcare processes are typically
highly dynamic, highly complex, and ad hoc [41].
another domain in which many data quality problems for the associated
event logs can be found is enterprise resource planning (erp). here, schol-
arly publications about the application of process mining have been published
about a procurement and billing process within sap r/3 [19]; a purchasing
process within sap r/3; and the process of booking gas capacity in a gas com-
pany [46]. the data quality problems arising here concern \ne-granular events",
\voluminous data", and \scoping". in particular, the identication of relation-
ships between events together with the large amounts of data that can be found
within an erp system are considered as important problems.
5 conclusions
process mining has made signicant progress since its inception more than a
decade ago. the huge potential and various success stories have fueled the inter-
est in process mining. however, despite an abundance of process mining tech-
niques and tools, it is still dicult to extract the desired knowledge from raw
event data. real-life applications of process mining tend to be demanding due to
data quality issues. we believe that problems related to the quality of event data
are the limiting factor. unfortunately, these problems have received limited at-
tention from process mining researchers. therefore, we identied ten categories
of data-related problems encountered in process mining projects. we hope that
our ndings will encourage systematic logging approaches (to prevent data qual-
ity problems) and repair techniques (to alleviate data quality problems). in the
20upcoming big-data era, process mining will not be limited by the availability of
data, but by the quality of event data.
acknowledgements
this research is supported by the dutch technology foundation stw, applied
science division of nwo and the technology program of the ministry of eco-
nomic aairs. htodo: repair bib file for
two styles used, e.g., missing
\van der" problems. i
references
1. aalst, w.: process mining: discovery, conformance and enhancement of business
processes. springer-verlag, berlin (2011)
2. bose, r.p.j.c., van der aalst, w.m.p.: analysis of patient treatment procedures:
the bpi challenge case study. technical report bpm-11-18, bpmcenter.org
(2011)
3. bose, r.p.j.c., van der aalst, w.m.p.: process mining applied to the bpi chal-
lenge 2012: divide and conquer while discerning resources. technical report,
bpmcenter.org (2012)
4. bose, r.p.j.c., van der aalst, w.m.p.: analysis of patient treatment procedures.
in daniel, f., barkaoui, k., dustdar, s., eds.: business process management work-
shops. volume 99 of lecture notes in business information processing. (2012)
165{166
5. aalst, w., medeiros, a., weijters, a.: genetic process mining. in ciardo, g.,
darondeau, p., eds.: applications and theory of petri nets 2005. volume 3536 of
lecture notes in computer science., springer-verlag, berlin (2005) 48{69
6. on process mining, i.t.f.: process mining manifesto. in daniel, f., dustdar, s.,
barkaoui, k., eds.: bpm 2011 workshops. volume 99 of lecture notes in business
information processing., springer-verlag, berlin (2011) 169{194
7. g org, c., pohl, m., qeli, e., xu, k.: visual representations. in kerren, a., ebert,
a., meye, j., eds.: human-centered visualization environments. volume 4417 of
lecture notes in computer science. springer-verlag, berlin (2007) 163{230
8. c. pedrinaci and j. domingue: towards an ontology for process monitoring and
mining. in m.hepp, hinkelmann, k., karagiannis, d., klein, r., stojanovic, n.,
eds.: semantic business process and product lifecycle management. volume 251.,
ceur-ws.org (2007) 76{87
9. de medeiros, a.k.a., van der aalst, w.m.p., carlos, p.: semantic process mining
tools: core building blocks. in golden, w., acton, t., conboy, k., van der
heijden, h., tuunainen, v.k., eds.: proceedings of the 16theuropean conference
on information systems (ecis 2008). (2008) 1953{1964
10. g unther, c.w., rozinat, a., van der aalst, w.m.p.: activity mining by global
trace segmentation. in rinderle-ma, s., sadiq, s., leymann, f., eds.: business
process mangement workshops. volume 43 of lecture notes in business informa-
tion processing., springer-verlag, berlin (2010) 128{139
11. g unther, c., van der aalst, w.: fuzzy mining: adaptive process simplication
based on multi-perspective metrics. in: international conference on business
process management (bpm 2007). volume 4714 of lecture notes in computer
science., springer-verlag, berlin (2007) 328{343
2112. bose, r.p.j.c., verbeek, e.h.m.w., van der aalst, w.m.p.: discovering hierar-
chical process models using prom. in nurcan, s., ed.: caise forum 2011. volume
107 of lecture notes in business information processing., springer-verlag, berlin
(2012) 33{48
13. bose, r.p.j.c., van der aalst, w.m.p.: abstractions in process mining: a tax-
onomy of patterns. in dayal, u., eder, j., koehler, j., reijers, h., eds.: business
process management. volume 5701 of lncs., springer-verlag (2009) 159{175
14. song, m., g unther, c.w., van der aalst, w.m.p.: trace clustering in process
mining. in ardagna, d., mecella, m., yang, j., eds.: business process manage-
ment workshops. volume 17 of lecture notes in business information processing.,
springer-verlag, berlin (2009) 109{120
15. weerdt, j.d., vanden broucke, s.k.l.m., vanthienen, j., baesens, b.: leveraging
process discovery with trace clustering and text mining for intelligent analy-
sis of incident management processes. in: 2012 ieee congress on evolutionary
computation (cec). (2012) 1{8
16. de medeiros, a.k.a., guzzo, a., greco, g., van der aalst, w.m.p., weijters,
a.j.m.m., van dongen, b.f., sacca, d.: process mining based on clustering:
a quest for precision. in ter hofstede, a.h.m., benatallah, b., paik, h., eds.:
business process management workshops. volume 4928 of lecture notes in com-
puter science., springer-verlag, berlin (2008) 17{29
17. greco, g., guzzo, a., pontieri, l., sacca, d.: discovering expressive process
models by clustering log traces. ieee transactions on knowledge and data
engineering 18(8) (2006) 1010{1027
18. bose, r.p.j.c., van der aalst, w.m.p.: context aware trace clustering: towards
improving process mining results. in: proceedings of the siam international
conference on data mining (sdm). (2009) 401{412
19. bose, r.p.j.c., van der aalst, w.m.p.: trace clustering based on conserved
patterns: towards achieving better process models. in: business process man-
agement workshops. volume 43 of lnbip., springer (2010) 170{181
20. rogers, s.: big data is scaling bi and analytics{data growth is about to acceler-
ate exponentially{get ready. information management-brookeld 21(5) (2011)
14
21. olofson, c.w.: managing data growth through intelligent partitioning: focus
on better database manageability and operational eciency with sybase ase.
white paper, idc, sponsored by sybase, an sap company (november, 2010)
22. manyika, j., chui, m., brown, b., bughin, j., dobbs, r., roxburgh, c., byers,
a.h.: big data: the next frontier for innovation, competition, and productivity.
technical report, mckinsey global institute (2011)
23. bose, r.p.j.c.: process mining in the large: preprocessing, discovery, and diag-
nostics. phd thesis, eindhoven university of technology (2012)
24. schonenberg, h., mans, r., russell, n., mulyar, n., van der aalst, w.m.p.: process
flexibility: a survey of contemporary approaches. in dietz, j., albani, a., barjis,
j., eds.: advances in enterprise engineering i. volume 10 of lecture notes in
business information processing., springer-verlag, berlin (2008) 16{30
25. bose, r.p.j.c., van der aalst, w.m.p., zliobait_ e, i., pechenizkiy, m.: handling
concept drift in process mining. in mouratidis, h., rolland, c., eds.: international
conference on advanced information systems engineering (caise 2011). volume
6741 of lecture notes in computer science., springer-verlag, berlin (2011) 391{
405
2226. carmona, j., gavald a, r.: online techniques for dealing with concept drift in
process mining. in: international conference on intelligent data analysis (ida
2012). (2012) to appear.
27. d luengo, m.s.: applying clustering in process mining to find dierent versions
of a process that changes over time. in florian daniel, k.b., dustdar, s., eds.:
business process management workshops. volume 99 of lecture notes in business
information processing., springer-verlag, berlin (2012) 153{158
28. stocker, t.: time-based trace clustering for evolution-aware security audits. in
florian daniel, k.b., dustdar, s., eds.: business process management workshops.
volume 100 of lecture notes in business information processing., springer-verlag,
berlin (2012) 471{476
29. ghionna, l., greco, g., guzzo, a., pontieri, l.: outlier detection techniques for
process mining applications. in an, a., matwin, s., ras, z.w., slezak, d., eds.:
foundations of intelligent systems. volume 4994 of lecture notes in computer
science., springer-verlag, berlin (2008) 150{159
30. folino, f., greco, g., guzzo, a., pontieri, l.: mining usage scenarios in business
processes: outlier-aware discovery and run-time prediction. data & knowledge
engineering 70(12) (2011) 1005{1029
31. 3tu data center: bpi challenge 2012 event log (2011)
doi:10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f.
32. 3tu data center: bpi challenge 2011 event log (2011)
doi:10.4121/uuid:d9769f3d-0ab0-4fb8-803b-0d1120cf54.
33. kim, w., choi, b.j., hong, e.k., kim, s.k., lee, d.: a taxonomy of dirty data.
data mining and knowledge discovery 7(2003) 81{99
34. gschwandtner, t., g artner, j., aigner, w., miksch, s.: a taxonomy of dirty
time-oriented data. in et al., g.q., ed.: cd-ares 2012. volume 7465 of lecture
notes in computer science. (2012) 58{72
35. rahm, e., do, h.: data cleaning: problems and current approaches. ieee
bulletin of the technical committee on data engineering 24(4) (2000)
36. m uller, h., freytag, j.c.: problems, methods, and challenges in comprehensive
data cleansing. technical report hub-ib-164, humboldt university berlin (2003)
37. oliveira, p., rodrigues, f., henriques, p.: a formal denition of data quality
problems. in: international conference on information quality (mit iq confer-
ence). (2005)
38. barateiro, j., galhardas, h.: a survey of data quality tools. datenbankspectrum
14(2005) 15{21
39. mans, r., schonenberg, m., song, m., van der aalst, w., bakker, p.: application
of process mining in healthcare : a case study in a dutch hospital. in fred,
a., filipe, j., gamboa, h., eds.: biomedical engineering systems and technolo-
gies (international joint conference, biostec 2008, funchal, madeira, portugal,
january 28-31, 2008, revised selected papers). volume 25 of communications in
computer and information science., springer-verlag, berlin (2009) 425{438
40. mans, r.: workow support for the healthcare domain. phd thesis, eindhoven
university of technology (june 2011) see http://www.processmining.org/blogs/
pub2011/workow support forthehealthcare domain.
41. rebuge, a., ferreira, d.: business process analysis in healthcare environments:
a methodology based on process mining. information systems 37(2) (2012)
42. lang, m., b urkle, t., laumann, s., prokosch, h.u.: process mining for clinical
workows: challenges and current limitations. in: proceedings of mie 2008.
volume 136 of studies in health technology and informatics., ios press (2008)
229{234
2343. poelmans, j., dedene, g., verheyden, g., van der mussele, h., viaene, s., peters,
e.: combining business process and data discovery techniques for analyzing and
improving integrated care pathways. in: proceedings of icdm'10. volume 6171
of lecture notes in computer science., springer-verlag, berlin (2010) 505{517
44. perimal-lewis, l., qin, s., thompson, c., hakendorf, p.: gaining insight from pa-
tient journey data using a process-oriented analysis approach. in: hikm 2012.
volume 129 of conferences in research and practice in information technology.,
australian computer society, inc. (2012) 59{66
45. blum, t., padoy, n., feuner, h., navab, n.: workow mining for visualization
and analysis of surgeries. international journal of computer assisted radiology
and surgery 3(2008) 379{386
46. maruster, l., beest, n.: redesigning business processes: a methodology based on
simulation and process mining techniques. knowledge and information systems
21(3) (2009) 267{297
24