data scientist: the engineer of the future
wil m.p. van der aalst
scientic director of the data science center eindhoven (dsc/e),
eindhoven university of technology, eindhoven, the netherlands.
w.m.p.v.d.aalst@tue.nl
abstract. although our capabilities to store and process data have
been increasing exponentially since the 1960-ties, suddenly many organi-
zations realize that survival is not possible without exploiting available
data intelligently. out of the blue, \big data" has become a topic in
board-level discussions. the abundance of data will change many jobs
across all industries. moreover, also scientic research is becoming more
data-driven. therefore, we reect on the emerging data science discipline.
just like computer science emerged as a new discipline from mathematics
when computers became abundantly available, we now see the birth of
data science as a new discipline driven by the torrents of data available
today. we believe that the data scientist will be the engineer of the fu-
ture. therefore, eindhoven university of technology (tu/e) established
the data science center eindhoven (dsc/e). this article discusses the
data science discipline and motivates its importance.
key words: data science, big data, process mining, data mining,
visual analytics, internet of things
1 always on: anything, anytime, anywhere
as described in [9], society shifted from being predominantly \analog" to \dig-
ital" in just a few years. this has had an incredible impact on the way we do
business and communicate [12]. society, organizations, and people are \always
on". data is collected about anything ,at any time , and at any place . gartner
uses the phrase \the nexus of forces" to refer to the convergence and mutual
reinforcement of four interdependent trends: social, mobile, cloud, and informa-
tion [10]. the term \big data" is often used to refer to the incredible growth of
data in recent years. however, the ultimate goal is not to collect more data, but
to turn data into real value. this means that data should be used to improve
existing products, processes and services, or enable new ones. event data are
the most important source of information. events may take place inside a ma-
chine (e.g., an x-ray machine or baggage handling system), inside an enterprise
information system (e.g., a order placed by a customer), inside a hospital (e.g.,
the analysis of a blood sample), inside a social network (e.g., exchanging e-mails
or twitter messages), inside a transportation system (e.g., checking in, buying a
ticket, or passing through a toll booth), etc. events may be \life events", \ma-2 w.m.p. van der aalst
chine events", or both. we use the term the internet of events (ioe) to refer to
all event data available. the ioe is composed of:
{ the internet of content (ioc): all information created by humans to increase
knowledge on particular subjects. the ioc includes traditional web pages,
articles, encyclopedia like wikipedia, youtube, e-books, newsfeeds, etc.
{ the internet of people (iop): all data related to social interaction. the iop
includes e-mail, facebook, twitter, forums, linkedin, etc.
{ the internet of things (iot): all physical objects connected to the network.
the iot includes all things that have a unique id and a presence in an internet-
like structure. things may have an internet connection or tagged using radio-
frequency identication (rfid), near field communication (nfc), etc.
{ the internet of locations (iol): refers to all data that have a spatial dimen-
sion. with the uptake of mobile devices (e.g., smartphones) more and more
events have geospatial attributes.
note that the ioc, the iop, the iot, and the iol are partially overlapping. for
example, a place name on a webpage or the location from which a tweet was
sent. see also foursquare as a mixture of the iop and the iol. content, people,
things, and locations together form the ioe as shown in figure 1.
internet of 
contentinternet of 
people
‚Äúsocial‚Äùinternet of 
thingsinternet of 
locations
‚Äúcloud‚Äù ‚Äúmobility‚Äù
internet of events‚Äúbig 
data‚Äù
fig. 1. the internet of events (ioe) is based on the internet of content (ioc), the
internet of people (iop), the internet of things (iot), and the internet of locations
(iol).
data science aims to use the dierent data sources described in figure 1 to
answer questions grouped into the following four categories:
{ reporting: what happened?
{ diagnosis: why did it happen?
{ prediction: what will happen?
{ recommendation: what is the best that can happen?data scientist: the engineer of the future 3
the above questions are highly generic and can be applied in very dierent
domains. wikipedia states that \data science incorporates varying elements
and builds on techniques and theories from many elds, including mathematics,
statistics, data engineering, pattern recognition and learning, advanced com-
puting, visualization, uncertainty modeling, data warehousing, and high perfor-
mance computing with the goal of extracting meaning from data and creating
data products" [21]. many alternative denitions of data science have been sug-
gested. for a short overview of the history of data science, we refer to [17].
the remainder is organized as follows. in section 2 we discuss the unprece-
dented growth of (event) data and put it in a historical perspective. section 3
compares data with oil, followed by section 4 which discusses the value of this
new oil. section 5 describes the required capabilities of the data scientist. sec-
tion 6 lists some of the core technologies available to transform data into results.
finally, section 7 describes the recently established data science center eind-
hoven (dsc/e).
2 our growing capabilities to store, process and
exchange data
figure 1 describes the dierent sources of data contributing to the internet of
events (ioe). as an example, take a modern smartphone like the iphone 5s.
as illustrated by figure 2 such phones have many sensors. these may be used
to collect data on a variety of topics ranging from location (based on gps) to
usage.
gpsproximity sensor
ambient light 
sensoraccelerometermagnetometergyroscopic sensor
wifi
touchscreencamera (front)camera (back)
bluetoothmicrophone
gsm/hsdpa/lte14+ sensors
finger-print 
scanner
fig. 2. modern smartphones have many sensors that can be used to collect data.
it is dicult to estimate the growth of data accurately. some people claim
that humanity created 5 exabytes (i.e., 5 billion gigabytes) of data from the stone4 w.m.p. van der aalst
age until 2003, that in 2011 that amount of data was created every 2 days,
and that now (2013) it takes about 10 minutes to generate 5 exabytes [18]. the
expanding capabilities of information systems and other systems that depend on
computing, are well characterized by moore's law. gordon moore, the co-founder
of intel, predicted in 1965 that the number of components in integrated circuits
would double every year. during the last fty years the growth has indeed been
exponential, albeit at a slightly slower pace. for example, as shown in figure 3,
the number of transistors on integrated circuits has been doubling every two
years. disk capacity, performance of computers per unit cost, the number of
pixels per dollar, etc. have been growing at a similar pace.
fig. 3. moore's law applies not only to the exponential growth of transistors on a
chip: it also applies to processor speeds, communication speeds, storage space on hard
disks, and pixels on a screen.
note that figure 3 uses a logarithmic scale: the number of transistors on
a chip increased by a factor 240=2= 1048576 over a 40-year period. to truly
grasp this development, let us illustrate this using a few comparisons. if trains
would have developed like computer chips, we could now travel by train from
eindhoven to amsterdam in approximately 5 milliseconds (1.5 hours divided
by 240=2). airplanes could y from amsterdam to new york in 24 milliseconds
(7 hours divided by 240=2), and we could drive around the world using only
38 milliliters of petrol. these examples illustrate the spectacular developments
associated to moore's law.
3 big data as the new oil
data science aims to answer questions such as \what happened?", \why did
it happen?", \what will happen?", and \what is the best that can happen?".data scientist: the engineer of the future 5
to do this, a variety of analysis techniques have been developed. however, such
techniques can only be applied if the right input data is available. fancy ana-
lytics without suitable data are like sports-cars without petrol. in fact, already in
2006 clive humby (co-founder of dunnhumby) declared: \data is the new oil" .
however, only recently it became evident that data indeed represents incredible
economic and societal value.
using the metaphor \data=oil" we can denitely see similarities:
{exploration : just like we need to nd oil, we need to locate relevant data before
we can extract it.
{extraction : after locating the data, we need to extract it.
{transform : clean, lter, and aggregate data.
{storage : the data needs to be stored and this may be challenging if it is huge.
{transport : getting the data to the right person, organization or software tool.
{usage : while driving a car one consumes oil. similarly, providing analysis
results requires data.
so the dierent stages from exploring crude oil to using it to drive a car also
apply to data science. however, there are also important dierences between
data and oil:
{ copying data is relatively easy and cheap. it is impossible to simply copy a
product like oil. (otherwise gas prices would not be so high.)
{ data is specic , i.e., it relates to a specic event, object, and/or period. dier-
ent data elements are not exchangeable . when going to a petrol station, this
is very dierent; drops of oil are not preallocated to a specic car on a specic
day. production to stock of data is seldom possible. typically, data elements
are unique; therefore it is dicult to produce them in advance.
{ typically, data storage and transport are cheap (unless the data is really \big
data"). in a communication network data may travel (almost) at the speed
of light and storage costs are much lower than the storage costs of oil.
as pointed out before, moore's law does not apply to classical means of
transport by car, trans, or plane (cf. speed, fuel consumption, etc.). the end of
moore's law has been wrongly predicted several times. however, it is clear that
the ultimate limits of the law come in sight. at some stage transistors cannot be
made any smaller and clock speeds cannot be further increased. therefore, the
only way to keep up with the growing demands for storage and communication is
to increase the number of computing entities. see the increasing numbers of cores
in processors and the trend to use large clusters of commodity hardware in the
context of hadoop. consider for example google. instead of relying on expensive
proprietary hardware to store and process data, google uses industry-standard
servers that both store and process the data, and can scale without limits by
using distributed parallel processing. such massive parallelization results in a
huge energy consumption. this is the reason why google invests in renewable
energy and decides on the location of its data centers based on the availability
of energy sources.6 w.m.p. van der aalst
costs of disk storage are 
steadily decreasingcosts of in-memory  
storage are steadily 
decreasingthe costs of in-memory storage 
correspond to the costs of disk 
storage a few years before
fig. 4. comparing the costs of dierent types of storage over time (taken from [13]).
energy costs and the costs of hardware are also inuencing the infrastruc-
ture most suitable for large-scale data science applications. figure 4 shows the
evolution of the costs of storage. the lower line refers to the decreasing costs
of disk storage. however, as shown in figure 4, the costs of in-memory storage
are decreasing at a similar pace. hence, the current prices of in-memory storage
are comparable to the prices of disk storage of a few years ago. this explains
the growing interest in in-memory databases and in-memory analytics. it now
becomes aordable to load entire databases in main memory. the sap hana
in-memory computing platform [16] is an illustration of this trend.
to understand the importance of storing data at the right place, consider
the characteristics of the xeon intel chip shown in figure 5. if the cpu requires
a data element and it is available in it's l1 cache, then this takes only 1.5
nanoseconds. assume that this corresponds to a distance of 90 centimeters. if the
data is not in the l1 cache, but in main memory, then this takes 60 nanoseconds.
this corresponds to a distance of 36 meters (using our earlier assumption that 90
centimeters equals 1.5 nanoseconds). if the data is not in main memory, but on a
solid-state drive (ssd) then this takes 200.000 nanoseconds. this corresponds
to a distance of 120 kilometers. to get the data from a regular hard disk takes
10.000.000 nanoseconds and corresponds to a distance of 6000 kilometers. hence,
shifting data from hard disk to main memory may result in incredible speed-ups.
having the right \oil infrastructure" is crucial for data science. moreover,
innovations in hardware and software infrastructures (e.g., hadoop) allow for
types of analysis previously intractable. when using mapreduce techniques and
distributed computing infrastructures like hadoop, we are trying to optimize thedata scientist: the engineer of the future 7
on table 
(90 cm)next building
(36 m)amsterdam
(120 km)
new york
(6000 km)
fig. 5. how to get new oil? the power of in-memory computing becomes obvious by
relating travel distances to the time required to fetch data in a computer.
alignment between data and computation (e.g., bringing computation to data
rather than bringing data to computation).
4 on the value of data
in [4] the value per user was computed by dividing the market capitalization by
the number of users for all main internet companies (google, facebook, twitter,
etc.). this study (conducted in 2012) illustrates the potential value of data.
most user accounts have a value of more than $100 dollar. via the website
www.twalue.com one can even compute the value of a particular twitter account,
e.g., the author's twitter account (@ wvdaalst ) was estimated to have a value of
$321. adding up the dierent social media accounts of a typical teenager may
yield a value of over $1000. such numbers should not be taken very serious, but
they nicely illustrate that one should not underestimate the value of data. often
the phrase \if you're not paying for the product, you are the product!" is used to
make internet users aware of the value of information. organizations like google,
facebook, and twitter are spending enormous amounts of money on maintaining
an infrastructure. yet, end-users are not directly paying for it. instead they
are providing content and are subjected to advertisements. this means that
other organizations are paying for the costs of maintaining the infrastructure in
exchange for end-user data.
the internet is enabling new business models relying on data science. some
examples:
{ patientslikeme.com connects patients having similar medical problems and
sells this information to professionals. the community platform is based on
the sharing of information that is resold to a third party.8 w.m.p. van der aalst
{ groupon.com provides a broker platform where customers can get a discount
by buying as a group. if the deal takes place, groupon gets parts of the
revenue.
{ airbnb.com connects people so that they can rent out spare rooms to one
another. airbnb gets commission.
in all cases data is used to connect people and organizations so that information,
products, or services can be exchanged.
besides enabling new business models, data science can be used to do things
more ecient of faster. moreover, data science plays a pivotal role in customer
relationship management (crm). for example, data originating from dierent
information sources (websites, sales, support, after sales, and social media) can
be used to map and analyze the so-called customer journey . organizations may
use analytics to maximize the opportunities that come from every interaction
customers have with them. loyal customers are more cost eective to retain
than acquiring new ones, since they are likely to purchase more products and
services, are less likely to leave, and may help to promote the brand.
fig. 6. survival of the ttest: results of a bain & company study suggesting that
companies with the best data science capabilities outperform the competition [15].
optimizing the customer journey is one of the many ways in which organi-
zations benet from data science and extract value from data. increased com-
petition makes data science a key dierentiator. organizations that do not use
data intelligently, will not survive. this is illustrated by various studies. see for
example the results of a bain & company study [15] shown in figure 6. we
believe that in the future organizations will compete on analytics.data scientist: the engineer of the future 9
5 data scientist: the sexiest job of the 21st century
hal varian, the chief economist at google said in 2009: \the sexy job in the
next 10 years will be statisticians. people think i'm joking, but who would've
guessed that computer engineers would've been the sexy job of the 1990s?". later
the article \data scientist: the sexiest job of the 21st century" [7] triggered
a discussion on the emerging need for data scientists. this was picked up by
several media and when analyzing job vacancies, one can indeed see the rapidly
growing demand for data scientists (see figure 7).
(a) itjobswatch
(b) linkedin jobs (c) indeed jobs
fig. 7. the demand for data scientists is growing.
so, what is a data scientist? many denitions have been suggested. for ex-
ample, [7] states \data scientists are the people who understand how to sh out
answers to important business questions from today's tsunami of unstructured
information". figure 8 describes the ideal prole of a data scientist. as shown,
data science is multidisciplinary. moreover, figure 8 clearly shows that data sci-
ence is more than analytics/statistics. it also involves behavioral/social sciences
(e.g., for ethics and understanding human behavior), industrial engineering (e.g.,
to value data and know about new business models), and visualization. just like
big data is more than mapreduce, data science is more than mining. besides
having theoretical knowledge of analysis methods, the data scientist should be
creative and able to realize solutions using it. moreover, the data scientist should
have domain knowledge and able to convey the message well.
it is important to realize that data science is indeed a new discipline . just
like computer science emerged from mathematics when computers became abun-
dantly available in the 1980-ties, we can now see that today's data tsunami is
creating the need for data scientists. figure 9 shows that data science is emerging10 w.m.p. van der aalst
data 
mining
process
miningvisualizationdata 
sciencebehavioral/
social
sciences
domain
knowledgemachine 
learning
distributed
computingstatistics
stochasticsindustrial
engineeringsystem
design
fig. 8. prole of the data scientist: dierent subdisciplines are combined to render an
engineer that has quantitative and technical skills, is creative and communicative, and
is able to realize end-to-end solutions.
from several more traditional disciplines like mathematics and computer science.
mathematics mathematics
computer sciencecomputer science
data sciencedata science
fig. 9. just like computer science emerged as a discipline when computers became
widely available, data science is emerging as organizations are struggling to make sense
of torrents of data.data scientist: the engineer of the future 11
6 turning data into value: from mining to visualization
although data science is much broader (cf. figure 8) we would now like to
briey describe three \data science ingredients": data mining, process mining,
and visualization.
in [8] data mining is dened as \the analysis of (often large) data sets to
nd unsuspected relationships and to summarize the data in novel ways that are
both understandable and useful to the data owner". the input data is typically
given as a table and the output may be rules, clusters, tree structures, graphs,
equations, patterns, etc. initially, the term \data mining" had a negative con-
notation especially among statisticians. terms like \data snooping", \shing",
and \data dredging" refer to ad-hoc techniques to extract conclusions from data
without a sound statistical basis. however, over time the data mining discipline
has become mature as characterized by solid scientic methods and many prac-
tical applications [2, 5, 8, 14, 22]. typical data mining tasks are classication
(e.g., constructing a decision tree), clustering ,regression ,summarization , and
association rule learning . all of these are based on simple tabular data where
the rows correspond to instances and the columns correspond to variables.
process mining aims to discover, monitor and improve real processes by ex-
tracting knowledge from event logs readily available in today's information sys-
tems [1]. starting point for process mining is an event log . each event in such a
log refers to an activity (i.e., a well-dened step in some process) and is related
to a particular case (i.e., a process instance ). the events belonging to a case
areordered and can be seen as one \run" of the process. event logs may store
additional information about events. in fact, whenever possible, process mining
techniques use extra information such as the resource (i.e., person or device)
executing or initiating the activity, the timestamp of the event, or data elements
recorded with the event (e.g., the size of an order).
event logs can be used to conduct three types of process mining [1]. the rst
type of process mining is discovery . a discovery technique takes an event log
and produces a model without using any a-priori information. process discovery
is the most prominent process mining technique. for many organizations it is
surprising to see that existing techniques are indeed able to discover real pro-
cesses merely based on example behaviors stored in event logs. the second type
of process mining is conformance . here, an existing process model is compared
with an event log of the same process. conformance checking can be used to
check if reality, as recorded in the log, conforms to the model and vice versa.
the third type of process mining is enhancement . here, the idea is to extend or
improve an existing process model thereby using information about the actual
process recorded in some event log. whereas conformance checking measures the
alignment between model and reality, this third type of process mining aims at
changing or extending the a-priori model. for instance, by using timestamps in
the event log one can extend the model to show bottlenecks, service levels, and
throughput times.
data and process mining techniques can be used to extract knowledge from
data. however, if there are many \unknown unknowns" (things we do not know12 w.m.p. van der aalst
the four data sets have similar 
statistical properties:
¬∑ the mean of x is 9
¬∑ the variance of x is 11
¬∑ the mean of y is approx. 7.50
¬∑ the variance of y is approx. 4.12
¬∑ the correlation is 0.816
as shown the linear regression 
lines are approx. y = 3.00 + 0.500x.
fig. 10. anscombe's quartet [3]: although the four data sets are similar in terms of
mean, variance, and correlation, a basic visualization shows that the data sets have
very dierent characteristics.
we don't know), analysis heavily relies on human judgment and direct inter-
action with the data. visualizations may reveal patterns that would otherwise
remain unnoticed. a classical example is anscombe's quartet [3] shown in fig-
ure 10. the four data sets have nearly identical statistical properties (e.g., mean,
variance, and correlation), yet the dierences are striking when looking at the
simple visualizations in figure 10.
the perception capabilities of the human cognitive system can be exploited
by using the right visualizations [20]. information visualization amplies human
cognitive capabilities in six basic ways: 1) by increasing cognitive resources, such
as by using a visual resource to expand human working memory, 2) by reducing
search, such as by representing a large amount of data in a small space, 3) by
enhancing the recognition of patterns, such as when information is organized in
space by its time relationships, 4) by supporting the easy perceptual inference
of relationships that are otherwise more dicult to induce, 5) by perceptual
monitoring of a large number of potential events, and 6) by providing a manip-
ulable medium that, unlike static diagrams, enables the exploration of a space
of parameter values [6, 19].
the term visual analytics was coined by jim thomas to advocate a tight
integration between automatic techniques and visualization. visual analytics
combines automated analysis techniques with interactive visualizations for an
eective understanding, reasoning and decision making on the basis of very large
and complex data sets [11]. for example, data and process mining can be used
in conjunction with interactive visualization.data scientist: the engineer of the future 13
7 data science center eindhoven (dsc/e)
in 2013, the data science center eindhoven (dsc/e) was established as eind-
hoven university of technology's (tu/e) response to the growing volume and
importance of data. about 20 research groups of the department of mathematics
& computer science, the department of electrical engineering, the department
of industrial engineering & innovation sciences, and the department of indus-
trial design of tu/e are involved in this center.
in line with the tu/e policy, dsc/e's research contributes to the challenges of
the tu/e thematic research areas: health, energy, and smart mobility. each
of these areas witnesses a rapid growing volume of data triggering a variety
of scientic challenges. data science is also highly relevant for the high-tech
industry in the brainport region (\the smartest region in the world"). however,
dsc/e is not limited to the tu/e's thematic research areas or the brainport
region. in fact, industries such as the nancial industry and the creative industry
heavily depend on data science.
data analytics:
turning data into information
internet of things:
gathering the dataunderstanding and 
influencing human 
behavior 
fig. 11. the three main research lines of dsc/e.
tu/e has strong research groups in areas related to data science: computer
science, mathematics, electrical engineering, industrial engineering, innovation
sciences, and industrial design. in subdisciplines such as process mining, which
are at the very heart of data science, tu/e is globally leading. the dsc/e aims
to further strengthen research in three broader areas (fig. 11):
{internet of things: gathering the data
{data analytics: turning data into information
{understanding and inuencing human behavior
dsc/e's research focuses on developing new insights (models, theories, tools) to
be able to add and extract value from real sets of heterogeneous data. on the one
hand, the groups involved will continue to conduct focused research in particular
areas relevant for data science. on the other hand, the dsc/e initiative will fuel
multidisciplinary research combining expertise in the dierent dsc/e research
groups contributing to dsc/e.14 w.m.p. van der aalst
given the empirical nature of data science, dsc/e collaborates with a wide
range of organizations. collaborations include larger joint research projects, phd
projects, master projects, and contract research. examples of organizations col-
laborating within dsc/e are philips, adversitement, perceptive software, mag-
naview, synerscope, and fluxicon.
references
1. w.m.p. van der aalst. process mining: discovery, conformance and enhancement
of business processes . springer-verlag, berlin, 2011.
2. e. alpaydin. introduction to machine learning . mit press, cambridge, ma, 2010.
3. f.j. anscombe. graphs in statistical analysis. american statistician , 27(1):17{21,
1973.
4. b. bergstein and m. orcutt. is facebook worth it? estimates of the historical
value of a user put the ipo hype in perspective. mit technology review, http:
//www.technologyreview.com/graphiti/427964/is-facebook-worth-it/ , 2012.
5. m. bramer. principles of data mining . springer-verlag, berlin, 2007.
6. s.k. card, j.d. mackinlay, and b. shneiderman. readings in information visual-
ization: using vision to think . morgan kaufmann publishers, 1999.
7. t.h. davenport and d.j. patil. data scientist: the sexiest job of the 21st century.
harvard business review , pages 70{76, october 2012.
8. d. hand, h. mannila, and p. smyth. principles of data mining . mit press,
cambridge, ma, 2001.
9. m. hilbert and p. lopez. the world's technological capacity to store, commu-
nicate, and compute information. science , 332(6025):60{65, 2011.
10. c. howard, d.c. plummer, y. genovese, j. mann, d.a. willis, and d.m.
smith. the nexus of forces: social, mobile, cloud and information.
http://www.gartner.com, 2012.
11. d. keim, j. kohlhammer, g. ellis, and f. mansmann, editors. master-
ing the information age: solving problems with visual analytics . vismaster,
http://www.vismaster.eu/book/, 2010.
12. j. manyika, m. chui, b. brown, j. bughin, r. dobbs, c. roxburgh, and a. by-
ers. big data: the next frontier for innovation, competition, and productivity.
mckinsey global institute, 2011.
13. j.c. mccallum. historical costs of memory and storage. http://hblok.net/
blog/storage/ .
14. t.m. mitchell. machine learning . mcgraw-hill, new york, 1997.
15. t. pearson and r. wegener. big data: the organizational challenge. bain
and company, san francisco, http://www.bain.com/publications/articles/
big_data_the_organizational_challenge.aspx/ , 2013.
16. h. plattner and a. zeier. in-memory data management: technology and appli-
cations . springer-verlag, berlin, 2012.
17. g. press. a very short history of data science. forbes
technology, http://www.forbes.com/sites/gilpress/2013/05/28/
a-very-short-history-of-data-science/ , 2013.
18. r. smolan and j. erwitt. the human face of big data . against all odds pro-
ductions, 2012.
19. j.j. thomas and k.a. cook, editors. illuminating the path: the research and
development agenda for visual analytics . ieee cs press, 2005.data scientist: the engineer of the future 15
20. j.j. van wijk. the value of visualization. in visualization 2005 , pages 79{86.
ieee cs press, 2005.
21. wikipedia. data science. http://en.wikipedia.org/wiki/data_science , 2013.
22. i.h. witten and e. frank. data mining: practical machine learning tools and
techniques (second edition) . morgan kaufmann, 2005.