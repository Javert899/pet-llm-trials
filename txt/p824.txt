pm2: a process mining project methodology
maikel l. van eck, xixi lu, sander j.j. leemans, and wil m.p. van der aalst
eindhoven university of technology, the netherlands
fm.l.v.eck,x.lu,s.j.j.leemans,w.m.p.v.d.aalst g@tue.nl
abstract. process mining aims to transform event data recorded in information
systems into knowledge of an organisation‚Äôs business processes. the results of
process mining analysis can be used to improve process performance or com-
pliance to rules and regulations. however, applying process mining in practice
is not trivial. in this paper we introduce pm2, a methodology to guide the ex-
ecution of process mining projects. we successfully applied pm2during a case
study within ibm, a multinational technology corporation, where we identiÔ¨Åed
potential process improvements for one of their purchasing processes.
keywords: process mining, methodology, case study, business process manage-
ment.
1 introduction
process mining techniques can be used to automatically discover process models, check
the conformance of process models to reality, and extend or improve process models
using data of actual process executions [1]. process mining analysis results can be used
to improve the performance of processes or an organisation‚Äôs compliance to rules and
regulations. hence, process mining provides the bridge between data mining or machine
learning techniques and the business process management discipline.
within the Ô¨Åeld of data mining, efforts have been made to establish methodolo-
gies to support organisations with their data mining projects [9, 12]. the aim of these
methodologies is to guide the planning and execution of such projects in order to save
time and costs, e.g. by helping to avoid the presentation of irrelevant insights. this also
results in a better understanding and acceptance of data mining projects [9]. two widely
used methodologies are crisp-dm [16], developed by a consortium led by spss, and
semma, developed by sas [12].
efforts have also been made to create project methodologies that are tailored toward
supporting process mining projects, as methodologies like crisp-dm and semma are
very high-level and provide little guidance for process mining speciÔ¨Åc activities [1]. to
the best of our knowledge, there are two well-known process mining methodologies:
process diagnostics method (pdm) [6], which has also been adapted for healthcare
environments [14], and the l* life-cycle model [1]. pdm is designed to quickly provide
a broad overview of a process, while l* covers many different aspects of process mining
and touches on broader topics like process improvement and operational support.
unfortunately, these methodologies are not suitable for every project. the scope of
pdm is limited, covering only a small number of process mining techniques and em-
phasises on avoiding the use of domain knowledge during the analysis [6], which makesit less applicable for larger, more complex projects [18]. l* covers more techniques, but
was primarily designed for the analysis of structured processes and aims at discovering
a single integrated process model. neither l* nor pdm explicitly encourages itera-
tive analysis, which proved vital for both our own case study as well as the case study
performed in [18]. moreover, both methodologies can beneÔ¨Åt from additional practical
guidelines to help inexperienced practitioners to overcome common challenges.
to address these issues, we present pm2: a process mining project methodology.
pm2is designed to support projects aiming to improve process performance or com-
pliance to rules and regulations. it covers a wide range of process mining and other
analysis techniques, and is suitable for the analysis of both structured and unstructured
processes. for each stage of pm2, we deÔ¨Åne its inputs andoutputs and discuss the
concrete steps to be executed, referred to as activities . pm2supports quick analysis it-
erations and evolving insights, taking existing best practices into account. we provide
practical guidance on using the methodology by discussing a case study performed to-
gether with ibm. there we applied pm2and used various process mining techniques
to answer research questions related to the performance of a purchasing process.
the structure of the paper is as follows. in sect. 2 we discuss the pm2methodology
and explain each of its stages. the case study is discussed in sect. 3 and the paper is
concluded in sect. 4.
2 the pm2methodology
in this section we present the pm2methodology. we Ô¨Årst give an overview of pm2and
then discuss each stage of the methodology in detail.
pm2guides organisations performing process mining projects aimed at improving
process performance orcompliance to rules and regulations. the goals of a process
mining project can be very concrete, e.g. achieving a cost reduction of 10% for a given
process, or more abstract, e.g. obtaining valuable insights regarding the performance
of several processes. through pm2, these goals are translated into concrete research
questions which are iteratively reÔ¨Åned and answered, resulting in Ô¨Åndings that are the
basis of improvement ideas for the selected process.
an overview of the pm2methodology is shown in fig. 1. the methodology con-
sists of six stages that relate to several different input and output objects of the following
types: goal-related objects, data objects, and models. the four goal-related objects are
(1)research questions derived from project goals, which are answered by (2) perfor-
mance Ô¨Åndings and (3) compliance Ô¨Åndings , leading to (4) improvement ideas to achieve
the goals. the data objects denote the three different representations of process-related
data: (1) information systems contain live process data in various forms, which can be
extracted and linked to discrete events to form (2) event data . event data can be trans-
formed into (3) event logs by deÔ¨Åning a case notion and event classes. we consider two
types of models: (1) process models and (2) analytic models . process models describe
the ordering of activities in a process, possibly enhanced with additional information
e.g. temporal constraints, resource usage or data usage. moreover, we also consider
business rules as (abstract) process models formally deÔ¨Åning constraints with respect toinitializationanalysis iterations
analysis iterations
analysis iterations
1. planning
2. extraction
3. data 
processing
4. mining
& analysis
5. evaluation
6. process 
improvement 
& support
  
discoveryconformanceenhancement
event 
logs
improve -
ment ideas
event 
data
information 
system
process  
model s
performance 
findings
compliance 
findings
stage
 output /
input
research 
questions
refined /new 
research 
questions
analytics
analytic 
models
business 
experts
process 
analystsfig. 1: an overview of the pm2methodology.
the execution of business processes. analytic models are any other type of models that
give insight into the process, e.g. decision trees.
the Ô¨Årst two stages of the methodology are (1) planning and (2) extraction , during
which initial research questions are deÔ¨Åned and event data are extracted. after the Ô¨Årst
two stages, one or more analysis iterations are performed, possibly in parallel. in gen-
eral, each analysis iteration executes the following stages one or more times: (3) data
processing , (4) mining &analysis , and (5) evaluation . an analysis iteration focusses
on answering a speciÔ¨Åc research question by applying process mining related activities
and evaluating the discovered process models and other Ô¨Åndings. such an iteration may
take anywhere from minutes to days to complete, mainly depending on the complexity
of the mining & analysis. if the Ô¨Åndings are satisfactory then they can be used for (6)
process improvement & support .
in the following we discuss each stage, its input and output, and the activities that
are performed in it.
2.1 stage 1: planning.
the objective of the planning stage is to set up the project and to determine the research
questions. we consider two main goals for starting process mining projects: improving
performance of a business process, or checking its compliance with respect to certain
rules and regulations.
the inputs of this stage are the organisation‚Äôs business processes . the outputs are
goal-related research questions and a set of information systems supporting the execu-
tion of the business processes to be analysed.
we identiÔ¨Åed three activities for this stage: identifying research questions (r.q.),
selecting business processes (b.p.), and composing project team . the order in which
these activities are executed may sometimes vary, as there may already be a speciÔ¨Åc
goal or research question before starting the process mining project.selecting business processes . a process mining project generally starts with selecting
the business processes to be analysed and improved. both the process characteristics
as well as the quality of event data should be taken into account since they have large
inÔ¨Çuence on the achievable results of the project [4]. bose et al. [4] identiÔ¨Åed the four
categories of problems related to the quality of event data: missing data, incorrect
data, imprecise data and irrelevant data. for example, imprecise timing information
for events affects the results of performance measurements, while the absence of
unique identiÔ¨Åers to link all related events makes the event log creation harder [13].
in addition to these two factors, we also consider the changeability of the business
processes, i.e. the organisation needs to be able to inÔ¨Çuence or adapt process exe-
cutions based on the Ô¨Åndings. this is important if process improvement is the main
project goal. after selecting the business processes, the set of information systems
that store the relevant process execution data is identiÔ¨Åed.
identifying research questions. during this activity, the goals are identiÔ¨Åed and trans-
lated into research questions , which we deÔ¨Åned as questions related to the selected
process that can be answered using event data. research questions can be related
to different aspects of business processes, e.g. quality, time, resource, cost. various
case studies [18] showed the importance of deÔ¨Åning concrete research questions for
a successful process mining project. however, we demonstrate in our case study that
abstract research questions from the initialization phase can be reÔ¨Åned through ex-
plorative analysis, resulting in concrete improvement ideas and valuable insights.
composing project team. the last activity involves selecting the people that work on
the project. earlier case studies and our own study show that project teams need ex-
perts with different backgrounds [18]. we deÔ¨Åne the following roles: business owners
(who are in charge of the business processes), business experts (who know the busi-
ness aspect and executions of the processes), system experts (who are familiar with
the it aspect of the processes and the systems supporting the processes), and pro-
cess analysts (who are skilled in analysing processes and applying process mining
techniques). the most important roles are the business experts and the process ana-
lysts, between which collaboration is essential to evaluate the analysis Ô¨Åndings and
to ensure that the Ô¨Åndings are relevant and usable.
2.2 stage 2: extraction.
theextraction stage aims to extract event data and, optionally, process models . inputs
for this stage are the research questions and the information systems that support the
execution of the selected business processes to be analysed. the outputs of this stage
areevent data , i.e. a collection of events without predeÔ¨Åned case notion or event classes,
and possibly process models .
we identiÔ¨Åed three activities for this stage: determining scope ,extracting event data ,
andtransferring process knowledge .
determining scope. this activity involves determining the scope of the data extrac-
tion, based on which the event data is to be created. we give four examples of ques-
tions to be considered: (1) with which granularity should be event data extracted
(e.g. considering events related to purchase orders but neglecting events related to theitems of purchase orders); (2) within which period; (3) which data attributes should
be extracted; (4) which correlation between data should be used to collect them.
extracting event data. once the extraction scope is determined, event data can be
created by collecting the selected process related data from the relevant information
systems and joining them into a single collection of events, for example, a table in
which each entry represents an event.
transferring process knowledge. this activity can be executed simultaneously with
the creation of event data. tacit knowledge related to the selected business processes
and the data attributes is exchanged between business experts and process analysts,
through e.g. interviews or brainstorm sessions, which enables the analysts to be effec-
tive in the data processing and mining stages. such process knowledge may include
written process documentation or hand-made process models. process knowledge is
shared throughout the project, but understanding of the process is essential for an
effective data processing stage.
in contrast to existing process mining methodologies, we explicitly divided the
event data extraction and the log creation and processing into two stages. one reason is
that the event data extraction is time-consuming and less frequently repeated than data
processing activities like Ô¨Åltering [13]. another reason is that it is possible to create
different views on the same event data that result in different event logs, as discussed in
the next section.
2.3 stage 3: data processing.
the main objective of the data processing stage is to create event logs as different views
of the obtained event data and to process event logs in such a way that it is optimal for
the mining and analysis stage. in addition to the event data as our main input, one can
also use process models as an input to Ô¨Ålter the event data. the outputs are event logs
that are used in the mining and analysis stage.
we identify four types of activities for this stage: creating views ,aggregating events ,
enriching logs andÔ¨Åltering logs . fig. 2 shows an overview of these activities and how
they are applied.
creating views. event logs are speciÔ¨Åc views on event data, created by deÔ¨Åning case
notions and event classes. case notion relate events such that together they form a
process instance, while event classes distinguish different activities within a process
instance. which view to create depends on the goal of the analysis, e.g. an order is
a logical case notion to analyse throughput times, while the resource is a better case
notion to analyse resource utilisation. a similar situation holds for deÔ¨Åning the event
classes: if every order is a process instance and the resource involved in an event is
its event class, then process discovery algorithms produce handover of work graphs.
aggregating events. aggregating events can help to reduce complexity and improve
structure of mining results [5]. we distinguish two types of aggregation: is-aandpart-
of. the is-aaggregation considers different types of events belonging to an equivalent
but more general event class while the number of events remains the same. for exam-
ple, two events labeled with simple manual analysis andcomplex manual analysis
are considered instances of event class manual analysis but remain as two events. in3. data processing
  creating views
aggregating 
eventsenriching logs
filtering logs
define case
define event class
slice and dice
variance based
compliance based
derive
correlate
is - a
part-of
fig. 2: an overview of different types
of data processing activities.
4. mining & analysis
process discovery conformance checking
process analytics
check quality aspect
check time aspect
check resource aspect
check cost aspect
enhancement
enhance quality aspect
enhance time aspect
enhance resource aspect
enhance cost aspect
data mining
...
 fig. 3: an overview of the activities in
mining and analysis stage.
contrast, the part-of aggregation merges multiple events into larger events, as is the
case with sub-processes. both types of aggregation can also be applied in reverse,
i.e. deÔ¨Åning a specialisation. a more general technique is to deÔ¨Åne a hierarchy based
on event attributes that can be used to aggregate events, as is discussed in [2], e.g.
considering location at a city, country or continent level.
enriching logs. event logs, as any other data, can be enriched with various addi-
tional attributes [11]. we discuss two ways of enriching an event log: (1) deriving or
computing additional events and data attributes based on the log itself, or (2) adding
external data. the throughput time of a case can be a computed data attribute, while
adding information on the weather at the time an event occurred is an example of
including external data.
filtering logs. finally, Ô¨Åltering is a well-known and frequently used data processing
step to reduce complexity or focus the analysis on a speciÔ¨Åc part of the dataset. this
activity is often performed multiple times in an analysis iteration to obtain different
perspectives on the event data. we distinguish three types of Ô¨Åltering techniques: slice
and dice (also known as attribute Ô¨Åltering), variance-based , and compliance-based .
-slice and dice can be used to remove events or traces based on the values recorded
for a speciÔ¨Åc attribute, e.g. activity name, resource identiÔ¨Åer or timestamps of
events, or based on simple statistics, e.g. number of events of a trace or case dura-
tions.
-variance based Ô¨Åltering groups similar traces, e.g. through clustering, which can
be used to partition the event log in order to discover simpler process models for
each of the partitions of a complex process [14].
-compliance based Ô¨Åltering can be used to remove traces or events that do not com-
ply with a given rule or Ô¨Åt a given process model, which is a very Ô¨Çexible form of
Ô¨Åltering.
2.4 stage 4: mining & analysis.
in the mining & analysis stage, we apply process mining techniques on event logs and
aim to answer answer research questions and gain insight into processes performance
andcompliance . if the research questions are more abstract, explorative techniquescombined with process discovery can be applied on event logs to get an overall view of
thebusiness process , e.g. its control-Ô¨Çow. once more speciÔ¨Åc research questions have
been deÔ¨Åned, the analysis can focus on answering concrete research questions, e.g.
the difference between the throughput times of the cases executed the activity manual
analysis and the cases that skipped this activity.
inputs for this stage are event logs . in addition, if process models are available,
they can also be used for conformance checking and enhancement activities. output
for this stage are Ô¨Åndings that answer research questions related to performance and
compliance goals.
we identify four types of activity for this stage: process discovery ,conformance
checking ,enhancement andprocess analytics . the Ô¨Årst three activities are well-known
process mining techniques [1]. process analytics are other complementary analysis
techniques, e.g. data mining and visual analytics, which can be applied in the context
of business processes [11]. fig. 3 shows an overview the four activities.
process discovery. given an event log as input, we generally start with process dis-
covery techniques, which return a fact-based process model as output. for discussions
on different process discovery techniques, see e.g. [7].
conformance checking. given a process model, discovered or documented, describ-
ing intended behaviour, and an event log recorded real behaviour, conformance check-
ing techniques aim at the detection of inconsistencies between a process model and
its corresponding execution log [15]. research questions related to the compliance
of business processes to different aspects, such as quality, time, resource and cost, can
be checked using conformance checking techniques, the results of which can also be
used to enhance process models.
enhancement. the enhancement activity is deÔ¨Åned as extending or improving an ex-
isting process model using information about the actual process recorded in an event
log[1], for example, by extending process model with performance information re-
lated to time or cost, or repairing the process model according to current executions
shown by the corresponding event log. the results of enhancement are process mod-
els, e.g. enhanced with different aspects, whereas the results of conformance checking
can be considered without any process model.
process analytics. in addition to the three process mining activities, other analysis
techniques can be applied in the context of event logs and process models, such as
data mining techniques [11] or visual analytics (e.g. histograms of events per case),
of which the results can be used to enhance process models with additional aspects.
2.5 stage 5: evaluation.
the objective of the evaluation stage is to relate the analysis Ô¨Åndings to improvement
ideas that achieve the project‚Äôs goals. the inputs are the process models ,performance
andcompliance Ô¨Åndings from the analysis stage. the outputs are improvement ideas or
new research questions .
the activities for this stage are: diagnose , and verify & validate (v&v).
diagnose. diagnosing the Ô¨Åndings obtained through mining and analysis includes
the following: (1) correctly interpreting the results (e.g. understanding the processmodel discovered), (2) distinguishing interesting or unusual results from the expected
ones (e.g. large set of abnormal executions), and (3) identifying or reÔ¨Åning research
questions for possible further iterations.
verify & validate. the correctness of the (unexpected) Ô¨Åndings is investigated. veri-
Ô¨Åcation compares the Ô¨Åndings obtained to the original data and system implementa-
tions, while validation compares the Ô¨Åndings to the claims of process stakeholders,
e.g. interviewing the resources involved in processes. both veriÔ¨Åcation and valida-
tion may help identifying the underlying root causes and designing ideas for possible
process improvements.
one of the challenges in process mining projects is often that the process analysts
are not domain experts for the process they are analysing [6,18], which means that they
may have difÔ¨Åculties determining the causes of unexpected analysis results. therefore,
it is essential that process experts are involved in the veriÔ¨Åcation and validation of the
results. ideally, they would already be involved during the previous mining stage, guid-
ing the analysis to make sure that the results are useful for the organisation.
2.6 stage 6: process improvement & support.
the objective of the process improvement & support stage is to use the gained insights
to modify the actual process execution. the inputs of this stage are the improvement
ideas from the evaluation stage. the outputs of this stage are process modiÔ¨Åcations .
the activities are: implementing improvements andsupporting operations .
implementing improvements. achieving process improvements is often the main mo-
tivation for a process mining project. however, the actual implementation of process
modiÔ¨Åcations is generally a separate project and a different area of expertise. the
results of a process mining project then form the fact-based input of such process
improvement efforts. approaches that focus on this area include business process re-
engineering and six sigma [8]. after changing the process, the improvements can be
measured in another analysis project.
supporting operations. process mining can provide operational support by detecting
problematic running cases, predicting their future or suggesting recommended ac-
tions. to use process mining for operational support it is essential that the results are
of high quality, and that there is an it infrastructure in place that links these results
to live event data. it is a challenging form of process mining, suitable only for very
structured processes [1].
3 ibm case study
in this section we describe how pm2was applied in a concrete case study. the case
study has been conducted at ibm, a leading multinational technology and consulting
corporation. among many other services, ibm provides hardware service plans. we
have analysed a supporting process in this area: the purchasing process for spare parts.
this process starts with the creation of a purchase requisition, after which an order isanalysis iterations ‚Äì emergency orders (eo)data driven analysis iterations 1. planning
selecting b .p.
identifying r .q.
composing team
2. extraction
determine scope
extracting event data
transferring knowledge
3. data processing
creating views
aggregating eventsfiltering logs
xesame , plugin csvi
plugins flav , ush
custom script
4. mining & analysis
process discovery
conformance 
process analyticsenhancement
5. evaluation
diagnosing v&v
3. data processing
filtering logs4. mining & analysis
process discovery
conformance 
enhancement5. evaluation
diagnosing
v&vanalysis iterations ‚Äì 
repair and warranty ...analysis iterations ‚Äì 
regular orders ...
abap /sql
plugin ivm
plugin 
rlp/c
dc, 
fp, 
lv
csv
plugins flav , ush
plugin ivm
 see fig . 5
3. data processing
filtering logs
4. mining & analysis
process analytics
plugins lv , ev5. evaluation
diagnosing
v&v
to provide the
option to delete 
an order and its 
corresponding 
requisition ...
1
2
3
4
5
see fig .6
1issues
(numbered )
well 
executed
tools 
used
parallel
iterations
activity
stage
plugins flav , ush
4
6
sap systems -
purchasing 
process
?what does 
the process 
look like ?
?+
cancelled 
eo
and why ..
?+what does the 
process of 
emergency 
orders look like ?
?+what are the 
differences between 
  regular orders ?
?+what drives perfor -
mance in repair and 
warranty ...?fig. 4: an overview of the case study execution according to the stages of pm2.
sent out to a supplier. the process ends when all ordered items are delivered, which oc-
casionally requires multiple deliveries. there are three different types of orders: regular
replenishment orders, emergency orders, and orders for the repair of parts. the process
is performed independently at several different ibm facilities around the world.
for our case study, we mainly used the open-source process mining framework
prom toolkit [19] to support applying our methodology. the prom is freely available1
and contains some of the latest developments in process mining research, implemented
as plug-ins.
in the following, we Ô¨Årst discuss the activities executed in each stage of the method-
ology and the analysis results, listing only the tools and plugins used that were most
important for our analysis. following that we summarize the lessons learned from this
case study.
3.1 execution
an overview of the case study‚Äôs activities is shown in fig. 4. it gives the concrete inputs
and outputs of each sage, which tools were used, and it shows where we deviated from
pm2or encountered issues.
planning. ibm‚Äôs primary goal was to get detailed insights in the spare parts purchasing
process‚Äô performance. this process was selected mainly due to the availability of good
1http://promtools.org/ ; november 2014quality event data. initially, there was only an abstract research question: what does
the process look like? a project team was created consisting of a team leader, two
business experts, two process analysts, and a system expert. only the process analysts
had previous experience with process mining.
extraction. the scope of the extraction was limited to all purchase orders created in
a single month. for each order, all events related to this order and its requisition were
extracted from the sap system, i.e. including events outside the speciÔ¨Åed month. events
related to the individual items of an order were not taken into account. this scope
ensured that there was sufÔ¨Åcient data to get a realistic overview of the workload of the
entire process, while still being of a manageable size. the extracted event data contained
hundreds of thousands of events related to thousands of orders.
explorative analysis iteration. the Ô¨Årst analysis iteration focussed on getting general
insight into the process. in the data processing stage, multiple event logs were created.
a view with the order as a case notion and the event types as the event classes was used
for most analyses. other views were created as well, e.g. the resource or supplier as
a case notion for social network analysis. part-of aggregation was used to divide the
process into three sub-processes (related to requisitions, orders and deliveries) to obtain
more structured models. filtering was used for various purposes, e.g. simplifying the
process by focussing on the events belonging to one sub-process.
the main activities performed during the mining & analysis stage were process an-
alytics and discovery, with minor use of enhancement. visual analytics and statistics
provided basic insights related to e.g. the number of events per order or the case du-
ration. the results showed clear weekly patterns and batch processing, such as orders
being sent to the suppliers on a speciÔ¨Åc day. process discovery on the aggregated log
and the Ô¨Åltered logs of the sub-processes returned fact-based process models, which
were enhanced with time information to show several bottlenecks.
we discuss some of the tools used during the data processing and mining stages.
event logs for different views are created using the xesame toolkit or by simply im-
porting a csv Ô¨Åle (csvi) in prom [19]. event log Ô¨Åltering is available through var-
ious plug-ins, e.g. filter log by attribute values (flav) orusing simple heuristics
(ush) . the sub-process aggregation was performed using a custom script. for process
analytics, the log visualizer (lv) provides basic statistics and can be used to inspect
individual cases and events. using the dotted chart (dc) plug-in [17], the events and
cases of the log are visualised against time, revealing time patterns, concept drift and
batch processing. there are many process discovery algorithms, but here the induc-
tive visual miner (ivm) [10] was mainly used because it is fast and produces structured
models that can be enhanced or analysed by other plug-ins. the replay a log for perfor-
mance/conformance analysis (rlp/c) [3] plug-in was used to enhance a model with
time or model quality information. the feature prediction (fp) plug-in enriches the
event log with additional attributes, e.g. case duration, and provides a general frame-
work for deriving and correlating process characteristics [11].
the evaluation of the results by business experts, done without involving the pro-
cess analysts, led to a clear deÔ¨Ånition of three separate processes (i.e. emergency orders,variant 2 : executed x but (almost) always skipped y and z
variant 1 : (almost) always skipped x but executed y and z
x y z
x y
zw
wfig. 5: two variants of cancelling an emergency order, generally mutually exclusive.
regular orders, and repair and warranty orders), and a list of very concrete research ques-
tions, which were mostly answered using process analytics techniques. in the following,
we discuss how the three processes were investigated.
analysis iteration - emergency orders. we started a new analysis iteration with the
reÔ¨Åned research question: what is the process model of emergency orders according
to the events recorded? during the data processing , the event log was Ô¨Åltered on the
order type and purchasing organisation to obtain the emergency orders. in addition, we
only retained the event classes indicated as relevant by the business experts. during the
mining and analysis stage, we discovered a structured process model and enhanced the
model with conformance results using ivm to show the frequencies of cases taking a
certain path through the process model. during the evaluation stage, business experts
identiÔ¨Åed an interesting combination of four event classes involved in two variants of
cancelling an order that are usually mutually exclusive, shown in fig. 5.
to further investigate the two variants, we started a second analysis iteration with
the research question what has exactly happened during these variants of executions
and why did these executions happen? thedata processing activities were executed to
focus on the two variants of cancelled emergency orders. we used analytic techniques
in the mining and analysis stage to view variants of the remaining cases (using the
prom plug-in explore variants (ev) ), while examining their corresponding complete
executions in the original event log, shown by fig. 6. we observed that the cancellation
of some orders happened closely before the cancellation of the corresponding requi-
sitions, which indicates users had to start two separate workÔ¨Çows to cancel the order
and its related requisition, i.e. double work. furthermore, we also observed that users
sometimes sent a second order cancellation request even though the initial request was
still pending in the workÔ¨Çow. during the evaluation stage, the business expert validated
the observations by using the sap system to see the workÔ¨Çows executed. the business
expert concluded that an idea for improvement would be to provide the option to cancel
an order and its corresponding requisition in one manual action and to inform users if
an order is still pending to be cancelled, to prevent double work.
analysis iteration - regular orders. we started another analysis iteration with the re-
Ô¨Åned research question: what are the differences between the business processes han-variant 2
x w yfig. 6: inspection of traces containing one variant of cancelling emergency orders.
geo 1
geo 2
geo 3a
b
c de
fgh
fig. 7: the logs of three geographies,
played-out on the model of the Ô¨Årst.
lighter activities are less executed.
a
b
cde
fg
h
abcd
ef ghgeo 1
geo 4fig. 8: a performance comparison for
two geographies. darker activities take
longer.
dling the regular orders of four different geographies? we Ô¨Årst used Ô¨Åltering to obtain
one event log for each geography. moreover, we only considered the 80% of cases with
mainstream behaviour (known as the ‚Äúhappy Ô¨Çow‚Äù) to make the differences more vis-
ible and Ô¨Ålter out noise. we then used ivm to discover process models, and used the
plug-in show deviations on process tree to show the deviations between each process
model and the other logs. the obvious differences between the mainstream processes,
shown in fig. 7, triggered the reÔ¨Åned research question: what are then the differences
between them with respect to the time aspect? again, we processed the logs and applied
the plug-in rlp/c , indicating different bottlenecks as shown in fig. 8.
analysis iteration - repair and warranty orders. this iteration‚Äôs research question
was: what drives performance differences for the repair and warranty orders for dif-
ferent geographies? in the data processing stage logs were created for each geography.
structured models were discovered with ivm and then manually compared, showingclear differences between geographies in the likelihood of orders being rejected by the
suppliers. detailed analysis revealed that one geography uses a pricing model where
ibm pays a fee to the supplier even if a part is not successfully repaired, while other
geographies use a pricing model where they only pay on successful repairs. finally, the
process models showed that in one geography an order conÔ¨Årmation from the supplier
is not always recorded. this results in a risk that parts are not available when needed,
so a concrete improvement idea is to implement order conÔ¨Årmations with all suppliers.
process improvement and support. at the time of writing, this stage is still ongoing.
the process owners are interested in further analysis and there are discussions on im-
plementing the suggested process improvements based on the project Ô¨Åndings.
3.2 discussion
in this section we discuss the lessons learned and the open challenges encountered
during the case study, as well as several good practices when using pm2.
by applying pm2during the case study, the project team has successfully executed
the process mining project. detailed insights in the performance of ibm‚Äôs spare part
purchasing process have been delivered, achieving the project‚Äôs goal. in addition, sev-
eral concrete process improvement ideas have been generated.
a lesson learned is that process mining is most effective when process analysts
work closely together with business experts in a highly iterative and interactive manner.
this was observed when comparing the data-driven analysis iteration with the later
analysis iterations. initially, there were no business experts in the project team (issue 1
in fig. 4) and the transfer of knowledge was limited (issue 3). the lack of domain
knowledge in the data processing stage resulted in incorrect Ô¨Åltering and aggregation
(issue 4), leading to Ô¨Åndings that were not representing the processes correctly (issue 6).
business experts were added to the project team at the end of the data-driven iteration
and the analysis that followed was executed in tandem by the business experts and
process analysts. this tandem execution was considered to be ‚Äúa golden combination‚Äù
according to the stakeholders at ibm, as it led to faster analysis iterations and concrete
process improvement ideas.
another learning point is that a basic understanding of process mining is beneÔ¨Åcial
for all those involved in the evaluation stage. however, the business experts that joined
the team had no previous experience with process mining. hence, interpretation of the
Ô¨Åndings was difÔ¨Åcult and time-consuming (issue 5). a full-day process mining work-
shop was organised to improve process mining understanding for the business experts
and business understanding for the process analysts. the evaluation stage became faster
and more effective as a result.
we also learned that abstract research questions can be reÔ¨Åned during the analysis
to obtain valuable insights. it is known that concrete research questions guide a process
mining project [18], however sometimes coming up with good research questions at the
start of a project is difÔ¨Åcult. data-driven exploration can generate unexpected Ô¨Åndings,
leading to concrete research questions to explain the Ô¨Åndings during further analysis.
a challenge we encountered during the project is that comparing process models
is difÔ¨Åcult. some research questions were related to the process execution for differentgeographies. to answer these questions, process models were discovered and compared
for each geography. however, manual comparison of these models is labour-intensive
and existing tool support is limited.
tool support for interactive analysis is important as well. in prom, it is currently
time-consuming to switch between different views or Ô¨Ålter applications on the same
data and to compare results. one possible solution would be to use process cubes [2].
a good practice that we identiÔ¨Åed during the project is to check the event data
for errors using statistical techniques and manual inspection. in our project the event
timestamps were not always created correctly (issue 2), which led to incorrect results
later in the analysis. once this was discovered, the event data extraction and the analysis
had to be redone. checking the event data for errors would have prevented this.
finally, a good practice for the data processing stage is to discuss ideas to simplify
future analysis. identifying sub-processes and process variants helped to reduce the
complexity of the models and stimulated speciÔ¨Åc research questions in our project.
similarly, discussing the importance of events or the expected relations between events
helped with the identiÔ¨Åcation of unexpected patterns during the analysis evaluation.
4 conclusion
in this paper, we have presented the pm2process mining project methodology. pm2
is highly iterative and emphasises the need for close collaboration between process
analysts and business experts. we have discussed and explained the inputs and outputs
of each stage of the methodology, as well as the concrete activities that can be executed.
to illustrate the feasibility of pm2in practise and to provide practical guidance for
its application, we performed a process mining project together with ibm. their spare
parts purchasing process was analysed in the case study using pm2. we described a
range of tools and techniques that were used, the results obtained, the challenges en-
countered and the lessons learned in the case study. we showed that we ran into issues
when we deviated from our methodology, while applying pm2led to valuable insights
and concrete process improvement ideas. ibm is currently discussing the implementa-
tion of the process improvement ideas generated in the project and considering the use
of process mining in a wider and more structural manner.
we plan on continuing to work with ibm to further reÔ¨Åne pm2, and to provide even
more discussion on its use. we also believe that there is still a need for more detailed
practical guidance to help process mining practitioners tackle various challenges, e.g.
determining how to choose a case notion, or when to use which mining algorithms.
such guidance could be given in the form of some kind of process mining ‚Äúcookbook‚Äù.
additionally, we plan to apply pm2in process mining projects within other organisa-
tions. finally, we encountered challenges for which tool support is missing, e.g. model
comparison, and we suggest further research into this.
acknowledgements. we would like to thank the employees of ibm and in particu-
lar: bart pastoor, jorn kerkhof, hans nijssen, and michiel kuipers, who have worked
closely with us during the case study described in this paper.references
1. aalst, w.v.d.: process mining: discovery, conformance and enhancement of business pro-
cesses. springer (2011)
2. aalst, w.v.d.: process cubes: slicing, dicing, rolling up and drilling down event data for
process mining. in: asia paciÔ¨Åc business process management, pp. 1‚Äì22. springer (2013)
3. aalst, w.v.d., adriansyah, a., dongen, b.v.: replaying history on process models for con-
formance checking and performance analysis. wiley interdisciplinary reviews: data mining
and knowledge discovery 2(2), 182‚Äì192 (2012)
4. bose, r.p.j.c., mans, r.s., van der aalst, w.m.p.: wanna improve process mining results?
in: computational intelligence and data mining (cidm), 2013 ieee symposium on. pp.
127‚Äì134. ieee (2013)
5. bose, r.j.c., van der aalst, w.m.: abstractions in process mining: a taxonomy of patterns.
in: business process management, pp. 159‚Äì175. springer (2009)
6. bozkaya, m., gabriels, j., werf, j.: process diagnostics: a method based on process min-
ing. in: information, process, and knowledge management, 2009. eknow‚Äô09. international
conference on. pp. 22‚Äì27. ieee (2009)
7. de weerdt, j., de backer, m., vanthienen, j., baesens, b.: a multi-dimensional quality as-
sessment of state-of-the-art process discovery algorithms using real-life event logs. informa-
tion systems 37(7), 654‚Äì676 (2012)
8. harmon, p.: business process change: a guide for business managers and bpm and six
sigma professionals. morgan kaufmann (2010)
9. kurgan, l.a., musilek, p.: a survey of knowledge discovery and data mining process
models. the knowledge engineering review 21(01), 1‚Äì24 (2006)
10. leemans, s., fahland, d., van der aalst, w.: exploring processes and deviations. in: busi-
ness process management workshops. p. to appear (2014)
11. de leoni, m., van der aalst, w.m., dees, m.: a general framework for correlating business
process characteristics. in: business process management, pp. 250‚Äì266. springer (2014)
12. mariscal, g., marb ¬¥an,¬¥o., fern ¬¥andez, c.: a survey of data mining and knowledge dis-
covery process models and methodologies. the knowledge engineering review 25(02),
137‚Äì166 (2010)
13. nooijen, e.h., van dongen, b.f., fahland, d.: automatic discovery of data-centric and
artifact-centric processes. in: business process management workshops. pp. 316‚Äì327.
springer (2013)
14. rebuge, ¬¥a., ferreira, d.r.: business process analysis in healthcare environments: a
methodology based on process mining. information systems 37(2), 99‚Äì116 (2012)
15. rozinat, a., van der aalst, w.m.p.: conformance checking of processes based on monitoring
real behavior. inf. syst. 33(1), 64‚Äì95 (2008)
16. shearer, c.: the crisp-dm model: the new blueprint for data mining. journal of data ware-
housing 5(4), 13‚Äì22 (2000)
17. song, m., van der aalst, w.: supporting process mining by showing events at a glance. in:
workshop on information technologies and systems. pp. 139‚Äì145 (2007)
18. suriadi, s., wynn, m.t., ouyang, c., ter hofstede, a.h., van dijk, n.j.: understanding pro-
cess behaviours in a large insurance company in australia: a case study. in: advanced
information systems engineering. pp. 449‚Äì464. springer (2013)
19. verbeek, h., buijs, j.c., van dongen, b.f., van der aalst, w.m.: xes, xesame, and prom 6.
in: information systems evolution, pp. 60‚Äì75. springer (2011)