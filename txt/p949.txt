a framework for detecting deviations in complex event
logs
li, g.; van der aalst, w.m.p.
published in:
intelligent data analysis
doi:
10.13140/rg.2.1.1848.7928
10.3233/ida-160044
published: 19/08/2017
document version
publisher’s pdf, also known as version of record (includes final page, issue and volume numbers)
please check the document version of this publication:
• a submitted manuscript is the author's version of the article upon submission and before peer-review. there can be important differences
between the submitted version and the official published version of record. people interested in the research are advised to contact the
author for the final version of the publication, or visit the doi to the publisher's website.
• the final author version and the galley proof are versions of the publication after peer review.
• the final published version features the final layout of the paper including the volume, issue and page numbers.
link to publication
citation for published version (apa):
li, g., & van der aalst, w. m. p. (2017). a framework for detecting deviations in complex event logs. intelligent
data analysis, 21(4), 759-779. doi: 10.13140/rg.2.1.1848.7928, 10.3233/ida-160044
general rights
copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners
and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
            • users may download and print one copy of any publication from the public portal for the purpose of private study or research.
            • you may not further distribute the material or use it for any profit-making activity or commercial gain
            • you may freely distribute the url identifying the publication in the public portal ?
take down policy
if you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately
and investigate your claim.
download date: 14. jan. 2018intelligent data analysis 21 (2017) 759–779 759
doi 10.3233/ida-160044
ios press
a framework for detecting deviations in
complex event logs
guangming liand wil m.p. van der aalst
eindhoven university of technology, eindhoven, the netherlands
abstract. deviating behavior within an organization can lead to unexpected results. the effects of deviations are often neg-
ative, but sometimes also positive. therefore, it is useful to detect deviations from event logs which record all the behavior
of the organization. however, existing model-based and cluster-based approaches are inaccurate or slow when dealing with
complex event logs, i.e. logs of less structured processes having many activities and many possible paths. this paper proposes
a novel approach that is faster than cluster-based approaches because it creates a so-called proﬁle which is less time-consuming
than creating clusters. furthermore, the approach is also more accurate than model-based approaches because we use an it-
erative approach to improve the result. our experiments show that approach outperforms existing techniques in a variety of
circumstances.
keywords: process mining, deviation detection, clustering, behavioral proﬁles
1. introduction
process mining is a family of techniques to extract knowledge about business processes from event
logs which record process executions consisting of different business activities [1]. process mining tech-
niques are widely used, not only in situations where processes are structured and well-deﬁned (e.g.,
procurement, ﬁnance, and e-government), but also in environments such as healthcare, customer rela-
tionship management (crm) and product development where things are less structured [2]. such envi-
ronments often allow for a higher degree of freedom and this may lead to unexpected deviations, e.g.,
a patient can directly visit a doctor without an appointment in an emergency. since deviations impact
business processes, it is of the utmost importance to detect them from event logs.
deviation detection is a signiﬁcant problem which has been explored within diverse research areas
and application domains [3], such as detecting failure behavior (e.g., bugs) in software systems [4],
detecting fraudulent claims in insurance companies [5] and detecting intrusions in a network [6]. the
lion’s share of deviation detection done in context of process mining focusses on conformance checking .
this requires a normative model and therefore knowledge of what constitutes a deviation. in this paper
we focus on deviation detection without a normative process model and just used the event data to detect
deviations.
deviation detection techniques can be divided into two categories, i.e., model-based approaches
and cluster-based approaches. techniques in the ﬁrst category basically employ conformance check-
ing method on a discovered process model to detect deviations [7,8]. these techniques ﬁrst mine an
corresponding author: guangming li, eindhoven university of technology, p.o. box 513, 5600 mb, eindhoven, the
netherlands. e-mail: g.li.3@tue.nl.
1088-467x/17/$35.00 c2017 – ios press and the authors. all rights reserved760 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
fig. 1. a model discovered from bpi challenge 2012.
appropriate model as a reference model and then classify cases which do not ﬁt the model as devi-
ations. this works well on structured processes, but has problems when dealing with less structured
processes. it is difﬁcult to specify an appropriate model in this setting due to the high variety of be-
havior. within the second category, techniques use clustering [9–11] to detect deviations based on the
idea of perceiving cases in small clusters as deviations. in this case there is no conformance checking
on a discovered reference model: by grouping similar cases the outliers become visible. clustering tech-
niques are more suitable for complex processes (i.e., no need to learn a reference model), but they are
more time-consuming compared to techniques in the ﬁrst category. in summary, although there already
exist techniques such as the ones mentioned above, deviation detection in less structured environments
remains a challenge. as an example, consider a log, recording the behavior for a loan process, extracted
from the bpi challenge 2012.1due to the high variability (13087 cases, 4366 traces and 36 activities),
discovered models look like the model in fig. 1 and cannot serve as a reference model. clustering on
the other hand takes to long. for example, the clustering algorithm (cf. section 6.3) takes 40 minutes on
this moderate sized event log. later we will show that the approach presented in this paper can uncover
deviations in a fraction of this time while avoiding the creation of a meaningless reference model.
in order to deal with these challenges, we propose a novel approach, whose basic principle is that a
case from a log is a deviation if it is not similar to the collection of mainstream cases in the log.
more precisely, as shown in fig. 2, (1) we sample the cases ( c) from the input log based on a norm
function (cf. section 4.1) to get a set of “more normal" cases (denoted as cs) as mainstream cases.
(2) once we have cs, the problem of specifying a deviation has been transformed into computing how
much a case is similar to cs. in order to compute the similarity, one ﬁrst has to ﬁgure out what makes a
case similar to cs. since the case and cshave different types of characteristics with respect to different
perspectives, we create a so-called proﬁle [19] to characterize them from speciﬁc perspectives. (3) then
we quantify the similarity based on the proﬁle and identify normal cases ( cn) and deviating cases ( cd)
1http://www.win.tue.nl/bpi/2012/challenge.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 761
fig. 2. the framework of detecting deviations.
according to their similarity. (4) adjust the norm function to increase (decrease) the likehood that normal
(deviating) cases are sampled. we improve the quality of detected class of deviating cases by iterating
the above steps, and return ﬁnal deviating cases in the last loop.
in summary, our contribution is based on creating a proﬁle, rather than models or clusters, to detect
deviations and improving the performance iteratively. our approach is more accurate than model-based
approaches and faster than cluster-based approaches when dealing with complex and less structured
logs.
the remainder of the paper is organized as follows. the next section summarizes existing approaches
for deviation detection. in section 3, we introduce preliminaries and provide a deﬁnition for the proﬁle
notion. section 4 proposes a framework for detecting deviations and we apply it to the control-ﬂow
perspective in section 5. the approach has been implemented in prom and in section 6 we evaluate the
approach. finally, section 7 concludes the paper.
2. related work
the idea of detecting deviations from event logs is not new and many approaches have been proposed.
in this section, we provide an overview of existing approaches and compare them with our approach.
model-based approaches. bezerra and wainer [12] propose three similar methods to detect deviating
cases, i.e., threshold ([13] extends it to dynamic thresholds), iterative andsampling algorithms. among
these methods, the last one gives the best result. it ﬁrst creates a sample log through sampling a given
log and then considers the cases, which do not perfectly ﬁt the model discovered from the sample log,
as deviations. in [14], authors propose an approach which consists of ﬁve steps: (i) scoping, (ii) process
discovery, (iii) ﬁltering of ﬁtting models, (iv) model selection, and (v) splitting of log. the key step in the
approach is to select the most appropriate model which is structurally simple and behavioraly speciﬁc
among ﬁtting models. similarly [15], employs the genetic algorithm [16] to discover an appropriate762 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
model from a preprocessed log and then classiﬁes the cases which are not instances of the model as
deviations.
the model-based approaches typically discover an appropriate model as a reference model, and then
use the conformance checking technique to classify cases which do not ﬁt the model as deviations.
however, it is a challenging (and often impossible) task to discover and select the appropriate model
from a complex log. in any case this requires domain-speciﬁc knowledge. in our approach, a proﬁle is
created to replace the appropriate model to detect deviations, which is much easier and more accurate.
cluster-based approaches. ghionna et al. [17] cluster cases based on frequent patterns extracted from
a log, and then treat cases in clusters whose sizes are below a threshold as deviationss [18] performs
a hierarchical clustering of a log, in which each case is seen as a point of a properly identiﬁed space
of features. the method was originally devoted to discovering an expressive process model from each
cluster, but we can also use it to detect deviations by classifying cases in small clusters as deviations.
song et al. [19] create a proﬁle (which is different from the notion used in this paper) to contain speciﬁc
attributes of a case and then map a case to a vector based on the proﬁle . in this way, the whole log is
mapped into a vector space. according to the distances between every two vectors, clusters are generated
and cases in small clusters are considered as deviations.
cluster-based approaches are more suitable for unstructured processes than model-based approaches.
however, cluster-based approaches are optimized to ﬁnd clusters rather than deviations. as a conse-
quence, they are time-consuming due to the time it takes to cluster cases. in contrast, our approach is
more efﬁcient since it detects deviations based on the similarity between each case and a sample log.
3. preliminaries
this paper proposes a novel method based a so-called proﬁle which characterizes “normal cases” from
speciﬁc perspectives. in this section, we ﬁrst deﬁne event logs because the deﬁnition of a log used in this
paper is quite different from the standard notation [1] and then provide a deﬁnition for the proﬁle notion.
deﬁnition 1 (universes) .in the remainder we assume the following universes:
–ais the set of all possible activity names,
–cis the set of all possible case (process instance) identiﬁers.
cases (process instances) are represented by a unique identiﬁer. this allows us to refer to a speciﬁc
case even if two cases have the same trace.
deﬁnition 2 (event logs) .l= (c;a; )is an event log if and only if:
–cc is a set of cases (i.e., process instances),
–aa is a set of activities, and
–2c!amaps each case onto a sequence of activities (i.e., a trace).
lis the set of all possible event logs.
letc=fc1;c2;:::;cngdenote a set of cases. (c) = [(c1);:::; (cn)]denotes the multiset of
traces(ci).p(c)is the powerset of c, i.e.,c02p(c)if and only if c0c.
deﬁnition 3 (proﬁle) .letl= (c;a; )2l be an event log. a proﬁling function onlis a function
profl2cp(c)![0;1]that quantiﬁes the similarity between a case and a set of cases (higher is
more similar).g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 763
if a proﬁle is based on some feature f, we writeprofl
f. a proﬁle can be considered as a set of
characteristics extracted from some cases. a proﬁling function computes the similarity which describes
to what extent the case contains characteristics in the set. a proﬁle is conﬁgurable notion and is not
limited to one speciﬁc feature. the user can create multiple proﬁle. next, we combine proﬁles to quantify
the similarity based on a set of features.
deﬁnition 4 (combining proﬁles) .letl= (c;a; )2l be an event log. let fbe a set of features
andw2f!r+a weight function. profl
f;w2cp(c)![0;1]takes the weighted average, i.e.,
profl
f;w(c;c0) =p
f2fw(f)profl
f(c;c0)p
f2fw(f)
the above deﬁnitions do not give concrete functions to quantity the similarity. the proﬁle functions
used vary when detecting deviations from different perspectives. in section 6, we concrete proﬁles fo-
cusing on the control-ﬂow perspective. in the remainder, we will drop the subscripts and simply assume
a given proﬁle function profl.
4. a framework for detecting deviations based on proﬁle
in section 3, we create a proﬁle to quantify the similarity between a case and csto judge if the
case is deviating. the quality of the judgement depends on cs, i.e., it is better if csonly contains
“more normal” cases. therefore we exploit the idea of iteratively and incrementally reﬁning cs. by
combining this idea with the proﬁle introduced in the previous section, a novel approach for detecting
deviations is proposed. the approach is a framework since the proﬁle is conﬁgurable by selecting or
detecting a suitable proﬁle function, i.e., users can create their own proﬁle to detect deviations from some
perspective. next, we ﬁrst illustrate the two main steps of the approach, i.e., sampling andclassifying ,
and then present the whole framework in section 4.3.
4.1. sampling
basically, we create csthrough sampling an event log. in order to make cscontain “more normal”
cases, we need to make these cases more likely to be selected in the sampling step. to this aim, ﬁrst a
function is created to assign a norm value to each case based on its normality.
deﬁnition 5 (norm function) .letl= (c;a; )2lbe an event log, and norm2c!r+is anorm
function that assigns a positive value to each case. the higher the value is, the “more normal” the case
is. cases with lower norm values correspond to deviating cases.
the norm function needs to be initialized by users and we will give more details in section 4.3. based
on norm values and a given sample size, we employ a function named sampling to derivecsfrom a log,
in which cases with higher norm values are more likely to be included.
deﬁnition 6 (sampling) .letl= (c;a; )2l be an event log, norm2c!r+a norm function,
andss2f0:::jcjga sample size. cs=sample (l;norm;ss )is a random set of cases sampled from
cusing a relative likelihood based on norm, i.e., jcsj=ss,csc, andc12cis k times as likely to
be included as c22cifnorm (c1) =knorm (c2).
note that the same case cannot be included twice in cs. of course, two cases having the same trace
may be included in csif this is mainstream behavior.764 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
4.2. classifying
after deriving csfrom a log, we compute the similarity by means of a proﬁle. then, based on the
similarity values of all cases and a given number which indicates the amount of deviating cases, we
partition the log into normal cases ( cn) and deviating cases ( cd), and update norm based on the
classiﬁcation, i,e., assign higher (lower) norm values to normal (deviating) cases.
deﬁnition 7 (classifying) .letl= (c;a; )2l be an event log, profl2cp(c)![0;1]
a proﬁling function, csca subset of sampled cases, norm2c!r+a norm function, and
nd2f1:::jcjgthe number of cases to be marked as deviating. classify (l;cs;nd;norm;profl) =
(cn;cd;norm0)such that
–cnandcdpartitionc, i.e.,c=cn[cdandcn\cd=;,
–jcdj=nd,
–for anycn2cnandcd2cd:profl(cn;cs)>profl(cd;cs),
–norm02c!r+such thatnorm0(cn) =norm (cn)rnforcn2cnandnorm0(cd) =
norm (cd)rdforcd2cd.
wherern>1and0<rd<1are used to adjust the norm values.
note that proflrefers to a generic proﬁling function. it can be based on any proﬁle or combining
proﬁles as users need, which indicates the algorithm proposed in this section is a generic framework for
detecting deviations.
4.3. the algorithm cyclicsc
the classiﬁcation step detects deviating cases and updates norm based oncsderived in the sampling
step, while the sampling step creates csbased on norm updated in the classifying step. the above steps
are related to each other and may need to be applied repeatedly to converge. as mentioned in section
3.2, we want to derive cswhich contains “more normal” cases. to this aim, we propose an algorithm
cycliccs which combines and iterates the above two steps to reﬁne cs, and ﬁnally returns the deviating
cases.
deﬁnition 8 (cycling) .letl= (c;a; )2l be an event log, norm12c!f1ga special norm
function that assigns value 1 to each case, ss2f0:::jcjga sample size, profl2cp(c)![0;1]
a proﬁling function, lt2n+a loop threshold, and nd2f1:::jcjgthe number of cases to be marked
as deviating. cyclicsc (l;ss;lt;nd ) = (cn;cd)based on the following procedure:
1. leti= 0andnorm =norm1,
2.cs:=sample (l;norm;ss),
3.(cn;cd;norm0) :=classify (l;cs;nd;norm;profl),
4.norm :=norm0,i:=i+ 1, and return to step 2 if i6ltor outputcnandcdifi>lt .
the above deﬁnition presents the input, output and steps of the cyclicsc algorithm, which is also
shown in fig. 3. initially, we set i= 0 to letnorm =norm1. in each loop, according to the given ss
and the new norm (i.e., generated in the previous loop), a set of cases csis selected from cusing the
function sample . next, based on cs,ndandprofl, we use the function classify to partitioncintocn
andcd, and update norm as the input of the function sample in the next loop. we iterate the above steps
until the ending condition is satisﬁed, i.e., the number of loops exceeds the threshold ltand then output
cnandcd.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 765
fig. 3. the ﬂowchart of the algorithm cyclicsc .
at the beginning of the algorithm, we need to conﬁgure some input constants and functions. in our
experiments in section 6, we let ss=jcj(1 dp)andnd=jcjdp, wheredpis a given value which
speciﬁes what fraction of the cases in lwill be detected as deviating cases. besides, ltalso need to be
speciﬁed to control the number of loops. the algorithm is designed in such a way that each reﬁnement
leads to a better csin most cases. however, this is not always the case. our experiments show that the
approach tends to converge to a “better” cs. however, we will also discuss its limitations.
if there is no a-priori or domain knowledge, function norm is initialized as norm1by default, i.e., we
assume all the cases are on the same “normal” level. however, the sampling stage can also beneﬁt from
a-priori or domain information. for instance, if we know some case is “more normal” in advance, we can
assign a “higher” norm value to it, which helps us achieve a better result, since the initial norm decides
the quality of the ﬁrst csand has a major inﬂuence on the ﬁnal csandcd. on the contrary, if we
assign “lower” norm values to “more normal” cases, the detected deviating cases may not be deviating.
function proflneeds to be created according to the speciﬁc application, and in next section, we present
how to create it for detecting deviations from the control-ﬂow perspective.
5. detecting deviations using the control-ﬂow perspective: an application of the framework
using the framework proposed in section 4, we can detect deviations from different perspectives
through creating different proﬁles. in this section, we present an application of the framework to detect
deviations from the control-ﬂow perspective. speciﬁcally, we create a combined proﬁle profl
fdf;deg;w1
based on the directly follows relation ( df, cf. section 5.1) and the dependency relation ( de, cf. sec-
tion 5.2) using weight function w12f!f1g, i.e.,w1(df) =w1(de) = 1 . the results are discussed in766 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
next section. next to the above two relations, we can use many other relations (e.g., for detecting devi-
ations from other perspectives) where the basic idea of the algorithm still holds. next, we demonstrate
how to create proﬁles based on the above two relations, respectively.
5.1. creating a proﬁle based on the directly follows relation
the directly follows relation is a key relation in process mining used in many model discovery meth-
ods, such as the -algorithm. it extracts the characteristics of successive relations between events.
the deﬁnition of the directly follows relation in this paper is different from the one in [1], where the
relation is in the context of a log and the frequency is not taken into consideration. here the directly
follows relation is in the context of a case (rather than a log) and the frequency is used to quantify the
directly follows relation. speciﬁcally, given a set of cases c0, we use, for instance, #c0(a;b)to count
the frequency that ais directly followed by bin the multiset of traces (c0). note that a directly follows
relation can occur multiple times in the same trace. next, we create a proﬁle based on the frequency to
compute the similarity between a case cand a set of cases c0. the idea is that the similarity is higher if
the directly follows relations in (c)have a higher frequency in (c0).
deﬁnition 9 (proﬁle based on directly follows relation) .letl= (c;a; )2 l be an event
log.profl
df2cp(c)![0;1]is the proﬁling function based on the directly follows relation.
with #c0(a;b) =p
c02c0jf16i <j(c0)j j((c0)i;(c0)i+1) = (a;b)gjandmaxfreq (c0) =
maxa;b2a#c0(a;b),
profl
df(c;c0) =8
<
:p
16i<j(c)j#c0((c)i;(c)i+1)
(j(c)j 1)maxfreq (c0);ifj(c)j>2
0 ;otherwise
in order to better understand the above deﬁnition, we use a small example. consider the set of cases is
c1with(c1) = [ha;c;d;fi10;ha;b;d;fi5;ha;c;d;e;b;d;fi5], and an example case is c1with(c1) =
ha;b;d;fi. now: #c1(a;b) = 5 ,#c1(b;d) = 10 ,#c1(d;f) = 20 andmaxfreq (c1) = 20 . the
similarity value can be computed as follows: profl
df(c1;c1) = (5 + 10 + 20) =(320) = 0:58.
5.2. creating a proﬁle based on the dependency relation
the proﬁle based on the directly follow relation only abstracts characteristics between two successive
events. therefore, it is not enough to adequately represent the control-ﬂow perspective. in order to derive
more features, we create another proﬁle based on a so-called dependency relation to abstract features
between two disconnected events.
deﬁnition 10 (co-occurrence relation) .letaa be a set of activities. =ht1;t2;t3;:::;tni2a
is a trace.avbif and only if there are i;j2f1;:::;ngandi6=jsuch thatti=aandtj=b.
the co-occurrence relation is commutative. for instance, avbis the same as bvawhich means
bothaandboccur in the trace . for an example trace 1=ha;b;d;fi, the following trace-based
co-occurrence relations can be found:
v1=f(a;b);(b;a);(a;d);(d;a);(a;f);(f;a);(b;d);(d;b);(b;f);(f;b);(d;f);(f;d)g:
based on the co-occurrence relation, we deﬁne the dependency relation to reﬂect the non-local depen-
dency between events.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 767
deﬁnition 11 (dependency relation) .letl= (c;a; )2lbe an event log and c02p(c).minconf
andminsupp are two threshold values. a)c0bif and only if freq(a;b)=freq (a)>minconf and
freq(a;b)=jc0j>minsupp withfreq(a) =jfcjc2c0^a2(c)gjandfreq(a;b) =jfcjc2
c0^av(c)bgj.
for a dependency relation a)cb,aandbare called antecedent andconsequent , respectively. note
that the arrow does not mean boccurs after ain a trace. it states that if we see a, then likely boccurs
in the same trace as well. the arrow means the dependency relation is directional, i.e., the reverse of a
dependency relation does not always hold. for instance, a)cbdoes not imply b)ca.
next, we create a proﬁle based on the dependency relation to compute the similarity between a case
cand a set of cases c0. the idea is that, for all dependency relations which are derived from c0and
whose antecedents occur in (c), we map the similarity to 1 if all their consequents also occur in (c).
otherwise, we map the similarity to 0. for convenience, we use fx2(c)gto represent the set of all the
activities in (c). for instance,fx2ha;b;a;fig=fa;b;fg.
deﬁnition 12 (proﬁle based on dependency relation) .letl= (c;a; )2 l be an event log.
profl
de2cp(c)!f0;1gis the proﬁling function based on the dependency relation, i.e.,
profl
de(c;c0) =
1;iffx2(c)gfb2aj9a2(c)a)c0bg
0; otherwise
in order to better understand the above deﬁnition, we use the same set of cases c1with(c1) =
[ha;c;d;fi10;ha;b;d;fi5;ha;c;d;e;b;d;fi5], and the same case c1with(c1) =ha;b;d;fito explain
it. when both minconf andminsupp are conﬁgured as 1, the set of dependency relations derived from
c1isfa)c1d;a)c1f;d)c1a;d)c1f;f)c1a;f)c1dg. hencefa;b;d;fgfa;d;fgand
profl
de(c1;c1) = 1 .
6. experiments and evaluation
thecyclicsc algorithm has been implemented as a plugin in the prom.2in this section, we test how
parameters of the cyclicsc algorithm inﬂuence its performance, and then compare it with other state-of-
the-art techniques both on synthetic and real-life logs.
6.1. creating synthetic logs and evaluation metrics
first we use synthetic data in order to do a controlled experiments where the ground-truth is known.
the basic process of evaluation on synthetic logs is: (i) creating normal logs, (ii) adding artiﬁcial devi-
ations into the logs, (iii) detecting deviations, and (iv) checking whether the detected deviations are real
deviations, i.e., artiﬁcial deviations.
in order to get convincing evaluation, it is necessary to test the algorithm on a wide variety of logs.
in this paper, we employ the method in [20] for the random generation of business processes and their
execution logs.
log generation . the method has four key parameters and (a),xor (x),loop (l) and deep (d)
to control the collection of randomly generated processes as shown in fig. 4 (e.g., a=100,x=
2https://svn.win.tue.nl/repos/prom/packages/liguangming/trunk/.768 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
table 1
created deviating cases using different deviation types
normal case a c k o n l d b
add a c d k o n l d b
remove a c k o l d b
replace a c k a n l d b
add, remove & replace a b k n o n d b
fig. 4. processes with different parameters.
0,l=0 andd=1 indicate there only exist and split-join patterns in the corresponding process
and the complexity of the process is at level 1). in order to achieve different processes with various
patterns, we set the ﬁrst three parameters to either 100 or 0, which means the processes contain or do not
contain the corresponding patters. besides, we vary deep from 1 to 2 to create processes having different
levels of complexity. for the same parameter setting, generated processes may be different. therefore,
we generate ﬁve models for each parameter setting, and then create 10 execution logs for each model.
eventually, 80 models and 800 logs are created, which cover a wide and representative set of logs. the
800 logs are considered as “normal logs” without deviants.
once we have synthetic logs, as shown in table 1, we inject artiﬁcial deviations using the following
three deviation types (dt) :
–addan event at a random place in a case;
–remove an event at a random place from a case;
–replace an event at a random place in a case.
in order to control how many normal cases are transformed into deviation, we introduce a parameter
deviation percentage (dp) which describes what fraction of the cases in the log are deviating after we
import deviations. with different deviation types (add, remove and replace) and percentages (0.1, 0.2
and 0.3), we create 9 deviating logs for each normal log, i.e., there are 9800logs for next experiments.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 769
evaluation metrics . in a sense, the deviation detection problem can be regarded as a classiﬁcation
problem with two classes of objects: deviating cases and normal cases. therefore we can create a con-
fusion matrix [1] to compute accuracy, precision andrecall to show the performance of an algorithm to
detect deviations. since the experiments are on a broad collection of logs ( 9800logs), we take the
average value as the ﬁnal result.
in experiments, the cyclicsc algorithm will always detect the same number of deviations as we create,
which is the reason that the precision and recall are always the same. however, for the naive andclus-
tering algorithms (cf. section 6.3), these numbers may be different. sometimes the deviating clusters or
variants may not only contain the right number of cases as we hope. in this case, we ﬁnd out the closest
number of deviations, i.e., closest to the number of real deviations.
6.2. controlled parameter settings
among the parameters of the cyclicsc algorithm, the proﬁling function profland the loop threshold
lthave great inﬂuence on the performance. in this part, we experiment on 9800logs always using
ss=jcj(1 dp),nd=jcjdp(cf. section 6.1 for dp) and the initial norm =norm1, to test the
inﬂuence and discuss limitations in some situations.
6.2.1. experiments with psroﬁling functions
in section 5, we create profl
dfandprofl
debased on the directly follows relation and dependency re-
lation, respectively, and then employ the combined proﬁling function profl
fdf;deg;w1to detect deviations
from the control-ﬂow perspective. in the following experiments, with lt= 1, we present the performance
of single and combined proﬁling functions on various logs with different patterns and different deviation
types. for convenience, the three functions are denoted as df;de andfdf;deg, respectively.
first we test proﬁling functions on logs with different patterns (e.g., a=100,x=0 andl=0 indicate
there only exist and split-join patterns in logs). the ﬁrst three bar charts in fig. 5 indicate the perfor-
mance on logs which only contain one type of behavior (e.g., only concurrency). in comparison, profl
df
works better for logs with xor split-join or loop patterns while profl
deperforms well for logs with
and split-join patterns. the last four charts reveal how different proﬁling functions work on logs with
multiple patterns. logically, when a new kind of patterns is added into logs, the performance will be
better (worse) if the proﬁling function can (cannot) manage the patterns. for instance, if we import xor
patterns into logs which do not contain xor split-join patterns (fig. 5(a)), the performance of profl
df
on new logs (fig. 5(d)) improves as profl
dfcan well deal with xor split-join patterns.
next we test the algorithm on logs with different deviations, i.e., different deviation types ( dt) and
percentages ( dp). as shown in fig. 6, in contrast, profl
dfsuits the adddeviation type while profl
deﬁts
theremove deviation type. they perform almost the same for the replace deviation type. for the ﬁrst
three charts, the deviating cases only contain one corresponding deviation. if we allow for different
types of deviations in a deviating case, e.g., always three types (fig. 6(d)), performance improves. if
there exist more deviations in a case, the deviating case is easier to detect.
as we can see in the last three charts, along with the increase of the deviation percentage, the per-
formance of profl
dedrops apparently while the accuracy of profl
dfdeclines slightly. in contrast, profl
df
outperforms profl
deand can achieve good recall and precision on logs with high deviation percentages.
in summary: the above experiments show that, profl
dfandprofl
debeneﬁt from particular settings,
while the combined proﬁling function can achieve the same or a better result than each individual pro-
ﬁling function. this suggests that we can improve performance through extracting more useful features
from a log.770 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
fig. 5. the performance of the cyclicsc algorithm conﬁgured with different proﬁling functions for logs with different patterns.
6.2.2. experiments with loop thresholds
in the algorithm, we try to get a log containing “more normal” cases by integrating the sampling and
classifying steps. however, the actual identiﬁcation from “normal” cases may fail. therefore, we test the
algorithm on various logs to reveal this limitation in particular situations.
since we know which cases are real deviating ones, we ﬁlter them to create a normal log. then, we
replace the sample log with the normal log to detect deviations. in this case, the performance should
be the best and we call it “perfect performance”. in the experiments profl=profl
fdf;deg;w1and we
compare the performance of the cyclicsc algorithm at different loops with the perfect performance.
if the former performance can approach the latter one, it means the algorithm succeeds in getting ag. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 771
fig. 6. the performance of the cyclicsc algorithm for logs with different deviations.
“normal” log. more precisely, we compare the former performance at the 1st loop (t1), the 6th loop
(t6), and the 15th loop (t15) with the perfect performance.
among all the logs, we choose four typical logs, log1 ( dp= 10%;dt=add;deep = 1;a= 0;x=
0;l= 0), log2 (dp= 20%;dt=add;deep = 1;a= 0;x= 100;l= 0), log3 (dp= 20%;dt=
add;deep = 1;a= 0;x= 0;l= 0) and log4 ( dp= 10%;dt=add;deep = 1;a= 0;x=
100;l= 0) to show performance with respect to the number of loops under different circumstances. as
shown in fig. 7, the performance of the algorithm for log1 reaches “perfect performance” after iterating
while the performance for log2 rises along with loops, but does not reach “perfect performance”. in
contrast, the performance for log3 and log4 remains the same when the number of loops increases.772 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
fig. 7. performance of the cyclicsc algorithm with respect to the number of loops.
to sum up, iteration does not reﬁne the result when the performance at the ﬁrst loop is too bad or as
good as “perfect performance”. otherwise, through iteration, we can make the performance approaching
“perfect performance”, but cannot guarantee it can reach “perfect performance”.
6.3. comparison with existing methods
in the following, we compare the cyclicsc algorithm with two other algorithms, i.e., the sampling
algorithm [12] which is a representative model-based method and the clustering algorithm [21] which is
a leading cluster-based method. besides, we import a baseline method, the naive method which considers
the low frequent variants created by disco as deviations (disco3is a famous software in process mining
ﬁeld and has a function to abstract variants from a given log).
except for the naive method, the other three algorithms have parameters to set for good performance.
we have discussed the parameters of the cyclicsc algorithm in previous experiments. for the sampling
algorithm, one needs to choose the mining algorithm (hm, im or ilp) and the sampling size (0v1.0).
theclustering algorithm needs us to conﬁgure the target ﬁtness (0v1.0) and the maximal cluster amount
(1v100). actually, there are more parameters than what is mentioned above. here we only consider the
most signiﬁcant parameters for each algorithm, i.e., the parameters which have a signiﬁcant inﬂuence on
the results. for each parameter setting, we compute the average performance over all the logs created in
section 5.1. then we take accuracy, precision, recall and time into consideration and select the best result
3http://www.ﬂuxicon.com/products/.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 773
fig. 8. the best performance of four algorithms with their parameter settings.
for each algorithm. by selecting the parameter setting which performs best, we avoid unfair comparisons
due to poor settings. the accuracy, precision, recall and the corresponding parameter setting are shown
in fig. 8 while the average time is shown in table 2.
overall, as shown in fig. 8, the naive algorithm performs worst while the other three have their own
advantages. the cyclicsc algorithm provides results with better accuracy than others, while the cluster-
ingalgorithm has high accuracy and precision, but low recall. the sampling algorithm has the best recall
but the accuracy is not good.
besides the overall performance, we also look into how each algorithm performs in different situations.
first we see how each algorithm works on logs of different complexity. more precisely, we classify the
logs into two categories in terms of the parameter deep (deep indicates the complexity of the models
which generate the synthetic logs), and the performance is shown separately in figs 9(a) and (b).
as shown in fig. 9(a), when the underlying model (i.e., the model to generate logs) is easy, the sam-
pling algorithm performs best since it can discover a model which is almost the same as the underlying
model. the cyclicsc andnaive algorithms also work well, while the clustering algorithm achieves bad
performance since it often classiﬁes normal and deviating cases into one cluster. when the underlying
model is complex, the sampling algorithm tends to obtain an easy model and considers normal cases
as deviating ones by mistake, which leads to high recall and low accuracy as shown in fig. 9(b). the
naive algorithm achieves bad performance since in this case, there exist many normal unique traces such
that cases having these traces may be classiﬁed as deviating cases. in contrast, the clustering algorithm
performs best while the cyclicsc algorithm achieves a medium performance.
next, we compare the performance of four algorithms on logs based on processes employing different
patterns. similarly, we split logs into two categories in terms of parameter a;x andl, respectively. for
instance, fig. 9(c) shows the performance on half of all the logs, i.e., on logs which do not contain and
split-join patterns.
in comparison, the cyclicsc algorithm performs better than others on logs with and patterns and logs
without xor or loop patterns, while the clustering algorithm has very different characteristics, i.e., logs
with xor and loop patterns are handled well, but does not perform well on logs with and patterns. the
naive approach works well when l= 0, but performs poorly in other situations. the sampling algorithm774 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
fig. 9. the performance of four algorithms on logs with different structure.
is impacted greatly by the and patterns, i.e., it achieves the best performance among the algorithms on
logs without and patterns, but the other way around on logs with and patterns. relatively, the xor
and loop patterns have little inﬂuence on the sampling algorithm.
figure 9 shows the comparison among four algorithms for logs with different structures. we also
tested how sensitive the four algorithms are with respect to different deviation percentages and devia-
tion types. as shown in the top three charts in fig. 10, along with the increase of deviation percentage,
the performance of clustering andsampling algorithms degrades while the performance of the cyclicsc
algorithm remains stable. the naive algorithm has bad precision and recall when the deviation percent-
age is low. in comparison, the cyclicsc algorithm is more suitable for high deviation percentage. for
different deviation types as shown in the bottom three charts, the clustering andsampling algorithms
perform poorly for the remove deviation type while the other two algorithms are hardly inﬂuenced byg. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 775
fig. 10. the performance of four algorithms for different deviations.
the different deviation types. when we mix the three different deviation types together, the cyclicsc
algorithm and the clustering algorithm outperform the other ones. overall, the cyclicsc algorithm has
better performance and stability than the others considering a range of different situations. this does
not imply that our approach always works best, however, it is more robust for the 9 800 event logs
considered.
next, we compare the running time of four algorithms over various logs as shown in table 2. more
speciﬁcally, we classify all logs into two or three categories based on each parameter and then compute776 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
table 2
the running time of four algorithms over various logs
log settings time(ms)
cyclicsc naive clustering sampling
x=0 346 (309) 141 (233) 2,917 (7,021) 698 (1,035)
x=100 408 (337) 198 (326) 5,451 (12,475) 836 (833)
a=0 404 (370) 154 (238) 1,004 (1,517) 722 (696)
a=100 350 (269) 185 (324) 7,364 (13,620) 812 (1135)
l=0 219 (127) 56 (158) 2,126 (6,794) 514 (849)
l=100 535 (381) 284 (334) 6,242 (12,388) 1020 (962)
dp=10% 382 (333) 161 (277) 3,729 (10,121) 787 (853)
dp=20% 376 (328) 170 (286) 4,217 (10,234) 778 (1128)
dp=30% 374 (315) 178 (293) 4,607 (10,285) 736 (819)
dt=add 380 (328) 173 (289) 4,365 (10,214) 772 (815)
dt=remove 376 (328) 165 (279) 3,834 (10,186) 713 (802)
dt=replace 375 (320) 169 (285) 4,348 (10,234) 813 (1165)
d=1 195 (54) 17 (12) 282 (218) 272 (117)
d=2 559 (376) 324 (339) 8087 (13321) 1262 (1288)
average 377 ( 324) 170 (285) 4185 (10184) 767 (941)
the average time on logs of each category for each algorithm. for instance, we separate all logs into
two parts based on parameter x(0 or 100) and the ﬁrst row in table 2 shows the average time on logs
of the part whose parameter x= 0. as we can imagine, the naive algorithm takes the least time in
all situations. on average, the cyclicsc algorithm is faster than the sampling andclustering algorithms.
speciﬁcally, the running time of the clustering algorithm rises dramatically when the complexity of logs
increases from d= 1tod= 2. in summary, the clustering algorithm takes far more time than the other
three algorithms, especially for complex logs, while the naive andcyclicsc algorithms use less time in
all situations.
6.4. example with a real-life log
the ﬁnal part of the experimental evaluation comprises the application of four algorithms to a real
life-log which we mentioned in section 1. the log is derived from bpi challenge 2012 and it contains
262,200 events in 13,087 cases which record the loans in a bank for a period of ﬁve months. for the
deviation detection based on such a real-life log, it is impossible to verify the accuracy, precision and
recall, since we do not know which cases are real deviations. therefore, we ﬁnd out one deviating case
from the log with each algorithm and compare the four cases on the activity frequency and running time
perspectives.
as shown in fig. 11, the cyclicsc algorithm achieves a deviating case (case id: 197216) which mainly
contains an infrequent activitiy, “w_beoordelen fraude” which means a fraudulent activity and occurs
very little (there are just 0.25% of all events in the log having this activity). the naive algorithm ﬁnds
an approved loan (case id: 173736) as a deviation. however, the detected deviating case is not a real
deviation, but a unique trace which has some frequent activities, e.g., “w_nabellen incomplete dossiers”
(8.7%). the sampling algorithm detects an unﬁnished loan (case id: 211182) while the clustering algo-
rithm takes quite a long time to get a cancelled loan (case id: 176894). the deviations detected by the
above two approaches contain a frequent activity “w_completeren aanvraag” (18%).
on the running time perspective, the naive algorithm spends the least time (44 seconds) among the four
algorithms. the sampling algorithm uses 65 seconds to ﬁnish the detecting process (i.e., conformance
checking). however, if we take the process to create and ﬁnd an appropriate model into consideration,g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 777
fig. 11. detected deviations from a real-life log.
the practical time is much more than 65 seconds. the clustering algorithm takes much longer time (40
minutes) than the others. in comparison, our approach uncovers deviations in a fraction of the time used
by clustering (140 seconds) while avoiding the creation of a reference model.
through analyzing the activity frequency and running time perspectives, we ﬁgure out the cyclicsc
algorithm can achieve a meaningful result and take limited time when applied to a real-life log.
7. conclusion and future work
traditional deviation detection approaches have problems in situations where event logs contain a
variety of process behavior. in this paper, we propose a novel algorithm named cyclicsc which is faster
than cluster-based approaches and more accurate than model-based approaches. besides, experiments
based on 80 models and 9 800 logs suggest that the cyclicsc algorithm is more robust than others.
the framework is conﬁgurable and we used it to create a concrete approach for detecting deviations
from control-ﬂow perspective. one can edit the proﬁling function to detect deviations from speciﬁc
perspectives. we compared the cyclicsc algorithm with existing methods on synthetic and real-life logs
to illustrate its properties.
thecyclicsc algorithm also has some limitations (cf. section 6.2.2). for instance, in some situations,
the loops are not handled properly, yet overall the approach is more robust than others. in future work we
would like to address these limitations and apply the approach to more real-life event logs for which we
engage with end-users to establish the ground truth (as we did for the synthetic data). we would also like
to test the approach in a streaming setting with concept drift. in this setting the process may gradually
change and including what constitute its mainstream behavior. the iterative nature of the approach seems
particulary suitable to handle this scenario.778 g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs
references
[1] w.m.p. van der aalst, process mining: discovery, conformance and enhancement of business processes, springer-verlag,
berlin, 2011.
[2] c.w. günther, process mining in ﬂexible environments, phd dissertation, eindhoven university of technology, 2009.
[3] v . chandola, a. banerjee and v . kumar, anomaly detection – a survey, technical report, university of minnesota, 2007.
[4] d. lo et al., classiﬁcation of software behaviors for failure detection: a discriminative pattern mining approach, in:
proceedings of the 15th acm international conference on knowledge discovery and data mining , 2009, pp. 557–566.
[5] w.s. yang and s.y . hwang, a process-mining framework for the detection of healthcare fraud and abuse, expert systems
with applications 31(1) (2006), 56–68.
[6] g. münz, s. li and g. carle, trafﬁc anomaly detection using k-means clustering, in: gi/itg workshop mmbnet , 2007.
[7] w.m.p. van der aalst and a.k.a. de medeiros,process mining and security: detecting anomalous process executions
and checking process conformance, electronic notes in theoretical computer science 121(2005), 3–21.
[8] j. swinnen, b. depaire and m.j. jans, a process deviation analysis – a case study, business process management
workshops, lnbip, 99(2012), 87–98, springer, heidelberg.
[9] j. de weerdt et al., leveraging process discovery with trace clustering and text mining for intelligent analysis of incident
management processes, in: ieee congress on evolutionary computation (cec 2012), 2012, 1–8.
[10] r.p.j.c. bose and w.m.p. van der aalst, trace clustering based on conserved patterns: towards achieving better process
models, in: business process management workshops, lnbip, 43(2009), pp. 170–181, springer, heidelberg.
[11] g. greco, a. guzzo and l. pontieri, discovering expressive process models by clustering log traces, ieee transactions
on knowledge and data engineering 18(8) (2006), 1010–1027.
[12] f. bezerra and j. wainer, algorithms for anomaly detection of traces in logs of process aware information systems,
information systems 38(1) (2013), 33–44.
[13] f. bezerra and j. wainer, a dynamic threshold algorithm for anomaly detection in logs of process aware systems, journal
of information and data management 3(3) (2012), 316–331.
[14] f. bezerra, j. wainer and w.m.p. van der aalst, anomaly detection using process mining, enterprise, business-process
and information systems modeling , 2009, 149–161.
[15] h. jalali and a. baraani, genetic-based anomaly detection in logs of process aware systems, world academy of science
engineering and technology 64(2010), 304–309.
[16] a.k. alves de medeiros, genetic process mining, phd dissertation, eindhoven university of technology, 2006.
[17] l. ghionna, g. greco and a. guzzo, outlier detection techniques for process mining applications, foundations of intel-
ligent systems, 2008, 150–159.
[18] g. greco, a. guzzo and l. pontieri, discovering expressive process models by clustering log traces, ieee transactions
on knowledge and data engineering 18(8) (2006), 1010–1027.
[19] m. song, c.w. günther and w.m.p van der aalst, trace clustering in process mining, in: business process management
workshops , lnbip 17(2009), pp. 109–120, springer, heidelberg.
[20] a. burattin and a. sperduti, plg: a framework for the generation of business process models and their execution logs,
in:business process management workshops , lnbip 66(2011), pp. 214–219. springer, heidelberg.
[21] j. de weerdt, j. vanthienen and b. baesens, active trace clustering for improved process discovery, ieee transactions
on knowledge and data engineering 25(12) (2013), 2708–2720.g. li and w.m.p . van der aalst / a framework for detecting deviations in complex event logs 779
appendix: the algorithm cyclicsc
in the paper, we describe the cyclicsc algorithm by providing deﬁnitions and illustrating the main
ideas using the ﬂowchart in fig. 3. in this appendix, we give the pseudo code of the algorithm to better
explain the algorithm.
algorithm 1 cyclicsc: discover deviating cases in an event log
input:
an event log l= (c;a; ), a sample size ss, a numberlt, a proﬁling function profl, a norm function norm1, a number
nd;
output:
a set of deviating cases cd;
1:norm =norm1
#norm is a function which assigns a value to each case based on its normality; it is initialized as norm1to assign a value
1 to each case
2:fori= 1toltdo
3:c0=c
4:cs=;
5:cd=;
phase 1: sampling
6: fori= 1tossdo
7:c=roulettewheelselection (norm;c0)
#roulettewheelselection is a function which randomly selects a case cfromc0using a relative likelihood based
onnorm , i.e.,c12c0is k times as likely to be selected as c22c0ifnorm (c1) =knorm (c2)
8:c0=c0nfcg
9:cs=cs[fcg
10: end for
phase 2: classifying
11: foreachc2cdo
12:similarity (c) =profl(c;cs)
#similarity is a function which assigns a value to each case to indicate how much it is similar to cs
13: end for
14:similaritylist =ranking (similarity;c )
#ranking is a function which returns a list in which all cases of care ranked based on their similarity values (from
small to large), i.e., c12cis beforec22cinsimilaritylist ifsimilarity (c1)6similarity (c2)
15: foreachc2cdo
16: ifsimilaritylist:indexof (c)>ndthen
17: norm (c) =norm (c)rn
18: else
19: norm (c) =norm (c)rd
20: cd=cd[fcg
21: end if
#similaritylist:indexof (c)indicates the index of cinsimilaritylist ;rn > 1and0< rd < 1are used
to adjust the norm values.
22: end for
23: end for