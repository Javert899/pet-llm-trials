rediscovering workflow models from  
event-based data using little thumb 
a.j.m.m. weijters  
a.j.m.m.weijters@tm.tue.nl w.m.p van der aalst   
w.m.p.v.d.aalst@tm.tue.nl
department of technology management, eindhoven university of technology, p.o. box 513, nl-
5600 mb, eindhoven, the netherlands, +31 40 2473857/2290 
 
abstract 
contemporary workflow management systems are driven by explicit process models, i.e., a 
completely specified workflow design is required in order to enact a given workflow process. 
creating a workflow design is a complicated time-consuming process and typically, there are 
discrepancies between the actual workflow processes and the processes as perceived by the 
management. therefore, we propose a technique for rediscovering workflow models. this 
technique uses workflow logs to discover the workflow process as it is actually being executed. the 
workflow log contains information about events taking place. we assume that these events are 
totally ordered and each event refers to one task being executed for a single case. this information 
can easily be extracted from transactional information systems (e.g., enterprise resource planning 
systems such as sap and baan). the rediscovering technique proposed in this paper can deal with 
noise and can also be used to validate workflow processes by uncovering and measuring the 
discrepancies between prescriptive models and actual process executions. 
 
keywords: process mining, workflow mining, knowledge discovery, petri nets 
 
1. introduction 
during the last decade workflow management concepts and technology [2,19,23] have been applied in many enterprise information systems. workflow management systems such as staffware, ibm 
mqseries, cosa, etc. offer generic modeling and enactment capabilities for structured business 
processes. by making graphical process definitions, i.e., models describing the life-cycle of a typical case (workflow instance) in isolation, one can configure these systems to support business 
processes. besides pure workflow management systems, many other software systems have adopted 
workflow technology. consider for example erp (enterprise resource planning) systems such as 
sap, peoplesoft, baan and oracle, crm (customer relationship management) software, etc. 
another example is the field of web services. all web services composition languages such 
bpel4ws, bpml, xlang, wsfl, wsci, etc. have adopted most workflow concepts.  despite 
its promise, many problems are encountered when applying workflow technology. as indicated by 
many authors, workflow management systems are too restrictive and have problems dealing with 
change [2,4,7,12,20]. many workshops and special issues of journals have been devoted to 
techniques to make workflow management more flexible (e.g., [2,4,20]). most of the research in 
this area is devoted to techniques to increase the flexibility either by allowing for ad-hoc changes 
(as reflected by workflow management systems such as inconcert [18]) or by providing 
mechanisms to migrate cases and evolve workflows [7,12].  
in this paper we take a different perspective with respect to the problems related to flexibility. 
we argue that many problems are resulting from a discrepancy between workflow design  (i.e., the 
construction of predefined workflow models) and workflow enactment (the actual execution of 
workflows). workflow designs are typically made by a small group of consultants, managers and 
specialists. as a result, the initial design of a workflow is often incomplete, subjective, and at a too 
high level. during the implementation of the workflow, i.e., configuring the workflow management 
system and training the workers involved, these issues cause many problems (“the devil is in the 
details”). therefore, we propose to “reverse the process”. instead of starting with a workflow 
design, we start by gathering information about the workflow processes as they take place. we 
assume that it is possible to record events such that (i) each event refers to a task (i.e., a well-defined step in the workflow), (ii) each event refers to a case (i.e., a workflow instance), and (iii) 
events are totally ordered. any information system using transactional systems such as erp, crm, 
or workflow management systems will offer this information in some form. note that we do not 
assume the presence of a workflow management system. the only assumption we make, is that it is 
possible to construct workflow logs with event data. these workflow logs are used to construct a 
process specification, which adequately models the behavior registered. we use the term process 
mining  for the method of distilling a structured process description from a set of real executions. in 
this paper, we propose a new technique for process mining. the remainder of this paper is as follows. first we discuss related work. section 3 introduces 
some preliminaries including a modeling language for workflow processes and the definition of a 
workflow log. then we present a new technique for process mining (section 4) and the 
implementation of this technique in the workflow-mining tool little thumb (section 5). in section 
6 we present our experimental results. finally, we conclude the paper by summarizing the main 
results and pointing out future work.   
 
2. related work 
the idea of process mining is not new [6,8,9,10,14,15,16,17,21,22,24,25,26].  cook and wolf have investigated similar issues in the context of software engineering processes.  in [8] they describe 
three methods for process discovery: one using neural networks, one using a purely algorithmic 
approach, and one markovian approach. the authors consider the latter two the most promising 
approaches. the purely algorithmic approach builds a finite state machine where states are fused if 
their futures (in terms of possible behavior in the next k steps) are identical. the markovian 
approach uses a mixture of algorithmic and statistical methods and is able to deal with noise. note 
that the results presented in [8] are limited to sequential behavior.  cook and wolf extend their 
work to concurrent processes in [9]. they also propose specific metrics (entropy, event type counts, 
periodicity, and causality) and use these metrics to discover models out of event streams. this approach is similar to the one presented in this paper. however, our metrics are quite different and 
our final goal is to find explicit representations for a broad range of process models, i.e., we 
generate a concrete petri net rather than a set of dependency relations between events. in [10], cook 
and wolf provide a measure to quantify discrepancies between a process model and the actual 
behavior as registered using event-based data. 
the idea of applying process mining in the context of workflow management was first 
introduced in [6]. this work is based on workflow graphs, which are inspired by workflow products 
such as ibm mqseries workflow (formerly known as flowmark) and inconcert. in this paper, two 
problems are defined. the first problem is to find a workflow graph generating events appearing in 
a given workflow log. the second problem is to find the definitions of edge conditions.  a concrete 
algorithm is given for tackling the first problem. the approach is quite different from the approach 
presented in this paper. given the nature of workflow graphs there is no need to identify the nature 
(and or or) of joins and splits. moreover, workflow graphs are acyclic. the only way to deal with iteration is to enumerate all occurrences of a given activity. in [22], a tool based on these algorithms 
is presented. 
schimm [24] has developed a tool to discover hierarchically structured workflows. his approach 
requires all splits and joins to be balanced. 
herbst and karagiannis also address the issue of process mining in the context of workflow 
management [14,15,16,17]. the approach uses the adonis modeling language and is based on 
hidden markov models where models are merged and split in order to discover the underlying 
process. the work presented in [14,16,17] is limited to sequential models. a notable difference with 
other approaches is that the same activity can appear multiple times in the workflow model.  the 
result in [15] incorporates concurrency but also assumes that workflow logs contain explicit causal 
information. the latter technique is similar to [6,22] and suffers from the drawback that the nature 
of splits and joins (i.e., and or or) is not discovered.  
compared to existing work we focus on workflow processes with concurrent behavior, i.e., 
detecting concurrency is one of our prime concerns [25]. therefore, we want to distinguish 
and/or splits/joins explicitly. to reach this goal we combine techniques from machine learning 
with workflow nets (wf-nets, [1]). wf-nets are a subset of petri nets. note that petri nets provide 
a graphical but formal language designed for modeling concurrency. moreover, the correspondence 
between commercial workflow management systems and wf-nets is well understood 
[1,2,13,19,23]. 
 
3. preliminaries: workflow nets and event logs 
workflows are by definition case-based , i.e., every piece of work is executed for a specific case. 
examples of cases are a mortgage, an insurance claim, a tax declaration, an order, or a request for 
information. the goal of workflow management is to handle cases as efficient and effective as 
possible. a workflow process is designed to handle similar cases. cases are handled by executing 
tasks  in a specific order. the workflow process model specifies which tasks need to be executed 
and in what order. alternative terms for such a model are: ‘procedure’, ‘workflow graph’, ‘flow 
diagram’ and ‘routing definition’. in the workflow process model, routing elements are used to 
describe sequential, conditional, parallel and iterative routing thus specifying the appropriate route 
of a case [19,23]. many cases can be handled by following the same workflow process definition. 
as a result, the same task has to be executed for many cases. petri nets [11] have been proposed for modeling workflow process definitions long before the 
term ”workflow management'” was coined and workflow management systems became readily 
available. consider for example information control nets, a variant of the classical petri nets, 
already introduced in the late seventies [13]. petri nets constitute a good starting point for a solid 
theoretical foundation of workflow management. clearly, a petri net can be used to specify the 
routing of cases (workflow instances). tasks  are modeled by transitions , and places and arcs model 
causal dependencies . as a working example we use the petri net shown in figure 1.  
 
 
sb
se p10
p8p9
p7p6 p4
p5 p3p2p1
t4t12
t9
t7t13 t11
t10t8
t6t5 t3 t2t1
 
figure 1: example of a workflow process modeled as a petri net. 
 
the transitions t1, t2, …, t13  represent tasks, the places sb, p1, …, p10, se  represent the causal 
dependencies. in fact, a place corresponds to a condition that can be used as pre- and/or post-condition for tasks. an and-split corresponds to a transition with two or more output places (from 
t2 to p2 and p3), and an and-join corresponds to a transition with two or more input places (from 
p8 and p9 to t11). or-splits/or-joins correspond to places with multiple outgoing/ingoing arcs 
(from p5 to t6 and t7, and from t7 and t10 to p8). at any time, a place contains zero or more 
tokens , drawn as black dots. transitions are the active components in a petri net: they change the 
state of the net according to the following firing rule : 
(1) a transition t is said to be enabled  if and only if each input place of t contains at least one 
token. 
(2) an enabled transition may fire. if transition t fires, then t consumes one token from each input 
place p of t and produces one token for each output place p of t. a petri net that models the control-flow dimension of a workflow is called a workflow net  (wf-
net) [1]. a wf-net has one source place ( sb) and one sink place ( se) because any case (workflow 
instance) handled by the procedure represented by the wf-net is created when it enters the 
workflow management system and is deleted once it is completely handled, i.e., the wf-net 
specifies the life-cycle of a case. an additional requirement is that there should be no “dangling 
tasks and/or conditions”, i.e., tasks and conditions which do not contribute to the processing of 
cases. therefore, all the nodes of the workflow should be on some path from source to sink. 
the wf-net focuses on the process perspective and abstracts from the functional, organization, 
information and operation perspectives [19]. these perspectives can be added using for example 
high-level petri nets, i.e., nets extended with color (data) and hierarchy. although wf-nets are very 
simple, their expressive power is impressive. in this paper we restrict our self to so-called sound  
wf-nets [1]. a workflow net is sound if the following requirements are satisfied: (i) termination is 
guaranteed, (ii) upon termination, no dangling references (tokens) are left behind, and (iii) there are 
no dead tasks, i.e., it should be possible to execute an arbitrary task by following the appropriate 
route. soundness is the minimal property any workflow net should satisfy. note that soundness 
implies the absence of livelocks and deadlocks. sound wf-nets can be used to model the basic 
constructs identified by the workflow management coalition [23] and used in contemporary 
workflow management systems. experience with leading workflow management systems such as 
staffware, cosa, mqseries workflow, etc. show that it is fairly straightforward to express these 
tool-specific languages in terms of the tool-independent language of wf-nets. for an introduction 
to wf-nets the reader is referred to [1].  
in this paper, we use workflow logs to discover workflow models expressed in terms of wf-nets. 
a workflow log is a sequence of events . an event is described by a case identifier  and a task 
identifier . an event e=(c,t)  corresponds to the execution of task t for a given case c. for reasons of 
simplicity, we assume that there is just one workflow process. note that this is not a limitation since 
the case identifiers can be used to split the workflow log into separate workflow logs for each 
process. one workflow log may contain information about thousands of cases. since there are no 
causal dependencies between events corresponding to different cases, we can project the workflow 
log onto a separate event sequence for each case without loosing any information. therefore, we 
can consider a workflow log as a set of event sequences where each event sequence is simply a sequence of task identifiers. an example of an event sequence of the petri net of figure 1 is given 
below: 
 
t1, t2, t4, t3, t5, t9, t6, t3, t5, t10, t8, t11, t12, t2, t4, t7, t3, t5, t8, t11, t13 
 
using the definitions for wf-nets and event logs we can easily describe the problem addressed in 
this paper: given a workflow log we want to discover a wf-net that (i) potentially generates as 
many event sequences appearing in the log as possible, (ii) generates as few event sequences not 
appearing in the log as possible, (iii) captures concurrent behavior, and (iv) is as simple and 
compact as possible. moreover, to make our technique practical applicable we want to be able to 
deal with noise. 
  
4. a heuristic process mining technique 
in this section, we present the details of our heuristic  process mining technique. in another paper 
[5], we describe a more formal  approach and the so-called α-algorithm for which it is proven that it 
can successfully rediscover a large class of practically relevant wf-nets. the α-algorithm is based 
on four ordering relation that can be easily derived from a workflow log. let a and b be events and 
w a workflow log, then 
 
(1) a>b   if and only if there is a trace line in w in which event a is directly followed by b, 
(2) a→b if and only if a>b  and not b>a,  
(3) a#b  if and only if not a>b  and not b>a, 
(4) a||b if and only if both a>b  and b>a. 
 
the a→b relation is the so-called dependency relation  (b depends (directly) on a), a#b relation is 
the so-called  non-parallel relation  (i.e. there is no direct dependency between them and parallelism 
is unlikely), and a||b relation is the so-called parallel relation (it indicates potential parallelism). in 
the α-algorithm the dependency relation is used to connect events, the non-parallel relation is used 
to detect the kinds of splits and joins.    however, the formal approach presupposes perfect information: (i) the log must be complete 
(i.e. if a task can follow another task directly, the log should contain an example of this behavior) 
and (ii) we assume that there is no noise in the log (i.e. everything that is registered in the log is 
correct). in practical situations, logs are rarely complete and/or noise free. therefore, in practice it 
becomes more difficult to decide if two events say a and b are in the a→b or a#b relation. for 
instance the causality relation ( a→b) between two tasks a and b only holds if in the log there is a 
trace in which a is directly followed by b (i.e. the relation a>b holds) and there is no trace in 
which b is directly followed by a (i.e. not b>a). however, in a noisy situation one abusive 
example can completely mess up the derivation of a right conclusion. for this reason we try to 
developed heuristic mining techniques which are less sensitive for noise and the incompleteness of 
logs. moreover, we try to conquer some other limitations of the α-algorithm (short loops and some 
petri-net constructs). the remainder of this section is used to present our heuristic mining 
techniques. in the next section, we describe little thumb, a workflow-mining tool based on the 
rules of thumb presented here. 
 in our heuristic mining approach we distinguish three mining steps: step (i) the construction 
of a dependency/frequency table (d/f-table), step (ii) the induction of a d/f-graph out of a d/f-
table, and step (iii) the reconstruction of the wf-net out of the d/f-table and the d/f graph.  
4.1 construction of the dependency/frequency table 
the starting point of our workflow mining technique is the construction of a d/f-table. for each 
task a the following information is abstracted out of the workflow log: (i) the overall frequency of 
task a (notation #a), (ii) the frequency of task a directly preceded by another task b (notation 
#b<a ), (iii) the frequency of a directly followed by another task b (notation # a>b ), (iv) a local 
metric that indicates the strength of the dependency relation between task a and another task b 
(notation $a!lb) and finally (v) a more global metric that indicates the strength of the dependency 
relation (notation $ a!b). 
 
metric (i) through (iii) seems clear without extra explanation. the definition of the local metric (iv) 
is as follows: $a!lb= (#a>b - #b>a) / (#a>b + #b>a +1) . remark that in this definition only 
local information is used (i.e. the a>b  relation). the effect of this definition is that if for instance 
event a is 5 times directly followed by event b but the other way around never accurse, the value of $a!lb= 5/6 = 0.833  indicating that we are not completely sure of the dependency relation (noise 
can have effected the result). however if a is 50 times followed by b but the other way around 
never occurs, the value of $a!lb= 50/51 = 0.980 indication that we are pretty sure of the 
dependency relation.  even if a is 50 times directly followed by b and noise has effected that b is 
ones followed by a the value of $a!lb= 49/52 = 0.942.   
 the last metric, metric (v), is more global than the other measurements because not only 
direct following events are involved. the underlying intuition is as follows. if it is always the case 
that, when task a occurs, shortly later task b also occurs, then it is plausible that task a causes the 
occurrence of task b. on the other hand, if task b occurs (shortly) before task a, it is implausible 
that task a is the cause of task b.  bellow we define the formalization of this intuition. if, in an 
event stream, task a occurs before task b and n is the number of intermediary events between them, 
the $ a!b-dependency counter is incremented with a factor (δ)n. δ is a dependency fall factor (δ  in 
[0.0…1.0]).  in our experiments δ is set to 0.8. the effect is that the contribution to the dependency 
metric is maximal 1 (if task b appears directly after task a then n=0) and decreases if the distance 
increases. the process of looking forward from task a to the occurrence of task b stops after the 
first next occurrence of task a or task b. the other way around, if task b occurs before task a and n 
is again the number of intermediary events between them, the $ a!b-dependency counter is 
decreased with a factor (δ)n. after processing the whole workflow log the a!b-dependency 
counter is divided by the minimum overall frequency of task a and b (min (#a, #b)).   
 
 b #b #b<a #a>b $a !!!!lb $a!!!!b 
t10 1035 0 581 0.998 0.803 
t5 3949 80 168 0.353 0.267 
t11 1994 0 0 0 0.193 
t13 1000 0 0 0 0.162 
t9 1955 50 46 -0.041 0.161 
t8 1994 68 31 -0.370 0.119 
t3 3949 146 209 0.177 0.019 
t6 1035 0 0 0 0.000 
t7 959 0 0 0 -0.011 
t12 994 0 0 0 -0.093 
t1 1000 0 0 0 -0.246 
t2 1994 0 0 0 -0.487 
t4 1994 691 0 -0.999 -0.825 
table 1: d/f-table for event t6 (i.e., a=t6).  
 
given the process model of figure 1 a workflow log with 1000 event sequences (23573 events) is 
generated. as an example, table 1 shows the above-defined metrics for task t6. notice that the task 
t6 belongs to one of two concurrent event streams (the and-split in t2). it can be seen from table 
1 that (i) t6 is never directly preceded by t10 (#b<a=0),  (ii) t6 is often directly followed by t10 
(#a>b=581), and (iii) both dependency measurements from t6 to t10 are relatively high (0.998 
and 0.803) . in the next section, we will use the d/f-table in combination with a relatively simple 
heuristic to construct a d/f-graph. 
4.2 induction of dependency/frequency graphs 
in the previous section, we observed that the information in the t6-d/f-table (table 1) strongly 
suggests that task t10 depends on task t6 because the dependency measurements between t6 and 
t10 are high, and t6 is never directly preceded by t10 and frequently directly followed by t10. in 
earlier approaches, cf. [25,26], we developed heuristic rules in line with this observation. as an 
illustration, consider the following rule:   
 if ((#a>b > σ ) and (#b<a ≤ σ ) and  ($a!lb ≥ n1) and  ($a!b ≥ n2)) then  a→b 
a→b is the dependency relation ( b depends (directly) on a) as introduced before. the four 
conditions demand that specific values of the d/f graph (#a>b, #b<a, $a !lb, $a!b) are higher 
or lower than a certain threshold value ( σ, n1, n2). in the perfect situation where we know that we 
have a workflow log that is totally free of noise, every task-pattern-occurrence is informative and 
for instance, the value of σ can be set to zero. however, if it is not clear if a workflow log is noise 
free, we must protect the induction process against inferences based on noise; only task-pattern-
occurrences above a threshold frequency are reliable enough for our induction process. the 
situation for the n1 and n2 parameters is less straightforward. especially the n2 parameter appears 
sensitive for parallelism in the workflow. the high number of parameters and the sensitivity of 
these parameters for noise are clear disadvantages of this kind of heuristic rules. in [21] we have 
attempted to use machine-learning techniques for threshold tuning.  
however, it seems not necessary to formulate a rule that for each pair of events a and b takes the 
decision if they are in the dependency relation or not. because we know that each not-first event 
must have at least one cause event, and each not-last event must have at least one dependent event. 
using this trivial information in our heuristic rule we can limit the search for “the best” candidate. 
this best helps us enormously in finding dependency relations. the above-formulated idea is 
implemented in the following first version of our heuristic rule. but first the definition of the 
dependency score  (ds) between event two events say x and y (notation ds(x,y) ), is given because 
we will use the ds in the formulation of the heuristic rule.   
- suppose x and y are events, then the dependency score ds(x,y) = (($x!ly)2 + ($x!y)2)/2.  
- first (temporally) version of mining rule 1. given a task a:  
a→x if and only if x is the event for which ds(a,x) is maximal, 
y→a if and only if y is the event for which ds(y,a) is maximal. 
remark that the new heuristic rule does not contain any parameter. applying this simple heuristic 
on the d/f-table (table 1 ) results in the d/f-graph of figure 2. if we compare the d/f-graph of 
figure 2 with the petri net of figure 1 (the petri-net used for generating the workflow log and d/f-
table in table 1), it can be seen that all the connections between the nodes are in accordance with 
underlying workflow model (all connections are correct and there are no missing connections). for each arc the dependency score (ds) is given and for each task the number of event occurrences in 
the log.   
 
 
 
t1
1000t2
1994t3
3949t4
1994t6
1035
t5
3949t7
959
t12
994t13
1000t11
1994
t8
1994t9
1995t10
1035
1.000
0.4200.502
0.8430.309
0.453 0.2470.7320.386
0.803
0.104
0.4230.4750.428
0.667
1.000 
figure 2: the extracted d/f-graph based on heuristic rule 1. 
 
however, the heuristic rule formulated above will not recognize all possible dependency relations. 
figure 3 gives an overview of the types of relations that will not be recognized: (i) some complex 
connection structures, (ii) recursion, and (iii) short loops. below we try to update the will discuss 
them.  
 
t9 t9t5
b2b1
a2a1
 
 
figure 3: an overview of the types of dependency relations not recognized by the first 
version of mining rule 1: (i) complex interconnected structures, (ii) recursion, and (iii) short 
loops. 
 
applying the first version of mining rule 1 on the first type of structure of figure 3 can result in 
missing dependency relations. for instance if the result of applying the rule on a1 gives a1→b1, on a2 gives a2→b2, on b1 gives again a1→b1, and finally on b2 gives again a2→b2, then the 
dependency relation a2→b1 will not be recognized. in line with these observations, the first 
version of our mining rule 1 is updated. 
 
- mining rule 1 (definite version). given a task a  
suppose x is the event for which ds(a,x)  = m is maximal. then a →y if and only if 
ds(a,y)< 0.95*m.  
suppose x is the event for which ds(x,a)  = m is maximal. then y →a if and only if 
ds(y,a)< 0.95*m . 
remark that again we have introduced a threshold value of 0.95. however, it is only one parameter, 
and the parameter seems robust for noise and concurrent processes. for this reason parameter 
tuning appears not necessary; the default value of 0.95 is appropriate. 
 the remaining two type of relations of figure 3 not covered by rule (1) are recursion (ii) and 
short loops (iii). recursion in for instance event t9 will result in patterns like t5, t4, t9, t9, t6, 
t9, t8 . recursion can be recognized by observing a high frequency of # t9>t9  in combination with 
a ds(t9,t9) value of about zero (normally ds(a,a) is only near to zero if #a>a is also near to 
zero).  
 a short loop from for instance t9 to t5 will result in patterns like t5, t4, t9, t5, t9, t6, t5, 
t8. at first sight short loops can be recognized by observing that both a high and exactly equal 
frequency of t5>t9  and t9<t5 in combination with dependency measurements ds(t5,t9)  and 
ds(t9,t5) both near to zero . however, the same behavior can be observed when t5 and t9 both 
depend on event x with x is an and-split.  
 in line with these observations, heuristic rule 1 is extended with two simple heuristic rules for 
recursion (rule 2) and for short loops (rule 3). details are omitted. 
4.3 generating wf-nets from d/f-graphs 
by applying the heuristic rules of subsection 4.1 on a workflow log it appears relatively simple to 
find the corresponding d/f-graphs. but the types of the splits and joins are not yet represented in the d/f-graph. however (i) information in the d/f-table, and (ii) the frequency of the nodes in the 
d/f-graph contain useful information to indicate the type of a join or a split.  for instance, if we have to detect the type of a split from a to b and/or c, we can look in the 
d/f-table to the values of b>c  and b<c . if a is an and-split, then pattern b, c and the pattern c, 
b can both appear, and we expect a positive value for both b>c  and c>b  (or in other words the 
b||c-relation holds). if it is an or-split, the patterns b,c and c,b  will not appear. 
as an example of (ii) we look to the frequency of the nodes in the d/f-graph of figure 2. t2 is 
an and-split (because #t4 = #t2  and there are no other incoming arcs for t4. t5 is an or-split 
(because #t5 = #t8 + #t9 ), and analogue t4 and t11. the join in node t11 is a little bit more 
complex: it appears a combination of a or-join between t7 and t10, combined with a and-join 
with t8 (#t7+#t10=#t8=#t11).   
in our mining tool the second observation (a frequency check) is used for the validation of the 
induced workflow model (see paragraph 5), a heuristic based on the first observation is used to 
determine the kinds of splits and joins. remark that for two events a and b exactly one of the 
following possible relations holds: a→b, b→a, a#b, or a||b.  based on the d/f-graph we already 
know if a→b or b→a holds. if that is not the case we only have to take the decision if a#b, or a||b  
holds. if this decision is taken, we can apply the α-algorithm of our formal approach [5] to translate 
this information into a wf-net. using the α-algorithm in this way we were able to reconstruct the 
types of the splits and joins appearing in our working example and to reconstruct the complete 
underlying wf-net (exactly the same one as the wf-net of figure 1). in the next section, (section 
5) we introduce the workflow mining tool little thumb. the tool is based on the workflow mining 
heuristics as presented in this section. in section 6 we will report our experimental results of 
applying the above-defined workflow mining heuristics to other workflow logs, with and without 
noise. 
 
5. little thumb 
little thumb1 is a tool that attempts to induce a workflow model from a workflow log. the 
workflow log may contain errors (noise) and can be incomplete. in figure 4 a screenshot of little 
                                                           
1 for two reasons we chose the name little thumb for our process mining tool: (i) it is based on 
heuristic rules also know as “rules of thumb”, and (ii) the analogy with the fairy tale. in the fairy tale little thumb and his brothers are left in the forest by their parents. fortunately, little thumb left a trail of white pebbles to find his way back. the second time, little thumb uses breadcrumbs instead of white pebbles to mark the way back. unfortunately, the breadcrumbs are partly eaten away by the birds, thus making it impossible to find the way back. the goal of the tool little thumb is given. in this example session, little thumb is used to analyze a workflow log. for this 
example we use a log with information about the processing of complaints. however, 5% of the 
cases in the workflow log contain errors (i.e. part of the event registration is missing or two events 
are interchanged). in the left upper corner, we see the d/f-table as discussed in section 4.2. 
 in the right side of the screenshot we see 5 tabs; (i) generate wf-log, (ii) select events, (iii) 
load wf-log, (iv) analyse wf-log, and finally (v) check wf-net. with the generate-wf-log tab 
(i) it is possible to load a wf-net and to generate workflow logs, with and without noise. in an 
experimental setting, these workflow-logs can be used as mining material to test the mining 
performance of our tool. with the select-events tab (ii) we can concentrate our mining process on 
the most frequent events and neglect low frequent events. the function of load-wf-log tab (iii) is 
trivial. the analyze-wf-log function (tab (iv)) is of more importance. below we will first illustrate 
the analyse-wf-log-function followed by a short illustration of the check-wf-net function (tab 
(v)).        
 
                                                                                                                                                                                                 
thumb is to deal with situations where the information in the log is incomplete and partly incorrect. 
this is analogous to finding the way back home on the basis of few breadcrumbs being left by the birds. 
   
figure 3: a screenshot of little thumb. 
 
we try to follow little thumb during his attempt to analyze the loaded workflow log. the same 
steps as mentioned in paragraph 4 can be distinguished. step (i) the construction of a 
dependency/frequency table (d/f-table). step (ii) the induction of a d/f-graph out of a d/f-table. 
step (iii) the induction of the and/or information for the different splits and joins and the 
reconstruction of the wf-net out of the d/f-table and the d/f graph.  
 
the first step is already executed (see the d/f-table in the screenshot of figure 4). the ‘evaluate’ 
event (frequency 989) is in focus. the ‘evaluate’-event is 356 times followed by the  
‘processing_required’-event, and 385 times by the ‘no_processing’-event. it easily to see that the 
workflow log contains noise: the 2 value for the #a<b-counter for the ‘no-processing’-event is 
caused by an error in the log (i.e. an unusual order). the result of step (ii) the induction of a d/f-
graph out of a d/f-table is will results in extra information in the d/f-table: the extra information 
indicates if events are in the ->, #, or || relation. with the show-d/f-graph option, it is possible to display the d/f-graph as illustrated in figure 5. for each connection the dependency score (ds) is 
given and for each event the number of occurrences in the log. it is also possible to show the 
$a!lb-value or the $a!b-value for each connection. the d/f-graph can be used for a first 
validation with a domain expert and possible changes can be easily implemented by changing the 
values in the extended d/f-table. 
 
 
register
9890.843evaluate
989check_processing
893processing_required
467processing_ok
465
process_quest
519
time_out
473no_processing
525processing_nok
427
archive
990process_complaint
895
send_quest
9851.000
0.9080.928 0.850 0.684 0.9880.5180.518
0.996
0.838
0.914
0.9220.626
0.6540.841
 
 
figure 5: the resulting d/f-graph of little thumb. 
 
 
the types of the splits and joins are not yet represented in the d/f-graph. in step (iii) we use the 
information in the extended d/f-table to indicate the type of a joins and splits. little thumb will 
use this information for the induction of the complete wf-net (represented as a petri-net). in the 
wf-net we can see that the split from ‘register’ to ‘send_quest’ and ‘evaluate’ is an and-split. 
again it is possible to evaluate the mined wf-net with a domain expert how knows the process that 
we try to model very well; possible changes can easily be implemented by editing the values in the 
extended d/f-table. register
989evaluate
989check_processing
893processing_required
467processing_ok
465
process_quest
519
time_out
473no_processing
525processing_nok
427
archive
990process_complaint
895
send_quest
9850.914b
e
 
 
figure 6: the wf-net as induced by little thumb. 
 
 
finally we will shortly discuss the last tab (v); the so-called check wf-net tab. this tab gives us the 
possibility to validate the wf-net. during the first check, we present all the traces in the workflow 
log of the wf-net; the wf-net checks if the trace can be parsed by the wf-net. if not, the position 
in the trace were the parsing stops is shown to the user. traces in the workflow log with noise will 
cause problems during tracing. however, if a high number of traces run in problems around the 
same event, this is a indication that there is a problem in the wf-net around that event. in the 
second check, we test out if the frequency information of the events (#a) is in accordance with the 
structure of the mined wf-net. for instance, the frequency information around ‘evaluate’ (989), ‘processing_required’ (467) and ‘no_processing’ (525) points out a or-split. small differences can 
be explained by noise, clear differences indicate an error in the mined wf-net. this concludes our 
introduction to little thumb. 
 
6. experiments 
6.1 first experiments 
to test our approach we use the petri-net representations of six different free-choice workflow models. the complexity of these models range from comparable with the complexity of our 
working model of figure 1 (13 tasks) to models with 16 tasks. all models contain concurrent 
processes and loops. for each model we generated three random workflow logs with 1000 event 
sequences: (i) a workflow log without noise, (ii) one with 5% noise, and (iii) a log with 10% noise.  to incorporate noise in our workflow logs we define four different types of noise generating 
operations: (i) delete the head of a event sequence, (ii) delete the tail of a sequence,  (iii) delete a 
part of the body, and finally (iv) interchange two random chosen events. during the deletion-
operations at least one event, and no more than one third of the sequence is deleted. the first step in 
generating a workflow log with 5% noise is to randomly generate a workflow log for the workflow 
model without errors. the next step is the random selection of 5% of the original event sequences 
and applying one of the four above described noise generating operations on it (each noise 
generation operation with an equal probability of 1/4). table 2 is the d/f-table for event t6 of the 
wf-net of figure 1 but now with some noise added. comparing this table with table 1 shows small 
differences between the values of the tables.  
 
 
b #b #b<a #a>b $a !!!!lb  $a!!!!b 
t10 1004 2 556 0.991 0.790 
t5 3817 77 162 0.354 0.267 
t11 1901 0 2 0.667 0.182 
t13 923 0 0 0.000 0.161 
t9 1902 50 46 -0.041 0.161 
t8 1908 66 26 -0.430 0.108 
t3 3814 141 203 0.180 0.030 
t6 1007 0 0 0.000 0.000 
t7 920 0 0 0.000 -0.011 
t12 972 0 0 0.000 -0.098 
t1 926 3 2 -0.167 -0.254 
t2 1904 0 1 0.500 -0.473 
t4 1921 664 4 -0.987 -0.808 
 
table 2: d/f-table for event t6 (i.e., a=t6)  but now from a workflow log with noise. 
 applying the above method on the six noise free workflow logs results in six perfect d/f-graphs 
(i.e. all the connections are correct and there are no missing connections), and exact copies of the 
underlying wf-nets.  if we add 5% or 10% noise to the workflow logs, the resulting d/f-graphs 
and wf-nets are still perfect.  
6.2 second experiments 
 
in a second series of experiments, we try to use workflow logs that resemble real workflows. we 
observe that at least four elements strongly influence the behavior of a wf-net and/or the workflow 
mining process: 
• the number of event types  (i.e. tasks) in the wf-net: four different workflow models are used 
with 12, 22, 32 and 42 event types. 
• the amount of material  in the workflow log: we generated logs with 100, 200, 600, 1000, 1400, 
and 2000 trace lines. 
• the amount of noise : we generated workflow log without noise, with 5% noise, 10% noise, 
20% noise, and 50% noise. 
• the unbalance refers to the (relative) probability that enabled event will fire. we consider four 
different settings: all events have a equal probability to fire, the probabilities vary a little bit 
(i.e. between  [0.9...1.1]), a little bit more (i.e. between [0.5...1.5]), and the probability vary 
strongly (i.e. between [0.1...1.9]). if for instance task x is an or-split to a and b and a has a 
probability or weight of 0.2 and b as weight 1.8, about in 10% of the cases a is taken and in 
90% of the cases b is taken. however, if task x is an and-split to a and b, then both a and b 
will be executed but in 10% of the cases a precedes b and in 90% of the cases b precedes a.  
this kind of unbalance can influence the behavior of wf-net and the material in the workflow 
log dramatically. 
 
we generated 480 different workflow logs by varying each of the above enumerated elements (i.e. 4  
x 6 x 5 x 4 =  480). based on the experiments it was possible to conclude under all circumstances 
most dependency relations, the type of splits and the type of joins are correctly found. the mining 
technique appears especially robust for the number of trace lines and the amount of unbalance. only if the amount of noise is increased to 50%, the mining technique runs into serious problems. a 
lower unbalance or the maximum number of trace lines (2000) did not help.  
 in about a quarter of the cases, exactly the right wf-net was found. in about 75% of the 
experiments, the check facility of little thumb found one or two errors in combination with an 
indication where in the wf-net the error seems to appear. we observed that most errors have to do 
with complex interconnected structures (figure 3) in combination with short loops. an 
improvement of the heuristic rules for short loops seems necessary.  
 
7. conclusion 
in this paper, we (i) introduced the context of workflow processes and process mining, (ii) some preliminaries including a modeling language for workflow processes, and (iii) a definition of a 
workflow log. hereafter, we presented the details of the three steps of our process mining 
technique: step (i) the construction of the d/f-table, step (ii) the induction of a d/f-graph out of a 
d/f-table, and step (iii) the reconstruction of the wf-net out of the d/f-table and the d/f graph.  
after this, we introduced little thumb, a workflow mining tool based on our process mining 
techniques.  
in section 6, we describe two series of experiments. in the first experiment, we applied our 
technique on six different workflow models with about 15 tasks. all models contain concurrent processes and loops. for each workflow model, we generated three random workflow logs with 
1000 event sequences: (i) without noise, (ii) with 5% noise, and (iii) with 10% noise. using the 
proposed technique, we were able to reconstruct the correct d/f-graphs and wf-nets. the 
experimental results on the workflow logs with noise indicate that our technique seems robust in 
case of noise.  
in the second experiment, not only the amount of noise was varied, but also the amount of 
material in the log, the complexity of the wf-net (with 12, 22, 32 and 42 event types), and amount 
of unbalance in the wf-net. again, the experimental results indicate that our technique is robust 
against these factors. however, there are still problems with our mining techniques; errors are made 
around complex interconnected structures in combination with short loops. an improvement of the 
heuristic rules for short loops seems necessary. 
notwithstanding the reported results and improvements, there is a lot of future work to do. more 
experimental work must be done especially on real workflow logs. in [3] we present a common xml-format for workflow logs. experience shows that it is fairly simple to extract information out 
of enterprise-specific information systems and translate this to xml format. little thumb can read 
the xml format. however, little thumb is only an experimental tool; if our mining technique 
becomes stable, we can put more effort in developing really interactive, and user friendly mining 
tool.  
finally, we will extend our mining technique in order to enlarge the set of underlying wf-nets 
that can be successfully mined. 
 
references 
1 w.m.p. van der aalst. the application of petri nets to workflow management. the journal of 
circuits, systems and computers, 8(1): 21-66, 1998. 
2 w.m.p. van der aalst, j. desel, and a. oberweis, editors. business process management: 
models, techniques, and empirical studies, volume 1806 of lecture notes in computer 
science. springer-verlag, berlin, 2000. 
3 w.m.p. van der aalst, b.f. van dongen, j. herbst, l. maruster, g. schimm, a.j.m.m. 
weijters. workflow mining: a survey of issues and approaches (working paper). 
http://www.tm.tue.nl/it/staff/wvdaalst/workflow/mining/. 
4 w.m.p. van der aalst and s. jablonski, editors. flexible workflow technology driving the 
networked economy, special issue of the international journal of computer systems, science, 
and engineering, volume 15, number 5, 2000. 
5 w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workflow mining: which 
processes can be rediscovered, working paper wp74, beta research school for operations 
management and logistic, eindhoven technical university, pp 1-25, 2002. 
6 r. agrawal, d. gunopulos, and f. leymann. mining process models from workflow logs. in 
the proceedings of the sixth international conference on extending database technology, 
pages 469-483, 1998. 
7 f. casati, c. ceri, b. pernici, and g. pozzi. workflow evolution. data and knowledge 
engineering, 24(3): 211-238, 1998. 
8 j.e. cook and a.l. wolf. discovering models of software processes from event-based data, 
acm transactions on software engineering and methodology, 7(3): 215-249, 1998. 9 j.e. cook and a.l. wolf. event-based detection of concurrency. in proceedings of the sixth 
international symposium on the foundations of software engineering (fse-6), orlando, fl, 
november 1998, pages 35-45.  
10 j.e. cook and a.l. wolf: software process validation: quantitatively measuring the 
correspondence of a process to a model. acm transactions on software engineering and 
methodology, 8(2): 147-176, 1999. 
11 j. desel and j. esparza. free choice petri nets, volume 40 of cambridge tracts in theoretical 
computer science. cambridge university press, cambridge, uk, 1995. 
12 c.a. ellis, k. keddara, and g. rozenberg. dynamic change within workflow systems. in n. 
comstock and c.a. ellis, editors, conf. on organizational computing systems, pages 10 - 21. 
acm sigois, acm. milpitas, california, 1995. 
13 c.a. ellis and g.j. nutt. modelling and enactment of workflow systems. in m. ajmone 
marsan, editor, application and theory of petri nets 1993, volume 691 of lecture notes in 
computer science, pages 1-16. springer, berlin, germany, 1993. 
14 j. herbst. a machine learning approach to workflow management.  in 11th european 
conference on machine learning, volume 1810 of lecture notes in computer science, pages 
183-194, springer, berlin, germany, 2000. 
15 j. herbst. dealing with concurrency in workflow induction in u. baake, r. zobel and m. al-
akaidi, european concurrent engineering conference, scs europe, ghent, belgium, 2000. 
16 j. herbst and d. karagiannis.  an inductive approach to the acquisition and adaptation of 
workflow models. in m. ibrahim and b. drabble, editors, proceedings of the ijcai'99 
workshop on intelligent workflow and process management: the new frontier for ai in 
business, pages 52-57, 1999. 
17 j. herbst and d. karagiannis.  integrating machine learning and workflow management to 
support acquisition and adaptation of workflow models. international journal of intelligent 
systems in accounting, finance and management, 9:67-92, 2000. 
18 inconcert. inconcert process designer's guide. inconcert inc, cambridge, massachusetts, 
1998. 
19 s. jablonski and c. bussler. workflow management: modeling concepts, architecture, and 
implementation. international thomson computer press, 1996. 20 m. klein, c. dellarocas, and a. bernstein, editors. adaptive workflow systems, special issue 
of the journal of computer supported cooperative work, volume 9, numbers 3-4, 2000. 
21 l. maruster, a.j.m.m. weijters, w.m.p. van der aalst, and a. van den bosch. process mining: 
discovering direct successors in process logs. proceedings of the 5th international 
conference on discovery science (discovery science 2002), volume 2534 of lecture notes in 
artificial intelligence, pages 364-373. springer-verlag, berlin, 2002. 
22 m.k. maxeiner, k. küspert, and f. leymann. data mining von workflow-protokollen zur 
teilautomatisierten konstruktion von prozeßmodellen. in proceedings of  datenbanksysteme in 
büro, technik und wissenschaft, pages 75-84, informatik aktuell springer, berlin, germany 
2001. 
23 p. lawrence (editor). workflow handbook 1997, workflow management coalition. john 
wiley and sons, new york, 1997. 
24 g. schimm. process miner - a tool for mining process schemes from event-based data. in s. 
flesca and g. ianni, editors, proceedings of the 8th european conference on artificial 
intelligence (jelia), volume 2424 of lecture notes in computer science, springer-verlag, 
berlin, 2002. 
25 a.j.m.m. weijters and w.m.p. van der aalst. process mining: discovering workflow models 
from event-based data. in b. kröse, m. de rijke, g. schreiber, and m. van someren, editors, 
proceedings of the 13th belgium-netherlands conference on artificial intelligence (bnaic 
2001), pages 283-290, 2001. 
26 a.j.m.m. weijters and w.m.p. van der aalst. rediscovering work ow models from event-
based data. in v. hoste and g. de pauw, editors, proceedings of the 11th dutch-belgian 
conference on machine learning (benelearn 2001), pages 93-100, 2001. 