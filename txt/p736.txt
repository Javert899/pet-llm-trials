proling event logs to congure
risk indicators for process delays
anastasiia pika1, wil m. p. van der aalst2;1, colin j. fidge1,
arthur h. m. ter hofstede1;2, and moe t. wynn1
1queensland university of technology, brisbane, australia
fa.pika,c.fidge,a.terhofstede,m.wynn g@qut.edu.au
2eindhoven university of technology, eindhoven, the netherlands
fw.m.p.v.d.aalst g@tue.nl
abstract. risk identication is one of the most challenging stages in
the risk management process. conventional risk management approaches
provide little guidance and companies often rely on the knowledge of ex-
perts for risk identication. in this paper we demonstrate how risk indi-
cators can be used to predict process delays via a method for conguring
so-called process risk indicators (pris). the method learns suitable
congurations from past process behaviour recorded in event logs. to
validate the approach we have implemented it as a plug-in of the prom
process mining framework and have conducted experiments using various
data sets from a major insurance company.
keywords: process risk indicators, process mining, risk identication
1 introduction
managing risks is one of the top priorities in corporate and government organ-
isations3. according to iso guide 73:2009, risk is the \eect of uncertainty on
objectives" where an eect is \a deviation from the expected | positive and/or
negative" [6]. risk identication is an essential starting point for risk manage-
ment. it is dened as a \process of nding, recognizing and describing risks" [6].
although many risk management approaches provide high-level guidance about
risk management strategy, they do not provide any tools to operationalize this
strategy [12, 15]. standard iso 31000 species that \risk identication can in-
volve historical data" [15], however it does not provide any further guidelines on
how to use historical data.
managing business processes is another important concern of an organisation.
business processes are exposed to dierent risks. for instance, a process may not
be nished within the time-frame dened by a service level agreement, it may
produce low-quality results, or it may exceed its budget. we refer to risks that
threaten the achievement of process goals as process-related. most organisations
3http://www.gartner.com/id=1957716, gartner report \ceo survey 2012: cios
must link risk management and compliance to business priorities"use information systems supporting their operational business processes. often
these systems also record information about process executions in event logs.
our belief is that this information can be exploited for the identication of
process-related risks.
in our preliminary work [14] we introduced the idea of using process risk
indicators (pris) to predict whether a deadline transgression is likely to happen
or not. for example, if an activity is repeated multiple times for a case, then
the likelihood of delay is signicantly higher. we also introduced a method for
instantiating these indicators from event logs based on statistical techniques for
outlier detection. however, our initial experiments showed that further work is
required to properly calibrate the indicators to reduce the number of \false pos-
itives", i.e., cases that are predicted to be late but in the end are not. in this
paper we present a novel method for conguration of pris that uses informa-
tion about outcomes from cases executed in the past. the method aligns these
indicators with the specics of a particular process to minimize the number of
false positives. we demonstrate the feasibility of the proposed approach using
case studies with data sets from an australian insurance company (suncorp).
the remainder of the paper is organized as follows. section 2 discusses re-
lated work. the general approach to process risk identication is presented in
section 3 followed by a description of eight pris. we then show how to congure
these pris using information about the outcomes of cases in the past. section 4
discusses our implementation in prom and reports on our experimental results.
section 5 concludes the paper.
2 related and previous work
few approaches exist that aim to identify and/or assess process risks [7, 8, 21].
wickboldt et al. proposed a framework that uses process execution data for
risk assessment [21]. risk assessment modules of the framework use information
about risk events reported during past activity executions. our approach also
predicts future risks based on past behaviours, but it does not require risk-related
information to be explicitly stored. jallow et al. [7] proposed a framework for
identication of operational process risks. however, estimation of the probabili-
ties and impacts associated with risk-related activities was assumed to be done
by experts. our approach avoids subjective opinions and learns such values from
historic event data. jans et al. [8] proposed using process mining for the identi-
cation of one particular type of risk (transactional fraud risk) and showed that
available process mining tools can help auditors detect fraud. by contrast, our
approach focuses on quantiable values such as delays or product quality and it
emphasises automatable techniques for risk identication that can be used for
run-time operational support [16].
van dongen et al. proposed an approach for predicting the remaining cycle
time of a case by applying non-parametric regression and using case data as
predictor variables [20]. the approach for predicting remaining process time
proposed by van der aalst et al. [18] is based on building an annotated transitionsystem and estimating the average remaining time of cases that visited the same
state previously. in contrast, our approach predicts the likelihood of case delay
rather than the remaining execution time.
grigori et al. presented a set of integrated tools that help manage process
execution quality supporting such features as analysis and prediction [3]. in
other work they propose a method for exception analysis, prediction, and pre-
vention [4]. a common feature of these approaches is that it is the responsibility
of users to specify what process properties (conditions, exceptions etc.) they
would like to analyse. our approach does not require such input and is based on
a set of risk indicators.
in our own earlier work we introduced the idea of using process risk indi-
cators for predicting case delays and proposed a method for instantiation of the
indicators from event logs [14]. the method is based on statistical techniques
for outlier detection. it used a simple analysis which assumed that process be-
haviours have normal distributions with xed thresholds being sucient to iden-
tify \risky" behaviours. our initial experiments revealed that risk indicators can
be used to predict case delays [14], but further work is required to properly
calibrate the indicators to reduce the number of false positives. in this paper
we present a method for conguration of risk indicators for process delays that
signicantly improves precision of case delays predictions.
3 risk identication method
3.1 approach
our goal is to develop a method that can identify the risk of delay for running
cases with a high degree of precision. our method analyses characteristics of
a current case, compares them with characteristics of similar cases executed
in the past and predicts a case delay if a \risky" behaviour is detected. our
overall approach consists of three major steps: (1) dene process risk indicators;
(2) congure pris; (3) identify the presence of pri instances in a current case.
first, we need to identify which behaviour of a process can be considered
\risky". in our initial work we introduced the use of process risk indicators
(pris) for predicting case delays. we dened a pri as \a pattern observable
in an event log whose presence indicates a higher likelihood of some process-
related risk" [14]. for example, an unusually large number of activity repetitions
per case may indicate a likely case delay or low-quality output because there
seems to be a problem processing this case.
in our preliminary work we also introduced a method for identifying the pres-
ence of a pri based on the \sample standard deviations" approach for outlier
detection [14]. for each pri we dened cut-o thresholds as x 2s. observations
whose values are higher than this value were considered outliers. a limitation of
the method is the assumption that some particular process behaviour follows a
normal distribution (e.g., activity repetitions in a case) which may not be valid
in many cases. we also assumed that atypical behaviour of a process can beconsidered \risky", e.g. when some activity in a case has an atypically long du-
ration it signals a higher likelihood of the case delay. however, while conducting
initial experiments we learned that though atypical behaviour is often associated
with case delays it is not always \risky". for example, if a process contains an
automated activity which typically takes a very small amount of time compared
to the total time that cases take, then variations to the execution time of such
an activity, even relatively large ones, do not aect the case duration.
to overcome these weaknesses of our initial work we present here a method
for conguration of indicators so that the specic characteristics of a particular
process are taken into account. we again use cut-o thresholds to identify \risky"
behaviours, however we introduce a way of learning the threshold values by using
information about outcomes of cases in the past. the method allows us to identify
atypical process behaviour that has been often associated with case delays in the
past rather than assuming anyoutlier indicates a risk.
3.2 process risk indicators (pris)
a pri is a pattern that signals an increased likelihood of some process-related
risk and which can be identied by analysing an event log. in our previous
work [14] we introduced the idea of using process risk indicators to identify the
risk of case delay. for the purpose of this paper we use several indicators that
can be discovered using basic event logs, information about case outcomes and
process models, all of which were available to us in our industrial case study.
below we dene eight process risk indicators for process delays.
pri 1: atypical activity execution time. the duration of an activity sig-
nicantly exceeds its typical duration. an activity may take more time than
usual due to human factors: an employee executing the activity may be inex-
perienced or occupied with many other tasks. fatigue is a common factor that
may cause a delay. another reason can be a complex or exceptional case that
requires additional investigation/learning. activity delay is also often caused by
a third party's involvement|reducing the number of contacts with third parties
is one of business process re-engineering's best practices [11].
pri 2: atypical waiting time. an activity has not been started for an
atypically long period of time. one possible explanation for long waiting times
is a lack of available resources. another possible reason is the \too hard basket"
syndrome, i.e., the situation where no one is willing to start an activity as it
is perceived to be too challenging or time consuming. also, some employees
tend to process certain tasks in batches, which may increase a particular task's
waiting time. a typical example is an approval task. removing batch-processing
is another of the bpr best practices [11], as is reducing waiting times because
these often occupy 95% of the throughput time of a case [9].
pri 3: multiple activity repetitions. the number of times an activity is
repeated in a case signicantly exceeds its usual value. it may be necessaryto repeat an activity if previous attempts fail. this can happen due to third
party involvement, e.g., not receiving an expected service from subcontractors
or failure to provide required information by a client. employees may also need
to repeat a task because of inexperience or complex case requirements.
pri 4: presence of a \risky" activity. a case contains a \risky" activity.
an activity is considered \risky" if the majority of the cases that contained
this activity in the past have been delayed. execution of a \risky" activity may
be related to a case's specics. for example, consultation with an expert or a
manager may be required for an exceptionally complex case.
pri 5: multiple resource involvement. more resources are involved in a
case than usually. one possible reason for such a situation is the so-called \hot
potato" phenomenon where a case is forwarded between dierent resources be-
cause nobody is willing to take charge of it. atypically high resource involvement
can also be needed for a very complex case. reducing the number of parties in-
volved in a case is another of the bpr best practices [11]. balasubramanian et
al. name frequent hand-overs of work between people in a process as one of the
factors that can lead to time overruns [2].
pri 6: atypical sub-process duration. the sum of activity duration and its
waiting time in a case (referred to here as a sub-process) is signicantly higher
than its typical value. we introduce this indicator to be able to work with event
logs that only record \complete" events for activities, as is often the case for real
event logs. this indicator tackles the same issues as pris 1 and 2.
pri 7: high resource workload. an activity has been assigned to or started
by a resource with a high workload. the workload of a resource at a point in
time is the number of items that were started by or assigned to the resource but
not yet completed. high resource workload is often mentioned in the literature
as a reason for such risks as time overruns or low-quality outputs [5, 13].
pri 8: use of a \risky" resource. an activity has been assigned to or
started by a\risky" resource. a \risky" resource for some activity is the one that
was often involved in execution of this activity in delayed cases. some human
resources may be incompetent or inexperienced when it comes to the execution of
some activities in a process. it is important to use recent data for identication
of this pri as the qualication levels and experience of resources will change
over time. another reason for a resource to be considered risky is a tendency to
postpone execution of certain activities, e.g., approval tasks.
3.3 conguring process risk indicators
our method for conguration of indicators requires information about known
outcomes from cases that happened in the past, i.e., whether they were delayed
or completed in time. we aim to nd for the pris the values of parameters thatcould predict delays with a required degree of precision in the past. if we cannot
detect such values for an indicator then it is not used for a particular process.
an input parameter to our method is a desired precision level. precision is the
fraction of cases predicted to be delayed that are actually delayed. increasing
precision is usually done at the expense of decreasing recall, which is dened
as the fraction of delayed cases that can be successfully predicted against the
actually delayed cases. if a user deals with a critical process, he may prefer
monitoring alerts with lower precision levels in order to increase recall, while for
a non-critical process he may want to check only those alerts that indicate a
very high likelihood of a case delay.
for each relevant process behaviour (e.g., the number of activity repetitions
in a case) we look for the smallest value that allows distinguishing between
delayed andin time cases with a required degree of precision. this value is used
as a cut-o threshold. in order to dene this threshold we need to check the
eectiveness of various candidate values. however, there could be a wide range
of these. analysing past logs can be time consuming, so in order to reduce the
search space we learn cut-o thresholds for the pris by checking only those
values from a pool of selected candidates. we use the following heuristic to
dene candidate values. first, we discard those values lower than the mean x
(which gives us a measure of central tendency). we then include those values
calculated as x ns, where sis the standard deviation (as a measure of
statistical dispersion), and nis in the range of 0 to 10 with an increment of 0.25
(these values were used for the experiments, they are input parameters). we do
not necessarily assume a normal distribution. nevertheless, these conventional
statistical measures provide a natural starting point for searching for thresholds.
we then check all values from the dened pool of candidates.
we are interested in indicators that can predict delays during a case's exe-
cution. therefore, while learning parameters of pris from past execution data,
our method considers only those events that happened before a deadline, i.e., we
discard activities that have been started after the deadline has been missed.
as an example of the calculation, consider pri 5 \ multiple resource involve-
ment". pri 5 is a case-based pri, i.e., it can have only one value per case and
we dene one cut-o threshold. in order to identify and use pri 5 the following
steps are performed:
1. dene candidate values tfor the cut-o threshold t:
(a) identify average number of resources involved in a case before deadline
(x) and standard deviation sof the population.
(b)ttx ns|npt0;0:25;0:50; :::;10uu
2. dene the cut-o threshold t:
for each tipt:
(a) collect a subset ctrueof the training set comprising all cases that are
delayed and whose number of resources involved before the deadline is
higher than ti;
(b) collect a subset cfalse of the training set comprising all cases that are
in time and whose number of resources is higher than ti;(c)pi#
|ctrue|{p|ctrue| |cfalse|q;ifp|ctrue| |cfalse|q¡0
0 otherwise.
here t= arg min tiptppi¡pq, where pis the desired precision level.
3. check the number of resources involved in the current case and alert a like-
lihood of a case delay if the number is higher than the value of the learned
threshold t.
for activity-based pris such as pri 1 (\atypical activity execution time"),
pri 2 (\atypical waiting time"), pri 3 (\multiple activity repetitions") and
pri 6 (\atypical sub-process duration") a similar procedure is repeated for
each activity to learn proper thresholds. a case can have multiple instances
of an activity-based pri, e.g., several activities may be delayed or repeated.
we consider that there is a chance of a case delay if the case contains at least
one instance of an activity-based pri. for resource-based pri 7 \high resource
workload" we learn appropriate values for cut-o thresholds for each resource. if
in a current case an activity is assigned to or started by a resource with a high
workload (dened by the learned threshold), a case delay is more likely.
pris 4 and 8 do not follow the general procedure described above. these are
examples of indicators that can only be identied using information about the
outcomes of cases in the past. to identify pri 4 \presence of a risky activity"
we check if there exists an activity that is executed mainly in delayed cases. for
pri 8 we check for each pair \activity-resource" if some resource's involvement
in the execution of an activity mainly occurs in delayed cases. then we check
if a current case contains a \risky" activity or if an activity is assigned to a
\risky" resource. identication of such behaviour signals increased likelihood of
case delay.
4 validation using real event logs
4.1 experimental setup
to estimate the quality of case delay predictions by our method we use hold-out
cross-validation [10]. this is a commonly used statistical practice that implies
partitioning of data into two subsets, where one subset is used for initial learn-
ing (a training set), and the results are validated using the other subset (a test
set). to facilitate validation of our approach we have implemented a plug-in
of the process mining framework prom 64. the plug-in takes as an input two
event logs. it uses one log as a training set to congure the pris, then it anal-
yses cases in the other log (a test set) to identify occurrences of these pris.
an input parameter is the expected case duration. cases that take longer than
this value are considered to be delayed. if any of the indicators is found in a
case it is predicted to be delayed. we compare predicted case delays with the
4http://www.promtools.org/prom6/actual case durations and evaluate the performance of the process risk identica-
tion method by estimating the values of \precision" and \recall". these metrics
are often used in dierent machine learning areas to estimate performance of
prediction techniques. precision is calculated as the fraction of cases correctly
predicted to be delayed against the total number of cases predicted to be de-
layed. recall is calculated as the fraction of delayed cases that are successfully
predicted against the number of cases that are actually delayed. these values are
calculated separately for each indicator to evaluate their individual performance.
we also calculate the values of precision and recall for all indicators combined
to evaluate their cumulative performance.
we used two dierent approaches to splitting data into a training set and a
test set. in one approach, we split event logs randomly, such that 75% of cases
were put into a training set and 25% of cases in a test set (referred to later
as a \random" split). in the other approach, cases that were completed during
one period of time (four months) were put into a training set while cases that
were started within the next period (two months) were put into the test set
(referred to later as a \time" split). as our approach is based on learning from
past execution data it is important to use large data sets for training, therefore
we decided to put more data in the training set while still having enough data
in the test set for meaningful validation.
before applying our method for risk identication it is important to perform
data pre-processing. processes tend to evolve over time. to avoid learning from
outdated information recent data should be used. for our experiments we picked
cases that were active over the same period of six months. the algorithm should
use only completed cases to properly congure pris, therefore partial traces
representing running process instances should be ltered out. the results of any
process mining algorithm depend on input data, therefore the quality of event log
data is crucial [1]. for example, if event log data contains mislabelled activities,
the performance of the algorithm may be aected, therefore it is important to
clean event log rst (e.g., ltering out mislabelled events). it is also important to
separately analyse cases that are executed in dierent contexts that aect their
durations. for example, the expected case duration may depend on the type of
customer (\gold" versus \silver") or type of service (\premium" versus \nor-
mal"). if such execution contexts are known, event log data should be rst split
and cases that are executed in dierent contexts should be analysed separately.
4.2 data properties and preprocessing
we evaluated our approach using two data sets from suncorp , a large australian
insurance company. both data sets represent insurance claim processes from
dierent organisational units, referred to here as data set a and data set b.
both event logs provided by suncorp contained only completed cases. data set
b contains cases from ve departments and was split into ve sets (referred to
here as b1{b5) which were used in separate experiments. each data set (a,
b1{b5) was split into a training set and a test set. the training set was used by
the algorithm for learning the cut-o thresholds. cases in the test set were used
for evaluating the performance of the pris.we rst cleaned up the data sets by ltering out cases with activities that
appear only once in the whole log. in most cases, such activities were not really
unique though their label was. typically this was a consequence of combining an
activity's name with the name of the employee who executed that activity. we
used original unltered data sets to more accurately estimate resource workloads
(required for pri 7).
to more accurately estimate waiting times (for pris 2 and 6) we used process
models. we rst identied the pre-set of an activity, i.e. the set of activities that
can directly precede a given activity. we then calculated the waiting time for the
activity as the dierence between its \start" time and the \complete" time of
the last activity from its pre-set preceding it in the case. since we did not have
process models, we instead used process mining to discover them from the event
logs. first we ltered the logs so that they contained only cases representing
mainstream process behaviour and used these ltered logs to discover process
models represented by petri nets with one of the prom process mining plug-
ins [19]. for data set a we used 95% of the cases representing the most frequent
process variants. data sets b1{b5 proved to have a large variety of process
variants. for these data sets only those cases were used for process discovery
that share the same process variant with at least four other cases. these ltered
logs were only used for process discovery and not in the experiments.
suncorp's business analysts provided us with indications about what they
feel should be the usual case durations for dierent departments. however, while
analysing the event logs we realized that these expectations are not realistic as
more than 50% of cases have durations higher than expected in four out of six
data sets. for these data sets we therefore learned the values for typical case
durations such that at least 50% of cases in a set are completed in time. these
values were used in the experiments. figure 1 shows as an example the distribu-
tion of case throughput times for data set b4. only cases highlighted in blue are
completed in time if we consider the value provided by the company's business
analysts to be the typical case duration. it is very likely that the behaviour of
a process is dierent when an explicit due date exists and is communicated to
workers. however, this should not aect the performance of our method since
process behaviour is still consistent across training and test data sets.
fig. 1. durations of cases in data set b4 (more than 50% of cases have durations higher
than expected by the company)data set a has some nice properties which make it suitable for our experi-
ments: a signicant number of cases, steady case arrival rates and similar case
duration distributions over time (low variability). figure 2 shows some basic
properties of data set a.
legend: sla { usual case duration in days
fig. 2. properties of data set a
for data sets b1{b5 additional ltering was required. we were informed by
suncorp that cases with claim amounts higher than a certain value are considered
\complex" and that it is normal for them to have long durations. we ltered the
event logs for our experiments and only used \simple" cases that are expected to
be completed within a certain time period. we found a large number of process
variants in these sets. high variability of the processes can be explained by the
fact that process models used by suncorp are not prescriptive and are only used
as guidance. high process variability may decrease precision of delay predictions
for two pris that use information about the order of activities (pri 2 \atypical
waiting time" and pri 6 \atypical sub-process duration"). the performance of
other pris is not expected to be aected since they do not rely on the order
of activity executions. also case arrival rates, case durations, and mean active
and waiting times were found to change over time. all these characteristics of
the process may have inuenced the results of the experiments. figure 3 depicts
basic characteristics of these ve data sets.
legend: sla { usual case duration in days
fig. 3. properties of data sets b1{b5
4.3 performance of the pris
we rst conducted our experiments with data set a. figure 4 depicts the results
of the experiments conducted with event log a using a random split and figure 5
depicts results of the experiments using a time split. an input parameter for the
algorithm is the \desired precision level". when we learn a cut-o threshold for
an indicator we pick the minimum value of the threshold that allowed predicting
case delays in a training set with a given precision level. we conducted exper-
iments for three precision levels: 95%, 90% and 80%. the columns represent
results for individual pris. the last column represents the cumulative result forall indicators: a case is predicted to be delayed if anyof the indicators is found
in the case. for a desired precision level the rst two rows represent the number
of true positives ( tp) and the number of false positives ( fp) produced. these
predictions are produced before expiry of the deadline. the next two rows are
the number of false negatives ( fn) and the number of true negatives ( tn).
tp fpis the number of cases predicted to be delayed. the precision is calcu-
lated as the fractiontp
tp fp.tp fnis the number of cases actually delayed
and can be used to compute the recall which is the fraction of delayed cases that
are successfully predicted and the actually delayed cases, i.e.,tp
tp fn. figures 4
and 5 show both precision and recall values for the test sets.
fig. 4. performance of the pris in data set a. \random" split experiment
the results of the experiments for the two dierent types of event log split
were comparable in terms of the indicators' performance. most predictions in
both cases came from pris 1, 2 and 6. some delays were indicated by pris 4
and 8. poorly performing indicators for this data set were pris 3, 5 and 7. in the
vast majority of cases it was only possible to identify pris 3 (\multiple activity
repetitions") and 5 (\multiple resource involvement") after the deadline was
missed. one of the reasons for the poor performance of pri 7 (\high resource
workload") for this log may be the fact that we do not have all data for the
process (incomplete cases were ltered out). we also assumed that resources
are involved full-time in this one particular process which may not be true.
figures 4 and 5 also demonstrate the number of delays that can be predicted with
these indicators for dierent precision levels. in the \random" split experiment
it can be observed that lowering the desired precision level leads to a decrease in
precision and an increase in recall. while this can also be observed in the \time"
split experiment the decrease of precision is more pronounced while the increase
in recall is less.fig. 5. performance of the pris in data set a. \time" split experiment
we have also applied to data set a the risk identication algorithm without
conguring the pris using a \random" 75/25% split. the results are depicted
in figure 6. for pris 1, 2, 3, 5, 6 and 7 the cut-o thresholds were dened
asx 2s, i.e., we assume normal distributions and use a 95% condence
interval. we did not use pris 4 and 8 in this experiment as they can only be
learned using information about the outcomes of past cases. precision levels for
all indicators were signicantly lower than the corresponding values from our
previous experiment where we congured the pris (depicted in figure 4). this
conrms that proper conguration of indicators is an essential step in the risk
identication method.
fig. 6. performance of the pris without congurations in data set a. \random" split
experiment
then we conducted the experiments with data sets b1{b5. figure 7 depicts
the results of the experiments for ve departments in data sets b1-b5. we have
used a random 75/25% split and 90% as the value for the desired precision level.
pris 1, 2, 6 and 8 demonstrated a good performance for all departments, and
a few delays were predicted with pris 3, 5 and 7. pri 4 (\presence of a risky
activity") did not predict any delays for these data sets because no single activity
was a strong predictor of delays in these logs.fig. 7. performance of the pris in data sets b1-b5. \random" split experiment
4.4 moment of delay prediction
we also evaluated the ability to predict delays early during a case's execution
which is obviously a highly desirable capability. in order to do so we checked
how many true positive and false positive predictions (coming from any of the
indicators) were generated before a given point in time during a case's execution,
to nd the earliest point when we can identify risks. since the event logs available
to us do not have \assign" events recorded, we consider the time of the \start"
event for an activity to be the discovery time for pris 3, 4, 5, 7 and 8, e.g., when
an activity has been started by a \risky" resource (pri 8), or by a resource with a
high workload (pri 7). the earliest time when we can observe pri 1 (\atypical
activity duration") is the time of the \start" event of an activity plus the value
of pri 1's threshold for this activity. for example, if an activity is not completed
within three days (the threshold value) after it has been started there is a higher
likelihood of the case delay, i.e., at this point we can already predict delay. the
earliest time when pri 2 (\atypical waiting time") can be observed is either thetime of the \complete" event of an activity plus the maximum of its successors'
pri 2 thresholds or the time of the \start" event of the next activity if it has
been started earlier and its wait duration is higher than its pri 2 threshold. for
example, if an activity is completed and none of its successors have been started
within two days (maximum of their pri 2 thresholds), we can say at this point
that a case delay is likely due to pri 2. a similar approach for calculating the
discovery time is used for pri 6.
figure 8(a) depicts the discovery times for data set a. recall that the dis-
covery time is the time at which a true positive or false positive predictions are
generated. figure 8(b) presents the discovery times for data set b5. the horizon-
tal axes in both diagrams represent the number of days since the beginning of a
case when the risk of the case delay was discovered. cases from data set a should
be completed within 14 days while the typical case duration for data set b5 is
120 days. the vertical axes depict the cumulative number of delay predictions
at a certain point in time. for example, figure 8(a) shows that more than 1000
correct delay predictions have been generated within the rst twelve days. for
data set a early predictions (below seven days) are coming mainly from pri 4
(\presence of a risky activity") and pri 8 (\use of a risky resource"). early
predictions for data set b5 (below 30 days) were generated mainly by pri 8
(\use of a risky resource") and pri 7 (\high resource workload").
paq p bq
fig. 8. pri discovery times for data sets a (a) and b5 (b), \random" split experiment
with 90% as the desired precision level
4.5 discussion
some of the limitations of the experiments described above are related to the
data available to us. one of the two data sets provided by suncorp displayed
high process variability. multiple process variants may have inuenced the per-
formance of pris that rely on the order of activities (pris 2 and 6), however
the performance of other indicators should not be aected. the other concern
is related to estimating the performance of pri 7 \high resource workload".
this is due to two reasons. the rst one is that the event logs available to us
contained only completed cases, i.e., traces corresponding to running process
instances were ltered out. we also assumed that all resources are involved in
one process. hence, the workload of resources may have been underestimated.in order to more accurately estimate the performance of this pri complete in-
formation about all processes in a company is required. this limitation should
not aect the performance of other indicators.
a limitation of the approach is our assumption that a process is in a steady
state, i.e. it is not changing over time. to deal with this limitation in this paper
we used data from a relatively short period (six months). however, if a process's
behaviour is constantly changing, the amount of available up-to-date data may
be insucient for proper conguration of pris.
we considered instance and process contexts, however we did not consider
social andexternal contexts using the terminology of [17], that may also inuence
case durations. this is a direction for possible future research. another direction
for future work is to investigate the relation between pris and the extent of the
expected delay.
5 conclusions
in this paper, we presented a method for conguration of risk indicators for
process delays. the method learns parameters of indicators by analysing event
logs and exploiting information about the outcomes of cases completed in the
past. such conguration of indicators takes the specics of a particular process
into account thus improving the accuracy of the predictions. we conducted a
number of experiments with dierent data sets from an australian insurance
company that conrmed that this approach decreases the level of false positive
alerts and thus signicantly improves the precision of case delay predictions.
the experiments demonstrated the ability to predict case delays with eight
selected pris. some of the indicators showed a consistently good performance
in all data sets (e.g., pris 1, 2 and 6), others are good predictors of delays for
some processes but did not predict delays for others (e.g., pris 4, 7 and 8).
pris 3 and 5 produced few predictions for this particular data set due to the
fact that it was typically possible to discover these indicators after the deadline
was missed. as is often the case in the data retrieval eld, there is a trade-o
between precision and recall. it is hard to predict more than 50% of case delays
with a high degree of precision using our indicators, while many delays can be
predicted with a degree of precision of 80%. we expect that our approach can
be applied for conguration of indicators for other types of process risks such as
cost overruns or low-quality outputs, but this should be explored in future work.
acknowledgements. this research is funded by the arc discovery project
\risk-aware business process management" (dp110100091). we would like to
thank suncorp for providing the data sets for analysis.
references
1. w.m.p. aalst et al. process mining manifesto. in business process management
workshops , pages 169{194. springer, 2012.
2. s. balasubramanian and m. gupta. structural metrics for goal based business
process design and evaluation. business process management journal , 11(6):680{
694, 2005.
3. d. grigori, f. casati, m. castellanos, u. dayal, m. sayal, and m.c. shan. business
process intelligence. computers in industry , 53(3):321{343, 2004.4. d. grigori, f. casati, u. dayal, and m.c. shan. improving business process quality
through exception understanding, prediction, and prevention. in 27th international
conference on very large databases (vldb 2001) . morgan kaufmann publishers
inc., 2001.
5. j.g. hollands and c.d. wickens. engineering psychology and human performance .
prentice hall new jersey, 1999.
6. international organization for standardization. risk management: vocabulary =
management du risque: vocabulaire (iso guide 73) . geneva, 2009.
7. a.k. jallow, b. majeed, k. vergidis, a. tiwari, and r. roy. operational risk
analysis in business processes. bt technology journal , 25(1):168{177, 2007.
8. m. jans, n. lybaert, k. vanhoof, and j.m. van der werf. a business process
mining application for internal transaction fraud mitigation. expert systems with
applications , 38(10):13351{13359, 2011.
9. m.h. jansen-vullers and h.a. reijers. business process redesign in healthcare:
towards a structured approach. quality control and applied statistics , 52(1):99,
2007.
10. ron kohavi et al. a study of cross-validation and bootstrap for accuracy estimation
and model selection. in international joint conference on articial intelligence ,
volume 14, pages 1137{1145. lawrence erlbaum associates ltd, 1995.
11. s.l. mansar and h.a. reijers. best practices in business process redesign: use and
impact. business process management journal , 13(2):193{213, 2007.
12. r. moeller. coso enterprise risk management: understanding the new integrated
erm framework , chapter 3: components of coso erm, pages 47{93. john wiley
& sons, inc., hoboken, nj, 2007.
13. j. nakatumba and w.m.p. van der aalst. analyzing resource behavior using pro-
cess mining. in business process management workshops , pages 69{80. springer,
2010.
14. a. pika, w.m.p. van der aalst, c.j. fidge, a.h.m. ter hofstede, and m.t. wynn.
predicting deadline transgressions using event logs. in m. la rosa and p. sof-
fer, editors, proceedings of the eighth international workshop on business process
intelligence (bpi12) , volume 132 of lecture notes in business information pro-
cessing , pages 211{216. springer-heidelberg, 2013.
15. standards australia and standards new zealand. risk management: principles
and guidelines (as/nzs iso 31000:2009) . sydney, nsw, wellington, nz, 3rd
edition, 2009.
16. w.m.p. van der aalst. process mining: discovery, conformance and enhancement
of business processes . springer-verlag, berlin, 2011.
17. w.m.p. van der aalst and schahram dustdar. process mining put into context.
internet computing, ieee , 16(1):82{86, 2012.
18. w.m.p. van der aalst, m.h. schonenberg, and m. song. time prediction based
on process mining. information systems , 36(2):450{475, 2011.
19. w.m.p. van der aalst, t. weijters, and l. maruster. workow mining: discov-
ering process models from event logs. knowledge and data engineering, ieee
transactions on , 16(9):1128{1142, 2004.
20. b. van dongen, r. crooy, and w.m.p. van der aalst. cycle time prediction: when
will this case nally be nished? on the move to meaningful internet systems:
otm 2008 , pages 319{336, 2008.
21. j.a. wickboldt, l.a. bianchin, r.c. lunardi, l.z. granville, l.p. gaspary, and
c. bartolini. a framework for risk assessment based on analysis of historical infor-
mation of workow execution in it systems. computer networks , 55(13):2954{2975,
2011.