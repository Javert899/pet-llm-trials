evaluating conformance measures in process mining
using conformance propositions
anja f. syring1, niek tax2, and wil m.p. van der aalst1;2;3
1process and data science (informatik 9),
rwth aachen university, d-52056 aachen, germany
2architecture of information systems,
eindhoven university of technology, eindhoven, the netherlands
3fraunhofer institute for applied information technology fit,
sankt augustin, germany
abstract. process mining sheds new light on the relationship between process
models and real-life processes. process discovery can be used to learn process
models from event logs. conformance checking is concerned with quantifying
thequality of a business process model in relation to event data that was logged
during the execution of the business process. there exist different categories of
conformance measures. recall , also called ﬁtness, is concerned with quantify-
ing how much of the behavior that was observed in the event log ﬁts the process
model. precision is concerned with quantifying how much behavior a process
model allows for that was never observed in the event log. generalization is con-
cerned with quantifying how well a process model generalizes to behavior that is
possible in the business process but was never observed in the event log. many
recall, precision, and generalization measures have been developed throughout
the years, but they are often deﬁned in an ad-hoc manner without formally deﬁn-
ing the desired properties up front. to address these problems, we formulate 21
conformance propositions and we use these propositions to evaluate current and
existing conformance measures. the goal is to trigger a discussion by clearly for-
mulating the challenges and requirements (rather than proposing new measures).
additionally, this paper serves as an overview of the conformance checking mea-
sures that are available in the process mining area.
keywords: process miningconformance checking evaluation measures
1 introduction
process mining [2] is a fast growing discipline that focuses on the analysis of event data
that is logged during the execution of a business process. events in such an event log
contain information on what was done, by whom, for whom, where, when, etc. such
event data are often readily available from information systems that support the execu-
tion of the business process, such as erp, crm, or bpm systems. process discovery ,
the task of automatically generating a process model that accurately describes a business
process based on such event data, plays a prominent role in process mining. throughout
the years, many process discovery algorithms have been developed, producing process
models in various forms, such as petri nets, process trees, and bpmn.event logs are often incomplete, i.e., they only contain a sample of all possible be-
havior in the business process. this not only makes process discovery challenging; it is
also difﬁcult to assess the quality of the process model in relation to the log. process
discovery algorithms take an event log as input and aim to output a process model that
satisﬁes certain properties, which are often referred to as the four quality dimensions [2]
of process mining: (1) recall : the discovered model should allow for the behavior seen
in the event log (avoiding “non-ﬁtting” behavior), (2) precision : the discovered model
should not allow for behavior completely unrelated to what was seen in the event log
(avoiding “underﬁtting”), (3) generalization : the discovered model should generalize
the example behavior seen in the event log (avoiding “overﬁtting”), and (4) simplicity :
the discovered model should not be unnecessarily complex. the simplicity dimension
refers to occam’s razor: “one should not increase, beyond what is necessary, the num-
ber of entities required to explain anything”. in the context of process mining, this is
often operationalized by quantifying the complexity of the model (number of nodes,
number of arcs, understandability, etc.). we do notconsider the simplicity dimension
in this paper, since we focus on behavior and abstract from the actual model repre-
sentation. recall is often referred to as ﬁtness in process mining literature. sometimes
ﬁtness refers to a combination of the four quality dimensions. to avoid later confusion,
we use the term recall which is commonly used in pattern recognition, information re-
trieval, and (binary) classiﬁcation. many conformance measures have been proposed
throughout the years, e.g., [2, 4, 6, 12–15, 24, 25, 27, 32, 33].
so far it remains an open question whether existing measures for recall, precision,
and generalization measure what they are aiming to measure. this motivates the need
for a formal framework for conformance measures. users of existing conformance mea-
sures should be aware of seemingly obvious quality issues of existing approaches and
researchers and developers that aim to create new measures should be clear on what
conformance characteristics they aim to support. to address this open question, this
paper evaluates state-of-the-art conformance measures based on 21 propositions intro-
duced in [3]. this paper supported by a detailed publicly available report detailing the
evaluations of existing techniques [29].
the remainder is organized as follows. section 2 discusses related work. section 3
introduces basic concepts and notations. the rest of the paper is split into two parts
where the ﬁrst one discusses the topics of recall and precision (section 4) and the sec-
ond part is dedicated to generalization (section 5). in both parts, we introduce the corre-
sponding conformance propositions and provide an overview of existing conformance
measures. furthermore, we discuss our ﬁndings of validating existing these measures
on the propositions. additionally, section 4 demonstrates the importance of the propo-
sitions on several baseline conformance measures, while section 5 includes a discussion
about the different points of view on generalization. section 6 concludes the paper.
2 related work
in early years, when process mining started to gain in popularity and the community
around it grew, many process discovery algorithms were developed. but at that time
there was no standard method to evaluate the results of these algorithms and to comparethem to the performance of other algorithms. based on this, rozinat et al. [28] called
on the process mining community to develop a standard framework to evaluate process
discovery algorithms. this led to a variety of ﬁtness/recall, precision, generalization
and simplicity notions [2]. these notions can be quantiﬁed in different ways and there
are often trade-offs between the different quality dimensions. as shown using generic
algorithms assigning weights to the different quality dimensions [10], one quickly gets
degenerate models when leaving out one or two dimensions. for example, it is very easy
to create a simple model with perfect recall (i.e., all observed behavior ﬁts perfectly)
that has poor precision and provides no insights.
throughout the years, several conformance measures have been developed for each
quality dimension. however, it is unclear whether these measures actually measure what
they are supposed to. an initial step to address the need for a framework to evaluate
conformance measures was made in [30]. five so-called axioms for precision measures
were deﬁned that characterize the desired properties of such measures. additionally,
[30] showed that none of the existing precision measures satisﬁed all of the formulated
axioms. in comparison to [30] janssenswillen et al. [19] did not rely on qualitative cri-
teria, but quantitatively compared existing recall, precision and generalization measures
under the aspect of feasibility, validity and sensitivity. the results showed that all recall
and precision measures tend to behave in a similar way, while generalization measures
seemed to differ greatly from each other. in [3] van der aalst made a follow-up step
to [30] by formalizing recall and generalization in addition to precision and by ex-
tending the precision requirements, resulting in a list of 21 conformance propositions.
furthermore, [3] showed the importance of probabilistic conformance measures that
also take into account trace probabilities in process models. beyond that, [30] and [3]
motivated the process mining community to develop new precision measures, taking the
axioms and propositions as a design criterion, resulting in the measures among others
the measures that are proposed in [26] and in [8]. using the 21 propositions of [3] we
evaluate state-of-the-art recall (e.g. [5, 26, 4, 16, 23, 27, 34]), precision (e.g. [4, 16, 17,
23, 13, 26, 27, 31]) and generalization (e.g. [4, 13, 16]) measures.
this paper uses the mainstream view that there are at least four quality dimensions:
ﬁtness/recall, precision, generalization, and simplicity [2]. we deliberately do not con-
sider simplicity, since we focus on behavior only (i.e., not the model representation).
moreover, we treat generalization separately. in a controlled experiment one can assume
the existence of a so-called “system model”. this model can be simulated to create a
synthetic event log used for discovery. in this setting, conformance checking can be re-
duced to measuring the similarity between the discovered model and the system model
[9, 20]. in terms of the well-known confusion matrix, one can then reason about true
positives, false positives, true negatives, and false negatives. however, without a system
model and just an event log, it is not possible to ﬁnd false positives (traces possible in
the model but not in reality). hence, precision cannot be determined in the traditional
way. janssenswillen and depaire [18] conclude in their evaluation of state-of-the-art
conformance measures that none of the existing approaches reliably measures this sim-
ilarity. however, in this paper, we follow the traditional view on the quality dimensions
and exclude the concept of the system from our work.whereas there are many ﬁtness/recall and precision measures there are fewer gen-
eralization measures. generalization deals with future cases that were not yet observed.
there is no consensus on how to deﬁne generalization and in [19] it was shown that
there is no agreement between existing generalization metrics. therefore, we cover gen-
eralization in a separate section (section 5). however, as discussed in [2] and demon-
strated through experimentation [10], one cannot leave out the generalization dimen-
sion. the model that simply enumerates all the traces in the log has perfect ﬁtness/recall
and precision. however, event logs cannot be assumed to be complete, thus proving that
a generalization dimension is needed.
3 preliminaries
amultiset over a setxis a function b:x!nwhich we write as [aw1
1;aw2
2;:::;awnn]
where for all i2[1;n]we haveai2xandwi2n.b(x)denotes the set of
all multisets over set x. for example, [a3;b;c2]is a multiset over set x=fa;b;cg
that contains three aelements, one belement and two celements.jbjis the num-
ber of elements in multiset bandb(x)denotes the number of xelements in b.
b1]b2is the sum of two multisets: (b1]b2)(x) =b1(x) +b2(x): b1nb2is
the difference containing all elements from b1that do not occur in b2. thus, (b1n
b2)(x) = maxfb1(x) b2(x);0g.b1\b2is the intersection of two multisets.
hence, (b1\b2)(x) = minfb1(x);b2(x)g.[x2bjb(x)]is the multiset of all
elements in bthat satisfy some condition b.b1b2denotes that b1is contained in
b2, e.g., [a2;b][a2;b2;c], but[a2;b3]6[a2;b2;c2]and[a2;b2;c]6[a3;b3].
process mining techniques focus on the relationship between observed behavior and
modeled behavior. therefore, we ﬁrst formalize event logs (i.e., observed behavior) and
process models (i.e., modeled behavior). to do this, we consider a very simple setting
where we only focus on the control-ﬂow, i.e., sequences of activities.
3.1 event logs
the starting point for process mining is an event log. each event in such a log refers to
anactivity possibly executed by a resource at a particular time and for a particular case.
an event may have many more attributes, e.g., transactional information, costs, cus-
tomer, location, and unit. here, we focus on control-ﬂow. therefore, we only consider
activity labels and the ordering of events within cases.
deﬁnition 1 (traces). ais the universe of activities . atracet2ais a sequence of
activities.t=ais the universe of traces.
tracet=ha;b;c;d;airefers to 5 events belonging to the same case (i.e., jtj= 5).
an event log is a collection of cases each represented by a trace.
deﬁnition 2 (event log). l=b(t)is the universe of event logs. an event logl2l
is a ﬁnite multiset of observed traces. (l) =ft2lgt is the set of traces appearing
inl2l.(l) =tn(l)is the complement of the set of non-observed traces.
event logl= [ha;b;ci5;hb;a;di3;ha;b;di2]refers to 10 cases (i.e., jlj= 10 ).
five cases are represented by the trace ha;b;ci, three cases are represented by the trace
hb;a;di, and two cases are represented by the trace ha;b;di. hence,l(ha;b;di) = 2 .a c
b d
c
da
b(a) a petri net model (with start and end transitions )
(b) a bpmn model allowing for the same behaviorm1
start
startend
endm2trace
abc
bac
abcdc
bacdcdcdcdc
(c) example logl12fig. 1: two process models m1andm2allowing for the same set of traces ( (m1) =
(m2)) with an example log l12(c).
3.2 process models
the behavior of a process model mis simply the set of traces allowed by m. in our
deﬁnition, we will abstract from the actual representation (e.g. petri nets or bpmn).
deﬁnition 3 (process model). mis the set of process models. a process model m2
mallows for a set of traces (m)t .(m) =t n(m)is the complement of the
set of traces allowed by model m2m .
a process model m2m may abstract from the real process and leave out unlikely
behavior. furthermore, this abstraction can result in (m)allowing for traces that can-
not happen (e.g., particular interleavings or loops).
we distinguish between representation andbehavior of a model. process model
m2m can be represented using a plethora of modeling languages, e.g., petri nets,
bpmn models, uml activity diagrams, automata, and process trees. here, we abstract
from the actual representation and focus on behavioral characteristics (m)t.
figure 1 (a) and (b) show two process models that have the same behavior: (m1) =
(m2) =fha;b;ci;ha;c;bi;ha;b;c;d;ci;hb;a;c;d;ci;:::g. figure 1(c) shows a pos-
sible event log generated by one of these models l12= [ha;b;ci3;hb;a;ci5;
ha;b;c;d;ci2;hb;a;c;d;c;d;c;d;c;d;c i2].
the behavior (m)of a process model m2m can be of inﬁnite size. we use
figure 1 to illustrate this. there is a “race” between aandb. afteraandb, activityc
will occur. then there is a probability that the process ends or dcan occur. let ta;k=
ha;bi(hc;di)khcibe the trace that starts with aand wheredis executedktimes.
tb;k=hb;ai(hc;di)khciis the trace that starts with band wheredis executedk
times.(m1) =(m2) =s
k0fta;k;tb;kg. some examples are given in figure 1(c).
since any log contains only a ﬁnite number of traces, one can never observe all
traces possible in m1orm2.a c
b d
start endfig. 2: a process model m3discovered based on log l3 =
[ha;b;ci5;hb;a;di3;ha;b;di2].
3.3 process discovery
a discovery algorithm takes an event log as input and returns a process model. for
example, the model m3in figure 2 could have been discovered based on event log
l3= [ha;b;ci5;hb;a;di3;ha;b;di2]. ideally, the process model should capture the
(dominant) behavior observed but it should also generalize without becoming too im-
precise. for example, the model allows for trace t=hb;a;cialthough this was never
observed.
deﬁnition 4 (discovery algorithm). a discovery algorithm can be described as a
function disc2l!m mapping event logs onto process models.
we abstract from concrete discovery algorithms. over 100 discovery algorithms
have been proposed in literature [2]. merely as a reference to explain basic notions, we
deﬁne three simple, but extreme, algorithms: disc ot,disc ut, and disc nt. letl2l
be a log. disc ot(l) =mosuch that(mo) =(l)produces an overﬁtting model that
allows only for the behavior seen in the log. disc ut(l) =musuch that(mu) =t
produces an underﬁtting model that allows for any behavior. disc nt(l) =mnsuch that
(mn) =(l)produces a non-ﬁtting model that allows for all behavior notseen in the
log.
4 recall and precision
many recall measures have been proposed in literature [2, 4, 6, 12–15, 24, 25, 27, 32,
33]. in recent years, also several precision measures have been proposed [7, 30]. only
few generalization measures have been proposed [4]. the goal of this paper is to evalu-
ate these quality measures. to achieve this, in the following the propositions introduced
in [3] are applied to existing conformance measures.
the notion of recall and precision are well established in the process mining com-
munity. deﬁnitions are in place and there is an agreement on what these two measures
are supposed to measure. however, this is not the case for generalization. there exist
different points of view on what generalization is supposed to measure. depending on
these, existing generalization measures might greatly differ from each other.
to account for the different levels of maturity in recall, precision and generaliza-
tion and to address the controversy in the generalization area, the following section
will solely handle recall and precision while section 5 focuses on generalization. bothsections establish baseline measures, introduce the corresponding propositions of [3],
present existing conformance measures and evaluate them using the propositions.
4.1 baseline recall and precision measures
we assume the existence of two functions: rec()andprec()respectively denoting recall
and precision. both take a log and model as input and return a value between 0 and 1.
the higher the value, the better.
deﬁnition 5 (recall). arecall measure rec2lm! [0;1]aims to quantify the
fraction of observed behavior that is allowed by the model.
deﬁnition 6 (precision). aprecision measure prec2lm! [0;1]aims to quantify
the fraction of behavior allowed by the model that was actually observed.
if we ignore frequencies of traces, we can simply count fractions of traces yielding
the following two simple measures.
deﬁnition 7 (trace-based l2m precision and recall). letl2l andm2m be
an event log and a process model. trace-based l2m precision and recall are deﬁned as
follows:
rectb(l;m) =j(l)\(m)j
j(l)jprectb(l;m) =j(l)\(m)j
j(m)j(1)
sincej(l)jis bounded by the size of the log, rectb(l;m)is well-deﬁned. however,
prectb(l;m)is undeﬁned when (m)is unbounded (e.g., in case of loops).
one can argue, that the frequency of traces should be taken into account when eval-
uating conformance which yields the following measure. note that it is not possible
to deﬁne frequency-based precision based on a process model that does not deﬁne the
probability of its traces. since probabilities are speciﬁcally excluded from the scope of
this paper, the following approach only deﬁnes frequency-based recall.
deﬁnition 8 (frequency-based l2m recall). letl2l andm2m be an event log
and a process model. frequency-based l2m recall is deﬁned as follows:
recfb(l;m) =j[t2ljt2(m)]j
jlj(2)
4.2 a collection of conformance propositions
in [3], 21 conformance propositions covering the different conformance dimensions
(except simplicity) were given. in this section, we focus on the general, recall and pre-
cision propositions introduced in [3]. we discuss the generalization propositions sepa-
rately, because they reason about unseen cases not yet recorded in the event log. most
of the conformance propositions have broad support from the community, i.e., there
is broad consensus that these propositions should hold. these are marked with a “ +”.
more controversial propositions are marked with a “ 0” (rather than a “ +”).general propositions the ﬁrst two propositions are commonly accepted; the com-
putation of a quality measure should be deterministic ( detpro+) and only depend on
behavioral aspects ( behpro+). the latter is a design choice. we deliberately exclude
simplicity notions.
proposition 1 (detpro+).rec(),prec(),gen()are deterministic functions, i.e., the
measures rec(l;m),prec(l;m),gen(l;m)are fully determined by l2l andm2m .
proposition 2 (behpro+).for anyl2 l andm1;m22 m such that(m1) =
(m2):rec(l;m 1) = rec(l;m 2),prec(l;m 1) = prec(l;m 2), and gen(l;m 1) =
gen(l;m 2), i.e., the measures are fully determined by the behavior observed and the
behavior described by the model (representation does not matter).
recall propositions in this subsection, we consider a few recall propositions .rec2
lm! [0;1]aims to quantify the fraction of observed behavior that is allowed by
the model. proposition recpro1+states that extending the model to allow for more
behavior can never result in a lower recall. from the deﬁnition follows, that this propo-
sition implies behpro+. recall measures violating behpro+also violate recpro1+
which is demonstrated as follows:
for two models m1;m2with(m1) =(m2)it follows from recpro1+that
rec(l;m 1)rec(l;m 2)because(m1)(m2). from recpro1+follows that
rec(l;m 2)rec(l;m 1)because(m2)(m1). combined, rec(l;m 2)rec(l;m 1)
andrec(l;m 1)rec(l;m 2)gives rec(l;m 2) = rec(l;m 1), thus, recall measures that
fulﬁll recpro1+are fully determined by the behavior observed and the behavior de-
scribed by the model, i.e., representation does not matter.
proposition 3 (recpro1+).for anyl2 l andm1;m22 m such that(m1)
(m2):rec(l;m 1)rec(l;m 2).
similarly to recpro1+, it cannot be the case that adding ﬁtting behavior to the event
logs, lowers recall ( recpro2+).
proposition 4 (recpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):rec(l1;m)rec(l2;m).
similarly to recpro2+, one can argue that adding non-ﬁtting behavior to event logs
should not be able to increase recall ( recpro30). however, one could also argue that
recall should not be measured on a trace-level, but should instead distinguish between
non-ﬁtting traces by measuring the degree in which a non-ﬁtting trace is still ﬁtting.
therefore, unlike the previous propositions, this requirement is debatable as is indicated
by the “ 0” tag.
proposition 5 (recpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):rec(l1;m)rec(l2;m).
for anyk2n:lk(t) =kl(t), e.g., ifl= [ha;bi3;hci2], thenl4= [ha;bi12;hci8].
we use this notation to enlarge event logs without changing the original distribution.
one could argue that this should not inﬂuence recall ( recpro40), e.g., rec([ha;bi3;hci2];m) = rec([ha;bi12;hci8];m). on the other hand, larger logs can provide more
conﬁdence that the log is indeed a representative sample of the possible behavior. there-
fore, it is debatable whether the size of the event log should have inﬂuence on recall as
indicated by the “ 0” tag.
proposition 6 (recpro40).for anyl2 l ,m2 m , andk1:rec(lk;m) =
rec(l;m).
finally, we provide a proposition stating that recall should be 1 if all traces in the
log ﬁt the model ( recpro5+). as a result, the empty log has recall 1 for any model.
based on this proposition, rec(l;disc ot(l)) = rec(l;disc ut(l)) = 1 for any logl.
proposition 7 (recpro5+).for anyl2 l andm2 m such that(l)(m):
rec(l;m) = 1 .
precision propositions precision ( prec2lm! [0;1]) aims to quantify the frac-
tion of behavior allowed by the model that was actually observed. initial work in the
area of checking requirements of conformance checking measures started with [30],
where ﬁve axioms for precision measures were introduced. the precision propositions
that we state below partly overlap with these axioms, but some have been added and
some have been strengthened. axiom 1 of [30] speciﬁes detpro+for the case of pre-
cision, while we have generalized it to the recall and generalization dimension. fur-
thermore, behpro+generalizes axiom 4 of [30] from its initial focus on precision to
also cover recall and generalization. precpro1+states that removing behavior from a
model that does not happen in the event log cannot lead to a lower precision. from the
deﬁnition follows, that this proposition implies behpro+. precision measures violat-
ingbehpro+also violate precpro1+. adding ﬁtting traces to the event log can also
not lower precision ( precpro2+). however, adding non-ﬁtting traces to the event log
should not change precision ( precpro30).
proposition 8 (precpro1+).for anyl2l andm1;m22m such that(m1)
(m2)and(l)\((m2)n(m1)) =;:prec(l;m 1)prec(l;m 2).
this proposition captures the same idea as axiom 2 in [30], but it is more general.
axiom 2 only put this requirement on precision when (l)(m1), while precpro1+
also concerns the situation where this does not hold.
proposition 9 (precpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):prec(l1;m)prec(l2;m).
this proposition is identical to axiom 5 in [30].
proposition 10 (precpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):prec(l1;m) =prec(l2;m).
one could also argue that duplicating the event log should not inﬂuence precision
because the distribution remains the same ( precpro40), e.g., prec([ha;bi20;hci20];m) =
prec([ha;bi40;hci40];m). similar to ( recpro30) and ( recpro40), the equivalents on
the precision side are tagged with “0”.proposition 11 (precpro40).for anyl2l,m2m , andk1:prec(lk;m) =
prec(l;m).
if the model allows for the behavior observed and nothing more, precision should be
maximal ( precpro5+). one could also argue that if all modeled behavior was observed,
precision should also be 1 ( precpro60). the latter proposition is debatable because it
implies that the non-ﬁtting behavior cannot inﬂuence perfect precision, as indicated by
the “0” tag. consider for example extreme cases where the model covers just a small
fraction of all observed behavior (or even more extreme situations like (m) =;).
according to precpro5+andprecpro60,rec(l;disc ot(l)) = 1 for any logl.
proposition 12 (precpro5+).for anyl2l andm2m such that(m) =(l):
prec(l;m) = 1 .
proposition 13 (precpro60).for anyl2l andm2m such that(m)(l):
prec(l;m) = 1 .
4.3 evaluation of baseline conformance measures
to illustrate the presented propositions and justify their formulation, we evaluate the
conformance measures deﬁned as baselines in section 4.1. note that these 3 baseline
measures were introduced to provide simple examples that can be used to discuss the
propositions. we conduct this evaluation under the assumption that l6= [ ] ,(m)6=;
andhi62(m).
general propositions. based on the deﬁnition of rectbandrecfbit is clear that all
measures can be fully determined by the log and the model. consequently, detpro+
hold for these two baseline conformance measures. however, prectbis undeﬁned
when(m)is unbound and, therefore, non-deterministic.
the behavior of the model is deﬁned as sets of traces (m), which abstracts from the
representation of the process model itself. therefore, all recall and precision baseline
conformance measures fulﬁll behpro+.
recall propositions. considering measure rectb, it is obvious that recpro1+holds
if(m1)(m2), because the intersection between (m2)and(l)will always
be equal or bigger to the intersection of (m1)and(l). the recpro2+proposi-
tion holds for rectb, ifl2=l1]l3and(l3)(m), because the additional ﬁt-
ting behavior is added to the nominator as well as the denominator of the formula:
j(l1)\(m)j+j(l3)j)=(j(l1)j+j(l3)j. this can never decrease recall. further-
more, recpro30propositions holds for rectbsince adding unﬁtting behavior cannot
increase the intersection between traces of the model and the log if l2=l1]l3and
(l3)(m). consequently, only the denominator of the formula grows, which de-
creases recall. similarly, we can show that these two proposition hold for recfb.
duplication of the event log cannot affect rectb, since it is deﬁned based on the set
of traces and not the multiset. the proposition also holds for recfbsince nominator and
denominator of the formula will grow in proportion. hence, recpro40holds for bothbaseline measures. considering rectb,recpro5+holds, since (l)\(m) =(l)if
(l)(m)and consequentlyj(l)\(m)j=j(l)j=j(l)j=j(l)j= 1. the same
conclusions can be drawn for recfb.
precision propositions. consider proposition precpro1+together with prectb. the
proposition holds, since removing behavior from the model that does not happen in the
event log will not affect the intersection between the traces of the model and the log:
(l)\(m2) =(l)\(m1)if(m1)(m2)and(l)\((m2)n(m1)) =;.
at the same time the denominator of the formula decreases, which can never decrease
precision itself. precpro2+also holds for prectb, since the ﬁtting behavior increases
the intersection between traces of the model and the log, while the denominator of
the formula stays the same. furthermore, precpro30holds for prectb, since unﬁtting
behavior cannot affect the intersection between traces of the model and the log.
duplication of the event log cannot affect prectb, since it is deﬁned based on the
set of traces and not the multiset, i.e. precpro40holds.
considering prectb,precpro5+holds, since (l)\(m) =(m)if(m) =(l)
and consequentlyj(l)\(m)j=j(m)j=j(m)j=j(m)j= 1. similarly, precpro60
holds for prectb.
4.4 existing recall measures
the previous evaluation of the simple baseline measures shows that the recall measures
fulﬁll all propositions and the baseline precision measure only violates one proposition.
however, the work presented in [30] demonstrated for precision, that most of the ex-
isting approaches violate seemingly obvious requirements. this is surprising compared
to the results of our simple baseline measure. inspired by [30], this paper takes a broad
look at existing conformance measures with respect to the previously presented propo-
sitions. in the following section, existing recall and precision measures are introduced,
before they will be evaluated in section 4.6.
causal footprint recall ( reca).van der aalst et al. [5] introduce the concept of the
footprint matrix, which captures the relations between the different activities in the log.
the technique relies on the principle that if activity ais followed by bbutbis never fol-
lowed bya, then there is a causal dependency between aandb. the log can be described
using four different relations types. in [2] it is stated that a footprint matrix can also be
derived for a process model by generating a complete event log from it. recall can be
measured by counting the mismatches between both matrices. note that this approach
assumes an event log which is complete with respect to the directly follows relations.
token replay recall ( recb).token replay measures recall by replaying the log on
the model and counting mismatches in the form of missing and remaining tokens. this
approach was proposed by rozinat and van der aalst [27]. during replay, four types of
tokens are distinguished: pthe number of produced tokens,cthe number of consumed
tokens,mthe number of missing tokens that had to be added because a transition wasnot enabled during replay and rthe number of remaining tokens that are left in the
model after replay. in the beginning, a token is produced in the initial place. similarly,
the approach ends by consuming a token from the ﬁnal place. the more missing and
remaining tokens are counted during replay the lower recall: recb=1
2(1 m
c) +
1
2(1 r
p)note that the approach assumes a relaxed sound workﬂow net, but it allows
for duplicate and silent transitions.
alignment recall ( recc).another approach to determine recall was proposed by van
der aalst et al. [4]. it calculates recall based on alignments, which detect process de-
viations by mapping the steps taken in the event log to the ones of the process model.
this map can contain three types of steps (so-called moves): synchronous moves when
event log and model agree, logmoves if the event was recorded in the event log but
should not have happened according to the model and model moves if the event should
have happened according to the model but did not in the event log. the approach uses
a function that assigns costs to log moves and model moves. this function is used to
compute the optimal alignment for each trace in the log (i.e. the alignment with the least
cost associated).
to compute recall, the total alignment cost of the log is normalized with respect to
the cost of the worst-case scenario where there are only moves in the log and in the
model but never together. note, that the approach assumes an accepting petri net with
an initial and ﬁnal state. however, it allows for duplicate and silent transitions in the
process model.
behavioral recall ( recd).goedertier et al. [16] deﬁne recall according to its deﬁ-
nition in the data mining ﬁeld using true positive (tp) and false negative (fn) coun-
ters.tp(l;m)denotes the number of true positives, i.e., the number of events in the
log that can be parsed correctly in model mby ﬁring a corresponding enabled tran-
sition.fn(l;m)denotes the number of false negatives, i.e., the number of events in
the log for which the corresponding transition that was needed to mimic the event was
not enabled and needed to be force-ﬁred. the recall measure is deﬁned as follows:
recd(l;m) =tp(l;m)
tp(l;m)+fn(l;m).
projected recall ( rece).leemans et al. [23] developed a conformance checking ap-
proach that is also able to handle big event logs. this is achieved by projecting the event
log as well as the model on all possible subsets of activities of size k. the behavior of
a projected log and projected model is translated into the minimal deterministic ﬁnite
automata (dfa)4. recall is calculated by checking the fraction of the behavior that
is allowed for by the minimal log-automaton that is also allowed for by the minimal
model-automaton for each projection and by averaging the recall over each projection.
continued parsing measure ( recf).this continued parsing measure was developed
in the context of the heuristic miner by weijters et al. [34]. it abstracts from the rep-
resentation of the process model by translating the petri net into a causal matrix. this
4every regular language has a unique minimal dfa according to the myhill–nerode theorem.matrix deﬁnes input and output expressions for each activity, which describe possible
in- and output behavior. when replaying the event log on the causal matrix, one has to
check whether the corresponding input and output expressions are activated and there-
fore enable the execution of the activity. to calculate the continued parsing measure
the number of events ein the event log, as well as the number of missing activated in-
put expressions mand remaining activated output expressions rare counted. note, that
the approach allows for silent transitions in the process model but excludes duplicate
transitions.
eigenvalue recall ( recg).polyvyanyy et al. [26] introduce a framework for the deﬁ-
nition of language quotients that guarantee several properties similar to the propositions
introduced in [3]. to illustrate this framework, they apply it in the process mining con-
text and deﬁne a recall measure. hereby they rely on the relation between the language
of a deterministic ﬁnite automaton (dfa) that describes the behavior of the model and
the language of the log. in principle, recall is deﬁned as in deﬁnition 7. however, the
measure is undeﬁned if the language of the model or the log are inﬁnite. therefore,
instead of using the cardinality of the languages and their intersection, the measure
computes their corresponding eigenvalues and sets them in relation. to compute these
eigenvalues, the languages have to be irreducible. since this is not the case for the
language of event logs, polyvyanyy et al. [26] introduce a short-circuit measure over
languages and proved that it is a deterministic measure over any arbitrary regular lan-
guage.
4.5 existing precision measures
soundness ( prech).the notion of soundness as deﬁned by greco et al. [17] states
that a model is precise if all possible enactments of the process have been observed in
the event log. therefore, it divides the number of unique traces in the log compliant
with the process model by the number of unique traces through the model. note, that
this approach assumes the process model in the shape of a workﬂow net. furthermore,
it is equivalent to the baseline precision measure prec tb.
simple behavioral appropriateness ( preci).rozinat and van der aalst [27] intro-
duce simple behavioral appropriateness to measure the precision of process models.
the approach assumes that imprecise models enable a lot of transitions during replay.
therefore, the approach computes the mean number of enabled transitions xifor each
unique trace iand puts it in relation to the visible transitions tvin the process model.
note, that the approach assumes a sound workﬂow net. however, it allows for duplicate
and silent transitions in the process model.
advanced behavioral appropriateness ( precj).in the same paper, rozinat and van
der aalst [27] deﬁne advanced behavioral appropriateness. this approach abstracts
from the process model by describing the relation between activities of both the log and
model with respect to whether these activities follow and/or precede each other. herebythey differentiate between never ,sometimes andalways precede/follow relations. to
calculate precision the set of sometimes followed relations of the log sl
fand the model
sm
fare considered, as well as their sometimes precedes relations sl
pandsm
p. the frac-
tion of sometimes follows/precedes relations of the model which are also observed by
the event log deﬁnes precision. note, that the approach assumes a sound workﬂow net.
however, it allows for duplicate and silent transitions in the process model.
etc-one/etc-rep ( preck) and etc-all ( precl).munoz-gama and carmona [25]
introduced a precision measure which constructs an automaton that reﬂects the states
of the model which are visited by the event log. for each state, it is evaluated whether
there are activities which were allowed by the process model but not observed by the
event log. these activities are added to the automaton as so-called escaping edges. since
this approach is not able to handle unﬁtting behavior, [7] and [4] extended the approach
with a preprocessing step that aligned the log to the model before the construction of the
automaton. since it is possible that traces result in multiple optimal alignments, there
are three variations of the precision measure. one can randomly pick one alignment
and construct the alignment automaton based on it (etc-one), select a representative
set of multiple alignments (etc-rep) or use all optimal alignments (etc-all). for each
variation, [4] deﬁnes an approach that assigns appropriate weights to the edges of the
automaton. precision is then computed by comparing for each state of the automaton,
the weighted number of non-escaping edges to the total number of edges.
behavioral speciﬁcity ( precm) and behavioral precision ( precn).goedertier et al.
[16] introduced a precision measure based on the concept of negative events that is de-
ﬁned based on the concept of a confusion matrix as used in the data mining ﬁeld. in
this confusion matrix, the induced negative events are considered to be the ground truth
and the process model is considered to be a prediction machine that predicts whether
an event can or cannot occur. a negative event expresses that at a certain position in
a trace, a particular event cannot occur. to induce the negative events into an event
log, the traces are split in subsequences of length k. for each event ein the trace, it is
checked whether another event encould be a negative event. therefore the approach
searches whether the set of subsequences contains a similar sequence to the one pre-
cedinge. if no matching sequence is found that contains enat the current position of e,
enis recorded as a negative event of e. to check conformance the log, that was induced
with negative events, is replayed on the model.
for both measures, the log that was induced with negative events is replayed on the
model. speciﬁcity and precision are measured according to their data mining deﬁnition
using true positive (tp), false positive (fp) and true negative (tn) counts.
goedertier et al. [16] ( prec m) deﬁned behavioral speciﬁcity precision as
prec m(l;m) =tn(l;m)
tn(l;m)+fp(l;m), i.e., the ratio of the induced negative events that
were also disallowed by m. more recently, de weerdt et al. [33] gave an inverse deﬁni-
tion, called behavioral precision ( prec n), as the ratio of behavior that is allowed by m
that does not conﬂict an induced negative event, i.e. prec n(l;m) =tp(l;m)
tp(l;m)+fp(l;m).weighted negative event precision ( preco).van den broucke et al. [31] proposed
an improvement to the approach of goedertier et al. [16], which assigns weights to
negative events. these weights indicate the conﬁdence of the negative events actually
being negative. to compute the weight, the approach takes the sequence preceding event
eand searches for the matching subsequences in the event log. all events that have never
followed such a subsequence are identiﬁed as negative events for eand their weight is
computed based on the length of the matching subsequence. to calculate precision the
enhanced log is replayed on the model, similar to the approach introduced in [33].
however, instead of increasing the counters by 1 they are increased by the weight of
the negative event. furthermore, van den broucke et al. [31] also introduced a modiﬁed
trace replay procedure which ﬁnds the best ﬁtting ﬁring sequence of transitions, taking
force ﬁring of transitions as well as paths enabled by silent transitions into account.
projected precision ( precp).along with projected recall ( rece) leemans et al. [23]
introduce projected precision. to compute precision, the approach creates a dfa which
describes the conjunction of the behavior of the model and the event log. the num-
ber of outgoing edges of dfa (mja)and the conjunctive automaton dfac (l;m;a )
are compared. precision is calculated for each subset of size kand averaged over the
number of subsets.
anti-alignment precision ( precq).van dongen et al. [13] propose a conformance
checking approach based on anti-alignments. an anti-alignment is a run of a model
which differs from all the traces in a log. the principle of the approach assumes that a
very precise model only allows for the observed traces and nothing more. if one trace
is removed from the log, it becomes the anti-alignment for the remaining log.
therefore, trace-based precision computes an anti-alignment for each trace in the
log. then the distance dbetween the anti-alignment and the trace is computed. this
is summed up for each trace and averaged over the number of traces in the log. the
more precise a model, the lower the distance. however, the anti-alignment used for
trace-based precision is limited by the length of the removed trace jj. therefore, log-
based precision uses an anti-alignment between the model and the complete log which
has a length which is much greater than the traces observed in the log. anti-alignment
precision is the weighted combination of trace-based and log-based anti-alignment pre-
cision. note, that the approach allows for duplicate and silent transitions in the process
model.
eigenvalue precision ( precr).polyvyanyy et al. [26] also deﬁne a precision measure
along with the eigenvalue recall ( recg). for precision, they rely on the relation between
the language of a deterministic ﬁnite automaton (dfa) that describes the behavior of
the model and the language of the log. to overcome the problems arising with inﬁnite
languages of the model or log, they compute their corresponding eigenvalues and set
them in relation. to compute these eigenvalues, the languages have to be irreducible.
since this is not the case for the language of event logs, polyvyanyy et al. [26] introduce
a short-circuit measure over languages and proof that it is a deterministic measure over
any arbitrary regular language.table 1: overview of the recall propositions that hold for the existing measures (under
the assumption that l6= [ ] ,(m)6=;andhi62(m)):pmeans that the proposition
holds for any log and model and means that the proposition does not always hold.
proposition name recarecbreccrecdrecerecfrecg
1 detpro+pppp
2 behpro+ppppp
3 recpro1+pppp
4 recpro2+ppppppp
5 recpro30p
6 recpro40ppppppp
7 recpro5+ppppp
ac b
d ef
fig. 3: a process model m4.
4.6 evaluation of existing recall and precision measures
several of the existing precision measures are not able to handle non-ﬁtting behavior
and remove it by aligning the log to the model. we use a baseline approach for the
alignment, which results in a deterministic event log: lis the original event log, which
is aligned in a deterministic manner. the resulting event log l0corresponds to unique
paths through the model. we use l0to evaluate the propositions.
evaluation of existing recall measures the previously presented recall measures
are evaluated using the corresponding propositions. the results of the evaluation are
displayed in table 1. to ensure the readability of this paper, only the most interesting
ﬁndings of the evaluation are addressed in the following section. for more details, we
refer to [29].
the evaluation of the causal footprint recall measure (reca) showed that it is deter-
ministic and solely relies on the behavior of the process model. however, the measure
violates several propositions such as recpro 1+,recpro 30, and recpro 5+. these vio-
lations are caused by the fact that recall records every difference between the footprint
of the log and the model. behavior that is described by the model but is not observed in
the event log has an impact on recall, although deﬁnition 5 states otherwise. to illus-
trate this, consider m4in figure 3, event log l4= [ha;b;c;d;e;fi;ha;b;d;c;e;fi]and
recpro 5+. the traces in l4perfectly ﬁt process model m4. the footprint of l4is shown
in table 2 (b). comparing it to the footprint of m4in table 2 (a) shows mismatches
althoughl4is perfectly ﬁtting. these mismatches are caused by the fact that the log
does not show all possible behavior of the model and, therefore, the footprint cannot
completely detect the parallelism of the model. consequently 10 of 36 relations of thetable 2: the causal footprints of m4(a),l4(b). mismatching relations are marked in red.
(a)
a b c d e f
a#!#!# #
b #! jj jj #
c# #jj jj !
d jj jj #!#
e#jj jj  #!
f# # # #(b)
a b c d e f
a#!# # # #
b #!! # #
c# #jj ! #
d# jj #!#
e# #  #!
f# # # # #
footprint represent mismatches: reca(l4;m4) = 1 10
36= 0:726= 1. van der aalst
mentions in [2] that checking conformance using causal footprints is only meaningful
if the log is complete in term of directly followed relations. moreover, the measure also
includes precision and generalization aspects, next to recall.
in comparison, recall based on token replay (recb) depends on the path taken
through the model. due to duplicate activities and silent transitions, multiple paths
through a model can be taken when replaying a single trace. different paths can lead
to different numbers of produced, consumed, missing and remaining tokens. therefore,
the approach is neither deterministic nor independent from the structure of the process
model and, consequently, violates recpro 1+. the continued parsing measure (recf)
builds on a similar replay principle as token-based replay and also violates detpro+.
however, the approach translates the process model into a causal matrix and is therefore
independent of its structure.
table 1 also shows that most measures violate recpro 30. this is caused by the fact,
that we deﬁne non-ﬁtting behavior in this paper on a trace level: traces either ﬁt the
model or they do not. however, the evaluated approaches measure non-ﬁtting behavior
on an event level. a trace consists of ﬁtting and non-ﬁtting events. in cases where the
log contains traces with a large number of deviating events, recall can be improved by
adding non-ﬁtting traces which contain several ﬁtting and only a few deviating events.
to illustrate this, consider token replay (recb), process model m5in figure 4, l5=
[ha;b;f;g ]andl6=l5][ha;d;e;f;gi]. the logl5is not perfectly ﬁtting and replaying
it on the model results in 6 produced and 6 consumed tokens, as well as 1 missing and
1 remaining token. recb(l5;m5) =1
2(1 1
6) +1
2(1 1
6) = 0:833. event logl6was
created by adding non-ﬁtting behavior to l5. replayingl6onm5results inp=c= 13 ,
r=m= 2andrecb(l7;m6) =1
2(1 2
13)+1
2(1 2
13) = 0:846. hence, the additional
unﬁtting trace results in proportionally more ﬁtting events than deviating ones which
improves recall: recb(l6;m6)<rec b(l7;m6).
to overcome the problems arising with the differences between trace-based and
event-based ﬁtness, one could alter the deﬁnition of recpro 30by requiring, that the
initial logl1only contains ﬁtting behavior ( (l1)(m)). however, to stay within the
scope of this paper, we decide to use the propositions as deﬁned in [3] and keep this
suggestion for future work.acb
d ef
hgfig. 4: petri net m5
table 3: overview of the precision propositions that hold for the existing measures
(under the assumption that l6= [ ] ,(m)6=;andhi 62(m)):pmeans that the
proposition holds for any log and model and means that the proposition does not
always hold.
prop. name prechpreciprecjpreckpreclprecmprecnprecoprecpprecqprecr
1 detpro+pppp
2 behpro+pppp
8precpro1+ppp
9precpro2+ppp
10 precpro30pp
11 precpro40ppppppppppp
12 precpro5+pppppppp
13 precpro60pppppppp
evaluation of existing precision measures the previously presented precision mea-
sures are evaluated using the corresponding propositions. the results of the evaluation
are displayed in table 3. to ensure the readability of this paper, only the most interest-
ing ﬁndings of the evaluation are addressed in the following section. for more details,
we refer to [29].
the evaluation showed that several measures violate the determinism detpro+
proposition. for example, the soundness measure ( prec h) solely relies on the number
of unique paths of the model j(m)jand unique traces in the log that comply with the
process modelj(l)\(m)j. hence, precision is not deﬁned if the model has inﬁnite
possible paths. additionally to detpro+, behavioral speciﬁcity ( recm) and behavioral
precision (recn) also violate behpro+. if during the replay of the trace duplicate or
silent transitions are encountered, the approach explored which of the available transi-
tions enables the next event in the trace. if no solution is found, one of the transitions
is randomly ﬁred, which can lead to different recall values for traces with the same
behavior.
table 3 shows that simple behavioral appropriateness ( prec i) violates all but one of
the propositions. one of the reason is that it relies on the average number of enabled
transitions during replay. even when the model allows for all exactly observed behavior
(and nothing more), precision is not maximal when the model is not strictly sequential.
advanced behavioral appropriateness ( prec j) overcomes these problems by relying ona
db
gc
e
f(a)
 (b)
fig. 5: petri net m6(a) and the alignment automaton describing the state space of =
ha;b;c;gi(b)
follow relations. however, it is not deterministic and depends on the structure of the
process model.
the results presented in [30] show that etc precision ( prec kandprec l), weighted
negative event precision ( prec o) and projected precision ( prec p) violate precpro1+.
additionally, all remaining measures aside from anti-alignment precision ( prec q) and
eigenvalue precision ( prec r) violate the proposition. the proposition states that remov-
ing behavior from a model that does not happen in the event log cannot lower precision.
consider, projected precision ( prec p) and a model with a length-one-loop. we remove
behavior from the model by restricting the model to only execute the looping activity
twice. this changes the dfa of the model since future behavior now depends on how
often the looping activity was executed: the dfa contains different states for each exe-
cution. if these states show a low local precision, overall precision decreases. further-
more, [30] showed that etc precision ( prec kandprec l), projected precision ( prec p)
and anti-alignment precision ( prec q) also violate precpro2+.
in general, looking at table 3 shows that all precision measures, except for sound-
ness (prec h) and eigenvalue precision ( prec r) violate precpro30, which states that
adding unﬁtting behavior to the event log should not change precision. however, for
example, all variations of the etc-measure ( prec k,prec l) align the log before con-
structing the alignment automaton. unﬁtting behavior can be aligned to a trace that
was not seen in the log before and introduce new states to the automaton. consider
process model m6, together with trace =ha;b;c;giand its alignment automaton dis-
played in figure 5. adding the unﬁtting trace ha;d;gicould result in the aligned trace
ha;d;e;giorha;d;f;gi. both aligned traces introduce new states into the alignment
automaton, alter the weights assigned to each state and, consequently, change preci-
sion. weighted negative precision ( prec o) also violates this proposition. the measure
accounts for the number of negative events that actually could ﬁre during trace replay
(fp). these false positives are caused by behavior that is shown in the model but not
observed in the log. as explained in the context of recpro 30, although the trace is not
ﬁtting when considered as a whole, certain parts of the trace can ﬁt the model. these
parts can possibly represent the previously missing behavior in the event log that leads
to the wrong classiﬁcation of negative events. adding these traces will, therefore, lead
to a decrease in false positives and changes precision. fp(l1;m)> fp (l2;m)and
tp(l1;m)
(tp(l1;m)+fp(l1;m))<tp(l2;m)
(tp(l2;m)+fp(l2;m)).table 3 shows that prec i,prec kandprec lviolate proposition precpro60, which
states that if all modeled behavior was observed, precision should be maximal and un-
ﬁtting behavior cannot effect precision. prec ionly reports maximal precision if the
model is strictly sequential and both etc measures ( prec kandprec l) can run into
problems with models containing silent or duplicate transitions.
the etc (prec k,prec l) and anti-alignment measures ( prec q) form a special
group of measures as they are unable to handle unﬁtting behavior without pre-processing
unﬁtting traces and aligning them to the process model. accordingly, we evaluate the
conformance measure based on this aligned log. the evaluation of precpro30and the
etc measure ( prec k,prec l) is an example of the alignment of the log resulting in a vi-
olation. however, there are also cases where the proposition only holds because of this
alignment. consider, for example, anti-alignment precision ( prec q) and proposition
precpro60. by deﬁnition, an anti-alignment will always ﬁt the model. consequently,
when computing the distance between the unﬁtting trace and the anti-alignment it will
never be minimal. however, after aligning the log, it exactly contains the modeled be-
havior, precision is maximal and the proposition holds.
5 generalization
generalization is a challenging concept to deﬁne, in contrast to recall and precision. as
a result, there are different viewpoints within the process mining community on what
generalization precisely means. the main reason for this is, that generalization needs to
reason about behavior that was not observed in the event log and establish its relation
to the model.
the need for a generalization dimension stems from the fact that, given a log, a
model can be ﬁtting and precise, but be overﬁtting. the algorithm that simply creates
a modelmsuch that(m) =ft2lgis useless because it is simply enumerating
the event log. consider an unknown process. assume we observe the ﬁrst four traces
l1= [ha;b;ci;hb;a;ci;ha;b;di;hb;a;di]. based on this we may construct the model
m3in figure 2 with (m3) =fha;b;ci;hb;a;ci;ha;b;di;hb;a;dig. this model al-
lows for all the traces in the event log and nothing more. however, because the real
underlying process in unknown, this model may be overﬁtting event log l1. based on
just four example traces we cannot be conﬁdent that the model m3in figure 2 will be
able to explain future behavior of the process. the next trace may as well be ha;cior
ha;b;b;ci. now assume that we observe the same process for a longer time and consider
the ﬁrst 100 traces (including the initial four): l2= [ha;b;ci25;hb;a;ci25;ha;b;di25;
hb;a;di25]. after observing 100 traces, we are more conﬁdent that model m3in fig-
ure 2 is the right model. intuitively, the probability that the next case will have a trace not
allowed bym3gets smaller. now assume that we observe the same process for an even
longer time and obtain the event log l2= [ha;b;ci53789;hb;a;ci48976;ha;b;di64543;
hb;a;di53789]. although we do not know the underlying process, intuitively, the prob-
ability that the next case will have a trace not allowed by m3is close to 0. this simple
example shows that recall and precision are not enough for conformance checking. we
need a generalization notion to address the risk of overﬁtting example data.it is difﬁcult to reason about generalization because this refers to unseen cases. van
der aalst et al. [4] was the ﬁrst to quantify generalization. in [4], each event is seen as
an observation of an activity ain some state s. suppose that state sis visitedntimes
and thatwis the number of different activities observed in this state. suppose that n
is very large and wis very small, then it is unlikely that a new event visiting this state
will correspond to an activity not seen before in this state. however, if nandware of
the same order of magnitude, then it is more likely that a new event visiting state swill
correspond to an activity not seen before in this state. this reasoning is used to provide
a generalization metric. this estimate can be derived under the bayesian assumption
that there is an unknown number of possible activities in state sand that probability
distribution over these activities follows a multinomial distribution.
it is not easy to develop an approach that accurately measures generalization. there-
fore, some authors deﬁne generalization using the notion of a “system” (i.e., a model
of the real underlying process). the system refers to the real behavior of the underlying
process that the model tries to capture. this can also include the context of the process
such as the organization or rules. for example, employees of a company might excep-
tionally be allowed to deviate from the deﬁned process model in certain situations [20].
in this view, system ﬁtness measures the fraction of the behavior of the system that is
captured by the model and system precision measures how much of the behavior of
the model is part of the system. buijs et al. [11] link this view to the traditional un-
derstanding of generalization. they state that both system ﬁtness and system precision
are difﬁcult to obtain under the assumption that the system is unknown. therefore,
state-of-the-art discovery algorithms assume that the process model discovered from an
event log does not contain behavior outside of the system. in other words, they assume
system precision to be 1. given this assumption, system ﬁtness can be seen as general-
ization [11]. janssenswillen et al. [20] agree that in this comparison between the system
and the model, especially the system ﬁtness, in fact is what deﬁnes generalization. fur-
thermore, janssenswillen and depaire [18] demonstrated the differences between the
traditional and the system-based view on conformance checking by showing that state-
of-the-art conformance measures cannot reliably assess the similarity between a process
model and the underlying system.
although capturing the unobserved behavior by assuming a model of the system is
a theoretically elegant solution, practical applicability of this solution is hindered by the
fact that is often impossible to retrieve full knowledge about the system itself. further-
more, [3] showed the importance of trace probabilities in process models. to accurately
represent reality, the system would also need to include probabilities for each of its
traces. however, to date, there is only one conformance measure that can actually sup-
port probabilistic process models [22]. this approach uses the earth movers’ distance
which measures the effort to transform the distributions of traces of the event log into
the distribution of traces of the model.
some people would argue that one should use cross-validation (e.g., k-fold check-
ing). however, this is a very different setting. cross validation aims to estimate the
quality of a discovery approach and not the quality of a given model given an event log.
of course, one could produce multiple process models using fragments of the event logand compare them. however, such forms of cross-validation evaluate the quality of the
discovery technique and are unrelated to generalization.
for these reasons, we deﬁne generalization in the traditional sense.
deﬁnition 9 (generalization). ageneralization measure gen2lm! [0;1]aims
to quantify the probability that new unseen cases will ﬁt the model.5
this deﬁnition assumes that a process generates a stream of newly executed cases.
the more traces that are ﬁtting and the more redundancy there is in the event, the more
certain one can be that the next case will have a trace that ﬁts the model. note that
we deliberately do not formalize the notion of probability, since in real-life we cannot
know the real process. also phenomena like concept drift and contextual factors make
it unrealistic to reason about probabilities in a formal sense.
based on this deﬁnition, we present a set of propositions. note that we do not claim
our set of propositions to be complete and invite other researchers who represent a
different viewpoint on generalization to contribute to the discussion.
5.1 generalization propositions
generalization ( gen2lm! [0;1]) aims to quantify the probability that new un-
seen cases will ﬁt the model. this conformance dimension is a bit different than the two
previously discussed conformance dimensions because it reasons about future unseen
cases (i.e., not yet in the event log). if the recall is good and the log is complete with
lots of repeating behavior, then future cases will most likely ﬁt the model. analogous
to recall, model extensions cannot lower generalization ( genpro1+), extending the log
with ﬁtting behavior cannot lower generalization ( genpro2+), and extending the log
with non-ﬁtting behavior cannot improve generalization ( genpro30).
proposition 14 (genpro1+).for anyl2l andm1;m22m such that(m1)
(m2):gen(l;m 1)gen(l;m 2).
similar to recall, this proposition implies behpro+. generalization measures vio-
lating behpro+also violate genpro1+.
proposition 15 (genpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):gen(l1;m)gen(l2;m).
proposition 16 (genpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):gen(l1;m)gen(l2;m).
duplicating the event log does not necessarily inﬂuence recall and precision. ac-
cording to propositions recpro40andprecpro40this should have no effect on recall
and precision. however, making the event log more redundant, should have an effect
5note that the term “probability” is used here in an informal manner. since we only have exam-
ple observations and no knowledge of the underlying (possibly changing) process, we cannot
compute such a probability. of course, unseen cases can have traces that have been observed
before.on generalization. for ﬁtting logs, adding redundancy without changing the distribution
can only improve generalization ( genpro4+). for non-ﬁtting logs, adding redundancy
without changing the distribution can only lower generalization ( genpro5+). note that
genpro4+andgenpro5+are special cases of genpro60andgenpro70.genpro60
andgenpro70consider logs where some traces are ﬁtting and others are not. for a log
where more than half of the traces is ﬁtting, duplication can only improve generaliza-
tion ( genpro60). for a log where more than half of the traces is non-ﬁtting, duplication
can only lower generalization ( genpro70).
proposition 17 (genpro4+).for anyl2l,m2m , andk1such that(l)
(m):gen(lk;m)gen(l;m).
proposition 18 (genpro5+).for anyl2l,m2m , andk1such that(l)
(m):gen(lk;m)gen(l;m).
proposition 19 (genpro60).for anyl2l,m2m , andk1such that most traces
are ﬁtting (j[t2ljt2(m)]jj[t2ljt62(m)]j):gen(lk;m)gen(l;m).
proposition 20 (genpro70).for anyl2l,m2m , andk1such that most traces
are non-ﬁtting (j[t2ljt2(m)]jj[t2ljt62(m)]j):gen(lk;m)gen(l;m).
when the model allows for any behavior, clearly the next case will also be ﬁtting
(genpro80). nevertheless, it is marked as controversial because the proposition would
also need to hold for an empty event log.
proposition 21 (genpro80).for anyl2 l andm2 m such that(m) =t:
gen(l;m) = 1 .
5.2 existing generalization measures
the following sections introduce several state-of-the-art generalization measures, be-
fore they will be evaluated using the corresponding propositions.
alignment generalization ( gens).van der aalst et al. [4] also introduce a measure
for generalization. this approach considers each occurrence of a given event eas ob-
servation of an activity in some state s. the approach is parameterized by a state m
function that maps events onto states in which they occurred. for each event ethat oc-
curred in state sthe approach counts how many different activities wwere observed in
that state. furthermore, it counts the number of visits nto this state. generalization is
high ifnis very large and wis small, since in that case, it is unlikely that a new trace
will correspond to unseen behavior in that state.
weighted negative event generalization ( gent).aside from improving the approach
of goedertier et al. [16] van den broucke et al. [31] also developed a generalization mea-
sure based on weighted negative events. it deﬁnes allowed generalizations agwhich
represent events, that could be replayed without errors and conﬁrm that the model istable 4: an overview of the generalization propositions that hold for the measures:
(assumingl6= [ ] ,(m)6=;andhi62(m)):pmeans that the proposition holds for
any log and model and means that the proposition does not always hold.
proposition name gensgentgenu
1 detpro+pp
2 behpro+p
14 genpro1+
15 genpro2+
16 genpro30
17 genpro4+ppp
18 genpro5+pp
19 genpro60ppp
20 genpro70pp
21 genpro80p
general and disallowed generalizations dg which are generalization events, that could
not be replayed correctly. if during replay a negative event eis encountered that actu-
ally was enabled the agvalue is increased by 1 weight (e). similarly, if a negative
event is not enabled the dg value is increased by 1 weight (e). the more disallowed
generalizations are encountered during log replay the lower generalization.
anti-alignment generalization ( genu).van dongen et al. [13] also introduce an
anti-alignment generalization and build on the principle that with a generalizing model,
newly seen behavior will introduce new paths between the states of the model, how-
ever no new states themselves. therefore, they deﬁne a recovery distance drecwhich
measures the maximum distance between the states visited by the log and the states
visited by the anti-alignment . a perfectly generalizing model according to van don-
gen et al. [13] has the maximum distance to the anti-alignment with minimal recovery
distance. similar to recall they deﬁne trace-based and log-based generalization. finally,
anti-alignment generalization is the weighted combination of trace-based and log-based
anti-alignment generalization.
5.3 evaluation of existing generalization measures
the previously presented generalization measures are evaluated using the correspond-
ing propositions. the results of the evaluation are displayed in table 4. to improve
the readability of this paper, only the most interesting ﬁndings of the evaluation are
addressed in the following section. for a detailed evaluation, we refer to [29].
table 4 displays that alignment based generalization ( gens) violates several propo-
sitions. generalization is not deﬁned if there are unﬁtting traces since they cannot be
mapped to states of the process model. therefore, unﬁtting event logs should be aligned
to ﬁt to the model before calculating generalization. aligning a non-ﬁtting log and du-
plicating it will result in more visits to each state visited by the log. therefore, addingnon-ﬁtting behavior increases generalization and violates the propositions genpro30,
genpro5+andgenpro70.
in comparison, weighted negative event generalization ( gent) is robust against the
duplication of the event log, even if it contains non-ﬁtting behavior. however, this mea-
sure violates detpro+,behpro+,genpro1+,genpro2+andgenpro30, which states
that extending the log with non-ﬁtting behavior cannot improve generalization. how-
ever, in this approach, negative events are assigned a weight which indicates how certain
the log is about these events being negative ones. even though the added behavior is
non-ﬁtting it might still provide evidence for certain negative events and therefore in-
crease their weight. if these events are then not enabled during log replay the value for
disallowed generalizations (dg) decreases dg(l1;m)< dg (l2;m)and generaliza-
tion improves:ag(l1;m)
ag(l1;m)+dg(l1;m)<ag(l2;m)
ag(l2;m)+dg(l2;m).
table 4 shows that anti-alignment generalization ( genu) violates several propo-
sitions. the approach considers markings of the process models as the basis for the
generalization computation which violates the behavioral proposition. furthermore, the
measure cannot handle if the model displays behavior that has not been observed in
the event log. if the unobserved model behavior and therefore also the anti-alignment
introduced a lot of new states which were not visited by the event log, the value of the re-
covery distance increases and generalization is lowered. this clashes with propositions
genpro1+andgenpro8+. finally, the approach also excludes unﬁtting behavior from
its scope. only after aligning the event log, generalization can be computed. as a result,
the measure fulﬁlls genpro5+,genpro60andgenpro70, but violates genpro30.
6 conclusion
with the process mining ﬁeld maturing and more commercial tools becoming available
[21], there is an urgent need to have a set of agreed-upon measures to determine the
quality of discovered processes models. we have revisited the 21 conformance propo-
sitions introduced in [3] and illustrated their relevance by applying them to baseline
measures. furthermore, we used the propositions to evaluate currently existing confor-
mance measures. this evaluation uncovers large differences between existing confor-
mance measures and the properties that they possess in relation to the propositions. it
is surprising that seemingly obvious requirements are not met by today’s conformance
measures. however, there are also measures that do meet all the propositions.
it is important to note that we do not consider the set of propositions to be complete.
instead, we consider them to be an initial step to start the discussion on what properties
are to be desired from conformance measures, and we encourage others to contribute to
this discussion. moreover, we motivate researchers to use the conformance propositions
as design criteria for the development of novel conformance measures.
one relevant direction of future work is in the area of conformance propositions that
have a more ﬁne-grained focus than the trace-level, i.e., that distinguish between almost
ﬁtting andcompletely non-ﬁtting behavior. another relevant area of future work is in the
direction of probabilistic conformance measures , which take into account branching
probabilities in models, and their desired properties. even though event logs provideinsights into the probability of traces, thus far all existing conformance checking tech-
niques ignore this point of view. in [1, 3], we already showed that probabilities can be
used to provide more faithful deﬁnitions of recall and precision. in [22], we moved
one step further and provide the ﬁrst stochastic conformance checking technique using
the so-called earth movers’ distance (emd). this conformance checking approach
considers the stochastic characteristics of both the event log and the process model. it
measures the effort to transform the distribution of traces of the event log into the dis-
tribution of traces of the model. this way one can overcome many of the challenges
identiﬁed in this paper.
acknowledgements we thank the alexander von humboldt (avh) stiftung for sup-
porting our research.
references
1. w.m.p. van der aalst. mediating between modeled and observed behavior: the quest for
the “right” process. in ieee international conference on research challenges in informa-
tion science (rcis 2013) , pages 31–43. ieee computing society, 2013.
2. w.m.p. van der aalst. process mining: data science in action . springer-verlag, berlin,
2016.
3. w.m.p. van der aalst. relating process models and event logs: 21 conformance propo-
sitions. in w.m.p. van der aalst, r. bergenthum, and j. carmona, editors, workshop on
algorithms & theories for the analysis of event data (ataed 2018) , pages 56–74. ceur
workshop proceedings, 2018.
4. w.m.p. van der aalst, a. adriansyah, and b. van dongen. replaying history on process
models for conformance checking and performance analysis. wires data mining and
knowledge discovery , 2(2):182–192, 2012.
5. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining: discovering
process models from event logs. ieee transactions on knowledge and data engineering ,
16(9):1128–1142, 2004.
6. a. adriansyah, b. van dongen, and w.m.p. van der aalst. conformance checking using
cost-based fitness analysis. in c.h. chi and p. johnson, editors, ieee international en-
terprise computing conference (edoc 2011) , pages 55–64. ieee computer society, 2011.
7. a. adriansyah, j. munoz-gama, j. carmona, b. f. van dongen, and w. m. p. van der aalst.
alignment based precision checking. in m. la rosa and p. soffer, editors, business process
management workshops , pages 137–149. springer, 2013.
8. a. augusto, a. armas-cervantes, r. conforti, m. dumas, m. la rosa, and d. reissner.
abstract-and-compare: a family of scalable precision measures for automated process dis-
covery. in m. weske, m. montali, i. weber, and j. vom brocke, editors, proceedings of the
international conference on business process management , pages 158–175, cham, 2018.
springer international publishing.
9. j.c.a.m. buijs. flexible evolutionary algorithms for mining structured process models . phd
thesis, department of mathematics and computer science, 2014.
10. j.c.a.m. buijs, b.f. van dongen, and w.m.p. van der aalst. on the role of fitness, pre-
cision, generalization and simplicity in process discovery. in r. meersman, s. rinderle,
p. dadam, and x. zhou, editors, otm federated conferences, 20th international confer-
ence on cooperative information systems (coopis 2012) , volume 7565 of lecture notes in
computer science , pages 305–322. springer-verlag, berlin, 2012.11. j.c.a.m. buijs, b.f. van dongen, and w.m.p. van der aalst. quality dimensions in process
discovery: the importance of fitness, precision, generalization and simplicity. interna-
tional journal of cooperative information systems , 23(1):1–39, 2014.
12. j. carmona, b. van dongen, a. solti, and m. weidlich. conformance checking: relating
processes and models . springer-verlag, berlin, 2018.
13. b.f. van dongen, j. carmona, and t. chatain. a uniﬁed approach for measuring precision
and generalization based on anti-alignments. in m. la rosa, p. loos, and o. pastor, editors,
international conference on business process management (bpm 2016) , volume 9850 of
lecture notes in computer science , pages 39–56. springer-verlag, berlin, 2016.
14. b.f. van dongen, j. carmona, t. chatain, and f. taymouri. aligning modeled and observed
behavior: a compromise between computation complexity and quality. in e. dubois and
k. pohl, editors, international conference on advanced information systems engineering
(caise 2017) , volume 10253 of lecture notes in computer science , pages 94–109. springer-
verlag, berlin, 2017.
15. l. garcia-banuelos, n. van beest, m. dumas, m. la rosa, and w. mertens. complete and
interpretable conformance checking of business processes. ieee transactions on software
engineering , 44(3):262–290, 2018.
16. s. goedertier, d. martens, j. vanthienen, and b. baesens. robust process discovery with
artiﬁcial negative events. journal of machine learning research , 10:1305–1340, 2009.
17. g. greco, a. guzzo, l. pontieri, and d. sacc `a. discovering expressive process models by
clustering log traces. ieee transaction on knowledge and data engineering , 18(8):1010–
1027, 2006.
18. g. janssenswillen and b. depaire. towards conﬁrmatory process discovery: making asser-
tions about the underlying system. business & information systems engineering , dec 2018.
19. g. janssenswillen, n. donders, t. jouck, and b. depaire. a comparative study of existing
quality measures for process discovery. information systems , 50(1):2:1–2:45, 2017.
20. g. janssenswillen, t. jouck, m. creemers, and b. depaire. measuring the quality of mod-
els with respect to the underlying system: an empirical study. in m. la rosa, p. loos,
and o. pastor, editors, business process management , pages 73–89, cham, 2016. springer
international publishing.
21. m. kerremans. gartner market guide for process mining, research note g00353970.
www.gartner.com, 2018.
22. s. leemans, a. syring, and w.m.p. van der aalst. earth movers’ stochastic conformance
checking. in t. hildebrandt, b.f. van dongen, m. r ¨oglinger, and j. mendling, editors,
business process management forum (bpm forum 2019) , volume 360 of lecture notes in
business information processing , pages 127–143. springer-verlag, berlin, 2019.
23. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. scalable process discovery and
conformance checking. software and systems modeling , 17(2):599–631, 2018.
24. f. mannhardt, m. de leoni, h.a. reijers, and w.m.p. van der aalst. balanced multi-
perspective checking of process conformance. computing , 98(4):407–437, 2016.
25. j. munoz-gama and j. carmona. a fresh look at precision in process conformance. in
r. hull, j. mendling, and s. tai, editors, business process management (bpm 2010) , volume
6336 of lecture notes in computer science , pages 211–226. springer-verlag, berlin, 2010.
26. a. polyvyanyy, a. solti, m. weidlich, c. di ciccio, and j. mendling. behavioural quotients
for precision and recall in process mining. technical report, university of melbourne, 2018.
27. a. rozinat and w.m.p. van der aalst. conformance checking of processes based on moni-
toring real behavior. information systems , 33(1):64–95, 2008.
28. a. rozinat, a.k. alves de medeiros, c.w. g ¨unther, a.j.m.m. weijters, and w.m.p. van der
aalst. the need for a process mining evaluation framework in research and practice. inm. castellanos, j. mendling, and b. weber, editors, informal proceedings of the interna-
tional workshop on business process intelligence (bpi 2007) , pages 73–78. qut, brisbane,
australia, 2007.
29. a.f. syring, n. tax, and w.m.p. van der aalst. evaluating conformance measures in pro-
cess mining using conformance propositions (extended version). corr , arxiv:1909.02393,
2019.
30. n. tax, x. lu, n. sidorova, d. fahland, and w.m.p. van der aalst. the imprecisions of
precision measures in process mining. information processing letters , 135:1–8, 2018.
31. s. k. l. m. vanden broucke, j. de weerdt, j. vanthienen, and b. baesens. determining
process model precision and generalization with weighted artiﬁcial negative events. ieee
transactions on knowledge and data engineering , 26(8):1877–1889, aug 2014.
32. j. de weerdt, m. de backer, j. vanthienen, and b. baesens. a multi-dimensional quality
assessment of state-of-the-art process discovery algorithms using real-life event logs.
information systems , 37(7):654–676, 2012.
33. j. de weerdt, m. de backer, j. vanthienen, and b. baesens. a robust f-measure for eval-
uating discovered process models. in n. chawla, i. king, and a. sperduti, editors, ieee
symposium on computational intelligence and data mining (cidm 2011) , pages 148–155,
paris, france, april 2011. ieee.
34. a.j.m.m. weijters, w.m.p. van der aalst, and a.k. alves de medeiros. process mining
with the heuristics miner-algorithm. beta working paper series, wp 166, eindhoven
university of technology, eindhoven, 2006.