decomposing replay problems: a case study
h.m.w. verbeek and w.m.p. van der aalst
department of mathematics and computer science,
eindhoven university of technology, the netherlands
{h.m.w.verbeek,w.m.p.v.d.aalst}@tue.nl
abstract. conformance checking is an important ﬁeld in the process
mining area. in brief, conformance checking provides us with insights
how well a given process model matches a given event log. to gain these
insights, we typically try to replaythe event log on the process model.
however, this replay problem may be complex and, as a result, may take
considerable time. to ease the complexity of the replay, we can decom-
posethegivenprocessmodelintoacollectionof(smaller)submodels,and
associate a (smaller) sublog to every submodel. instead of replaying the
entireeventlogontheentireprocessmodel,wecanthenreplaythecorre-
sponding sublog on every submodel, and combine the results. this paper
tests this divide-and-conquer approach on a number of process models
and event logs while using existing replay techniques. results show that
the decomposed replay may indeed be faster by orders of magnitude, but
that success is not guaranteed, as in some cases a smaller model and a
smaller log yield a more complex replay problem.
1 introduction
in the area of process mining, we typically distinguish three diﬀerent ﬁelds:
discovery ,conformance checking , andenhancement [1]. the discovery ﬁeld is
concerned with mining an event log for a process model (for the remainder
of this paper, we assume that these process models are represented using petri
nets). we are given an event log, and we try to come up with some process model
that nicely represents the observed behavior in the event log. the conformance
checking ﬁeld is concerned with checking whether an event log matches a process
model. we are given an event log and a process model, and we try to provide
insights how well these two match. the enhancement ﬁeld is concerned with
enhancing a process model using an event log. we are given an event log and
a process model, and we copy data as found in the event log into the process
model. a popular enhancement technique is to add durations to a process model
based on timestamps in the log. this allows us to detect bottlenecks in a process
model. this paper focuses on the conformance checking ﬁeld [10].
as mentioned, for conformance checking, it is vital that we can match the
event log with the process model. a popular approach for matching both is to
replay the entire event log on the process model. every trace of the event log is
replayed in the process model in such a way that mismatches are minimized insome way. of course, our aim is to match every event in the trace to a possible
action in the process model. however, this is not always possible. in some cases,
we cannot match an event to any possible action in the process model, or vice
versa, we cannot match an necessary action according to the model to an event
in the log. the result of this matching process is an alignment [4] between every
trace of the event log and the process model. a situation where the alignment
cannot match an event to any action can be considered as a situation where we
decided to skip a position in the log during replay. this is referred to as a log
move: we handle the event in the log (by skipping it) but do not reﬂect this
move in the model. vice versa, a situation where the alignment cannot match an
action to any event can be considered as a situation where we decided to skip the
action. such a situation is referred to as a model move . the remaining situation
where an event and an action are matched is referred to as a synchronous move .
by associating coststo these situations, we can obtain a best alignment by
minimizing these costs. based on this idea of associating costs, replay techniques
have been implemented in the process mining tool prom6 [3].
earlier work [5,2] has shown that we can apply a divide-and-conquer ap-
proach to these cost-based replay techniques. instead of replaying the entire
event log on the entire process model, we ﬁrst decompose the process model into
acollectionofsubmodels.second,wecreateasublogfromtheentireeventlogfor
every submodel. behavior not captured by a submodel will be ﬁltered out of the
corresponding sublog, only behavior captured by the submodel will be ﬁltered
in. third, we compute subcosts for every combination of submodel and sublog.
fourth and lasts, we replay every sublog on the corresponding submodel in such
a way that the subcosts are minimized. it has been proven that the weighted
accumulated subcosts are a lower bound for the actual overall costs [11]. more-
over, the fraction of ﬁtting cases is the same with or without decomposition [11].
as a result, the accumulated subcosts is a lower bound for the actual costs.
conceptually, the entire model can be regarded as a decomposition of itself
intoasinglesubmodel,withasinglesublog.nevertheless,properdecompositions
may exist as well. an example of a (most likely) proper decomposition is the
maximal decomposition as described in [2,11]. this maximal decomposition uses
an equivalence class on the arcs in the process model to decompose the model
into submodels. this equivalence class corresponds to the smallest relation for
which the incident arcs of places, silent transitions, and visible transitions with a
non-unique label are equivalent. as a result, the maximal decomposition results
in submodels that only have visible transitions with unique labels in common.
nevertheless, other proper decompositions of a process model may exist (like [7])
which may be of interest for the decomposed replay problem.
replaying all sublogs on the corresponding submodels has two possible ad-
vantages:
1. it may highlight problematic areas (submodels) in the model, that is, sub-
models with a bad ﬁtness. this information can hence be used for diagnostic
purposes.220 pnse’13 – petri nets and software engineering2. it may be much faster than replaying the entire log on the entire model,
while the fraction of ﬁtting cases is the same for both. obviously, this may
save time while maintaining quality.
this paper focuses on the second advantage, that is on the possible speed-up of
theentirereplaywhenusingdivide-and-conquertechniques.astheoverallmodel
is bigger than any of its submodels, it is expected that the replay problem of
the entire log is more complex than the replay problem of all sublogs. as a
result, determining the actual costs may take simply way too much time, while
determining a decent lower limit may take an acceptable amount of time. this
paper takes four diﬀerent process models with six corresponding diﬀerent event
logs, and several ways to decompose the process models into submodels, and
checks whether determining the costs of more ﬁne-grained decomposed replay
problems are indeed faster than that of more coarse-grained decomposed replay
problems. furthermore, if it is indeed faster, it checks whether the obtained costs
are still acceptable. note that a technique that return a trivial lower bound (like
0) for the costs is very fast, but is not acceptable as we do not gain any insights
by such a technique.
2 preliminaries
this section introduces the basic concepts of process models and event logs as
used in this paper. furthermore, it details how we determine the cost associated
by replaying an event log on a process model. for the remainder of this paper,
we useuto denote the universe of labels.
a process model contains a labeled petri net [9,8] (p,t,f,l ), wherepa set
of places,tis a set of transitions such that p∩t=∅,f⊆(t×p)∪(p×t)
a set of arcs, and l∈(t/negationslash→u )a partial function that maps a transition onto
its label. for function l, we use dom(l)to denote the set of transitions that are
mapped onto some label, and rng(l)to denote the set of labels that are mapped
onto by some transition. a marking mof a net (p,t,f,l )is a multiset of places,
denotedm∈b(p). the input set of a place or transition n∈p∪tis denoted
•nand corresponds to the set of all nodes that have an arc going to n, that is,
•n={n/prime|n/primefn}. in a similar way, the output set is deﬁned: n•={n/prime|nfn/prime}.
transitiont∈tisenabledby marking min netn= (p,t,f,l ), denoted as
(n,m )[t/angbracketright, if and only if mcontains a token for every place in the input set of t,
that is, if and only if m≤•t. an enabled transition t∈tmayﬁre, which results
inanewmarking m/primewhereatokenisremovedfromeveryplaceintheinputsetof
tand a token is added for every place in its output set, that is, m/prime=m−•t+t•.
we use (n,m )[t/angbracketright(n,m/prime)to denote that transition tis enabled by marking
m, and that ﬁring transition tin marking mresults in marking m/prime. letσ=
/angbracketleftt1,t2,...,t n/angbracketright∈t∗be a sequence of transitions. (n,m )[σ/angbracketright(n,m/prime)denotes that
there is a set of markings m0,m1,...,m nsuch thatm0=m,mn=m/prime, and
(n,m i)[ti/angbracketright(n,m i+1)for0≤i<n. a marking m/primeisreachable from a marking
mif there exists a σsuch that (n,m )[σ/angbracketright(n,m/prime). a transition t∈dom(l)is
calledvisible, a transition t/negationslash∈dom(l)is called invisible. an occurrence of ae. verbeek et al.: decomposing replay problems: a case study 221visible transition tcorresponds to an observable activity l(t). we can project a
transition sequence σ∈t∗onto its sequence of observable activities σv∈u∗in
a straightforward way, where all visible activities are mapped onto their labels
and all invisible transitions are ignored. we use (n,m )[σv(n,m/prime)to denote
that there exists a σ∈t∗such that (n,m )[σ/angbracketright(n,m/prime)andσis mapped onto
σv.
furthermore, a process contains an initial state and a ﬁnal state, that is a
process modelpis a triple (n,m 0,mn), wheren= (p,t,f,l )is a labeled petri
net,m0∈b(p)isitsinitialmarking,and mn∈b(p)isitsﬁnalmarking.theset
ofvisibletracesforaprocessmodel p= (n,m 0,mn),denotedφ(p),corresponds
to the set of sequences of observable activities that start in the initial marking
and end in the ﬁnal marking, that is, φ(p) ={σv|(n,m 0)[σv(n,m n)}.
an event log [1] lis a multiset of traces, where a trace is a sequence of
activities. thus, if a⊆uis the set of activities, then σ∈a∗is atraceand
l∈b(a∗)is anevent log . an event log lcan be projected onto some set of
activitiesa/prime, denotedl/harpoonuprighta/prime, in a straightforward way: all events not in a/primeare
ﬁltered out, and all events in a/primeare ﬁltered in.
thereplay problem [4] for an event log land a process model pcan now be
described as ﬁnding, for every trace σl∈l, the transition sequence σp∈t∗for
which its corresponding sequence of observable activities σp,v∈φ(p)matches
σlbest. to determine which transition ﬁts a trace best, we alignσlandσp,v
and associate coststo every misaligned transition and/or event. a misaligned
transitionmeansthatweneedtoexecuteavisibletransitionintheprocessmodel
that is not reﬂected in the event log, and is called a model move . a misaligned
event means that an activity has been executed and logged that is not reﬂected
by a corresponding visible transition in the process model, and is called a log
move. an aligned transition-event pair means that both the event log and the
process model agree on the next activity, and is called a synchronous move . by
associating costs to model moves and log moves, and by minimizing the costs,
we can determine the transition sequence from the process model that best ﬁts
a trace from the event log.
for thedecomposed replay problem for an event log land a process model p,
we ﬁrst decompose the process model into a collection of smaller process models
{p1,...,pm}. second, we map the costs for replaying the entire log on the entire
net to costs for every smaller process model pi= ((pi,ti,fi,li),m0,i,mn,i). in
earlier work [11], we have shown that these costs can be mapped in such away
that the accumulated costs for the decomposed replay problem is a lower bound
for the costs of the original replay problem. third, we ﬁlter the log for every
smaller process model into a smaller log li=l/harpoonuprightrng(li). fourth, we replay every
smaller log lion the corresponding smaller process model pi, using the adapted
costs. fifth and last, we accumulate the resulting costs into a single costs, which
is a lower bound for the actual costs.
in [11], we have shown how to decompose a process model p= ((p,t,f,l ),
m0,mn)in such a way that the decomposition is maximal. for this maximal
decomposition, we introduce an equivalence class on the arcs of f. arcs will222 pnse’13 – petri nets and software engineeringtable 1. characteristics of process models
process model transitions places arcs labels
repairexample 12 12 26 8
a32 32 32 74 32
bpic2012a 11 14 28 10
bpic2012 58 44 124 36
end up in the same submodel piif and only if they are equivalent, where this
equivalence is deﬁned as the smallest relation for which the following rules hold:
r1an incident (input or output) arc of a placeis equivalent to all incident arcs
of that place.
r2an incident arc of an invisible transition is equivalent to all incident arcs of
that transition.
r3an incident arc of a visible transition with a non-unique label (that is, there
exist other transitions that have the same label) is equivalent to all incident
arcs of all transitions with that label.
as a result of these rules, any place, any invisible transition, and any visible
transition with non-unique label will be part of a single submodel only, whereas
visible transitions with unique labels may be split over diﬀerent submodels. as
a result, only these visible transitions with unique label interface between the
diﬀerent submodels. in [11], we have proved that this decomposition preserves
perfect replay: the entire event log lcan be replayed perfectly (that is, no costs
and/or mismatches) on process model pif and only if every sublog lican be
replayed perfectly on submodel pi. moreover, a trace perfectly ﬁts the overall
model if and only if its projection ﬁts each of the submodels. hence, the fraction
of ﬁtting traces can be computed precisely using any decomposition.
3 case study setting
for the case study, we will use four diﬀerent process models ( repairexample ,
a32,bpic2012a ,andbpic2012 )withsixcorrespondingdiﬀerentlogs( repair-
example ,a32f1n00 ,a32f1n10 ,a32f1n50 ,bpic2012a , and bpic2012 ).
table 1 shows the characteristics of these models, whereas table 2 shows the
characteristics of the corresponding logs.
therepairexample model comes with a single event log, which is typically
used for demonstration (or tutorial) purposes. the a32model comes with three
event logs, which contain a varying amount of noise. the ﬁrst log, a32f1n00 ,
containsnonoise(‘ 0%’),thesecondlog, a32f1n10 ,containssomenoise(‘ 10%’),
and the third log, a32f1n50 , contains much noise (‘ 50%’). this model and these
logs were used to test genetic mining algorithms on their ability to recreate the
original model from noisy event logs. the bpic2012a model and event log stem
from the bpi 2012 challenge log and originate from [6]. note that this log ise. verbeek et al.: decomposing replay problems: a case study 223table 2. characteristics of event logs
event log cases events event classes
repairexample 1104 11,855 12
a32f1n00 1000 24,510 32
a32f1n10 1000 24,120 32
a32f1n50 1000 22,794 32
bpic2012a 13,087 60,849 10
bpic2012 13,087 262,200 36
table 3. replay results of event logs
event log running time costs
(in seconds)
repairexample 0.25 0.197
a32f1n00 11 0.000
a32f1n10 17 0.993
a32f1n50 32 4.521
bpic2012a 0.59 1.293
bpic2012 480 14.228
a real-life log, which was obtained from a company. the bpic2012a event log
is obtained by ﬁltering out all events that start with either “o” or “w”, that is,
it contains only the events that start with “a”. the corresponding model was
hand-made based on results of running the transition system miner on this
ﬁltered log (cf. [13]). the bpic2012 model and log also stem from this challenge
log. in contrast with the bpic2012a log, the bpic2012 log contains all events
from the challenge event log, and some more. these extra events allowed us to
mine this log using a passage-based technique, of which the bpic2012 model is
a result [2].
table 3 shows the results of running the replayer on the six event logs, where
running times have been rounded to the two most signiﬁcant digits (this is done
throughout the paper). for details on the replayer and other prom6 plug-ins
used in this case study, we refer to [14]. note that the absolute values for these
running times are not that important for this paper, as we only want to compare
them. in the next section, we will compare these results to the results of running
the decomposed replayer on these logs. our goal will be to check whether the
decomposed replayer returns still-acceptable costs in better times for certain
decompositions.
4 case study results
the ﬁrst decomposition we will check for the results, is a near-maximal decom-
position. in section 2, we have detailed a maximal decomposition based on an
equivalence class between arcs. in this maximal decomposition, it is possible224 pnse’13 – petri nets and software engineeringtable 4. decomposed replay results of event logs using near-maximal decomposition
event log submodels running time costs
(in seconds)
repairexample 6 (171%) 0.43 (99%) 0.196
a32f1n00 30 (12%) 1.3 (100%) 0.000
a32f1n10 30 (7%) 1.1 (45%) 0.444
a32f1n50 30 (4%) 1.2 (48%) 2.155
bpic2012a 8 (378%) 2.2 (49%) 0.629
bpic2012 12 dnf
to have a submodel ((pi,ti,fi,li),m0,i,mn,i)that subsumes some other sub-
model ((pj,tj,fj,lj),m0,j,mn,j)when it comes down to the labels, that is,
rng(li)⊆rng(lj). a typical submodel where this might happen is the submodel
that contains the source (or sink) place in case of a workﬂow net [12]. as ev-
ery label of the subsumed submodel is contained in the subsuming model, every
event of the ‘subsumed’ event log will be contained in the ‘subsuming’ event log.
as a result, we can replay (almost for free) the ‘subsumed’ log while replaying
the ‘subsuming’ log. for this reason, we add the subsumed submodel to every
subsuming submodel (there may be more subsuming submodels), and do not
replay the subsumed submodel on its own. the resulting decomposition is called
a near-maximal decomposition.
table 4 shows the results of replaying the event logs on the submodels as
obtained by the near-maximal decomposition. to enable easy comparison, ta-
ble 4 also shows the reduction percentages for both running times and costs (for
example, 0.43 seconds is 171% of 0.25 seconds). these results show that the a32
logs are replayed much faster than with the regular replay (though the costs are
signiﬁcantly lower), but that the other replays got worse, with the bpic2012 log
as most negative example. for this log, the decomposed replay simply did not
ﬁnish (“dnf”), as it eventually ran out of resources (4 gb of memory). from
these results, we conclude that decomposing a replay problem need not improve
the time required for replay.
figure 1 shows the submodel that was the bottleneck for the decomposed
replay of the bpic2012 log. this submodel contains 12invisible transitions (the
entire net contains 22, so more than half of these invisible transitions ended up in
this submodel), of which 3loop back, 5source transitions, and 5sink transitions.
as a result, the replay of the corresponding sublog on this speciﬁc submodel is
verycomplex: inanyreachable state,thereareatleast 5enabledtransitions, and
in many states there will be some invisible transitions enabled as well). it might
help the replay if we restricted the unlimited freedom of these source transitions
in some way. for this reason, we introduced a place that eﬀectively restricts the
number of tokens, say n, that may be present in the submodel. initially, this
placepcontainsntokens. any transition tin the original submodel is changed
in the following way:e. verbeek et al.: decomposing replay problems: a case study 225fig. 1.problematic submodel that results from near-maximal decomposition of
bpic2012
–ifthas more incoming arcs than outgoing arcs, that is, if |•t|>|t•|, thenx
outgoing arcs from transition tto placepare added, where x=|•t|−|t•|.
–ifthas more outgoing arcs then incoming arcs, that is, if |t•|>|•t|, thenx
incoming arcs from place pto transition tare added, where x=|t•|−|•t|.
–ifthas as many incoming arcs as outgoing arcs, that is, if |t•|=|•t|, then
nothing is added.
the exceptions to these rules are the arcs from source places and the arcs to sink
places, as these arcs will not be counted. reason for doing so is that otherwise
allntokens may end up in a sink place, which results in a dead submodel.
as a result of applying these changes, every transition in the resulting net has
as many incoming arcs as outgoing arcs (excluding source and sink places),
and hence the number of tokens in every reachable marking (again excluding
source and sink places) is constant, that is, equal to n. this amount of tokens
nis determined in such a way that the execution of a source transition can be
followed by the execution of a sink transition. therefore, we will put a single
token into the place for a source transition, and as many tokens as needed to
ﬁre (once) all other (non-source) transitions in the preset of this place. figure 2
shows the resulting submodel, where the place in the middle (the one with the
many incident arcs) is the place that restricts the otherwise unlimited behavior
of the former source transitions. given the structure of this submodel, this place
contains initially a single token, which allows for only a single thread in this
submodel during replay.
with this change made to the decomposition of the entire net (note that
only the behavior of the problematic submodel was restricted in the way as226 pnse’13 – petri nets and software engineeringfig. 2.restricted problematic submodel
described, other submodels were unaﬀected), the decomposed replay ﬁnished in
470seconds (98%), with costs of 8.722(61%). the replay of the problematic
submodel took 460seconds (costs 5.210), which explains the better part of the
replay time (please recall that for the decomposed replay we allowed 4threads
to be run simultaneously). although the decomposed replay now ﬁnished, it
ﬁnished in almost the same time as the regular replay with signiﬁcant lower
costs. furthermore, by restricting the behavior of this submodel, we cannot
claim anymore that the resulting costs are a lower bound for the actual costs.
for this reason, we tried to reduce the number of invisible transitions instead.
an invisible transition with single input and single output can be reduced
in such a way, that the behavior of the resulting model is not a restriction,
but an extension of the behavior of the original model. in general, the single
input place of the invisible transition will have input transitions and additional
(not counting this invisible transition) output transitions. likewise, its single
output place will have additional input transitions (not counting this invisible
transition) and output transitions. in general, a path from such an additional
input transition to such an additional output transition will not be possible in
the model, as the invisible transition works ‘the wrong way’. however, all other
paths are possible, like a path from an input transition to an output transition,
and a path from an input transition to an additional output transition. thus,
if we merge the single input place and the single output place, and remove the
invisible transition, than we only add behavior in the model. as a result, the
replay costs might go down (which is safe for a lower bound), but cannot go up.
figure 3 shows the resulting submodel.
withthischangemade,thedecomposedreplayﬁnishedin 190seconds(40%),
with costs of 6.676(47%). the replay of the problematic submodel took 140e. verbeek et al.: decomposing replay problems: a case study 227fig. 3.reduced problematic submodel
seconds (costs 3.163). the bottleneck submodel is now a diﬀerent submodel,
which requires a replay time of 180seconds.
obviously, the reduction of invisible transitions has led to a 60% decrease in
running time (from 480seconds to 190seconds), but also to a 53% decrease in
the computed costs (from 14.228to6.676). as we know that the decomposed
replay returns a lower bound for the costs, this decrease in costs is okay, but
perhaps we can do better by not reducing all invisible transitions. possibly, there
is some trade-oﬀ somewhere between the invisible transitions to reduce and the
decrease in computed costs. for this reason, we again focus on the reduction
of an invisible transition. clearly, if (1) the single input place of an invisible
transition has no additional output transitions and (2) the single output place of
thatinvisibletransitionhasnoadditionalinputtransitions,thenthereductionof
this invisible transitions leads to no extra behavior. in contrast, if there would be
3additional output transitions and 4additional input places, then this reduction
would lead to 3×4 = 12possible new paths. if we want to retain as much of the
original behavior in the reduced net, then we want to reduce invisible transitions
for which the product of the number of additional output transitions and the
number of additional input transitions is below some threshold. if this threshold
equals 0, then no new paths are allowed, if it is suﬃciently large (say, 100),
then any reduction is allowed. to check the eﬀect of this threshold, we have run
the decomposed replay for a number of possible thresholds. table 5 shows the
results. please note that the result of the reduction with some ﬁxed threshold
needs not be unique, the resulting net might be non-deterministic. we are aware
of this, but want to check its eﬀect anyway.
this table indicates that if we set the threshold to 32, that then all invisible
transitions will be reduced (running time approx. the same and costs the same).
furthermore, this table indicates that, to get a good lower bound for the costs228 pnse’13 – petri nets and software engineeringtable 5. eﬀects of reducing invisible transitions on running time and computed costs
threshold running time costs
(in seconds)
32 (42%) 200 (47%) 6.676
16 (58%) 280 (50%) 7.091
8 (386%) 1900 (55%) 7.849
(if possible at all), we need to spend more running time than we would need for
the original (not decomposed) replay problem: to obtain a lower bound for the
costs that is slightly above half of the actual costs, we need to spend more than
three times as much running time. as a result, we conclude that this reduction
technique has its merits (it actually ﬁnishes with costs that are reasonable for
the running time required), but that there is no signiﬁcant gain in ﬁne-tuning
this technique.
a third option to cope with the problematic submodel could be to organize
the submodels into submodels that are better suited for the replay at hand.
apparently, the replay techniques we are using have problems with certain sub-
models (many source transitions, many invisible transitions), so we should try to
avoid generating such submodels. for this reason, we propose a slightly changed
notion of equivalence, the only change being that the following rule is added:
r4thei-th incoming arc of a visible transition with unique label is equivalent
to thei-th outgoing arc of that transition, if both arcs exist.
note that only the fourth rule on visible transitions with unique labels has been
added. also note that we now assume that some order exists among the incident
arcs of such a transition, aswe linkthe i-th incoming arc tothe i-th outgoing arc,
if possible. as a result, again, the result may be non-deterministic. nevertheless,
this approach may help in suppressing submodels with many source transitions,
as the result of the fourth rule may be that a submodel with a visible sink
transitiontis merged with a submodel with a visible source transition t. we use
the term near-minimal decomposition to refer to the decomposition that results
from these adapted equivalence rules, as many submodels will be linked together
using these new rules. table 6 shows the results for this decomposition.
this table shows that we typically obtain good results (costs-wise) using
the near-minimal decomposition, but that we sometimes use slightly more time.
furthermore, this table shows that for the bpic2012a andbpic2012 event logs
the near-minimal decomposition turns out to be the minimal decomposition, as
both resulted in only a single submodel. in both cases this is caused because
every parallel construct includes invisible transitions (either as split or as join).
tobeabletosplittheseconstructs,wecanrelaxtwoofthefourequivalencerules
in such a way that we also allow the decomposition to split invisible transitions
in the same way as it splits visible transitions with unique labels:
r2’thei-th incoming arc of an invisible transition is equivalent to the i-th
outgoing arc of that transition, if both arcs exist.e. verbeek et al.: decomposing replay problems: a case study 229table 6. decomposed replay results of event logs using near-minimal decomposition
event log submodels running time costs
(in seconds)
repairexample 2 (136%) 0.31 (100%) 0.197
a32f1n00 4 (18%) 1.9 (100%) 0.000
a32f1n10 4 (13%) 2.1 (94%) 0.929
a32f1n50 4 (10%) 3.1 (96%) 4.322
bpic2012a 1 (125%) 0.74 (100%) 1.293
bpic2012 1 (101%) 480 (100%) 14.228
table 7. decomposed replay results of event logs using near-minimal (with invisible
transitions) decomposition
event log submodels running time costs
(in seconds)
bpic2012a 3 (391%) 2.3 (98%) 1.272
bpic2012 3 (83%) 400 (100%) 14.227
this relaxation maintains the property that any perfectly ﬁtting trace in the
overall model is a perfectly ﬁtting trace in the submodels (as we are still possible
to replay such a perfectly ﬁtting trace in the submodels). however, it may break
thepropertythataperfectlyﬁttinginthesubmodelsisaperfectlyﬁttingtracein
theoverallmodel,astheinvisibletransitionsontheborderofthesubmodelsmay
not agree. when replaying a trace on the submodels, such a disagreement will
not be noticed, but when replaying it on the overall model, it will be noticed,
which leads to a mismatch and extra costs. table 7 shows the results of this
decomposition for the two event logs. both computed costs are quite good, and
the running time of the bpic2012 log is better than that of the regular replay,
but the running time of the bpic2012a log is worse.
another way to split up the near-minimal decomposition a bit further is to
deﬁne a set of (visible) transitions for which the relaxed equivalence does not
hold, that is, that assume that all incident arcs are not equivalent. in this paper,
we will use the term milestone transitions for such transitions. as a result, we
then have the following changes in the rules for equivalence:
r2”thei-th incoming arc of an invisible non-milestone transition is equivalent
to thei-th outgoing arc of that transition, if both arcs exist.
r4”thei-th incoming arc of a visible non-milestone transition with unique
label is equivalent to the i-th outgoing arc of that transition, if both arcs
exist.
note that the equivalence of milestone transitions is derived from these rules.
initially, incident arcs of milestone transitions are not equivalent, but they may
become equivalent if this equivalence is the result of any combination of the four230 pnse’13 – petri nets and software engineeringrules mentioned above. as such, it is still possible that multiple incident arcs for
a milestone transition are equivalent.
as an example of this milestone decomposition, we have selected six such
milestone transitions in the bpic2012 model:
1. t14402
2. t16523
3. w_beoordelen fraude+schedule
4. w_nabellen incomplete dossiers+schedule
5. w_valideren aanvraag+complete
6. w_valideren aanvraag+start
pleasenotethattheﬁrsttwomilestonetransitionsareinvisibletransitions,which
have no counterpart in the event log, whereas the remaining four are visible.
together, the ﬁrst four transitions form a sparsest cut in the model, where the
middle two transitions link a submodel containing parallelism to a submodel
containing an invisible transitions that allows the former submodel to be started
over and over again during replay. using this decomposition, the decomposed
replay takes only 100seconds (22%) and results in a costs of 11.722(82%), which
is quite acceptable. the left submodel (the large one) took 100seconds to replay
(costs 7.102), the top-right submodel took 97seconds (costs 2.806), the middle-
right submodel took 66seconds (costs 0.951), and the bottom-right submodel
(the parallel one) took 2.8seconds to replay (costs 0.863). obviously, the fact
that we could run the decomposed replay on 4diﬀerent threads helped a lot in
reducing the running time, as the total running time would have taken not 100
but100 + 96 + 66 + 2 .8≈260seconds.
5 tool implementation
the decomposed replay has been implemented in the prom61passage package,
which depends on the pnetreplayer package for the replayer. for sake of com-
pleteness, we mention that for this case study we have used version 6.1.122 of the
former and version 6.1.160 of the latter package. the passage package contains
a number of plug-ins that are relevant for the decomposed replay.
create decomposed replay problem parameters usingthisplug-in,the
user can set the parameters which are to be used for both decomposing a
modelandalogintosubmodelsandsublogs,andthedecomposedreplaythat
may follow this decomposition. both an automated and a manual version
exist for this plug-in. the automated version takes a model (a petri net)
and generates default parameters for this model. the manual version takes
either a model with default parameters or existing parameters, and allows
the user to change these parameters through a dialog. figure 4 shows this
dialog. for additional details on the implementation, we refer to [14].
1a nightly build version of prom6 containing the required packages can be down-
loaded from http://www.promtools.org/prom6/nightly.e. verbeek et al.: decomposing replay problems: a case study 231fig. 4.dialog for setting decomposed replay parameters
select visible transitions using this list, the user can select the transi-
tions that are allowed to be present in multiple submodels. only selected
transitions can appear on the interface between diﬀerent submodels. by
default, the visible transitions will be selected.
select milestone transitions using this list, the user can select the mile-
stone transitions. when determining the equivalence classes between the
arcs, only the non-selected transitions will be taken into account. by
default, the visible transitions will be selected.
select ilp threshold using this slider, the user can inﬂuence for which
submodels the ilp-based replay will be used. this replayer will be used
forallsubmodelsforwhichthenumberoftransitionsexceedsthisthresh-
old. the non-ilp-based replayer will be used otherwise. by default, this
threshold is set to 8.
select max loss using this slider, the user can inﬂuence to what extend
singe-input-single-output invisible transitions are to be reduced. such
a transition will be reduced if the product of its number of additional
inputs and its number of additional outputs (that is, the number of new
paths) is below this threshold. by default, this threshold is set to 0.
select loss threshold using this slider, the user can inﬂuence for which
submodels single-input-single-output invisible transitions are to be re-
duced. these transitions are only reduced if the number of transitions in
the submodel exceeds this threshold. by default, this threshold is set to232 pnse’13 – petri nets and software engineering100(the maximal value), which means that these transitions will only
be reduced for very large submodels.
select thread threshold usingthisslider,theusercaninﬂuenceforwhich
submodels the behavior of the source transitions will be restricted by
adding a place. the submodel will only be restricted if the number
of transitions in the submodel exceeds this threshold. by default, this
threshold is set to 100(the maximal value), which means that this re-
stricting place will only be added for very large submodels.
select threads using this slider, the user can inﬂuence the number of
tokens in the place restricting the behavior of (otherwise) source tran-
sitions. the number of tokens equals the number set by this slider plus
the number of outgoing arcs from this place to non-source transitions.
by default, this number is set to 1.
select runs with this slider, the user can inﬂuence how many times the
regular replay and decomposed replay are to be run. the regular replay
and decomposed replay will be run as many times as indicated by this
slider.
enter label with this textbox, the user can provide a meaningful name
to the selected parameter setting. this name will be used by prom6 to
identify this parameters setting. by default, this name equals the name
of the process model preﬁxed by “parameters for ".
show replay costs using this plug-in, the user can run the regular replay
and the decomposed replay as many times as indicated by the parameters
that are required as input. additional inputs are the process model (a petri
net) and the event log. this plug-in will result in an overview that contains
the parameter settings (for sake of reference) and the results (both running
times and costs for both the regular replay and the decomposed replay,
including the running times and costs for the replay of every sublog on the
corresponding submodel).
create decomposed replay problem usingthisproblem,theusercancon-
struct a decomposed replay problem. required inputs are the parameters,
the process model (a petri net), and an event log. please note that this
plug-in only constructs the decomposed replay problem, it does not solve it
by actually replaying it. to actually do this replay, the user can select the
“visualize replay” visualization, which ﬁrst performs the replay and then
shows the results.
6 concluding remarks
in this paper, we have applied our decomposed replay techniques on six event
logs and four corresponding process models. for two out of six event logs any
decomposed replay we tried took longer than the regular replay. however, for
exactly these two event logs the regular replay takes less than a second. the four
remaining event logs all take longer than second to replay, where the actual time
needed varies from 11seconds to 480seconds. for all of these four event logs,e. verbeek et al.: decomposing replay problems: a case study 233table 8. result of (decomposed) replay for event logs that take longer than a second
to replay
event log submodels running time costs
(in seconds)
a32f1n00 1 11 0.000
4 (18%) 1.9 (100%) 0.000
a32f1n10 1 17 0.993
4 (13%) 2.1 (94%) 0.929
a32f1n50 1 32 4.521
4 (10%) 3.1 (96%) 4.322
bpic2012 1 480 14.228
4 (22%) 100 (82%) 11.722
we could achieve better running times at acceptable decreases in costs. table 8
shows the result of the decomposed replay for these event logs, and compares
these results to the results of the regular replay (where there is only a single
submodel).
although this shows that we can use decomposed replay to achieve better
running times at acceptable costs, there is an issue with the actual decomposi-
tion. especially the bpic2012 event log has been a hard nut to crack, as the
replay of this event log turns out to be very sensitive to the actual decomposi-
tion. the replay simply fails to ﬁnish if we decompose the process model and
event log in as many submodels and sublogs as possible. especially the replay of
one of the sublogs on its corresponding submodel caused the replay to not ﬁnish.
in the paper, we have shown that we can take several approaches to this
replay problem. first of all, we can restrict the possible behavior of the prob-
lematic submodel. this leads to a replay that ﬁnishes, but not to a costs that is
by deﬁnition a lower bound for the actual costs, which is what we need. second,
we can reduce the single-input single-output invisible transitions such that none
remains. as a result of this reduction, we possibly introduce new paths (new
behaviors) in the resulting submodel, but this is safe as this only lowers the
costs. we also tried to ﬁne-tune this reduction of invisible transitions, but that
did not help much: the costs did not improve more than the running time of the
replay did increase. third, we can aim for submodels that suit the actual replay
techniques we are using in some way. doing so leads to a decomposed replay
that ﬁnishes, although it might decompose the model in a single submodel.
to be able to use the decomposed replay technique eﬀectively in practice, we
need to be able to come up with a good decomposition that decreases running
times while keeping the estimated costs at an acceptable level. we have shown
that even for the hard bpic2012 log this was possible, but we haven’t shown
that we can do this for any log. for this, we need more research on what these
good decompositions are, how they look like, and how we can derive them from
a given model (possibly, given the corresponding event log). as shown, for some234 pnse’13 – petri nets and software engineeringlogs,thedefault( near-maximal )decompositionworksﬁne,butotherlogsrequire
more sophisticated decompositions.
references
1. w.m.p. van der aalst. process mining: discovery, conformance and enhancement
of business processes . springer-verlag, 2011.
2. w.m.p. van der aalst. decomposing process mining problems using passages. in
s. haddad and l. pomello, editors, applications and theory of petri nets 2012 ,
volume 7347 of lecture notes in computer science , pages 72–91. springer-verlag,
2012.
3. w.m.p. van der aalst, b.f. van dongen, c.w. günther, r.s. mans, a.k. alves
de medeiros, a. rozinat, v. rubin, m. song, h.m.w. verbeek, and a.j.m.m.
weijters. prom 4.0: comprehensive support for real process analysis. in j. kleijn
anda.yakovlev,editors, application and theory of petri nets and other models of
concurrency (icatpn 2007) , volume 4546 of lecture notes in computer science ,
pages 484–494. springer-verlag, 2007.
4. a. adriansyah, b. van dongen, and w.m.p. van der aalst. conformance checking
using cost-based fitness analysis. in c.h. chi and p. johnson, editors, ieee
international enterprise computing conference (edoc 2011) , pages 55–64. ieee
computer society, 2011.
5. j. carmona, j. cortadella, and m. kishinevsky. divide-and-conquer strategies for
process mining. in u. dayal, j. eder, j. koehler, and h. reijers, editors, business
process management (bpm 2009) , volume 5701 of lecture notes in computer
science, pages 327–343. springer-verlag, 2009.
6. b. f. van dongen. bpi challenge 2012. dataset.
http://dx.doi.org/10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f, 2012.
7. j.munoz-gama,j.carmon,andw.m.p.vanderaalst. hierarchicalconformance
checking of process models based on event logs. in atpn 2013 , lncs. springer,
2013. accepted for publication.
8. t. murata. petri nets: properties, analysis and applications. proceedings of the
ieee, 77(4):541–580, april 1989.
9. w. reisig and g. rozenberg, editors. lectures on petri nets i: basic models ,
volume 1491 of lecture notes in computer science . springer-verlag, 1998.
10. a. rozinat and w.m.p. van der aalst. conformance checking of processes based
on monitoring real behavior. information systems , 33(1):64–95, 2008.
11. w. m. p. van der aalst. decomposing petri nets for process mining: a generic
approach. technical report bpm-12-20, bpmcenter.org, 2012.
12. wilvanderaalst,aryaadriansyah,andboudewijnvandongen. replayinghistory
on process models for conformance checking and performance analysis. wiley
interdisciplinary reviews: data mining and knowledge discovery , 2(2):182–192,
2012.
13. h. m. w. verbeek. bpi challenge 2012: the transition system case. in
m. la rosa and p. soﬀer, editors, bpm 2012 workshops , volume 132 of lnbip,
pages 225–226. springer, 2013.
14. h. m. w verbeek and w. m. p. van der aalst. decomposing replay problems: a
case study. technical report bpm-13-09, bpmcenter.org, 2013.e. verbeek et al.: decomposing replay problems: a case study 235236 pnse’13 – petri nets and software engineering