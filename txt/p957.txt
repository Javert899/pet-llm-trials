divide and conquer: a tool framework for supporting
decomposed discovery in process mining
verbeek, h.m.w.; munoz gama, j.; van der aalst, w.m.p.
published in:
the computer journal
doi:
10.1093/comjnl/bxx040
published: 01/11/2017
document version
publisher’s pdf, also known as version of record (includes final page, issue and volume numbers)
please check the document version of this publication:
• a submitted manuscript is the author's version of the article upon submission and before peer-review. there can be important differences
between the submitted version and the official published version of record. people interested in the research are advised to contact the
author for the final version of the publication, or visit the doi to the publisher's website.
• the final author version and the galley proof are versions of the publication after peer review.
• the final published version features the final layout of the paper including the volume, issue and page numbers.
link to publication
citation for published version (apa):
verbeek, h. m. w., munoz gama, j., & van der aalst, w. m. p. (2017). divide and conquer: a tool framework for
supporting decomposed discovery in process mining. the computer journal, 60(11), 1649-1674. doi:
10.1093/comjnl/bxx040
general rights
copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners
and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
            • users may download and print one copy of any publication from the public portal for the purpose of private study or research.
            • you may not further distribute the material or use it for any profit-making activity or commercial gain
            • you may freely distribute the url identifying the publication in the public portal ?
take down policy
if you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately
and investigate your claim.
download date: 14. jan. 2018© the british computer society 2017. all rights reserved.
for permissions, please e-mail: journals.permissions@oup.com
advance access publication on 8 may 2017 doi:10.1093 /comjnl /bxx040
divide and conquer: a tool
framework for supporting
decomposed discovery in process
mining
h.m.w. v erbeek1*, w.m.p. van der aalst1andj. m unoz -gama2
1architecture of information systems group, department of mathematics and computer science,
eindhoven university of technology, eindhoven, the netherlands
2information systems group, department of computer science, ponti ﬁcia universidad cat ólica de
chile, santiago de chile, chile
*corresponding author: h.m.w.verbeek@tue.nl
process mining has been around for more than a decade now, and, in that period, several discovery
algorithms have been introduced that work fairly well on average-sized event logs, that is, event logsthat contain ∼50 different activities. nevertheless, these algorithms have problems dealing with big
event logs, that is, event logs that contain 200 or more different activities. for this reason, a genericapproach has been developed which allows such big problems to be decomposed into a series ofsmaller (say, average-sized or even smaller) problems. this approach offers formal guarantees forthe results obtained by it and makes existing algorithms also tractable for larger logs. as a result,discovery problems may become feasible, or may become easier to handle. this paper introduces atool framework, called divide and conquer that fully supports this generic approach and that has
been implemented in prom 6. using this novel framework, this paper demonstrates that signi ﬁcant
speed-ups can be achieved for discovery. this paper also discusses the fact that decomposition may
lead to different results, but that this may even turn out to have a positive effect.
keywords: process mining; decomposition; discovery; prom
received 15 april 2016; revised 13 february 2017; editorial decision 6 april 2017
handling editor: rada chirkova
1. introduction
the ultimate goal of process mining [1] is to gain process-
related insights based on event logs created by a wide variety
of systems. an event log then contains a sequence of events
for every case that was handled by the system. as an example,table
1shows data related to a typical event recorded for
some system, which can be interpreted as follows [ 2]:
on 1 october 2015, resource 112 has completed activity a1.
a sequence of events contained in an event log is commonly
referred to as a trace . from the data associated with the trace,
we can derive for which particular case activity a1was
completed.
typically, research done in the process mining area can be
divided into three sub ﬁelds: process discovery ,processconformance andprocess enhancement . in this paper, we will
only consider process discovery.
theﬁeld of process discovery [1] deals with discovering a
process model from an event log. example process discovery
algorithms include the alpha miner [ 3], the ilp miner [ 4]
and the inductive miner [ 5]. the alpha miner was the ﬁrst
process discovery algorithm to discover concurrency
adequately. the integer linear problem (ilp) miner basicallyconverts the discovery problem into many ilp and solves the
discovery problem by solving all these ilps. the inductive
miner is the most recent discovery algorithm of these three,which discovers block-structured [
6] models in limited time
by using a powerful divide-and-conquer approach.
as indicated, the ilp miner may use many ilps to solve
the problem at hand. the size of these ilps is mainly
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017determined by the number of different activities present in the
event log, and much less by the number of traces in the eventlog. for example, consider the 57/52/nevent log from the is
2014 data set [
7]. this log contains 2000 traces, 57 activities,
an average trace length of 52 and noise. for this event log,we have created nine increasingly smaller event logs by
repeatedly ﬁltering out the last 200 traces . figure
1shows the
typical computation times needed by the ilp miner on theselogs as implemented in the process mining framework prom
6[
8]. the ﬁgure shows that splitting the log in this way does
not really help in speeding up the ilp miner. granted, asublog containing only 200 traces requires much less time
than the overall log containing 200 traces, but if we have
to run the ilp miner on 10 such sublogs and then mergethe results,
1we do not gain much. for the same event log,
we also have created 10 smaller event logs by repeatedly
ﬁltering out ﬁve random activities .2figure 2shows the
typical computation times needed by the same ilp miner
on these logs. the ﬁgure shows that if we would be able to
split the event log into ﬁve sublogs each containing 17
activities, the ilp miner might only need 60 seconds
instead of almost 1400.
to be able to deal with big event logs containing 200 or
more different activities, ref. [ 9] has proposed a theoret-
icaldecomposition approach for process discovery. instead
of discovering a process model from the overall event log
(the monolithic approach), this d ecomposed approach ﬁrst
decomposes the event log into a number of sublogs that
each contains only a subset of the activities from the over-all event log, second it employs the discovery algorithm
on each of these sublogs resulting in as many process sub-
models, and third it merges all submodels into an overallprocess model. this decomposition approach may be sig-
niﬁcantly faster than the monolithic approach, provided
that
(1) the event log can be decomposed reasonably fast
over the sublogs,
(2) the discovery algorithm is signi ﬁcantly faster on the
sublogs, possibly by running it concurrently on thesesublogs on different machines, and
(3) the discovered submodels can be merged in a reason-
ably fast way, even though it may require solvingsome ilp to remove redundant places [
9].figure 1. the effect of different numbers of traces using the ilp
miner .
figure 2. the effect of different numbers of activities using the
ilp miner .table 1. event e1.
key value
concept:name a1
lifecycle:transition complete
org:resource 112
time:timestamp 2015-10-01t00:38:44.546
1note that as the 10 results may disagree with each other, this merge may
be (close to) impossible.
2the activities have been removed in the following batches of ﬁve:ﬁrst
ji lox,4 , , ,{ },az ap ag ad n,, ,,{ },as r aw c d,, ,,{ },an ax,,{
aka ha y,, },qyi ia a,, 2 , ,{ },ac at w h i,, , , 5{ },ab af b,, ,{
arf,},aj t v s al,,, ,{ },zi ga qa,3 , , ,{} , last ae ai p au e,, , ,{ },
which leaves the activities a vua oa mmi k,, , ,, 1 ,{ }for the ﬁnal log.1650 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017this paper introduces the divide and conquer tool frame-
work, which instantiates the theoretical decomposed discov-ery approach as introduced in [
9], and which has been
implemented in prom 6. this framework offers an easy inte-
gration of existing discovery algorithms, that is, existing algo-rithms can be decomposed in an easy way. this paper also
includes two evaluations of this framework: a ﬁrst using a
number of arti ﬁcial data sets varying in size and complexity
[
10,11,7] and a second using real-life data sets varying in
size and complexity [ 12,13]. finally, this paper discusses the
fact that using decomposed discovery may lead to differentresults, but it also shows that this may have a positive effect
as it was needed to win a contemporary process discovery
contest [
14]. results show that (i) decomposition may pro-
vide results in cases where the monolithic approach fails, (ii)
for larger cases decomposition provides results in less time,
(iii) decomposition may actually result in a model thatexplains the log at hand at-least-as-good (that is, it may at-
least-as-good classify whether or not a new trace originates
from the same system that generated the log) and (iv) thedecomposition overhead is insigni ﬁcant when using a com-
plex miner like the ilp miner.
the remainder of this paper is organized as follows. section
2introduces the concepts necessary for the other sections,
these include activity logs and accepting petri nets. note that
the remainder of this paper will use accepting petri nets as pro-cess models. section
3introduces the tool framework, which
includes (i) different heuristics to split an overall event log into
sublogs, (ii) information on how to add an existing discoveryalgorithm to the framework, (iii) an approach to merge many
discovered subnets into an overall accepting petri net and (iv)
the implementation of the framework in prom 6. section
4
introduces the two evaluations conducted with the ilp miner
and the tool framework. the ﬁrst evaluation uses arti ﬁcial logs,
whereas the second evaluation uses real-life logs. section 5
discusses the fact that decomposed discovery may lead to dif-
ferent results but also that these differences may be positive.
furthermore, this section discusses the use of the other, non-ilp miner, discovery algorithms within the framework.
section
6concludes the paper.
2. preliminaries
this section presents the key concepts informally. see [ 9] for
formalizations of these concepts.
2.1. logs
in this paper, we often consider activity logs , which are an
abstraction of the event logs as found in practice.
anactivity log is a collection of traces, where every trace
is a sequence of activity occurrences . table 2shows the
example activity log l1, which contains information on 20cases. for example, four cases followed the trace
aa a aa,,,,12458 . in total, the log contains eight activities
(aa,,18{¼ }) and 1 31 79299459+ ++´++´++
9 551 735551 5 6+++ +´++= activity occurrences.
anevent log is a collection of traces, where every trace is
a sequence of events . table 1shows a typical event from an
event log, containing the following attributes [ 2]:
concept:name the activity name of the event, in this
case the events refer to the activity known as a1.
lifecycle:transition the activity transition of the
event, in this case activity a1has been completed
(other options include starting, suspending, resumingand aborting an activity [
2]).
org:resource the resource that triggered the event, in
this case the resource which is known by number 112in the organization.
time:timestamp the date and time the event occurred,
in this case, some time on 1 october 2015.
we assume that two events never have the same attribute
values. this can be enforced by giving each event a uniqueidenti ﬁer. an activity log can be obtained from an event log
by using a so-called classi ﬁer[
2], which is a set of attribute
keys. using such a classi ﬁer an activity log is obtained by
replacing every event in the log with the combined values of
the classi ﬁer. typically [ 2], this will be the value of the con-
cept:name attribute (see, for example, table 2), or the
combined value of the concept:name andlifecycle:
transition attributes.
in the remainder of this paper, a log corresponds to an
activity log, unless it is explicitly stated that it is an event log.table 2. activity log l1in tabular form.
trace frequency
aaaaaaaaaaaaa,,,,,,,,,,,,1245624564257 1
a aaaaaaaaaaaaa
aaa,,,,,,,,,,,,,,
,,12456345643562
4571
aaaaaaaaa,,,,,,,,124563457 1
aaaaaaaaa,,,,,,,,124563458 2
aaaaa aaaa,,,,,,,,124564357 1
aaaaa,,,,12458 4
aaaaa aaaa,,,,,,,,134 5 6 4357 1
aaaaa aaaa,,,,,,,,134 5 6 4358 1
aaaaa,,,,134 58 1
aaaaaaaaaaaaa
aaaa,,,,,,,,,,,,,
,,,1425642563456
24581
aaaaa,,,,14257 3
aaaaa,,,,14258 1
aaaaa,,,,14357 1
aaaaa,,,,14358 11651 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 20172.2. petri nets
a petri net can model a process using three different types of
elements: places ,transitions and arcs. figure 3shows an
example petri net containing 10 places ( pp,,11 0{¼ }), 11 tran-
sitions ( tt,,11 1{¼}) and 24 arcs.
the dot in place p1is called a token . all tokens together
indicate the current state of the petri net, which is called a
marking . in the example, the marking contains only a single
token in place p1, denoted p1[], but it could also contain the
two tokens in place p1and three tokens in place p2, denoted
pp,12
23éëêùûú. this latter marking would be visualized by putting
two dots in place p1and three dots in place p2.
as usual [ 15], a transition tisenabled in some marking m,
denoted mt[, if all its input places (that is, places from
which there is an arc to transition t) contain tokens in marking
m. in the example petri net, only transition t1is enabled in
the example marking p1[], that is, pt11[[]. a transition enabled
in a marking mmay ﬁre, resulting in a new marking m¢,
denoted mtméë¢, where m¢equals mwhere one token is
removed from every input place of tand one token is added
to every output place of t. in the example petri net, if trans-
ition t1ﬁres at marking p1[], the new marking would be p2[],
that is, pt p11 2[[][].
aﬁring sequence is a sequence of markings and transitions
such that every transition is enabled in its predecessor markingand results in its successor marking. for example, in the example
net, the sequence
ptptp ptp p,, , , , ,, ,1 1 2 2 34 3 45 [] [] éëùûéëùûis aﬁr-
ing sequence. a transition sequence is aﬁring sequence pro-
jected onto the transitions. for example, the example ﬁring
sequence yields tt t,,123 as a transition sequence.
as usual in process mining [ 1], we extend petri nets with
labels, an initial marking and a set of ﬁnal markings, yielding
anaccepting petri net. figure 4shows an accepting petri net
n1based on the example petri net, with labels (like a1and
a8), an initial marking p1([ ]) and one ﬁnal marking p10([ ]) .
the labels are used to link transitions in the petri net to
activities in an activity log. as an example, transition t1is
linked to activity a1. transitions that are linked to activ-
ities are called visible transitions. transitions that are not
linked to activities, like transition t2,a r ec a l l e d invisible
transitions. invis ible transitions ar e visualized using a
black square.as a result of the labeling, we can obtain an activity
sequence from a transition sequence by removing all invisible
transitions while replacing every visible transition with its
label. for example, the example transition sequence tt t,,123
yields activity sequence aa,12 (because t2is invisible).
the initial marking and ﬁnal markings are included to have
a well-de ﬁned start and end, just like the traces in the log.
when replaying an activity log on a petri net, the petri netneeds to have an initial marking to start with, and ﬁnal mark-
ings to conclude whether the replay has reached a proper ﬁnal
state. in the example, a replay of some trace starts from mark-ing
p1[], and the replay will only be successful if marking
p10[] is reached.
in the remainder of this paper, a net corresponds to an
accepting petri net, unless it is explicitly stated that it is a
petri net.
2.3. discovery algorithms
adiscovery algorithm (see fig. 5) is an algorithm that takes
as an input an overall log (like l1) over some set of activities
aand that creates as output a net (like net n1) over the same
set of activities a. note that we do assume that the labeling
function of the created net is surjective (there is at least one
transition for every activity), but that we do not assume that itis injective (there may be multiple transitions labeled with the
same activity). example discovery algorithms that do result in
an injective labeling function include the alpha miner [
3],
p3 p5
p2
p4 p6p7
p8p9
p10
t2t1
t8t3
t4
t5 t7t10
t11
t6 t9p1
figure 3. a petri net.a1
a4 a5 a6a2
a3a7
a8p3 p5
p2
p4 p6p7
p8p9
p10
t2t1
t8t3
t4
t5 t7t10
t11
t6 t9p1
figure 4. an accepting petri net n1. note that transitions are
labeled and there is a well-de ﬁned start and end.
log
netdiscovery
algorithm
figure 5. conceptual view on a discovery algorithm.1652 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017the heuristics miner [ 16], the hybrid ilp miner [ 17], the ilp
miner [ 4] and the inductive miner [ 5]. example discovery
algorithms that may result in a non-injective labeling function
include the evolutionary tree miner [ 18].
3. tool framework
the goal of decomposed discovery is to apply an existing dis-
covery algorithm on a series of sublogs instead of one overall
log, where every sublog contains signi ﬁcantly less different
activities than the overall log. under the assumption that (i)
the complexity of the discovery algorithm is signi ﬁcantly
worse than linear (in the number of different activities) and(ii) the additional overhead of having to decompose the log
beforehand and merge the submodels afterwards do not spoil
the bene ﬁts, the decomposed discovery algorithm is expected
toﬁnish well before the monolithic discovery algorithm.
for this reason, the decomposed discovery algorithm ﬁrst
determines small sets of different activities that are expectedto have direct causal relations among themselves. these sets
of activities are referred to as activity clusters in the remain-
der of this paper. figure
6then shows a conceptual view on a
decomposed discovery algorithm. first, the algorithm uses
different heuristics to construct a collection of possible activ-
ity cluster sets, and selects the best activity cluster set fromthat collection. second, for every activity cluster in the
selected set, the algorithm ﬁlters the overall activity log into a
sublog. third, the algorithm discovers a subnet from the sub-
log using the provided discovery algorithm. fourth and last,
the subnets are merged into an overall net.
this section ﬁrst introduces each of these steps in detail.
second, it introduces the implementation of the decomposed
discovery algorithm in prom 6.
3.1. discover clusters
the goal of this step is to obtain an as-best-as-possible set of
small activity clusters, where the activities within a single clus-ter have direct causal relations among themselves. figure
7
shows the approach the decomposed discovery algorithm usesto achieve this. first, a matrix is discovered from the overalllog indicating for every pair of activities how strong the direct
causal relation is from the ﬁrst to the second. second, a graph
is derived from this matrix containing only the strongest rela-tions. third, an initial set of activity clusters is derived from
this graph. fourth, a set of grouped activity clusters is derived
overall log
discover
clusters
filter
sublog
discovery
algorithm
merge
subnets
overall netfor every ac-
tivity clustersublog
subnetbest
clusters
section  3.2section  3.1
section  3.3
section  3.4
figure 6. conceptual view on a decomposed discovery algorithm.overall log
select best
clustersgraphcreate
graphmatrixdiscover
matrix
grouped
clustersgroup
clustersclusterscreate
clusters
for many
heuristics
best
clusterssection  3.1.1
section  3.1.2
section  3.1.3
section  3.1.4
section  3.1.5
figure 7. conceptual view on discovering activity clusters.1653 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017from the initial set of clusters by grouping very small or very
coherent clusters together.
these four steps are executed using a collection of different
settings (different heuristics), leading to a collection of as many
cluster sets. fifth and last, the best set from the collection is
selected and returned as a result. in the remainder of this section,
we provide the necessary details for these ﬁve steps.
3.1.1. discover matrix
this step discovers a matrix (usi ng some heuristics) that con-
tains for every pair of two activi ties the estimated strength of the
direct causal relation from the ﬁrst activity to the second. we
will refer to such a matrix as a causal activity matrix .t a b l e 3
shows an example causal activity matrix m1for log l1.
the strengths in a causal activity matrix range from −1.0
(weakest) to 1.0(strongest), which should be interpreted as
follows:
a value of 1.0 indicates that it is sure that there is a
direct causal relation.
a value of 0.5 indicates that it is likely that there is a
direct causal relation.
a value of 0.0 indicates that we do not know whether
there is a direct causal relation or not.
a value of −0.5 indicates that it is likely that there is
nodirect causal relation.
a value of −1.0 indicates that it is sure that there is no
direct causal relation.
for example, based on m1, we are sure that there is a direct
causal relation from a4toa5(asmaa,1 . 0 15 5() = ), and we
are sure that there is no direct causal relation from a2toa1
(asmaa,1 . 0 121() = - ).
table 4shows an overview of the heuristics currently
implemented in the tool framework for discovering a causalactivity matrix from a log.
3.1.2. create graph
in this step, we create a graph containing the stronger direct
causal relations. this is done by removing the relations fromthe casual activity matrix that are not strong enough to satisfya preset threshold. we will refer to such graph as a causal
activity graph . figure
8shows a causal activity graph g1cre-
ated from causal activity matrix m1. the strengths of the arcs
in the causal activity graph range from 0.0 (exclusive) to 1.00(inclusive). the closer the strength is to 1.0, the moretable 3. example causal activity matrix m1for log l1.
from /to a1 a2 a3 a4 a5 a6 a7 a8
a1 −0.41 0.91 0.75 0.88 −1.00 −1.00 −1.00 −1.00
a2 −1.00 −0.79 −1.00 0.29 0.88 −1.00 −1.00 −1.00
a3 −1.00 −1.00 −0.76 0.10 0.86 −1.00 −1.00 −1.00
a4 −1.00 −0.29 −0.13 −0.86 1.00 −1.00 −1.00 −1.00
a5 −1.00 −1.00 −1.00 −1.00 −1.00 0.93 0.90 0.92
a6 −1.00 0.75 0.83 0.86 −1.00 −0.60 −1.00 −1.00
a7 −1.00 −1.00 −1.00 −1.00 −1.00 −1.00 −0.62 −1.00
a8 −1.00 −1.00 −1.00 −1.00 −1.00 −1.00 −1.00 −0.63
table 4. heuristics to discover a causal activity matrix.
heuristic description
heuristics a simple heuristic based on how often ais directly
followed by a¢inland vice versa
fuzzy a more involved heuristic that also takes second-order
effects (like how often ais directly followed by a¢
compared to how often ait directly followed by a¢)
alpha a heuristic based on the alpha miner. if the alpha
miner creates a place between the transitions labeled
with aanda¢, then this heuristics returns 1.0, otherwise
−1.0
random a heuristic that returns a random value (for testing
purposes only)
average a meta heuristic that returns the average value of the
heuristics ,fuzzy andalpha heuristics
mini a meta heuristic that returns the minimal value of the
heuristics ,fuzzy andalpha heuristics
midi a meta heuristic that returns the middle value of the
heuristics ,fuzzy andalpha heuristics
maxi a meta heuristic that returns the maximal value of the
heuristics ,fuzzy andalpha heuristics
a4a1
a6a8a7
a5
a3a2
0.82
0.750.500.75
0.710.71
1.000.670.50
0.870.830.80
figure 8. example causality graph g1for matrix m1.1654 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017conﬁdent we are that there is indeed such a direct causal rela-
tion. for example, based on g1, we are sure that there is a
causal relation from a4toa5(weight is 1.0), and there might
be a causal relation from a6toa2(weight is 0.5).
to create a causal activity graph from a causal activity
matrix m, we simply take all values from mthat exceed 0.0.
however, prior to doing this, we ﬁrst apply two transforma-
tions on m, which might affect the outcome.
theﬁrst transformation is the zero value transformation,
which takes a new zero value z 1.0, 1.0î( - ) and transforms
mtom z^using the following rule:
ma amaa
maa z
zmaa z
maa z
maa z
zmaa z
maa,1.0 if , 1.0;
,
1.0if , , 1.0 ;
0.0 if , ;
,
1.0if , 1.0, ;
1.0 if , 1.0.z^(¢)=ì
íïïïïïïïïïï
îïïïïïïïïïï(¢)=
(¢)-
-(¢)î( )
(¢)=
(¢)-
+(¢)î( - )
-( ¢)=-
clearly, this transformation has an effect on which values in
the matrix will be selected for arcs in the graph: any valueexceeding value zwill be selected, any other value will not.
as an example, causal activity graph g
1was obtained from
matrix m10 . 5^.
the second transformation is the concurrency threshold
transformation, which takes a concurrency threshold
c 0.0, 1.0î( ] and transforms mtomcusing the following
rule:
ma amaa ma a c
maa,0.5 if , , ;
,o t h e r w i s e .c(¢)=ì
íïï
îïï-( ¢)- ( ¢)<
(¢)∣∣
this transformation can be used to downplay values in the
matrix in case the relation between activities are balanced,
which may be caused because both activities can be executed
concurrently [ 1]. in case of concurrent activities, direct causal
relations are not wanted.
3.1.3. create clusters
this step creates an initial set of activity clusters from a cau-
sal activity graph. these activity clusters are created by ﬁrst
assigning an equivalence class on the arcs in the graph. for
this equivalence class, two arcs are directly equivalent if one
of the following conditions hold:
input arcs : both arcs share the same source node. as
an example, the arcs aa,12() ,aa,13() and aa,14() in
fig.8belong to the same equivalence class, as they all
have a1as source node.
output arcs : output arcs of the same target node. as
an example, the arcs aa,12() and aa,62() in fig. 8belong to the same equivalence class, as they both
have a2as target node.
as now aa,12() is equivalent to both aa,13() and aa,62() ,
aa,13() andaa,62() are equivalent as well (if xis equivalent
toyandyis equivalent to z,t h e n xis equivalent to z). in a
similar way, the arcs aa,14() ,aa,64()and aa,63() are also
equivalent to aa,12() . however, the arc aa,35() is not equiva-
lent to aa,12() , as there are no nodes equivalent to aa,12()
that have either a3as source node or a5as target node. note
that although there are equivalent arcs that have a3astarget
node, there are no arcs that have a3assource node. second, a
single cluster is created for every equivalence class. this clus-
ter contains all arcs in that equivalence class, and all nodesconnected to these arcs. as an example, the arcs
aa,12() ,
aa,13() ,aa,14() ,aa,62() ,aa,63() andaa,64()form a cluster
together with the nodes a1,a2,a3,a4anda6. likewise, the
arcs that share node a5as target node form a cluster, as do
the arcs that share a5as source node. figure 9shows the set
of activity clusters created from causal activity graph g1.
to prevent any confusion in the remaining steps, the set of
activity clusters are ordered. as a result, there will be a ﬁrst
cluster, a second cluster, etc. this ordering allows us to keeptrack of which subnet was discovered from which sublog.
3.1.4. group clusters
this step changes the initial set of activity clusters by group-
ing clusters that are strongly related to each other. as anexample, the two leftmost clusters as shown in fig.
9have
three activities in common( a2,a3anda4), whereas the other
pairs of clusters only have a single activity in common (eithera
5ora6). when having to group clusters, it is, therefore, bet-
ter to group the two leftmost clusters. figure 10shows the
resulting set of activity clusters.
a2
a3
a4a5
a1a2
a3
a4a6a5
a6a7
a8
figure 9. example activity clusters c1for graph g1.
a1a2
a3
a4a6a5
a6a7
a8a5
figure 10. example grouped activity clusters c2.1655 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017this grouping of clusters, along with the reason for doing
this, has been described in detail in [ 19]. in short, a set of
activity clusters is considered to be better if it scores better on
the weighted quality metrics as shown in table 5. each of
these metrics provides a value between 0.0 and 1.0, and usingthe provided relative weights, an end score is determined.
this step starts with the initial set of clusters and requires a
percentage of clusters as input. as long as the number of
clusters divided by the number of initial clusters exceeds the
given percentage, this step selects the best two different activ-
ity clusters to be merged, and merges them.3.1.5. select best clusters
instead of relying on a single heuristic, the decomposed dis-covery algorithm relies on three different discovery heuristics
with four different zero values each. table
6shows an over-
view of the discovery heuristics used for selecting the bestactivity clusters. for each of these combinations, the set of
activity clusters is determined. from these sets of clusters, the
best one is selected.
the reason for selecting these heuristics is that experiments
have shown that sometimes one works best, and sometimes
another. the reason for using the values −0.5, 0.0 and 0.5 as
zero values is to have some coverage of the entire space of
these parameters, that is,
1.0, 1.0(- ) . the reason for adding
the values −0.6 and 0.9 is that we have seen empirically that
for these values these heuristics often provide good results.
3.2. filter sublog
the goal of this step it so split the overall activity log into a
sublog for every activity cluster. the sublogs are ordered in
the same way as the activity clusters are ordered. as a result,
theﬁrst sublog corresponds to the ﬁrst cluster, the second
sublog to the second cluster, etc. for a given cluster, a sublog
is obtained from the log by ﬁltering in those activities that
correspond to nodes in that cluster. as an example, table 7
shows the sublogs resulting from ﬁltering log l1using the
activity clusters c1.
this ﬁltering works for most of the existing discovery
algorithms, but the alpha miner is a known exception. as an
example of this, fig. 11shows the result of running the
alpha miner on the ﬁrst sublog, that is on the log that corre-
sponds to the cluster aa aa a,,,,12346{ }. obviously, the
alpha miner is unable to properly handle the activity a4cor-
rectly, which is caused by the fact that it appears both as atable 6. collection of heuristics to select the best activity clusters
from.
heuristic zero values concurrency threshold
heuristics 0.5, 0.0, 0.5, 0.9{- } 0.005{}
fuzzy 0.6, 0.5, 0.0, 0.5{-- } 0.005{}
midi 0.5, 0.0, 0.5, 0.9{- } 0.005{}table 5. quality metrics for activity clusters. although other
metrics are possible as well, we limit ourselves here to those de ﬁned
in [19].
metric description
cohesion the causal relation strengths within every single cluster
should be maximal
coupling the causal relation strengths between every two different
clusters should be minimal
size the sizes of the clusters should be distributed evenly
overlap the overlap (common activities) between every two
different clusters should be minimal
table 7. filtered traces for activity log l1and activity clusters c1in fig. 9in tabular form.
cluster aaaaa,,,,12346{} cluster aaaa,,,2345{} cluster aaaa,,,5678{}
aaaa aaa aa,,,,,,,,124624642 aaaaaaaaa,,,,,,,,245245425 aaaaaa,,,,,565657
aaaaaaaaaaaa,,,,,,,,,,,124634643624 aaaaaaaaaaaa,,,,,,,,,,,245345435245 aaaaaaaa,,,,,,,56565657
aaaa aa,,,,,124634 aaaaaa,,,,,245345 aaaa,,,5657
aaaa aa,,,,,124634 aaaaaa,,,,,245345 aaaa,,,5658
aaaaaa,,,,,124643 aaaaaa,,,,,245435 aaaa,,,5657
aaa,,124 aaa,,245 aa,58
aaaaaa,,,,,134 6 43 aaaaaa,,,,,345435 aaaa,,,5657
aaaaaa,,,,,134 6 43 aaaaaa,,,,,345435 aaaa,,,5658
aaa,,134 aaa,,345 aa,58
aaaaaaaaaaaa,,,,,,,,,,,1 42642634624 aaaaaaaaaaaa,,,,,,,,,,,425425345245 aaaaaaaa,,,,,,,56565658
aaa,,142 aaa,,425 aa,57
aaa,,142 aaa,,425 aa,58
aaa,,143 aaa,,435 aa,57
aaa,,143 aaa,,435 aa,581656 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017ﬁnal activity (like in the trace aa a,,124 ) and in the middle
of a trace (like in the trace aa a,,142 )[9]. for the second
cluster, the fact that activity a4appears both as an initial
activity and in the middle of a trace, results in a similar
problem.
the typical work-around to overcome this problem is to
introduce an artiﬁcial start activity αand an artiﬁcial end
activity ω. these two arti ﬁcial transitions prevent that an ini-
tial or a ﬁnal activity also occurs in the middle of a trace.
table 8shows the result of adding these two arti ﬁcial activ-
ities to the ﬁrst sublog. figure 12shows the result of runningthe alpha miner on this sublog. clearly, the resulting net now
handles activity a4in a proper way.
for this reason, when ﬁltering the overall log for a sublog using
an activity cluster, we include the option to add arti ﬁcial start and
end activities to the resulting sublog. obviously, this creates theobligation to remove the transitions labeled with these activities
later on, that is, when merging the subnets into an overall net.
3.3. discovery algorithm
the goal of this step is to discover a subnet from every sub-
log by using the provided discovery algorithm. table
9shows
a list of existing discovery algorithms in prom 6 that are cur-rently supported by the framework. some of the existing dis-
covery algorithms do not discover a petri net, and require a
conversion algorithm to convert the discovered model into apetri net. although the ideas in this paper are not petri-net
speciﬁc, the framework is tailored toward petri nets to allow
for a modular approach. without this, we would need to cus-tomize things for every discovery approach.
the main problem with this step is that these existing algo-
rithms discover a petri net with an initial marking but withoutindicating explicit ﬁnal markings. recall that the replay needs
to check which traces are accepted by the model. as a result,
not only the initial marking but also the ﬁnal markings are
important. therefore, we assume that a petri net has both an
explicit initial marking and an explicit collection of ﬁnal
markings. such a petri net we call an accepting petri net.
the framework offers two solutions for this problem:
(1) some discovery algorithms do in fact discover a
petri net with an explicit initial marking and a collec-
tion of explicit ﬁnal markings . example discovery
algorithms for which this holds include the inductiveminer and the evolutionary tree miner. for such
algorithms, a wrapper is available that ﬁrstﬁnds
these initial and ﬁnal markings for the petri net at
hand, and second constructs an accepting petri net
from them.
(2) other discovery algorithms only discover a petri net
with an implicit initial marking (containing a single
token in every source place) and a collection of
implicit ﬁnal markings (where each ﬁnal marking
contains a single token in a single sink place).
example discovery algorithms for which this holds
include the alpha miner, the heuristics miner, theilp miner and the hybrid ilp miner. the alpha
miner and heuristics miner always discover a petri
net with a single source place and a single sink place,with the underlying assumption that the initial mark-
ing contains a single token in the source place and
the only ﬁnal marking contains a single token in the
sink place. the ilp miner and the hybrid ilp minera4
a6a2
a3 a1
figure 11. result of the alpha miner on the sublog obtained for
cluster aaaaa,,,,12346{} .
a4
a6a2
a3 a1 α ω
figure 12. result of the alpha miner on the ﬁrst sublog with arti-
ﬁcial start and end activities.table 8. filtered traces with arti ﬁcial start and end activities
added for the ﬁrst cluster in table 7.
cluster aaaaa,,,,12346{}
aaaa aaa aa, ,,,,,,,,,124624642aw
aaaaaaaaaaaa,,,,,,,,,,,,,124634643624aw
aaaa aa,, ,, ,,,124634aw
aaaa aa,, ,, ,,,124634aw
aaaaaa,,,,,,,124643aw
aaa,, ,,124aw
aaaaaa,,,,,,,134 6 43aw
aaaaaa,,,,,,,134 6 43aw
aaa,,,,134aw
aaaaaaaaaaaa,,,,,,,,,,,,,1 42642634624aw
aaa,,, ,142aw
aaa,,, ,142aw
aaa,,,,143aw
aaa,,,,143aw1657 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017always discover a petri net with any number of
source places and no sink places, with the underlying
assumption that the initial marking contains a tokenin every source place, and that the only ﬁnal marking
is the empty marking. for these algorithms, a differ-
ent wrapper is available that ﬁrst creates these initial
andﬁnal markings from the net at hand, and second
constructs an accepting petri net from them.
using these two wrappers, all discovery algorithms men-
tioned in table
9could be added with ease to the framework.
in case a discovery algorithm does not provide any initial andﬁnal markings (be it implicit or explicit), or in case the algo-
rithm has different implicit markings than the ones men-
tioned, then a speci ﬁc wrapper needs to be created for it. this
is allowed by the framework but it will take some effort.
as an example, fig.
13shows the resulting subnets that the
hybrid ilp miner discovered from the sublogs that areshown in table
7.
note that all these discovery algorithms are oblivious to
the fact that the sublog may contain arti ﬁcial activities. these
algorithms will just discover a petri net from the sublog that
was provided to them.
3.4. merge subnets
the goal of this step is to merge the discovered subnets into
one overall net. this merge is done in three steps: joining the
subnets, hiding all transitions labeled with arti ﬁcial activities
and reducing the net. the result after reduction will be the
accepting petri net that results from the merge.
3.4.1. join subnets
this step joins a collection of subnets into one overall net
using the following rules (cf. [ 9]):
place : every place from every subnet is copied into
the overall net.
invisible transitions : every invisible transition from
every subnet is copied into the overall net.visible transitions : for every label, a single visible
transition with that label is selected as proxy for allother transitions in all subnets with that label. only the
proxy transition is copied into the overall net.
arc: every arc from every subnet is copied into the
overall net, where a transition is replaced by its proxy
if it has a proxy.
initial marking : the initial markings of all subnets are
combined into the overall initial marking.
final markings : for every possible combination of
ﬁnal markings in the small net, an overall ﬁnal mark-
ing will be created. note that markings are multisets of
tokens that can be combined easily.
assume, for the sake of argument, that some discovery algo-
rithm has discovered the two subnets as shown in fig.
14.j o i n i n g
these two subnets results in the overall net shown in fig. 15.
note that in this step, we join allvisible transition with the
same label by selecting a proxy and by rerouting all arcs to
and from this proxy. however, this will not work in case one(or more) of the subnets contains duplicate transitions (that is,
multiple visible transitions sharing the same label). as a result
of the rules, these two transitions would be joined as well. asan example, consider the subnet as shown in fig.
16. in this
net, the visible transitions t4andt6share label a6. obviously,
joining these transitions is not desired: t4andt6cannot both
occur at the same time, so merging them leads to a deadlock.
as a result, before joining the subnets, we need to make sure
that every subnet does not have duplicate transitions.
figure 17shows the solution used to solve this problem: in
every subnet, if duplicate transitions exist, then the construct
as shown in this ﬁgure is applied. every ﬁring of transitions
t4and t6in the subnet is now replaced by the transitiontable 9. discovery algorithms in prom 6[ 8] supported by the framework. the conversion plug-ins listed are necessary to convert a native
result (like a heuristics net or a process tree) into a petri net.
discovery algorithm discovery plug-in conversion plug-in
alpha miner [3] ‘alpha miner ’
heuristics miner [16] ‘mine for a heuristics net using heuristics miner ’‘ convert heuristics net into petri net ’
hybrid ilp miner [17] ‘ilp-based process discovery ’
ilp miner [4] ‘ilp miner ’
inductive miner [5] ‘mine petri net with inductive miner, with parameters ’
evolutionary tree miner [18] ‘mine a process tree with etmd using parameters and classi ﬁer’‘ convert process tree to petri net ’
a1
a4 a6a2
a3
a4a5a2
a3 a5a7
a8
a6
figure 13. result of the hybrid ilp miner on all sublogs from
table 7.1658 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017sequences tt t,,ia o
46 4áñ and tt t,,iao
66 6áñ and vice versa. the
places pa
4andpa
6guarantee that any ﬁring of any transition
labeled with a6gets routed into the right direction. as an
example, transition to
4can only ﬁre ifti
4hasﬁred before. as a
result, we obtain an adapted subnet that has similar behaviorbut which does not contain duplicate transitions.
to avoid joining duplicate transitions in a single subnet,
before joining all subnets, all duplicate transition in a singlesubnet are removed ﬁrst by adapting every subnet.3.4.2. hide transitions labeled with arti ﬁcial activities
this step removes the labels of the arti ﬁcial activities that
may have been inserted into the sublogs in an earlier step. toremove these labels, the corresponding transitions are simply
made invisible. as an example, fig.
18shows the result of
performing this step on the net as shown in fig. 15.
3.4.3. reduce net
this step reduces the size of the overall petri net by applying
variants on classical behavior-preserving reduction rules [ 20]
and by removing places that are structurally redundant [ 21].
the classical behavior-preserving reduction rules had to be
adapted to take initial markings and visible transitions intoaccount. note that we only reduce invisible transitions and
need to keep track of initial and ﬁnal markings.
as a result of applying a reduction rule, the initial marking of
the overall net may need to be u pdated. consider, for example,
the silent transition in fig.
18that corresponds to the transition
labeled αin fig. 15. this transition and its input places can be
removed from the net, but then the tokens from the initial mark-
ing need to be moved from the input places to the output places.
otherwise, the initial marking would get lost.
no reduction step should remove a visible transition. only
invisible transitions and places may be reduced by these rules,
but all visible transition should remain. consider, for example,the transition labeled a
2in fig. 15. this transition has the samea1
a4 a6a2
a3
a5ω α
a5
a6a7
a8α ω
figure 14. possible nets resulting from discovery algorithm.
cluster {a1,a2,a3,a4,a5,a6}
cluster {a5,a6,a7,a8}a1
a4a2
a3
a5α
a6a7
a8ω
figure 15. net that result from joining the subnets as shown in
fig.14.
a5
a6a7
a6α ω
p1t1p2t3
p3t5
t4 t6p4t7p5
figure 16. possible discovered net that contains two duplicate
transitions labeled a6.a5
a6a7
α ω
p1t1p2t3
p3t5
ta
6p4t7p5
pi
6po
6ti
4pa
4to4ti
6pa
6to6
figure 17. similar net that contains only one visible transition
labeled a6.
a1
a4a2
a3
a5
a6a7
a8
figure 18. net from fig. 15with arti ﬁcial labels made invisible.1659 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017input places and the same output places as the transition labeled
a3. as a result, the so-called fusion of parallel transitions
reduction rule [ 20] could remove one of these activities. clearly,
this is not desired, as these transitions are there to explain the
behavior as found in the log. removing them now would defeatthe purpose of the process d iscovery from event data.
as an example, fig.
19shows the result of performing this
step on the net as shown in fig. 18. in this example, the reduc-
tion was able to remove all invisible transitions. however, in
general, this might not be the case (e.g. skip transitions).
3.5. implementation
the decomposed discovery algorithm has been implemented
as the discover using decomposition action in the publicly
available decomposedminer package of prom 6. figure 20
shows the dialog for this action, which allows the user to
select the conﬁguration , the classi ﬁer (see section 2.1), and
the discovery algorithm (see section 3.3).
a con ﬁguration of the algorithm determines prede ﬁned
values for the settings in the algorithm. the following con ﬁg-
urations can be selected:
decompose : this con ﬁguration uses all steps (as
described before) with defaul t values. for discovering a
matrix (see section 3.1.1), the aforementioned 12 con ﬁg-
urations (see table 6) are used. for creating a graph (see
section 3.1.2), the zero value is set to 0.0 and the concur-
rency threshold to 0.005. for creating initial clusters (see
section 3.1.3), no parameters are required. for ﬁltering
the overall log (see section 3.2), empty traces are not
removed, and arti ﬁcial start and end activities (called
‘start>’and‘[end]’) are added only in case the alpha
miner is selected as discovery algorithm. for discovering
the subnets from the sublogs (see section 3.3), the
selected discovery algorithm is used. merging the sub-nets into an overall net (see section
3.4)ﬁrst removes
the structural redundant places and then reduces the
result using the improved classical reduction rules.
decompose 75 %: this con ﬁguration is identical to the
decompose conﬁguration, except that the clusters are
now grouped to 75% of the original number of clusters(see section
3.1.5). as an example, if the best initialclusters contained 20 clusters, then these clusters would
be grouped into 15 clusters by this con ﬁguration.
decompose 50 %: this default conﬁguration is identi-
cal to the decompose conﬁguration, except that the
clusters are now grouped to 50% of the original num-ber of clusters. this results in 10 clusters in case there
are 20 clusters in the best clustering.
do not decompose : this con ﬁguration creates an activ-
ity cluster array with a single cluster containing all
activities, and it does not add arti ﬁcial start and end
events. as a result, the selected miner is run with theselected classi ﬁer on the original log. this con ﬁgur-
ation corresponds to the monolithic approach and
offers a baseline for comparison.
theﬁrst three con ﬁgurations allow the user to select the level
of decomposition from maximal ( decompose ) to three-quarters
of maximal ( decompose 75 %) and half of maximal ( decompose
50%). the last con ﬁguration allows the user to check the result
of applying the regular (monolithic, or ‘decompose 0 %’) discov-
ery algorithm in an easy way.
4. evaluations
this section evaluates the implemented tool framework on
existing arti ﬁcial and real-life data sets. for both evaluations,
we use the ilp miner , as this discovery algorithm is known
to have an exponential complexity [
4] in the number of differ-
ent activities in the log. although other discovery algorithms
are supported by the framework (see also table 9), in this
evaluation, we focus only on the ilp miner.
to assess the quality of the discovered models, we use the
state-of-the-art alignment-based conformance metrics preci-
sion andgeneralization . in the area of process mining, four
such quality metrics are generally accepted [ 1]:a1
a4 a5 a6a2
a3
a7
a8
figure 19. net from fig. 18after reductions.
figure 20. dialog for discover using decomposition action in
prom 6.1660 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017fitness : the extent to which the model allows for the
behavior as seen in the event log.
precision : the extent to which the model does not
allow for behavior completely unrelated to behavior as
seen in the event log.
generalization : the extent to which the model gener-
alizes the behavior as seen in the event log.
simplicity : the extent to which the model is the sim-
plest model that explains the behavior as seen in the
event log.
as the ilp miner guarantees a perfect ﬁtness (the discovered
model always allows for all behavior as seen in the event
log), there is no reason to assess ﬁtness. furthermore, the
metrics for simplicity are rather subjective and do not relate
to the model ’s behavior. therefore, we restrict this quality
assessment to the precision and generalization metrics.
to evaluate the decomposed discovery algorithm for a
single case, that is, for a given event log, a given con ﬁgur-
ation and a given discovery algorithm, we perform the fol-lowing steps:
(1) we import the event log. we assume that the ﬁrst
classi ﬁer in that event log provides us with the activ-
ity log.
(2) we run the decomposed discovery algorithm using
the given con ﬁguration and the given discovery algo-
rithm. any computation time reported relates only to
this step, and not to any of the other steps. in theend, this results in an accepting petri net , which is
saved to ﬁle.
(3) next, we measure the quality of the resulting net
with respect to the log using the precision and gener-
alization metrics. for this, we replay [
22] the given
event log on the discovered net. this provides uswith the log alignment needed for the next step.
(4) we calculate the generalization and precision of
the log and the net using the measure precision /
generalization plug-in as available in prom 6.(5) we output a text ﬁle containing the diagnostic results
in condensed form.
these steps are implemented as the evaluate decomposed
discovery plug-in.
first, we compare the monolithic discovery algorithm (as
implemented by the do not decompose conﬁguration, see
section
3.5) with the maximal-decomposition discovery algo-
rithm (as implemented by the decompose conﬁguration).
second, we compare the maximal-decomposition discovery
algorithm with both the non-maximal-decomposition discov-ery algorithms (as implemented by the decompose 75 % con-
ﬁguration and the decompose 50 % con ﬁguration).
the reported computation times for the monolithic discov-
ery (that is, for the do not decompose conﬁguration) include
only the computation time needed for the discovery algorithm
itself (see fig.
6), that is, it excludes the computation times
for discovering activity clusters, ﬁltering the overall log and
merging the subnets. for all other con ﬁgurations, the compu-
tation times include all these steps.
all plug-ins used for doing the evaluations are available
through the divide and conquer test package in prom 6.
this package can be downloaded from https://svn.
win.tue.nl/repos/prom/packages/divideand
conquertest/trunk , which is a folder in our subver-
sion repository.
the evaluations are performed on a desktop computer with
an intel core i7-4770 cpu at 3.40 ghz, 16 gb of ram, run-
ning windows 7 enterprise (64-bit), and using a 64 bit versionof java 7 where 4 gb of ram is allocated to the java vm.
note that the approach can be distributed over multiple compu-
ters, but we only use one computing node.
4.1. arti ﬁcial data sets
table
10shows the list of three artiﬁcialdata sets containing
59 event logs (with their characteristics) that are used for this
evaluation.
table 10. artiﬁcial data sets used in the evaluation.
artiﬁcial data set description
dmkd 2006 [10] 20 synthetic events logs generated from four petri nets, containing 12, 22, 32 and 42 activities, 1000 traces, and
different noise levels. this data set uses case labels like a32f0n10 , where a32indicates that this case contains 32
activities, and n10indicates that in 10% of the traces noise was introduced
is 2014 [7] 32 synthetic event logs generated from four highly structured petri nets, containing 59, 48, 32 and 57 activities,
2000 traces, four different average trace lengths ( ∼15–55), with and without noise. this data set uses case labels
like59/55/n, where 59indicates the reported number of activities in [ 7],55indicates the average trace length, and n
indicates that this log contains noise
bpm 2013 [11] 7 synthetic event logs ( a–g) generated from seven highly structured petri nets, containing 317, 317, 317, 429, 275, 299
and 335 activities, log ccontains 500 traces, all other logs contain 1200 traces, log bis 100% ﬁtting its model, all other
events logs do not ﬁt 100%. this data set uses case labels like pram6 , which directly relates to the case from the data set1661 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 20174.1.1. monolithic vs. maximal-decomposition
first, we show for which logs in the arti ﬁcial data sets both
conﬁgurations are feasible. second, we show the computation
times required by both con ﬁgurations, and compare them
where possible. next, we provide results for the precision and
generalization metrics for both con ﬁgurations. note that, as
we are using the ilp miner, ﬁtness is guaranteed to be 1, so
we do not discuss the ﬁtness metric here. to compute preci-
sion and generalization, we need to replay the arti ﬁcial log on
the discovered net [ 22]. therefore, third, we show for which
artiﬁcial logs this replay is feasible. fourth, we show the feas-
ible precision values obtained by both con ﬁgurations, and
compare them where possible. fifth, we do the same for gen-
eralization. finally, we summarize our ﬁndings.
feasibility. table 11shows for every log from the arti ﬁcial
data sets whether they are feasible using the do not decom-
pose anddecompose conﬁgurations.both con ﬁgurations run
out of memory for the prdm6 andprfm6 logs from the bpm
2013 data set, while do not decompose runs out of time (that
is, it needs to be stopped after a week) for the prgm6 log
from the same data set. this table shows that one more log(the prgm6 log) is feasible with the decompose conﬁgur-
ation, and that hence we can only compare computation times
for those logs that are feasible with do not decompose .
computation times. figure
21shows the feasible computa-
tion times for do not decompose , and the speed-ups obtained
by using decompose . for example, this ﬁgure shows that do
not decompose takes almost 75.000 seconds ( >20 hours) to
discover a net from the prcm6 log, and it also shows that
decompose is∼150 times as fast, needing only ∼500 seconds
(<10 minutes). decompose outperforms do not decompose
for all logs where the latter takes >10 seconds.
figure 21clearly shows that the speed-up obtained by
decompose depends on the computation time of do not
decompose . this is especially clear for the dmkd 2006 and
is 2014 data sets: the higher the computation time needed bydo not decompose , the higher the speed-up of decompose .
theﬁgure ﬁnally shows that the speed-up also depends on
the data set the arti ﬁcial log originates from. for example, the
speed-up for a log from the dmkd 2006 data set is typically
higher than the speed-up for a log from the is 2014 data set.
this is surprising, as we assumed both data sets to be of simi-
lar complexity.
figure 22shows, for the 10 most time-consuming arti ﬁcial
logs, the feasible computation times for both con ﬁgurations,
and also where time is spent. first, time is spent on the dis-
covery of the subnets, that is, on running the discovery algo-rithm on the sublogs. figure
22shows the percentage of time
spent on this (see the bottom-most bars, labeled discovery ).
second, time is spent on the reduction of the discovered over-all net, which is shown using the middle bars, labeled
reduction . third, time is spent on, for example, computing
the best activity cluster, splitting the log or merging the sub-nets into an overall net, which is accumulated in the top-mosttable 11. feasible arti ﬁcial logs for all con ﬁgurations. ‘yes’indicates that both discovery and replay are feasible, ‘yes/no’that discovery is
feasible but replay is not, and ‘no’that discovery is not feasible.
data set event log do not decompose decompose 50 % decompose 75 % decompose
dmkd 2006 all yes yes yes yes
is 2014 all yes yes yes yes
bpm 2013 pram6 yes yes /no yes /no yes /no
prbm6 yes yes /no yes /no yes /no
prcm6 yes /no yes yes yes
prdm6 no no no no
prem6 yes /no yes /no yes /no yes /no
prfm6 no no no noprgm6 no yes /no yes /no yes /no
pram6prbm6prcm6
prem6y = 0.0033x0.7222
r2= 0.9919
y = 0.0014x0.725
r2 = 0.9931
0.1250.528321285122048
0.1 10 100 1000 10000 100000speed-up using decompose
computation time using do not decomposesecondsdiscovery - artificial
speed-up
dmkd 2006 is 2014
bpm 2013 power (dmkd 2006)
power (is 2014)1
figure 21. comparison of feasible computation times for the arti-
ﬁcial logs. the speed-up by decomposition tends to be high when
computation times are long.1662 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017bars, labeled other . clearly, decompose spends the majority
of its time (at least 86% for the logs shown in fig. 22,a tl e a s t
83% for all feasible logs) in the decomposed discovery, only a
fraction is spent on the overhead of the decomposition
approach. this shows that when using the decomposed ilpminer, there is no urgent need to improve on, for example, the
reduction of the net, as the entire approach would hardly bene-
ﬁtf r o mt h i s .
figure
22also shows the computation times using
decompose for the prgm6 log, for which do not decompose
is infeasible. it takes decompose ∼62 000 seconds ( ∼17
hours) to discover a net from the prgm6 log. given the fact
thatdo not decompose for this log needs to be stopped after
a week, the speed-up of decompose for this log is at least 10.
feasibility of replay. table 11also shows for which of the
feasible arti ﬁcial logs the replay is feasible. as mentioned
earlier, this replay is required to compute the precision andgeneralization metrics. for the arti ﬁcial logs for which the
replay is not feasible (that is, for all logs from the bpm 2013
data set except the prcm6 log), the evaluation runs out of
time. as a result, we can only compare precision and general-
ization for all logs in the dmkd 2006 data set and all logs in
theis 2014 data set.
precision. figure
23shows the precision values obtained using
do not decompose , and the precision gain /loss as obtained using
decompose . as an example, the precision obtained with do not
decompose on the 48/12/nlog is∼0.86, and decompose results
in a precision loss of ∼0.33, which results in a precision of
0.33 0.86 0.28´» ). this ﬁgure also shows that, in general,
decompose results in the same or less precision as do notdecompose . the only exceptions to this are the 32/18/nlog (gain
of 1.0123) and the 32/18/-log (gain of 1.0042).
figure 24shows why precision can be lower when using
decompose . the net that is discovered with decompose con-
tains three source transitions (transitions without incoming
a42f0n00
a32f0n00a22f0n0032/18/n
48/12/-
48/12/n
0.00.20.40.60.81.01.2
0 0.2 0.4 0.6 0.8 1precision gain/loss using decompose  
precision using do not decomposediscovery - artificial
precision
figure 23. comparison of precision metrics on all feasible arti ﬁ-
cial logs.
110100100010000100000
0%10%20%30%40%50%60%70%80%90%100%
secondsdiscovery - artificial
perc. of time spend and comp. time
discovery reduction other do not decompose decompose
figure 22. categorized percentages of computation times for the
most-hard feasible arti ﬁcial logs together with their computation times.
figure 24. example a32f0n00 explaining why precision can be
lower when using decompose . the top net is the result from do not
decompose , the bottom net from decompose .1663 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017arcs), which are always enabled. as these transitions are
enabled in all possible states, but only executed in few states,this net is less precise (0.28 instead of 0.67).
in contrast, fig.
25shows that precision can also be
(slightly) higher when using decompose . the net that is dis-
covered with decompose contains two additional source places
(places without incoming arcs), which are initially marked.
one of these places effectively prevents the transition labeledi2+complete from being executed more than once, which is
possible in the net discovered by do not decompose , but which
does not occur in the log. as a result, the net discovered withdecompose is slightly more precise (0.85 instead of 0.84).
generalization. figure
26shows the generalization values
obtained using do not decompose , and the generalization gain /
loss as obtained using decompose .t h i s ﬁgure shows that, in
general, decompose results in a better generalization than do not
decompose , although the differences are typically very small.
conclusions. if the monolithic discovery algorithm (that is,
do not decompose ) can discover a net from a log, then the
decomposition discovery algorithm (that is, decompose ) can
also discover a net from this log. however, the decompositionalgorithm can also discover nets from logs on which the
monolithic algorithm fails. as such, the decomposition algo-
rithm can be applied on larger and more complex logs thanthe monolithic algorithm.
the decomposition algorithm is typically faster than the
monolithic algorithm. if the monolithic algorithm takes >100
seconds, then the speed-up is at least 7.5, but can be >100.
the decomposition algorithm typically results in nets that
have an equal or worse value for precision, where the latter istypically due to the introduction of additional source transi-
tions. however, it is also possible that the decomposition algo-
rithm results in a slightly higher precision, as a result of theintroduction of additional initially marked source places.the decomposition algorithm typically results in a net that
has an equal or better value for generalization, although theimprovements are minor.
4.1.2. maximal-decomposition vs. non-maximal-
decomposition
first, we show for which arti ﬁcial logs all three con ﬁgura-
tions ( decompose ,decompose 75 % and decompose 50 %)
are feasible. second, we show the computation times required
bydecompose and the speed-ups obtained using the other
two con ﬁgurations. third, we show for which arti ﬁcial logs
(and discovered nets) the replay [
22] is feasible, as again this
is needed to compute the precision and generalization.
fourth, we show the precision values obtained usingdecompose and the percentages obtained by the other two
conﬁgurations. fifth, we do the same for generalization.
finally, we summarize our ﬁndings.
feasibility. table
11shows for which arti ﬁcial logs the
decompose 50 % con ﬁguration (simply called decompose
50% henceforth) and the decompose 75 % con ﬁguration (sim-
ply called decompose 75 % henceforth) are feasible.both
decompose 75 % and decompose 50 % only fail for the
prdm6 andprfm6 logs from the bpm 2013 data set (by also
running out of memory). as a result, we can compare compu-
tation times for all logs that are feasible with decompose .
computation times. figure 27shows the computation times
required by decompose , and the speed-ups obtained using
decompose 75 % and decompose 50 %. as an example, it
takes decompose ∼210 000 seconds ( ∼58 hours) to discover
a net from the prgm6 log, and the speed-up of decompose
a12f0n00 a42f0n0048/12/n
48/12/-a32f0n00
a22f0n00
0.99951.00001.00051.00101.00151.00201.00251.00301.0035
0 0.2 0.4 0.6 0.8 1generalization gain/loss using decompose  
generalization using do not decompose  discovery - artificial
generalization
figure 26. comparison of generalization metrics on all feasible
artiﬁcial logs. decompose results in very similar generalization values.
figure 25. example 32/18/nexplaining why precision can be
higher when using decompose . the top net is the result from do not
decompose , the bottom net from decompose .1664 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 201750%i s∼1.09, resulting in a required computation time of
∼193 000 seconds ( ∼54 hours). for the easier logs,
decompose 50 % outperforms decompose , but for the harder
logs, there seems to be no improvement.
feasibility of replay. table 11also shows for which of the
feasible arti ﬁcial logs the replay is feasible. the replay on the
nets discovered using decompose 50 % is feasible for exactly
the same set of logs as for which decompose 75 % is feasible.
like with the decompose conﬁguration, the replay for
decompose 75 % and decompose 50 % runs out of time for all
logs from the bpm 2013 data set but the prcm6 log. as a
result, we can compare precision and generalization for all
logs that are feasible with decompose .
precision. figure 28shows the precision values obtained
using decompose , and the gains /losses using decompose 75 %
anddecompose 50 %. as an example, the precision value for
thea32f0n00 log as obtained using decompose is∼0.28, and
the gains for the two other con ﬁgurations are ∼2.37, resulting
in a precision value of ∼0.67 (that is, the same value as
obtained by do not decompose ). apparently, both decompose
75%a n d decompose 50 % were able to avoid the introduction
of the additional source transitions for these logs. still, for
some other logs ( a42f0n00 ,48/12/-and48/12/n), these con ﬁg-
urations do not improve precision to the same level as do not
decompose . possibly, we need an even more coarse-grained
decomposition (like 25%) to get the same precision.
generalization. figure 29shows the generalization values
obtained using decompose , and the gains /losses using
decompose 75 % and decompose 50 %. as an example, the
precision value for the a32f0n00 log as obtained usingdecompose is∼0.98, and the losses for both other con ﬁgura-
tions are ∼0.997, resulting in a precision value of ∼0.98.
conclusions. the non-maximal decomposition discovery
algorithms (that is, decompose 75 % and decompose 50 %)
can discover nets from the same set of logs that the maximaldecomposition algorithm ( decompose ) can. on an average,
the 50% decomposition algorithm takes a bit more time
(104%) than the maximal decomposition algorithm, and the75% decomposition algorithm takes also a bit more (105%).a32f0n00
a22f0n00
48/12/n
0.00.51.01.52.02.5
0 0.2 0.4 0.6 0.8 1precision gain/loss using decompose 75%/50%  
precision using decomposedecomposed discovery - artificial
precision
decompose 75% decompose 50%
figure 28. comparison of decomposed precision metrics for arti-
ﬁcial logs.
48/12/n
a32f0n00a22f0n00
0.99650.99700.99750.99800.99850.99900.99951.00001.0005
0 0.2 0.4 0.6 0.8 1generalization gain/loss using decompose 75%/50%  
generalization using decompose  decomposed discovery - artificial
generalization
decompose 75% decompose 50%
figure 29. comparison of decomposed generalization metrics for
artiﬁcial logs.pram6
prbm6prgm6
00.20.40.60.811.21.41.61.8
1 10 100 1000 10000 100000speed-up using decompose 75%/50%  
computation time using decompose  secondsdecomposed discovery - artificial
speed-up
decompose 75% decompose 50%
figure 27. comparison of decomposed computation times for
artiﬁcial logs.1665 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017for the logs that take <100 seconds, the 50% decomposition
algorithm is the fastest, but it is considerably slower for somelogs that require more time, like the pram6 andprbm6 logs.
apparently, for these logs, the 50% decomposition algorithm
results in sublogs that are harder to handle for the ilp minerthan the sublogs for the other decomposition algorithms. the
non-maximal decomposition algorithms result in equal or bet-
ter precision values. sometimes the precision values obtainedmatch the ones obtained using the monolithic algorithm
(which is perfect). the non-maximal decomposition algo-
rithms result in equal or worse generalization values, but ifworse the difference is only minor.
4.2. real-life data sets
table
12shows the list of real-life data sets (with their char-
acteristics) that are used for this evaluation. as an example,fig.
30shows a graphical overview of the bpic 2012 event
log, which nicely shows that in the underlying process the
vast majority of the work is done on working days (the verti-cal gaps in the overview correspond to the weekends).
4.2.1. monolithic vs. maximal-decomposition
first, we show for which logs in the real-life data sets both
conﬁgurations are feasible. second, we show the computation
times required by both con ﬁgurations, and compare them
where possible. third, we show for which logs the replay
[
22] required for precision and generalization is feasible.
fourth, we show the feasible precision and generalization
values obtained by both con ﬁgurations, and compare them
where possible. finally, we summarize our ﬁndings.
feasibility. table 13shows for every real-life data set and
both con ﬁgurations, the set of logs that are feasible. the do
not decompose conﬁguration runs out of time (that is, it needsto be stopped stopped after a week) for all logs from the
bpic 2015 data set. this table clearly shows that more real-
life logs are feasible with the decompose conﬁguration, and
that hence we can only compare computation times for those
logs that are feasible with do not decompose .
computation times. figure 31shows, for all real-life logs,
the feasible computation times for both con ﬁgurations (where
possible), and also where time is spent. clearly, decompose
spends the majority of its time (about than 90% for the bpic
2012 log and more than 99% for the bpic 2015 logs) in the
decomposed discovery, only a fraction is spent on the overhead
of the decomposition approach. like with the arti ﬁcial logs,
this shows that when using the ilp miner, there is no urgent
need to improve on, for example, the reduction of the net, as
the entire approach would hardly bene ﬁtf r o mt h i s .
figure 31also shows that on the bpic 2012 log,decompose
is∼2 5t i m e sa sf a s ta s do not decompose . furthermore, it shows
the computation times using decompose for the bpic 2015
logs, for which do not decompose is infeasible. for example, it
takes decompose 2167 seconds ( ∼36 minutes) to discover a net
from the bpic2015 _5log. as do not decompose for this log
needs to be stopped after a week, the speed-up of decompose
for this log is at least 280.
feasibility of replay. table 13also shows for which of the
real-life logs the replay is feasible.as the discovery using
do not decompose is not feasible for any of the bpic 2015
logs, we can only compare precision and generalization forthebpic 2012 log.
precision and generalization. the precision and generaliza-
tion values obtained using decompose are exactly the same
as the values using do not decompose . this is not surprising,
as both discover the same net. figure
32shows the few
table 12. real-life data sets used in the evaluation.
real-life data set description
bpic 2012 [12] a real-life log taken from a dutch financial institute. this log contains 13 087 cases, ∼262 200 events, and 36 event
classes. apart from some anonymization, the log contains all data as it came from the ﬁnancial institute. the process
represented in the event log is an application process for a personal loan or overdraft within a global ﬁnancing
organization. the event log is a merger of three intertwined sub processes
bpic 2015 [13] five real-life event logs, provided by ﬁve dutch municipalities. the data contains all building permit applications over a
period of ∼4 years. there are many different event classes present. the cases in the log contain information on the main
application as well as objection procedures in various stages. furthermore, information is available about the resource thatcarried out the task and on the cost of the application. some statistics on the logs:
log 1 : 1199 cases, 52 217 events, 398 event classes
log 2 : 832 cases, 44 354 events, 410 event classes
log 3 : 1409 cases, 59 681 events, 383 event classes
log 4 : 1053 cases, 47 293 events, 356 event classes
log 5 : 1156 cases, 59 083 events, 398 event classes1666 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017connected parts of this net. the remainder of the net contains
only disconnected transitions, indicating that the ilp miner
has had its problems with this real-life log. but where do not
decompose takes 1420 seconds ( ∼24 minutes) to discover this
(disappointing) result, decompose takes only 56 seconds (less
than a minute). as such, decompose is obviously an
improvement over do not decompose for this log.
conclusions. do not decompose can only discover a net for
thebpic 2012 log, whereas decompose can discover a netfor every log in the real-life data sets. furthermore, for the
only log that do not decompose can handle successfully,
decompose is∼25 times as fast. as a result, decompose is
clearly always better than do not decompose on the real-life
data sets.
4.2.2. maximal-decomposition vs. non-maximal-
decomposition
first, we show for which logs all three con ﬁgurations
(decompose ,decompose 75 %a n d decompose 50 %) are
figure 30. a dotted chart of the real-life bpic 2012 event log. cases are plotted on the y-axis, time on the x-axis, the color of a dot denotes
the event class, and the shape of the dot denotes the day of the week.
table 13. feasible real-life logs for all con ﬁgurations. ‘yes’indicates that both discovery and replay are feasible, ‘yes/no’that discovery is
feasible but replay is as not, and ‘no’that discovery is not feasible.
data set event log do not decompose decompose 50 % decompose 75 % decompose
bpic 2012 bpic2012 yes yes yes yes
bpic 2015 bpic2015_1 no yes yes yes /no
bpic2015_2 no yes /no yes /no yes /no
bpic2015_3 no yes yes yesbpic2015_4 no yes yes yes
bpic2015_5 no yes /no yes /no yes /no1667 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017feasible. second, we show the computation times required by
decompose and the speed-ups obtained using the other two
conﬁgurations. third, we show for which logs (and discovered
nets) the replay [ 22] is feasible, as again this is needed to com-
pute the precision and generalization. fourth, we show the pre-
cision and generalization values obtained using decompose
and the percentages obtained by the other two con ﬁgurations.
finally, we summarize our ﬁndings.
feasibility. table 13shows for which real-life logs decompose
50%a n d decompose 75 % are feasible.as table 13shows, we
can compare computation times for all real-life logs.
computation times. figure 33shows the computation times
required by decompose on the real-life logs, and the speed-
ups obtained using decompose 75 % and decompose 50 %.apparently, there seems to be no real improvement in using
either decompose 75 %o r decompose 50 % over decompose .
in fact, there are two logs ( bpic2012 andbpic2015 _3) for
which decompose 50 % needs signi ﬁcantly more time than
the other two con ﬁgurations.
feasibility of replay . table 13also shows for which of the
feasible logs the replay is feasible.the replay for decompose
75% and decompose 50 % runs out of memory for the
bpic2015 _2log and out of time for the bpic2015 _5log. as
a result, we can compare precision and generalization for all
logs that are feasible with decompose .
precision and generalization. both decompose 75 %a s
decompose 50 % typically return the same values for precision and
generalization. the only exception to this is the precision of the
net discovered from the bpic 2012 log using decompose 50 %,
which drops to 97%. the latter is caused by the fact that
decompose 50 % does not discover the place (see fig. 32) between
o_selected +complete ando_created +complete .
conclusions . both decompose 75 % and decompose 50 % can
discover nets from the same real-life logs that decompose
can. where decompose 75 % requires about the same time as
decompose requires, decompose 50 % may require signi ﬁ-
cantly more. apparently, some of the clusters become to big
to be handled comfortably by the ilp miner. typically, bothdecompose 75 % and decompose 50 % preserve precision and
generalization, although in one case the precision dropped to
97%.
4.3. wrapping up
the three decomposed ilp miners outperform the monolithic
ilp miner in two ways. first, the decomposed ilp minersbpic2012bpic2015_3
00.20.40.60.811.2
1 10 100 1000 10000 100000speed-up using decompose 75%/50%  
computation time using decomposesecondsdecomposed discovery - real-life
speed-up
decompose 75% decompose 50%
figure 33. comparison of decomposed computation times for
real-life logs.110100100010000100000
0%10%20%30%40%50%60%70%80%90%100%
secondsdiscovery - real-life
perc. of time spend and comp. time
discovery reduction other donot decompose decompose
figure 31. categorized percentages of computation times for the
real-life logs together with their computation times.
figure 32. connected parts of the accepting petri net discovered
using either decompose ordo not decompose .1668 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017can discover nets from logs on which the monolithic ilp
miner simply fails. as an example, the monolithic ilp minerfails on ﬁve out of the six real-life logs, whereas the three
decomposed ilp miners succeed on all of them. second, if
the monolithic ilp miner is able to discover a net, then thethree decomposed ilp miners can discover a net much faster
(up to a factor 280). the net as discovered by a decomposed
ilp miner may be different (we will discuss this in the nextsection), but typically it will result in a net with equals or bet-
ter precision, and equal or slightly worse generalization.
using a non-maximal-decomposed ilp miner has a posi-
tive effect for the smaller logs, but no effect or a negative
effect for the larger logs. for some larger real-life logs, the
negative effect may even be called considerable (about threetimes as slow). as such, starting with the maximal-
decomposed ilp miner seems to be a good idea.
5. discussions
in the previous section, we have seen that sometimes decom-
posed discovery may result in a net that is different from the
net as discovered without (or with less) decomposition. in
this section, we ﬁrst discuss the possible differences in the
discovered nets. second, we show that the effect may be posi-
tive. to demonstrate this, we discuss the drfurby classi ﬁer
contribution to the ‘process discovery contest @ bpm
2016’[
14]. this contest was won by the ﬁrst author using the
decomposition approach described here. from all submis-
sions, most traces (193 out of 200) were classi ﬁed correctly.
furthermore, we have seen that the ilp miner can be suc-
cessfully decomposed using the tool framework. however, this
may not hold for the other discovery algorithms, as these algo-rithms may have different characteristics. therefore, we discuss
the use of the tool framework with the other discovery algo-
rithms as implemented in the tool framework. finally, we con-clude this section by summarizing our ﬁndings.
5.1. differences
the decomposition approach as presented in [
9] offers the
following formal guarantees:
perfect ﬁtness is preserved : the entire log is perfectly
ﬁtting the resulting merged net if and only if every
sublog is perfectly ﬁtting the subnet that was discov-
ered from it.
upper bound for misalignment costs : the misalign-
ment costs of replaying every sublog on the net that
was discovered from it is an upper bound for the mis-alignment costs of replaying the entire log on the
merged net. these misalignment costs are closely
related to the ﬁtness metric, as a trace is perfectly ﬁt-
ting if and only if these costs are 0.same fraction of perfectly ﬁtting traces : the fraction
of traces from the entire log perfectly ﬁtting the
merged net equals the fraction of traces perfectly ﬁt-
ting every subnet.
the above guarantees prove that discovery of a perfectly ﬁt-
ting net can be successfully decomposed. nevertheless, the
approach does not guarantee that the result of the decom-posed discovery will be the same as the result from the mono-
lithic discovery, even when all discovered nets are perfectly
ﬁtting. recall that the decomposition splits the entire log into
a collection of sublogs, and that it calls on the discovery algo-
rithm for every sublog. clearly, the discovery algorithm can
only use the information that is contained in the providedsublog, as the information contained in the other sublogs is
withheld from it. as such, it might take different decisions
than it would have if all information would be available.examples of such situations are shown in figs
24and25.
nevertheless, as mentioned earlier and as we will show with
the following process discovery contest, the differences mayjust be for the better.
5.2. process discovery contest
the aim of the ‘process discovery contest @ bpm 2016 ’
[
14] was to evaluate the state-of-the-art in process discovery.
to this end, the organizers of t he contest created 10 process
models, say m m,,11 0¼ , which were not disclosed to the contest-
ants. for every created process model mi(with i 1, ,10î{ ¼ }) ,
the organizers also created two event logs: a march itraining log
containing 1000 traces, which was disclosed in march 2016, anda
juneitest log containing 20 traces, which was disclosed in
june 2016. the test log also contained negative cases, that is,
traces impossible according to the original model.
a submission should include a discovery algorithm dand
a way to classify the traces of every juneilog as positive
(perfectly ﬁtmi) or negative (do not perfectly ﬁtmi), using
the model as discovered by algorithm dfrom the march ilog.
using the undisclosed process model mi, the organizers then
determine how many traces from every juneilog are correctly
classi ﬁed by the submission. in the end, the submission
which classi ﬁes the most traces over all juneilogs correctly
wins the contest. in case of a tie, the time required for dto
discover the models tip the balance. clearly, the better the 10
discovered process models match the 10 undisclosed process
models, the better the classi ﬁcation.
to allow the contestants to test their submission prior to
submitting it, the organizers also created for every undis-
closed process model mianaprilitest log (disclosed in april
2016) and a mayitest log (disclosed in may 2016). both the
apriliand the mayitest log are known to contain 10 positive
traces and 10 negative traces. by using this information, thecontestants can test and calibrate their submission.1669 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 20175.2.1. the drfurby classi ﬁer
theﬁrst author participated in this contest using the decompos-
ition approach presented in this paper. the basic idea behind the
drfurby classi ﬁer[23] is (i) to minimize the number of false
negatives (i.e. positive traces that are classi ﬁed as negative
traces) by only including algor ithms that guarantee perfect ﬁt-
ness and (ii) to minimize the numbe r of false positives (i.e. nega-
tive traces that are classi ﬁed as positive traces) by including
many of these algorithms. ho wever, to minimize the time
r e q u i r e db yi t ,t h e drfurby classi ﬁerincludes only a minimal
set of these algorithms that provide the maximal result.
examples of relevant algorithms that guarantee perfect ﬁt-
ness include:
inductive miner : the ‘inductive miner infrequent ’
with noise threshold set to 0.0 [ 5].
ilp miner : the ‘ilp miner ’[4].
hybrid ilp miner (default) : the ‘hybrid ilp miner ’
with default settings [ 17].
hybrid ilp miner (single) :t h e ‘hybrid ilp miner ’
that uses only a single variable per causal relation [ 17].
next to these discovery algorithms, the drfurby classi ﬁer
also exploits the tool framework presented in this paper, as it
alsopreserves (perfect) ﬁtness [ 9].
thedrfurby classi ﬁertakes a march ilog and a juneilog
as input, and creates a classi ﬁedjunei+log as output. to cre-
ate the output from the inputs, it iteratively uses a number of
perfect- ﬁtness-guaranteeing discovery algorithms with differ-
ent decomposition settings. for each combination of a discov-
ery algorithm dand a setting s,i tﬁrst discovers an accepting
petri net dmarchsi() using the framework. second, it checks
which traces of the juneilog can be perfectly replayed on
dmarchsi() [22]. if a trace is perfectly replayed on all such
combinations, it is classi ﬁed as positive by the drfurby
classi ﬁer, otherwise, it is classi ﬁed as negative.
conﬁguration. theapriliandmayilogs were used to deter-
mine a minimal set of (possibly decomposed) discoveryalgorithms that provides maximal result. for this sake, all
discovery algorithms mentioned earlier were tested, andvarious decomposition settings (do not decompose, maximal
decomposition, decompose by 80%, by 60%, by 40%, …). in
the end, two decomposed discovery algorithms were found
that provide the desired maximal result:
the inductive miner with maximal decomposition
(called
dim
100henceforth).
the hybrid ilp miner without decomposition (called
dhim
0henceforth).
as a result, the drfurby classi ﬁeris con ﬁgured with only
these two decomposed discovery algorithms, leading to a dis-covery algorithm
dfurby.
implementation. the drfurby classi ﬁeris implemented as
thedrfurby classi ﬁerplug-in in prom 6.6, where it can be
found in the divide and conquer package.
to enrich the classi ﬁed test log with the necessary classi ﬁca-
tion attributes, a drfurby extension is implemented as well (see
[2] for details on log extensions), which uses the pre ﬁxdrfur-
by. this extension de ﬁnes attributes as listed in table 14.
as a result, by inspecting the output log, the user can see
how many traces are classi ﬁed positive (negative), which
traces are classi ﬁed positive (negative), etc. furthermore, to
get a quick overview of which traces are classi ﬁed positive,
thedrfurby classi ﬁerappends a plug sign ( +) to the name
of every trace that is classi ﬁed positive.
results. table 15shows the number of traces classi ﬁed cor-
rectly by the drfurby classi ﬁerfor all aprili,mayiand
juneilogs. as a result, for the juneilogs, the drfurby
classi ﬁerclassi ﬁes 193 out of 200 traces correctly. in add-
ition, fig. 34shows two views on the resulting classi ﬁed
logjune3+. these views show that 13 out of 20 traces are
classi ﬁed as positive, and that only seven are classi ﬁed as
negative.
5.2.2. maximal decomposition vs. no decomposition
instead of using the inductive miner with maximal decom-position, the drfurby classi ﬁercould also been con ﬁgured
table 14. the attributes of the drfurby extension which are added to the output log to allow the user to inspect the classi ﬁcation results.
attribute level type description
classi ﬁcation trace string classi ﬁcation of the trace ( ‘positive ’or‘negative ’)
him0costs trace continuous the costs of replaying this trace on the accepting petri net as discovered by the dhim
0discoverer
im100costs trace continuous the costs of replaying this trace on the accepting petri net as discovered by the dim
100discoverer
millis log discrete the number of milliseconds it takes to classify the test log
name log literal the name of the test log
negative log discrete the number of traces in the test log classi ﬁed negative
positive log discrete the number of traces in the test log classi ﬁed positive
totalcosts trace continuous the accumulated costs of replaying this trace on all discovered accepting petri nets1670 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017with the inductive miner without decomposition (called dim
0
henceforth). for 9 out of the 10 march ilogs, this would not
make a difference, as the results would be the same
(d march d marchim
iim
i 0 100() = () ). however, for march 3,i t
would make a difference.
table 16shows the classi ﬁcation results on the may3log.
this table shows that the drfurby classi ﬁerclassi ﬁes eight
traces as negative. furthermore, it shows that if we wouldhave used
dim
0instead of dim
100, it would only classify ﬁve
traces as negative. as a result, using the maximal-
decomposed inductive miner ( dim
100) instead of the monolithic
inductive miner ( dim
0), the drfurby classi ﬁercorrectly clas-
siﬁes three more traces as negative. for the june 3log, two
more traces (7 instead of 5) are correctly classi ﬁed as
negative.
as a result, the drfurby classi ﬁerusing a maximal-
decomposed inductive miner works better (7 false positiveson all
juneilogs, no false negatives) than it would using the
monolithic inductive miner (9 false positives on all junei
logs, no false negatives). that is, with decomposition, the
drfurby classi ﬁerclassi ﬁes 193 out of 200 traces correctly,
whereas without decomposition it would only have classi ﬁed
191 correctly.
in the end, this improvement from 191 to 193 made the
drfurby classi ﬁerwin [ 14] the contest, as the two runner-
ups in the contest classify both 192 traces correctly. onerunner-up did not use any decomposition techniques, but the
other runner-up actually used the ilp miner with maximal
decomposition as supported by the tool framework for somelogs (like the
march 3log). for the other logs (like the
march 1log), the second runner-up used the inductive miner
without decomposition. as a result, in the top three of thecontest, two submissions actually use the tool framework pre-
sented in this paper, and one of them won. although the main
goal of the paper is the feasibility and speed-ups of thedecomposed approach, this result shows the competitive
value of the approach on the quality perspective.
5.3. other discovery algorithms
as shown earlier in table
9, six different discovery algo-
rithms are implemented in the tool framework. the previous
figure 34. two views on the results of the drfurby classi ﬁerfor
thejune 3log. the top view shows that the traces 1 –4, 7–11, 16, 17,
19 and 20 are classi ﬁed as positive. the bottom view shows that 7
traces are classi ﬁed as negative.
table 16. classi ﬁcation ( += positive, −=negative) of the may3log using the drfurby classi ﬁer. the third row ( dfurby) can be viewed as
the conjunction of the ﬁrst two rows. clearly, the inductive miner with maximal decomposition ( dim
100) complements dhim
0much better than the
inductive miner without decomposition ( dim
0).
trace 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
dim
100 +−−− +++++ −− +++++ −− ++
dhim
0 +−+−++++++ − ++++ − +− ++
dfurby+−−− +++++ −− ++++ −−− ++
dim
0 +−+−++++++ − ++++ − +− ++table 15. numbers of traces classi ﬁed correctly by the drfurby
classi ﬁer. note that the optimal answer is 20 in all cases, hence it
perfectly classi ﬁes 22 out of 30 logs.
i 123456789 1 0
aprili20 20 20 20 20 20 20 19 20 20
mayi 20 20 18 20 19 20 20 18 20 20
june i 20 20 17 20 20 19 19 18 20 201671 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017section has shown that one of them, the ilp miner [ 4], can
be successfully decomposed using the framework to signi ﬁ-
cantly speed up analysis. but as the other ﬁve algorithms may
have different characteristics, these positive results may not
transfer to them. in this section, we discuss the use of the toolframework on these other ﬁve algorithms. the running times
as reported in this section are obtained on the same computer
as we did the evaluation in the previous section on.
alpha miner. the alpha miner [
3] was the ﬁrst discovery
algorithm able to discover concurrency in process models.
under certain preconditions, the alpha miner can rediscoverthe net from the event log successfully (see [
3]). although
the alpha miner is generally conceived to be a very simple
and straightforward algorithm, its complexity is not that sim-ple. especially, the step that computes the maximal activity
setsa
1anda2such that (i) every activity in a1can be directly
followed by any activity in a2, (ii), every activity in a1can-
not be directly followed by any activity in a1, and (iii) every
activity in a2cannot be directly followed by any activity in
a2, is potentially very time-consuming when there are many
different activities. as a result, although for many event logs,
decomposing the alpha miner may not result in a speed-up,
for some event logs, it could. as an example, we take theprgm6 log from the bpm 2013 [
11] data set. to discover the
net from this log takes the monolithic alpha miner ∼800
seconds. the maximally decomposed alpha miner decom-poses this log into 57 sublogs, and to discover the 57 subnets
takes only ∼87 seconds. this shows that the alpha miner may
beneﬁt from the decomposition. however, the overhead of the
decomposed alpha miner spoils this gain of >700 seconds as
it takes days (if not weeks or worse) to reduce the resulting
merged net (section
3.4.3). the main bottleneck is the algo-
rithm used to reduce the structural redundant places [ 21],
which uses ilps to compute these structural redundant places.
heuristics miner. the heuristics miner [ 16] is a very ef ﬁ-
cient discovery algorithm, with a very low complexity. for
this discovery algorithm, we do not expect any speed-upswhen using decomposition. as an example, it takes the
monolithic heuristics miner ∼5 seconds to discover a net
from the prgm6 log, whereas the maximally decomposed
heuristics miner takes almost 95 seconds, of which it needs
∼36 seconds to discover the 57 subnets. hence, although on
average the heuristics miner takes less time to discover a sin-gle subnet, it takes more to discover them all, and the over-
head of the decomposition is signi ﬁcant.
hybrid ilp miner. like the ilp miner, the hybrid ilp
miner [
17] uses ilps to solve the discovery problem.
although (i) the actual ilp when using both discovery algo-
rithms may be different, and (ii) the hybrid ilp miner is typ-ically faster, we expect the hybrid ilp miner to also bene ﬁtfrom the decomposition. if we take the prgm6 log again as
example, the monolithic hybrid ilp miner crashes as the ilpsolver runs out of memory in 3 hours, whereas the maximally
decomposed hybrid ilp miner ﬁnishes after ∼60 000 seconds
(∼16 hours and 40 minutes). hence, although the maximally
decomposed hybrid ilp miner takes >16 hours, at least it
does discover a net, whereas the monolithic hybrid ilp
miner fails to do so
inductive miner. the inductive miner [
5] is the youngest
member of the process discovery family, and has quickly
grown to be the most often used process discovery algorithm
in prom. in its way, this discovery algorithm uses a divide-and-conquer approach to tackle the discovery problem at
hand. nevertheless, it requires the algorithm some efforts to
decide which division is best at some point in time. as aresult, it takes the monolithic inductive miner ∼29 seconds to
discover a net from the prgm6 log, whereas the maximally
decomposed inductive miner takes almost 52 seconds, ofwhich it needs ∼33 seconds to discover the 57 best clusters,
∼8 seconds to discover the 57 subnets, and 6 seconds to do
the necessary reductions. as such, the inductive miner maybeneﬁt as well from the decomposition, but like with the
heuristics miner, the overhead of the decomposition may out-
weigh the bene ﬁts.
evolutionary tree miner. the evolutionary tree miner [
18]
is a genetic discovery algorithm, which uses replay [ 22]t o
check the quality of a discovered net. being an evolutionaryalgorithm, the higher the quality of a net, the higher the
chances that it survives. as such, provided suf ﬁcient space
and time, it will provide a high-quality net, provided thatsuch a net exists. however, it often lacks the suf ﬁcient time,
as by default it stops after 10 minutes, after which it returns the
best net found so far. although from a user-perspective thisdeadline of 10 minutes is understandable, it will typically be
too short for the algorithm to return a high-quality net. on the
examples as introduced in the previous section, this discoveryalgorithm will typically stop because of this 10-minute dead-
line. as a result, it will take 10 minutes on the prgm6 log,
whereas the maximally decomposed evolutionary tree minerwill take at least 570 minutes (10 minutes for every of the 57
sublogs). as such, decomposing this discovery algorithm only
makes sense if the deadline is set to a far larger value than 10minutes, or if it is removed altogether. the latter forces the dis-
covery algorithm to stop on another stopping criteria, which all
have to do with the quality of the best net.
5.4. summarizing the ﬁndings
thedrfurby classi ﬁer[
23] uses the tool framework and was
the winning submission for the ‘process discovery contest1672 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017@ bpm 2016 ’[14]. by replacing the inductive miner with-
out decomposition by the inductive miner with maximaldecomposition, we effectively reduced the number of errors
(misclassi ﬁed traces) by the drfurby classi ﬁerfrom 9 to 7 in
theﬁnal test logs. if we also include the calibration logs, then
the number of errors is reduced from 21 to 13. this shows
that the tool framework has effectively improved the effect-
iveness of the drfurby classi ﬁer, and has made this submis-
sion the winning submission [
14], as the two runner-ups both
misclassi ﬁed eight traces.
discovery algorithms other than the ilp miner can bene ﬁt
from the tool framework as well, including the alpha miner
[3], the inductive miner [ 5] and the hybrid ilp miner [ 17].
for all these algorithms, discovering the subnets from thesublogs may take less time than discovering a net from the
entire log.
however, the overhead of the tool framework may spoil
any computation time bene ﬁts obtained by the decomposition
for the ﬁrst two. we have seen that for the alpha miner the
net reduction (section
3.4.3 ) may be a problem. apparently,
the nets as discovered by the alpha miner may be problem-
atic for the implemented reduction techniques, especially
those techniques that use ilps. for the inductive miner, thereduction techniques are less of a problem, but the discovery
of the best cluster may take too long to bene ﬁt from the
decomposition. the heuristics miner [
16] does not bene ﬁt
from the decomposition, as its complexity is very low. the
evolutionary tree miner [ 18] takes so much time that is typ-
ically capped with a 10-minute deadline to have it deliver aresult within reasonable time. using decomposition with this
algorithm only makes sense if time is not the limiting factor
in the discovery.
6. conclusions
this paper presents the divide and conquer framework. this
framework fully supports the decomposed discovery as intro-duced in [
9], and has been implemented in prom 6. as such,
the framework allows for easy decomposed discovery, using
existing discovery algorithms. the current framework sup-ports six discovery algorithms, but can easily support more.
for the decomposed discovery, the framework allows
the end user to select the classi ﬁer to use (which maps the
event log at hand to an activity log), the miner (or discov-
ery algorithm) to use and a conﬁguration to use. available
conﬁgurations include do not decompose (monolithic dis-
covery algorithm), decompose (maximal decomposition
discovery algorithm), decompose 75 % (75% decomposition
discovery algorithm) and decompose 50 % (50% decompos-
ition discovery algorithm). the selected level of decomposition
(maximal, 75% or 50%) determines the number of sublogs to
overall log will be split into. for the maximal decomposition,this number will be maximal. whatever classi ﬁe r ,m i n e r ,a n dconﬁguration the user selects, the end result will be an overall
net discovered for the log at hand.
adding a new miner to the framework is easy, provided
that the miner either results in (i) a net with an explicit initial
marking and an explicit set of ﬁnal markings, or (ii) a net
with an implicit initial marking (one token in every source
place) and an implicit set of ﬁnal markings (a token in one
sink place). however, if a new miner emerges that does notsatisfy these requirements, then it can still be added, but a
wrapper needs to be created that assigns an initial marking
and a set of ﬁnal markings to the discovered net.
the ilp miner [
4] bene ﬁts clearly from the framework.
logs that take the ilp miner more than a week, can be dis-
covered within half an hour with a decomposed ilp miner.this shows that decomposition indeed can speed up a com-
plex discovery algorithm signi ﬁcantly. other discovery
algorithms may also bene ﬁt, like the hybrid ilp miner
[
17], the alpha miner [ 3] and the inductive miner [ 5].
however, for the latter two algorithms, any computation
time bene ﬁt obtained by the decomposition may be spoiled
by the overhead of the decomposition. as a result, we need
to investigate whether we can reduce this overhead for these
algorithms.
it is a fact that using decomposed discovery may lead to
different results. however, the drfurby classi ﬁer[23] sub-
mission to the ‘process discovery contest @ bpm 2016 ’
[14] shows that this may very well have a positive effect. by
making the state-of-the-art inductive miner [ 5] decomposed
in this submission, the number of misclassi ﬁed traces could
be reduced from 9 to 7, thereby winning the contest [ 14]a s
the two runner-ups had both eight misclassi ﬁcations.
although this approach does not provide any guarantees, it
clearly shows that the quality of the nets (de ﬁned as the per-
centage of correctly classi ﬁed traces) obtained by decom-
posed discovery may exceed the quality of the nets asobtained by non-decomposed discovery. however, there is
guarantee for such a positive effect.
because of the formal guarantees as provided by the
decomposition, the results of the decomposed discovery pro-
vide a valuable and reliable alternative. as examples, preci-
sion may drop, while generalization may increase. thepossible drop in precision may be mitigated by using a non-
maximal decomposition algorithm, like the 50% decompos-
ition algorithm.
future work on the framework includes additional non-
maximal decomposition algorithms and possible improve-
ments on the imposed overhead. our evaluation shows that indiscovery we can go from maximal decomposition to 50%
decomposition while maintaining high speed-ups. discovery
may take more time, but on average the computation timesare still reasonable, and the results get only better. therefore,
for discovery, we aim to check whether, for example, a 25%
decomposition algorithm is even better, both in computationtimes and in results.1673 divide and conquer :at oolframework for supporting decomposed discovery in process mining
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017references
[1] van der aalst, w.m.p. (2016) process mining: data science in
action (2nd edn). springer.
[2] günther c.w. and verbeek h.m.w. (2014) xes standard
deﬁnition. technical report bpm-14-09.
[3] van der aalst, w.m.p., weijters, a.j.m.m. and maruster, l.
(2004) work ﬂow mining: discovering process models from
event logs. ieee trans. knowl. data eng. ,16, 1128 –1142.
[4] van der werf, j.m.e.m., van dongen, b.f., hurkens, c.a.j.
and serebrenik, a. (2009) process discovery using integer lin-
ear programming. fundam. inf. ,94, 387 –412.
[5] leemans, s.j.j., fahland, d., and van der aalst, w.m.p.
(2013) discovering block-structured process models fromevent logs —a constructive approach. in colom, j.-m. and desel,
j. (eds.), application and theory of petri nets and concurrency ,
lecture notes computer science, 7927, pp. 311 –329.
[6] dumas, m., van der aalst, w.m.p. and ter hofstede, a.h.m.
(2005) process-aware information systems: bridging people
and software through process technology , wiley & sons.
[7] munoz-gama, j., carmona, j. and van der aalst, w.m.p.
(2014) single-entry single-exit decomposed conformancechecking. inf. syst. ,46, 102 –122.
[8] verbeek, h.m.w., buijs, j.c.a.m., van dongen, b.f. and van der
aalst, w.m.p. (2010) prom 6: the process mining toolkit. proc.
bpm demonstration track 2010 , pp. 34 –39. ceur-ws.org.
[9] van der aalst, w.m.p. (2013) decomposing petri nets for pro-
cess mining: a generic approach. distrib. parallel database ,
31, 471 –507.
[10] maruster, l., weijters, a.j.m.m., van der aalst, w.m.p. and
van den bosch, a. (2006) a rule-based approach for processdiscovery: dealing with noise and imbalance in process logs.data min. knowl. discov. ,13,6 7–87.
[11] munoz-gama, j., carmona, j. and van der aalst, w.m.p.
(2013) conformance checking in the large: partitioning andtopology. in daniel, f., wang, j. and weber, b. (eds), proc.
11th int. conf. business process management (bpm 2013) ,
beijing, china, august 26 –30, lecture notes computer
science, 8094, pp. 130 –145.
[12] van dongen, b.f. (2012). bpi challenge 2012 data set. doi: 10.
4121 /uuid:3926db30-f712-4394-aebc-75976070e91f.
[13] van dongen, b.f. (2015). bpi challenge 2015 data set. doi: 10.
4121 /uuid:31a308ef-c844-48da-948c-305d167a0ec1.[14] carmona, j., de leoni, m., depaire, b. and jouck, t. process
discovery contest @ bpm 2016.
https: //www.win.tue.nl /
ieeetfpm /lib/exe/fetch.php?media =shared:downloads:process
discoverycontest.pdf (accessed october 13, 2016).
[15] girault, c. and valk, r. (2001) petri nets for system
engineering: a guide to modeling, veri ﬁcation, and
applications , springer new york, inc., secaucus, nj, usa.
[16] weijters, a.j.m.m. and van der aalst, w.m.p. (2003)
rediscovering work ﬂow models from event-based data using
little thumb. integr. comput. aided eng. ,10, 151 –162.
[17] van zelst s.j., van dongen b.f. and van der aalst w.m.p.
(2015) ilp-based process discovery using hybrid regions.proc. int. workshop on algorithms & theories for the analysisof event data, ataed 2015, satellite event of the conferences:36th int conf. application and theory of petri nets and
concurrency petri nets 2015 and 15th int. conf. application
of concurrency to system design (acsd 2015) , brussels,
belgium, june 22 –23, pp. 47 –61. ceur-ws.org.
[18] buijs j.c.a.m. (2014) flexible evolutionary algorithms for
mining structured process models. phd thesis, eindhovenuniversity of technology.
[19] hompes, b.f.a., verbeek, h.m.w. and van der aalst, w.m.p.
(2015) finding suitable activity clusters for decomposedprocess discovery. in ceravolo, p., russo, b. and accorsi, r.(eds), data-driven process discovery and analysis: 4th int.
symposium, simpda 2014 , milan, italy, november 19 –21,
revised selected papers, lecture notes business information,237, pp. 32 –57.
[20] murata, t. (1989) petri nets: properties, analysis and applica-
tions. proc. ieee ,77, 541 –580.
[21] berthelot, g. (1987) transformations and decompositions of
nets. in brauer, w., reisig, w. and rozenberg, g. (eds),advances in petri nets 1986 part i: petri nets, central modelsand their properties , lecture notes computer science, 254,
pp. 360 –376.
[22] van der aalst, w.m.p., adriansyah, a. and van dongen, b.f.
(2012) replaying history on process models for conformancechecking and performance analysis. data min. knowl. discov. ,
2, 182 –192.
[23] ver beek, h.m.w. and mannhardt, f. (2016) the
drfurby classi ﬁer submission to the process discovery
contest @ bpm 2016. bpm center report bpm-16-08.bpmcenter.org.1674 h.m.w. v erbeek et al.
section c: c omputational intelligence ,machine learning and dataanalytics
thecomputer journal ,vol.6 0n o. 11, 2017
downloaded from https://academic.oup.com/comjnl/article-abstract/60/11/1649/3804254
by technische universiteit eindhoven user
on 22 november 2017