ieee transactions on services computing, vol. 1, no. 1, september 2015 1
processes meet big data: connecting data
science with process science
wil van der aalst senior member, ieee, and ernesto damiani senior member, ieee
abstract ‚Äîas more and more companies are embracing big data, it has become apparent that the ultimate challenge is to relate
massive amounts of event data to processes that are highly dynamic. to unleash the value of event data, events need to be tightly
connected to the control and management of operational processes. however, the primary focus of big data technologies is currently
on storage, processing, and rather simple analytical tasks. big data initiatives rarely focus on the improvement of end-to-end
processes. to address this mismatch, we advocate a better integration of data science ,data technology andprocess science . data
science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering
the ‚Äúevidence‚Äù hidden in the data. process mining aims to bridge this gap. this paper discusses the interplay between data science
and process science and relates process mining to big data technologies, service orientation, and cloud computing.
index terms ‚Äîprocess mining, data science, process science, big data, service orientation, and cloud computing.
f
1 i ntroduction
bigdata is changing the way we do business, socialize,
conduct research, and govern society [1], [2]. big data
has become a board-level topic and organizations are in-
vesting heavily in related technologies. at the same time
it is not always clear how to derive value from data. col-
lecting large amounts of data does not necessarily lead to
better processes and services. moreover, analytics are often
targeting particular tasks rather than the end-to-end process.
we would like to stress the importance of the process
perspective in big data initiatives. the aim of this paper is
twofold.
1) we connect data science and process science by
sketching the history of both disciplines. the data
science discipline combines techniques from statis-
tics, data mining, machine learning, databases, visu-
alization, ethics, and high performance computing.
process science can be seen as the broader discipline
covering process-centric approaches including busi-
ness process management (bpm), workÔ¨Çow man-
agement (wfm), business process reengineering
(bpr), operations research (or), etc. we argue that
one needs to carefully combine process-centric and
data-centric approaches. this seems obvious, yet
most data science (process science) approaches are
process (data) agnostic. process mining techniques
aim to bridge this gap [3], [4].
2) although process mining techniques have been
around for a few years and tools are readily avail-
able (prom, disco, aris ppm, qpr, celonis, snp ,
w. van der aalst is with the department of mathematics and computer
science, eindhoven university of technology, po box 513, nl-5600 mb
eindhoven, the netherlands.
e-mail: see http://vdaalst.com
e. damiani is with khalifa university/ebtic, abu dhabi, uae, on
leave from the department of computer science, universit¬¥ a degli studi di
milano, italy. e-mail: ernesto.damiani@kustar.ac.ae
manuscript received september 23rd, 2015.minit, myinvenio, perceptive, etc.), they are not a
Ô¨Årst-priority in most big data projects. the focus
of big data technology projects (apache hadoop,
spark, storm, impala, etc.) is mostly handling huge
volumes of highly diverse data while providing
simple reporting and traditional analytics, targeting
speciÔ¨Åc steps in the process. consequently, there are
few process mining efforts tailored towards big data
applications.
therefore, we discuss the interplay between process
mining and big data technologies, service orientation, and
cloud computing in this paper.
this paper is also intended as an introduction to our
special issue of ieee transactions on services computing
focusing on ‚Äúprocesses meet big data‚Äù. in a way, the contri-
butions we selected for this special issue and the content of
this paper can be seen as complementary: the former present
novel techniques and technologies that promise to bridge
the gap between process and data science, while the latter
focuses on open issues that still need to be solved for that
bridge to become robust and easy to cross. in their paper
‚Äúmultilevel process mining for financial audits‚Äù, michael
werner and nick gehrke present some sophisticated analyt-
ics techniques for process data and show how they can be
used to detect hidden irregularities when auditing Ô¨Ånancial
processes.
the work ‚Äúonline discovery of declarative process
models from event streams‚Äù by andrea burattin, marta
cimitile, fabrizio maggi and alessandro sperduti deals
with extracting models from high-bandwidth event streams.
in their work ‚Äúevent correlation analytics: scaling pro-
cess mining using mapreduce-aware event correlation
discovery techniques‚Äù, hicham reguieg, boualem bena-
tallah, hamid r. motahari nezhad and farouk toumani
describe how the most widespread big data computational
paradigm (see also section 5.1 of this paper) can be applied
to efÔ¨Åciently compute correlation analytics. a different con-ieee transactions on services computing, vol. 1, no. 1, september 2015 2
process science data science process mining
data mining
machine learning
visual analyticsstatisticsnosql
mapreduce
ethicspredictive analytics
business process managementworkflow management
operations research
business process reengineering
scientific managementconcurrency theoryverificationsimulationclusteringprocess discovery
conformance checking
...
fig. 1. process mining as the bridge between process science and data science.
tribution to process discovery focusing on artefacts rather
than on models is described in the paper ‚Äúdiscovering
interacting artifacts from erp systems‚Äù, by xixi lu, marijn
nagelkerke, dennis van de wiel, and dirk fahland.
other papers featured in this special issue focus more
on process model synthesis, either by introducing data-
intensive techniques for process model design, like the
paper ‚Äúan automated approach for assisting the design
of conÔ¨Ågurable process models‚Äù by nour assy, nguyen
ngoc chan and walid gaaloul, or by presenting bottom-
up techniques to foster the emergence of processes via
‚Äúspontaneous‚Äù service compositions, like ‚Äúa dataÔ¨Çow-
pattern-based recommendation framework for data ser-
vice mashup‚Äù by guiling wang, yanbo han, zhongmei
zhang and shouli zhang.
finally, the papers ‚Äúanalysis of technology trends based
on diverse data sources‚Äù by aviv segev, sukhwan jung
and seungwoo choi, and ‚Äúcollaborative agents for dis-
tributed load management in cloud data centers using
live migration of virtual machines‚Äù by j. octavio gutierrez-
garcia and adrian ramirez-nafar, discuss different aspects
of the ongoing integration between process mining toolkits
and big data and cloud computing technologies (see also
section 5.2 of this paper).
in the remainder of this paper, we Ô¨Årst review the history
of process science (section 2) and the history of data science
(section 3). then, section 4 introduces process mining as
a means to bridge the gap between both. section 5 dis-
cusses the implications and opportunities of implementing
process mining tools on top of big data technologies and
platforms. first, we discuss the mapreduce programming
model in the context of process mining (section 5.1). subse-
quently, we zoom in on the relationship between future big-
data-enabled process-mining-as-a-service, service orienta-
tion and cloud computing (section 5.2). section 6 concludes
the paper.
2 a b rief history of process science
in recent years, ‚Äúdata science‚Äù has become a common term
to refer to an emerging discipline revolving around the
widespread availability of data. we would like to confront
‚Äúdata science‚Äù with the umbrella term ‚Äúprocess science‚Äù
which refers to the broader discipline that combines knowl-
edge from information technology and knowledge from manage-
ment sciences to improve and run operational processes . process
science aims at process improvements in terms of time,
costs, quality, speed, Ô¨Çexibility, and reliability. this can be
done through automation, but process science is deÔ¨Ånitely
not limited to that. the process science discipline encom-
passes sub-disciplines such as business process manage-ment (bpm). in this section, we provide a brief history of
process science.
since the industrial revolution, productivity has been
increasing because of technical innovations, improvements
in the organization of work, and the use of information
technology. adam smith (1723-1790) showed the advan-
tages of the division of labor. frederick taylor (1856-1915)
introduced the initial principles of scientiÔ¨Åc management.
henry ford (1863-1947) introduced the production line for
the mass production of ‚Äúblack t-fords‚Äù.
here we consider the birth of scientiÔ¨Åc management as
the starting point of process science. around 1880 frederick
taylor, the ‚Äúfather of scientiÔ¨Åc management‚Äù, got interested
in answering questions like ‚Äúwhat is the best way to do
a job?‚Äù. for example, taylor found out that the optimal
weight that a worker could lift using a shovel was 21 pounds
[5]. this resulted in the ‚Äúwyoming 21-pound shovel‚Äù that
exploited this scientiÔ¨Åc knowledge (see figure 2).
fig. 2. fragment of an advertisement by wyoming shovels from 1913
illustrating the origins of process science. frederick taylor empirically
determined the optimal size of a shovel and this ‚ÄúscientiÔ¨Åc knowledge‚Äù
was used to promote the ‚Äúwyoming 21-pound shovel‚Äù claiming that it
could double or triple the productivity of workers.
over time operational processes increased in complexity
and the need to consider the ‚Äúprocess as whole‚Äù rather
than optimizing individual tasks has become more pressing.
rather than selecting the optimal size of a shovel, the
need to orchestrate highly dynamic processes involving
many entities emerged. the second industrial revolution
addressed this need by realizing assembly lines and opti-
mized processes (e.g., to produce ‚Äúblack t-fords‚Äù as fast
and cheap as possible). all of this was done without relying
on sophisticated it systems.
around 1950, computers and digital communication in-
frastructures slowly started to inÔ¨Çuence business processes.ieee transactions on services computing, vol. 1, no. 1, september 2015 3
two decades later the effects of this third industrial revo-
lution became noticeable in every sector. the adoption of
it systems resulted in dramatic changes in the organization
of work and enabled new ways of doing business. today,
innovations in computing and communication are still the
main drivers behind change in almost all business processes.
business processes have become more complex, heavily rely
on information systems, and may span multiple organiza-
tions. people are now using the term ‚Äúindustry 4.0‚Äù [6] to
refer to a fourth industrial revolution. the goal is to create
‚Äúsmart‚Äù manufacturing systems using a combination of
embedded systems, sensors, networks, service orientation,
big data, and analytics.
concurrent with the various stages of the industrial rev-
olution, management science developed and started to use
more sophisticated mathematical models to analyze and im-
prove processes. operations research (or) emerged as a sci-
entiÔ¨Åc discipline [7]. or employs a broad range of problem-
solving techniques to improve decision-making and process
performance, e.g., simulation, mathematical optimization,
queueing theory, and markov decision processes. most of
these techniques involve the construction of (mathematical)
models that attempt to describe the underlying processes.
hence, or is an essential ingredient of process science.
over time, information systems (rather than people)
started to ‚Äúrun‚Äù the operational processes. see, for exam-
ple, the spectacular development that has brought sap , a
german corporation founded in 1972, to become the multi-
national world leader in enterprise software for managing
business operations. today, most large organizations use
enterprise software from vendors like sap , oracle, ibm, hp ,
and microsoft. information systems manage and store data
and these are used to support and automate processes .
process descriptions (implicit or explicit) are used to
create products and deliver services. as a result, process
modeling has become of the utmost importance. process
models assist in managing complexity by providing insights
and by documenting procedures. information systems need
to be conÔ¨Ågured and driven by precise instructions. cross-
organizational processes can only function properly if there
is agreement on the required interactions. as a result, pro-
cess models are widely used in today‚Äôs organizations [8].
the importance of process models is also reÔ¨Çected by the
uptake of workÔ¨Çow management (wfm) and business process
management (bpm) systems [8], [9], [10]. these are based on
ideas already present in the early ofÔ¨Åce information systems .
already in the seventies, people like skip ellis, anatol holt,
and michael zisman worked on ofÔ¨Åce information systems
driven by explicit process models [8]. ellis et al. developed
ofÔ¨Åce automation prototypes such as ofÔ¨Åcetalk-zero and
ofÔ¨Åcetalk-d at xerox parc in the late 1970s. these systems
used variants of petri nets to model processes. another ex-
ample from the same period is scoop (system for comput-
erizing of ofÔ¨Åce processes), developed by michael zisman.
scoop also used petri nets to represent business processes.
ofÔ¨Åcetalk, scoop and other ofÔ¨Åce information systems
were created in a time where workers were typically not
connected to some network. hence, these systems were not
widely adopted. nevertheless, the vision realized in today‚Äôs
bpm systems was already present.
in the mid-nineties there was the expectation that wfmsystems would get a role comparable to database manage-
ment (dbm) systems [9]. there was indeed a burst of wfm
systems offered by a range of vendors. however, wfm
systems were not widely adopted and did not manage to
become and integral part of most information systems (like
dbm systems). the early wfm systems were focusing too
much on automation, not acknowledging the management
aspects and the need for Ô¨Çexibility. moreover, processes can
also be captured in conventional programming languages.
hence, workÔ¨Çows are often hidden in code.
bpm can be seen as an extension of workÔ¨Çow manage-
ment (wfm) [8]. wfm primarily focuses on the automation
of business processes, whereas bpm has a broader scope:
from process automation and process analysis to operations
management and the organization of work. on the one
hand, bpm aims to improve operational business processes,
possibly without the use of new technologies. for example,
by modeling a business process and analyzing it using
simulation, management may get ideas on how to reduce
costs while improving service levels. on the other hand,
bpm is often associated with software to manage, control,
and support operational processes.
despite the availability of wfm/bpm systems, process
management is not ‚Äúsubcontracted‚Äù to such systems at
a scale comparable to dbm systems. the application of
‚Äúpure‚Äù wfm/bpm systems is still limited to speciÔ¨Åc in-
dustries such as banking and insurance. process management
turned out to be much more ‚Äúthorny‚Äù than data management.
bpm is multifaceted, complex, and difÔ¨Åcult to demarcate. given
the variety in requirements and close connection to business
concerns, it is often impossible to use generic bpm/wfm
solutions. therefore, bpm functionality is often embedded
in other systems. moreover, bpm techniques are frequently
used in concert with conventional information systems.
process science, ranging from scientiÔ¨Åc management and
operations research to wfm and bpm, is relevant for any
organization. considering the developments over the last
century, productivity increased dramatically thanks to novel
management principles, analysis techniques, and our ability
to develop information systems that support and automate
processes.
process modeling and the analysis of process models
both play an important role in process science. hence, this
brief history of process science would be incomplete without
acknowledging this aspect [8]. over time, many process
modeling techniques have been proposed. in fact, the well-
known turing machine described by alan turing (1912-
1954) can be viewed as a process model. it was instrumental
in showing that many questions in computer science are
undecidable. moreover, it added a data component (the
tape) to earlier transition systems. also markov chains,
named after andrey markov (1856-1922), can be viewed as
simple process models. however, turing machines, transi-
tion systems, and markov chains do not capture concur-
rency explicitly. petri nets, proposed by carl adam petri
(1926-2010) in 1962, were the Ô¨Årst formalism able to model
concurrency. petri nets play a prominent role in bpm: they
are graphical and most of the contemporary bpm notations
and systems use token-based semantics adopted from petri
nets. concurrency is very important as in business processes
many things may happen in parallel. many cases may beieee transactions on services computing, vol. 1, no. 1, september 2015 4
handled at the same time and resources may operate inde-
pendently. even within a case there may be various enabled
or concurrently running activities. therefore, systems and
notations should support concurrency natively.
the analysis of process models should not be confused
with the analysis of the process itself. consider for example
veriÔ¨Åcation techniques (e.g., using model checking). these
are more concerned with the internal consistency of the
model (or software) rather than the performance of the pro-
cess. also simulation, queueing theory and markov analysis
only analyze models of the process rather than the process
itself. hence, the validity of the analysis results heavily
depends on the faithfulness of the model. by exploiting
event data generated by processes, the connection between
model and reality can be ensured (cf. section 4).
this concludes our brief, and very incomplete, history
of process science. we skipped many developments and
approaches in process science, e.g., business process reengi-
neering (bpr) [11], six sigma [12] and case management
(cm) [13]. the many different ingredients illustrate the
broadness and complexity of the discipline.
3 a b rief history of datascience
data science emerged as new discipline, simply because
of the torrents of data that are collected about anything ,
at any time , and at any place . the overall volume of data
recorded by mankind has been growing for decades [14],
so in principle nothing is new. however, in many appli-
cation domains it seems that a ‚Äútipping point‚Äù has been
reached. gartner uses the phrase ‚Äúthe nexus of forces‚Äù
to refer to the convergence and mutual reinforcement of
four interdependent trends: social (people want to connect,
share, work and interact), mobile (people want do this at
any location and at any time using multiple devices), cloud
(people want to access any piece of data from anywhere),
and information (people want to consume and generate infor-
mation) [15]. this is directly impacting the way individuals,
organizations, and societies interact, work, and do business
[2]. data science is not limited to ‚Äúbig data‚Äù: also smaller
data sets that easily Ô¨Åt on a single hard-drive may hold
valuable information potentially changing the future of an
organization. at the same time, also ‚Äúsmall data‚Äù may be
incredibly difÔ¨Åcult to analyze (from both a computational
and comprehension perspective).
data science aims to use different data sources to answer
questions grouped into the following four categories [1]:
reporting: what happened?
diagnosis: why did it happen?
prediction: what will happen?
recommendation: what is the best that can happen?
obviously, the above questions are highly generic illustrat-
ing the broadness of the data science discipline.
over time, many alternative deÔ¨Ånitions of data science
have been suggested [16]. in july 1966, turing award winner
peter naur proposed the term datalogy in a letter to the
editor of the communications of the acm. he deÔ¨Åned
datalogy as the ‚Äúscience of the nature and use of data‚Äù and
suggested to use this term rather than ‚Äúcomputer science‚Äù.
peter naur also used the term ‚Äúdata science‚Äù long beforeit was in vogue. in [17], naur writes: ‚Äúa basic principle
ofdata science , perhaps the most fundamental that may
be formulated, can now be stated: the data representation
must be chosen with due regard to the transformation to be
achieved and the data processing tools available‚Äù. the book
from 1974 also has two parts considering ‚Äúlarge data‚Äù: ‚Äúpart
5 - processes with large amounts of data‚Äù and ‚Äúpart 6 -
large data systems‚Äù. in the book, ‚Äúlarge amounts of data‚Äù
are all data sets that cannot be stored in working memory.
the maximum capacity of magnetic disk stores considered
in [17] ranges between 1.25 and 250 megabytes. not only
the disks are orders of magnitude smaller than today‚Äôs
disks, also the notion of what is ‚Äúlarge/big‚Äù has changed
dramatically since the early seventies. some of the core
principles of data processing may have remained invariant.
however, in the early seventies it was impossible to imagine
the amounts of data being collected today. our ability to
store and process data have developed in spectacular way
as reÔ¨Çected by ‚Äúmoore‚Äôs law‚Äù and all of its derivatives.
data science is an amalgamation of different subdisciplines.
different authors will stress different ingredients. here, we
name a few:
statistics : the discipline is typically split into de-
scriptive statistics (to summarize the sample data,
e.g., mean, standard deviation, and frequency) and
inferential statistics (using sample data to estimate
characteristics of all data or to test a hypothesis).
clearly, data science has its roots in statistics.
data mining : in [18] data mining is deÔ¨Åned as ‚Äúthe
analysis of (often large) data sets to Ô¨Ånd unsuspected
relationships and to summarize the data in novel
ways that are both understandable and useful to the
data owner‚Äù. the input data is typically given as
a table and the output may be rules, clusters, tree
structures, graphs, equations, patterns, etc. clearly,
data mining builds on statistics, databases, and al-
gorithms. compared to statistics, the focus is on
scalability and practical applications.
machine learning : the difference between data mining
and machine learning is not a clear-cut. the Ô¨Åeld
of machine learning emerged from within artiÔ¨Åcial
intelligence (ai) with techniques such as neural net-
works. here, we use the term machine learning to
refer to algorithms that give computers the capabil-
ity to learn without being explicitly programmed.
to learn and adapt, a model is built from input
data (rather than using Ô¨Åxed routines). the evolving
model is used to make data-driven predictions or
decisions.
process mining : adds the process perspective to ma-
chine learning and data mining. starting point is
event data that are related to process models, e.g.,
models are discovered from event data or event data
are replayed on models to analyze compliance and
performance.
stochastics : stochastic systems and processes behave
in an unpredictable manner due to the inÔ¨Çuence of
random variables. the stochastics discipline aims to
estimate properties (e.g., Ô¨Çow time) in such uncertain
contexts. examples are markov models and queue-ieee transactions on services computing, vol. 1, no. 1, september 2015 5
data 
mining
process
mining
visualization
data 
sciencebehavioral /
social
sciences
domain
knowledgemachine 
learning
large scale 
distributed
computingstatistics industrial
engineeringdatabases
stochastics
privacyalgorithms
visual 
analytics
formal 
methods
bpm /wfm
concurrencyscientific 
management
process 
science
model
checkingoperations 
research
fig. 3. process mining as the linking pin between process science and data science.
ing networks whose parameters can be estimated
from data.
databases : no data science without data. hence, the
database discipline forms one of the cornerstones
of data science. database management (dbm) sys-
tems serve two purposes: (1) structuring data so
that they can be managed easily and (2) providing
scalability and reliable performance so that applica-
tion programmers do not need to worry about data
storage. until recently, relational databases and sql
(structured query language) were the norm. due to
the growing volume of data, massively distributed
databases and so-called nosql databases emerged.
moreover, in-memory computing (cf. sap hana)
can be used to answer questions in real-time.
algorithms : to handle large amounts of data and to
answer complex questions, it is vital to provide al-
gorithms that scale well. algorithmic improvements
allow for the analysis of problems that are orders
of magnitude larger than before. consider for exam-
ple distributed algorithms based on the mapreduce
paradigm or algorithms that provide approximate
results in a fraction of time.
large scale distributed computing : sometimes problems
are too large to be solved using a single computer.
moreover, data need to be moved around from one
location to another. this requires a powerful dis-
tributed infrastructure. apache hadoop is an ex-
ample of open-source software framework tailored
towards large scale distributed computing.
visualization and visual analytics : data and process
mining techniques can be used to extract knowledge
from data. however, if there are many ‚Äúunknown
unknowns‚Äù (things we don‚Äôt know we don‚Äôt know),analysis heavily relies on human judgment and di-
rect interaction with the data. the perception capa-
bilities of the human cognitive system can be ex-
ploited by using the right visualizations [19]. visual
analytics, a term coined by jim thomas, combines
automated analysis techniques with interactive visu-
alizations for an effective understanding, reasoning
and decision making on the basis of very large and
complex data sets [20].
behavioral/social sciences : behavioral science is the sys-
tematic analysis and investigation of human behav-
ior. social sciences study the processes of a social sys-
tem and the relationships among individuals within
a society. to interpret the results of various types
of analytics it is important to understand human
behavior and the social context in which humans
and organizations operate. moreover, analysis results
often trigger questions related to coaching and posi-
tively inÔ¨Çuencing people.
industrial engineering : data science is often applied
in a business context where processes should be as
efÔ¨Åcient and effective as possible. this may require
knowledge of accounting, logistics, production, Ô¨Å-
nance, procurement, sales, marketing, warehousing,
and transportation.
privacy and security : privacy refers to the ability to
seclude sensitive information. privacy partly over-
laps with security which aims to ensure the con-
Ô¨Ådentiality, integrity and availability of data. data
should be accurate and stored safely, not allowing
for unauthorized access. privacy and security need to
be considered carefully in data science applications.
individuals need to be able to trust the way data is
stored and transmitted.ieee transactions on services computing, vol. 1, no. 1, september 2015 6
organizations , devices , people ,  
processes , ...
conformance 
checking
process 
discoveryevent
data 
event
data 
performance 
analysisextract , load , 
transform event 
data from 
operational 
processeselt
modeling , i.e., 
creating normative 
or descriptive 
process models
hand -made 
and 
automatically 
discovered 
models
event
data !!! deviation
!!! bottleneck
01001001
11001101
00111010
finding out 
what really 
happens in 
real-life 
processes
understanding real -life 
process performance 
(e.g., bottlenecks )understanding and 
quantifying 
deviations
fig. 4. process mining overview showing three common types of process mining process discovery, conformance checking, and performance
analysis.
ethics : next to concrete privacy and security breaches
there may be ethical notions related to ‚Äúgood‚Äù and
‚Äúbad‚Äù conduct. not all types of analysis possible are
morally defendable. for example, mining techniques
may favor particular groups (e.g., a decision tree may
reveal that it is better to give insurance to white
middle-aged males and not to elderly black females).
moreover, due to a lack of sufÔ¨Åcient data, minority
groups may be wrongly classiÔ¨Åed. discrimination-
aware data mining techniques address this problem
by forcing the outcomes to be ‚Äúfair‚Äù [21].
the above list nicely illustrates the interdisciplinary na-
ture of data science.
4 p rocess mining : bridging the gap
process mining techniques can be used to extract process-
related information from event logs [3]. they can be seen
as part of the broader data science and process science
disciplines. in fact, as figure 3 shows, process mining is
the linking pin between both disciplines. process mining
seeks the confrontation between event data (i.e., observed
behavior) and process models (hand-made or discovered
automatically). the interest in process mining is rising as is
reÔ¨Çected by the growing numbers of publications, citations
and commercial tools (disco, aris ppm, qpr, celonis, snp ,
minit, myinvenio, perceptive, etc.). in the academic world,
prom is the de-facto standard (www.processmining.org)
and research groups all of the world have contributed to
the hundreds of prom plug-ins available.
the starting point for any process mining effort is a
collection of events commonly referred to as an event log
(although events can also be stored in a database). each
event corresponds to:
acase (also called process instance), e.g., an order
number, a patient id, or a business trip,anactivity , e.g., ‚Äúevaluate request‚Äù or ‚Äúinform cus-
tomer‚Äù,
atimestamp , e.g., ‚Äú2015-09-23t06:38:50+00:00‚Äù,
additional (optional) attributes such as the resource
executing the corresponding event, the type of event
(e.g., start, complete, schedule, abort), the location of
the event, or the costs of an event.
event logs can be used for a wide variety of process mining
techniques. here we focus on the three main types of
process mining: process discovery ,conformance checking , and
performance analysis (see figure 4).
the Ô¨Årst type of process mining is discovery . a discov-
ery technique takes an event log and produces a process
model (e.g., petri net or bpmn model) without using any a
priori information. process discovery is the most prominent
process-mining technique. techniques ranging from the ba-
sic alpha algorithm [3] to sophisticated inductive mining
techniques [22] are available. for many organizations it is of-
ten surprising to see that these techniques are indeed able to
discover real processes merely based on example behaviors
stored in event logs. the discovered process models provide
valuable insights, and provide the basis for other types
of process mining (bottleneck analysis, deviation analysis,
predictions, etc.).
the second type of process mining is conformance check-
ing[3], [23]. here, an existing process model is compared
with an event log of the same process. conformance check-
ing can be used to check if reality, as recorded in the log,
conforms to the model and vice versa. the process model
used as input may be hand-made or discovered. to check
compliance often a normative hand-crafted model is used.
however, to Ô¨Ånd exceptional cases, one often uses a dis-
covered process model showing the mainstream behavior. it
is also possible to ‚Äúrepair‚Äù process models based on event
data.
state-of-the-art conformance checking techniques create
so-called alignments between the observed event data andieee transactions on services computing, vol. 1, no. 1, september 2015 7
the process model [23]. in an alignment each case is mapped
onto the closest path in the model, also in case of deviations
and non-determinism. as a result, event logs can always be
replayed on process models. this holds for both discovered
processes and hand-made models. the ability to replay
event data on models and the availability of timestamps
enables the third type of process mining: performance analy-
sis. replay may reveal bottlenecks in the process. moreover,
the tight coupling between model and data also helps to
Ô¨Ånd root causes for performance problems. the combination
of event data with process-centric visualizations enables
novel forms of performance analyses that go far beyond
traditional business intelligence (bi) tools.
5 a pplying big-datatechniques to process
mining
the growing availability of data over the last two decades
has given rise to a number of successful technologies,
ranging from data collection and storage infrastructures
to hardware and software tools for analytics computation
and efÔ¨Åcient implementations of analytics. recently, this
technology ecosystem has undergone some radical change
due to the advent of big data techniques [24]. a major
promise of big data is enabling ‚Äúfull data‚Äù analysis [25]), i.e.,
the computation of analytics on all available data points, as
opposed to doing so on selected data samples. in the case
of process mining, ‚Äúgoing full data‚Äù means that the process
miner will be able to analyse huge amounts of event data,
obtaining a complete picture of the process including all
possible variations and exceptions.
5.1 mapreduce in the context of process mining
big data technology often relies on mapreduce [26], a ba-
sic computational paradigm, which has been remarkably
successful in handling the heavy computational demands
posed by huge data sets. mapreduce models a parallel
computation as a sequence of rounds, each of which consists
in computing a map and a reduce function over lists of
(key, value) pairs. this paradigm is reminiscent of classic
functional programming inasmuch access to input data is
entirely by value : any change to data values that takes
place on the computational nodes does not automatically
propagate back to the original data entries. communication
occurs by generating new (key, value) lists which are then fed
into the next round of execution.
unlike what happens in classic functional programming,
however, in mapreduce output lists need not have the same
cardinality as the input ones. rather, the map function maps
its input list into an arbitrary (but usually lower) number
of values. even more, the reduce function usually turns
a large list of pairs into one (or a few) output values.
mapreduce performance gain comes from the fact that all
of the map output values are not reduced by the same
reduce node. rather, there are multiple reducers, each of
which receives a list of (key, value) pairs having the same key,
and computes its reduce function on it independently of
reduce operations taking place at other nodes. the parallel
operation of mapreduce is summarized in figure 5.
in principle, the mapreduce paradigm looks well suited
to implementing process mining algorithms. nevertheless,some issues still stand in the way of mapreduce implemen-
tations of pm techniques. indeed, mapreduce implemen-
tations of process mining algorithms like alpha miner, the
flexible heuristics miner (fhm), and the inductive miner
[27] have been described only recently [28], [29].
let us now focus on what needs to be done to use
mapreduce in the context of process mining. most open
issues regard the map function. mapreduce implementa-
tions of process mining algorithms should compute keys
based on datatypes that are available within (or reachable
from) process log entries, such as task, location and operator
ids or timestamps. in other words, the map phase should
put under the same key all process data that must be
considered together when performing the Ô¨Ånal computation
(the reduce phase).
to understand why this is still an issue, let us consider a
mapreduce program that computes the average (and the
standard deviation) of a process‚Äô wall clock duration for
different age intervals of the human operators who carry it
out. this simple computation can be performed in a single
round. firstly, the program will map the list of (operator id,
process duration) pairs into a list of (age bucket code, pro-
cess duration) . then, it will use the (age bucket code) to route
(key, value) pairs to the reduce nodes. thirdly, each reduce
node will compute the average and standard deviation of its
own (key, value) pairs. this computation is straightforward
because the map function is nothing else than a look-up
table mapping employee ids to age buckets. however, it
is easy to see that if the process miner wanted to perform
the same computation by skill/competence level rather than
by age, computing the keys would become more complex,
possibly involving inferences over equivalent competences.
in case of complex mappings, the impact of the map
function on the total computation time can become very
high, especially in the case of multiple rounds. research
is therefore needed to develop efÔ¨Åcient map functions for
process mining, involving pre-computation of keys and
even smaller scale parallelization.
another open issue regards load balancing . the over-
all performance of mapreduce is highly dependent on
carefully balancing the size of the lists to be handled at
reduce nodes. such balancing can sometimes be achieved
by writing intelligent map functions. in our example, an
intelligent map function could consider the distribution of
age among personnel, and send to the same reduce node
the age buckets that are known to hold few employees,
while reserving an entire node for larger buckets. however,
estimating (e.g., from process model analysis) or learning
(e.g., by preliminary sampling event streams) the cardinality
distribution of keys is by no means straightforward.
some preliminary work has been done [30] on how to
use domain taxonomies to improve the efÔ¨Åciency of mapre-
duce. basically, if a set of events has been tagged using a
taxonomy, a map function can be written that generates keys
corresponding to taxonomy subtrees of different depths, to
generate lists of the same cardinality. for instance, let us
assume that we want to tag process events that encode calls
to a help desk by using a simple taxonomy of activities
rooted in request . our taxonomy includes two subtrees,
rooted respectively in repair andreplacement . if we
know (or can guess) that there will be nearly as many callsieee transactions on services computing, vol. 1, no. 1, september 2015 8
fig. 5. the gist of mapreduce computational paradigm: all (key, value) pairs with the same key are sent to the same reduce node
to be tagged with each child of replacement as there are
with repair , we can use each child of replacement (e.g.
batterysubstitution ) to generate a key and repair to
generate another, balancing the cardinalities of (key, value)
lists to be processed by mapreduce. however, estimating
a priori the ‚Äúright‚Äù depth of each subtree to be chosen for
balancing the lists is not straightforward.
in the future, we hope to see tools for deÔ¨Åning map
and reduce functions on the basis of the business process
model data types, of the relations among them and of other
semantics-rich context information [31]. new techniques
are also needed for exploiting semantics to balance load
at the reduce nodes. the process miner should be helped
to write key-generation rules that automatically generate
the same number of pairs for each key value, achieving
balanced parallelization. support for semantics-aware, intel-
ligent map functions is still very patchy in current big data
toolkits. indeed, most of the initial big data applications
were targeted to unstructured processes (such as social
media conversations), and many big data environments do
not yet support process models.
in terms of implementation, map functions can be easily
scripted to include simple conformance checks (section 4)
like spotting illegal event ids. unfortunately, scripting com-
plex conformance checks may prove trickier. an initial at-
tempt at automating checks within big data tools was made
by ibm within its business insight toolkit (bitkit) [32]. via
its bigsql utility, ibm insight allows for deÔ¨Åning a pseudo-
sql schema and to write pseudo-sql queries against it that
are automatically compiled for hadoop as parallel compu-
tations. so, a business process miner could potentially spot
a business process model violation simply by writing such
a pseudo-sql query. however, the expressive power of this
approach is conÔ¨Åned to the simple queries one can write in
pseudo-sql.
recently, big data technology has focused on building
systems supporting a declarative, workÔ¨Çow-oriented ap-
proach to executing mapreduce computations. to mention
but a few, these systems include apache oozie, azkaban
(originated at linkedin) and luigi (developed at spotify).
these tools can be seen as potentially ‚Äúbpm-friendly‚Äù in
the sense of modelling big data analytics itself as a work-
Ô¨Çow and allowing the process miner to insert user-deÔ¨Åned
modules in the data analysis pipeline. however, they do
not yet include libraries for supporting standard processmodel checks, or other recurrent steps of bpm. apache
flink, developed within the stratosphere research project
[33] looks promising for process mining applications. flink
combines stream processing and mapreduce computations,
and supports inserting user deÔ¨Åned functions, as well as
complex data types.
last but not least, a key requirement of process mining
is automated support for anonymization , also known as de-
identiÔ¨Åcation , to protect privacy of personal data showing up
in process logs [34] and preventing unauthorized uses of
process-based metrics for worker discrimination. in the past
decade, many computational notions of de-identiÔ¨Åcation
such as k-anonymity ,l-diversity , and t-closeness have been
introduced to customize privacy-preserving data mining to
the requirements of speciÔ¨Åc domains [35]. however, unlike
some bpm data preparation tools [36], big data import
toolkits do not yet support automated anonymization [37].
in particular, it is still unclear whether enforcement of pri-
vacy, trustworthiness and access control on process events is
better done before mapping events to generate key values,
or as part of the mapping, automatically bringing process
(key, value) pairs to the granularity and detail level compati-
ble with the privacy preferences and regulation compliance
requirements of the process owners.
5.2 cloud-based deployment for process mining
conventional process mining tools are often deployed on
the process owners‚Äô premises. however, cloud-based de-
ployment seems a natural choice for process-mining-as-
a-service. indeed, the elasticity of the cloud provisioning
model [38] has suggested since long to deliver mapreduce
on virtual clusters that can ‚Äúscale out‚Äù, requesting additional
cloud resources when needed [39]. on the process analysis
side, some process mining tools like zeus [40] already in-
cluded a scale-out capability on an internal grid architecture.
elastic provisioning of mapreduce clusters promises to add
two key features to pm tools:
co-provisioning of data and analytics, i.e. run-time,
computer-assisted choice of matching data represen-
tation and process mining algorithms according to
user requirements.
negotiable service level agreements (slas), in
terms of delivery time and accuracy of the process
mining results.ieee transactions on services computing, vol. 1, no. 1, september 2015 9
in the long term, performing process analysis on public
clouds in co-tenancy with other process owners should
facilitate cross-organizational process mining, encouraging
organizations to learn from each other and improve their
processes [41]. however, it is important to remark that
deploying mapreduce on virtual (as opposed to physical)
machines in a cloud infrastructure does not automatically
exploit the elasticity that cloud computing can offer, because
there is a non-trivial semantic gap that still needs to be
bridged.
as we have discussed in section 5.1, mapreduce par-
allelization is based on nodes that independently compute
local instances of the map and reduce functions. however,
today‚Äôs cloud resource managers do not ‚Äúsee‚Äù these nodes;
rather, they allocate computation units as virtual machines
(vms), which in turn are dynamically assigned to physical
blades. as intuition suggests, lousy node-to-vm and vm-to-
blade allocations could impair all the efforts made to achieve
balanced key-to-node assignments in the mapreduce com-
putation. in terms of the simple example we introduced
in section 5.1, what would be the point of a devising a
balanced assignment of age buckets to reduce nodes if these
nodes would get randomly mapped to cloud vms (and vms
to blades)?
work is currently underway [42] on designing Ô¨Çexible
mapreduce runtimes that seamlessly integrate semantics-
aware reduce functions with cloud resource allocation, but
much remains to be done before this goal is fully reached.
6 c onclusion
companies and organizations worldwide are increasingly
aware of the potential competitive advantage they could get
by timely and accurate ‚Äúfull data‚Äù process mining based
on (1) sophisticated discovery and visualization techniques
and (2) the bigdata computational paradigm. however, as
highlighted in this paper, several research and technology
challenges remain to be solved before process mining, data
science and big data technologies can seamlessly work
together.
the special issue ‚Äúprocess analysis meets big data‚Äù of
the ieee transactions on services computing features some
notable contributions toward overcoming the hurdles that
still prevent us from reaping the full beneÔ¨Åts of big data
techniques in process mining. in this extended editorial
paper, we have discussed the relation between process and
data science, identiÔ¨Åed some of the remaining difÔ¨Åculties
and outlined a research strategy that we believe should
underlie the community‚Äôs efforts toward full integration of
data and process science. hopefully, this vision will soon
bring about scalable process mining analytics on top of big
data toolkits, while enabling them to be easily tailored to
domain-speciÔ¨Åc requirements.
further research is also needed to deliver the software
environment for this to take place, possibly taking advan-
tage of the available ‚Äúarchitectural glue‚Äù to integrate process
mining modules in the big data pipeline. hopefully, once
the marriage between big data and process mining has
become a reality, process mining services will become more
affordable, driving costs of process-aware analytics well
within reach of organizations that do not have the expertise
and budget for it today.references
[1] w. van der aalst, ‚Äúdata scientist: the engineer of the future,‚Äù in
proceedings of the i-esa conference , ser. enterprise interoperability,
k. mertins, f. benaben, r. poler, and j. bourrieres, eds., vol. 7.
springer-verlag, berlin, 2014, pp. 13‚Äì28.
[2] j. manyika, m. chui, b. brown, j. bughin, r. dobbs, c. roxburgh,
and a. byers, ‚Äúbig data: the next frontier for innovation, com-
petition, and productivity,‚Äù 2011, mckinsey global institute.
[3] w. van der aalst, process mining: discovery, conformance and en-
hancement of business processes . springer-verlag, berlin, 2011.
[4] w. van der aalst, ‚Äúprocess mining as the superglue between
data science and enterprise computing,‚Äù in ieee international
enterprise distributed object computing conference (edoc 2014) ,
m. reichert, s. rinderle-ma, and g. grossmann, eds. ieee
computer society, 2014, pp. 1‚Äì1.
[5] f. taylor, the principles of scientiÔ¨Åc management . harper and both-
ers publishers, new york, 1919.
[6] bundesministerium f ¬®ur bildung und forschung, industrie 4.0:
innovationen f¬® ur die produktion von morgen . bmbf, bonn, ge-
many, 2014, http://www.bmbf.de/pub/broschuere industrie-4.
0-gesamt.pdf.
[7] j. moder and s. elmaghraby, handbook of operations research:
foundations and fundamentals . van nostrand reinhold,new york,
1978.
[8] w. van der aalst, ‚Äúbusiness process management: a com-
prehensive survey,‚Äù isrn software engineering , pp. 1‚Äì37, 2013,
doi:10.1155/2013/507984.
[9] w. van der aalst and k. van hee, workÔ¨Çow management: models,
methods, and systems . mit press, cambridge, ma, 2002.
[10] m. dumas, m. rosa, j. mendling, and h. reijers, fundamentals of
business process management . springer-verlag, berlin, 2013.
[11] m. hammer and j. champy, reengineering the corporation . nicolas
brealey publishing, london, 1993.
[12] t. pyzdek, the six sigma handbook: a complete guide for green
belts, black belts, and managers at all levels . mcgraw hill, new
york, 2003.
[13] w. van der aalst, m. weske, and d. gr ¬®unbauer, ‚Äúcase handling: a
new paradigm for business process support,‚Äù data and knowledge
engineering , vol. 53, no. 2, pp. 129‚Äì162, 2005.
[14] m. hilbert and p . lopez, ‚Äúthe world‚Äôs technological capacity to
store, communicate, and compute information,‚Äù science , vol. 332,
no. 6025, pp. 60‚Äì65, 2011.
[15] c. howard, d. plummer, y. genovese, j. mann, d. willis, and
d. smith, ‚Äúthe nexus of forces: social, mobile, cloud and infor-
mation,‚Äù 2012, http://www.gartner.com.
[16] g. press, ‚Äúa very short history of data science,‚Äù forbes
technology, http://www.forbes.com/sites/gilpress/2013/05/
28/a-very-short-history-of-data-science/, 2013.
[17] p . naur, concise survey of computer methods . studentlitteratur
lund, akademisk forlag, kobenhaven, 1974.
[18] d. hand, h. mannila, and p . smyth, principles of data mining . mit
press, cambridge, ma, 2001.
[19] j. wijk, ‚Äúthe value of visualization,‚Äù in visualization 2005 . ieee cs
press, 2005, pp. 79‚Äì86.
[20] d. keim, j. kohlhammer, g. ellis, and f. mansmann, eds., mas-
tering the information age: solving problems with visual analytics .
vismaster, http://www.vismaster.eu/book/, 2010.
[21] d. pedreshi, s. ruggieri, and f. turini, ‚Äúdiscrimination-aware data
mining,‚Äù in proceedings of the 14th acm sigkdd international
conference on knowledge discovery and data mining . acm, 2008,
pp. 560‚Äì568.
[22] s. leemans, d. fahland, and w. van der aalst, ‚Äúscalable process
discovery with guarantees,‚Äù in enterprise, business-process and
information systems modeling (bpmds 2015) , ser. lecture notes in
business information processing, k. gaaloul, r. schmidt, s. nur-
can, s. guerreiro, and q. ma, eds., vol. 214. springer-verlag,
berlin, 2015, pp. 85‚Äì101.
[23] w. van der aalst, a. adriansyah, and b. van dongen, ‚Äúreplaying
history on process models for conformance checking and per-
formance analysis,‚Äù wires data mining and knowledge discovery ,
vol. 2, no. 2, pp. 182‚Äì192, 2012.
[24] o. terzo, p . ruiu, e. bucci, and f. xhafa, ‚Äúdata as a service (daas)
for sharing and processing of large data collections in the cloud,‚Äù
incomplex, intelligent, and software intensive systems (cisis), 2013
seventh international conference on , july 2013, pp. 475‚Äì480.ieee transactions on services computing, vol. 1, no. 1, september 2015 10
[25] t. pfeiffer, ‚Äútowards distributed intelligence using edge-heavy
computing,‚Äù 2015, http://ec.europa.eu/information society/
newsroom/cf/dae/document.cfm?action=display&doc id=
7682/.
[26] j. dean and s. ghemawat, ‚Äúmapreduce: simpliÔ¨Åed data process-
ing on large clusters,‚Äù communications of the acm , vol. 51, no. 1,
pp. 107‚Äì113, jan. 2008.
[27] w. van der aalst, t. weijters, and l. maruster, ‚ÄúworkÔ¨Çow mining:
discovering process models from event logs,‚Äù knowledge and data
engineering, ieee transactions on , vol. 16, no. 9, pp. 1128‚Äì1142, sept
2004.
[28] j. evermann, ‚Äúscalable process discovery using map-reduce,‚Äù
services computing, ieee transactions on , vol. pp , no. 99, pp. 1‚Äì1,
2014.
[29] s. hernandez, s. zelst, j. ezpeleta, and w. van der aalst, ‚Äúhan-
dling big(ger) logs: connecting prom 6 to apache hadoop,‚Äù in
proceedings of the bpm2015 demo session , ser. ceur workshop
proceedings, vol. 1418. ceur-ws.org, 2015, pp. 80‚Äì84.
[30] e. jahani, m. j. cafarella, and c. r ¬¥e, ‚Äúautomatic optimization for
mapreduce programs,‚Äù proceedings of the vldb endowment , vol. 4,
no. 6, pp. 385‚Äì396, mar. 2011.
[31] l. bodenstaff, e. damiani, p . ceravolo, c. fugazza, and k. reed,
‚Äúrepresenting and validating digital business processes,‚Äù in web
information systems and technologies , lecture notes in business
information processing, j. filipe and j. cordeiro, eds. springer
berlin heidelberg, 2008, vol. 8, pp. 19‚Äì32.
[32] h. ossher, r. bellamy, d. amid, a. anaby-tavor, m. callery,
m. desmond, j. de vries, a. fisher, t. frauenhofer, s. krasikov,
i. simmonds, and c. swart, ‚Äúbusiness insight toolkit: flexible
pre-requirements modeling,‚Äù in software engineering - companion
volume, 2009. icse-companion 2009. 31st international conference
on, may 2009, pp. 423‚Äì424.
[33] a. alexandrov, r. bergmann, s. ewen, j.-c. freytag, f. hueske,
a. heise, o. kao, m. leich, u. leser, v . markl, f. naumann,
m. peters, a. rheinl ¬®ander, m. j. sax, s. schelter, m. h ¬®oger,
k. tzoumas, and d. warneke, ‚Äúthe stratosphere platform for big
data analytics,‚Äù the vldb journal , vol. 23, no. 6, pp. 939‚Äì964, dec.
2014.
[34] k. e. emam, f. dankar, r. issa, e. jonker, d. amyot, e. cogo, j.-
p . corriveau, m. walker, s. chowdhury, r. vaillancourt, t. roffey,
and j. bottomley, ‚Äúa globally optimal k-anonymity method for the
de-identiÔ¨Åcation of health data,‚Äù journal of the american medical
informatics association , vol. 16, no. 5, 2009.
[35] a. cavoukian and j. jonas, ‚Äúprivacy by design in the age
of big data,‚Äù ofÔ¨Åce of the information and privacy commis-
sioner, 2012, https://privacybydesign.ca/content/uploads/2012/
06/pbd-big data.pdf.
[36] c. gunther and w. van der aalst, ‚Äúa generic import framework
for process event logs,‚Äù in proceedings of business process manage-
ment workshops , sept. 2006, pp. 81‚Äì92.
[37] a. cavoukian and d. castro, ‚Äúbig data and innova-
tion, setting the record straight: de-identiÔ¨Åcation does
work,‚Äù ofÔ¨Åce of the information and privacy commissioner,
2014, https://www.privacybydesign.ca/content/uploads/2014/
06/pbd-de-identiÔ¨Åcation itif1.pdf.[38] m. armbrust, a. fox, r. grifÔ¨Åth, a. d. joseph, r. katz, a. konwin-
ski, g. lee, d. patterson, a. rabkin, i. stoica, and m. zaharia, ‚Äúa
view of cloud computing,‚Äù communications of the acm , vol. 53,
no. 4, pp. 50‚Äì58, apr. 2010.
[39] c. ardagna, e. damiani, f. frati, g. montalbano, d. rebeccani,
and m. ughetti, ‚Äúa competitive scalability approach for cloud
architectures,‚Äù in cloud computing (cloud), 2014 ieee 7th inter-
national conference on , june 2014, pp. 610‚Äì617.
[40] a. azzini and p . ceravolo, ‚Äúconsistent process mining over big
data triple stores,‚Äù in big data (bigdata congress), 2013 ieee
international congress on , june 2013, pp. 54‚Äì61.
[41] w. van der aalst, ‚ÄúconÔ¨Ågurable services in the cloud: supporting
variability while enabling cross-organizational process mining,‚Äù
inon the move to meaningful internet systems: otm 2010 - confed-
erated international conferences , lecture notes in computer science
6426, springer 2010, pp. 8‚Äì25.
[42] d. cheng, j. rao, y. guo, and x. zhou, ‚Äúimproving mapreduce
performance in heterogeneous environments with adaptive task
tuning,‚Äù in proceedings of the 15th international middleware confer-
ence, ser. middleware ‚Äô14. new york, ny, usa: acm, 2014, pp.
97‚Äì108.
wil van der aalst prof.dr.ir. wil van der aalst is a
full professor of information systems at the tech-
nische universiteit eindhoven (tu/e). at tu/e
he is the scientiÔ¨Åc director of the data science
center eindhoven (dsc/e). since 2003 he holds
a part-time position at queensland university of
technology (qut) and is also a member of the
royal netherlands academy of arts and sci-
ences, royal holland society of sciences and
humanities, and academia europaea. his per-
sonal research interests include workÔ¨Çow man-
agement, process mining, petri nets, business process management,
process modeling, and process analysis.
ernesto damiani prof. ernesto damiani is cur-
rently a full professor and director of the in-
formation research security centre at khalifa
university, abu dhabi, uae, where he also leads
the big data initiative of the etisalat british tele-
com innovation centre (ebtic). he is on leave
from universit ¬¥a degli studi di milano, italy, where
he is the director of the sesar research lab.
ernesto holds a visiting position at tokyo denki
university, japan and is a fellow of the japanese
society for the progress of science. his re-
search interests include big data for cyber-security, process discovery
and service-oriented computing.