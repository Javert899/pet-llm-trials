what if process predictions are not
followed by good recommendations?
marcus dees1;2, massimiliano de leoni3,
wil m.p. van der aalst4;2and hajo a. reijers5;2
1uitvoeringsinstituut werknemersverzekeringen (uwv), the netherlands
2eindhoven university of technology, eindhoven, the netherlands
3university of padova, padova, italy
4rwth aachen university, aachen, germany
5utrecht university, utrecht, the netherlands
marcus.dees@uwv.nl, deleoni@math.unipd.it,
wvdaalst@pads.rwth-aachen.de, h.a.reijers@uu.nl
abstract. process-aware recommender systems (par systems) are information
systems that aim to monitor process executions, predict their outcome, and rec-
ommend effective interventions to reduce the risk of failure. this paper discusses
monitoring, predicting, and recommending using a par system within a ﬁnan-
cial institute in the netherlands to avoid faulty executions. although predictions
were based on the analysis of historical data, the most opportune intervention
was selected on the basis of human judgment and subjective opinions. the re-
sults showed that, although the predictions of risky cases were relatively accu-
rate, no reduction was observed in the number of faulty executions. we believe
that this was caused by incorrect choices of interventions. although a large body
of research exists on monitoring and predicting based on facts recorded in his-
torical data, research on fact-based interventions is relatively limited. this paper
reports on lessons learned from the case study in ﬁnance and identiﬁes the need
to develop interventions based on insights from factual, historical data.
keywords: process mining recommender systems prediction intervention
a/b test.
1 introduction
process-aware recommender systems (hereafter shortened as par systems) are a new
breed of information systems. they aim to predict how the executions of process in-
stances are going to evolve in the future, to determine those that have higher chances
to not meet desired levels of performance (e.g., costs, deadlines, customer satisfaction).
consequently recommendations are provided on which effective contingency actions
should be enacted to try to recover from risky executions. par systems are expert sys-
tems that run in the background and continuously monitor the execution of processes,
predict their future, and, possibly, provide recommendations. examples of par systems
are discussed by conforti et al. [1] and schobel et al. [2].
a substantial body of research exists on evaluating risks, also known as process
monitoring and prediction; see, e.g., the surveys by m ´arquez-chamorro et al. [3] and
by teinemaa et al. [4]. yet, as also indicated in [3], “little attention has been given to
copyright © 2019 for this paper by its authors. use permitted under creative 
commons license attribution 4.0 international (cc by 4.0).
2 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
providing recommendations” . in fact, it has often been overlooked how process par-
ticipants would use these predictions to enact appropriate actions to recover from those
executions that have a higher risk of causing problems. it seems that process participants
are tacitly assumed to take the “right decision” for the most appropriate corrective ac-
tions for each case. this also holds for approaches based on mitigation / ﬂexibility “by
design” [5]. unfortunately, the assumption of selecting an effective corrective action
is not always met in reality. when selecting an intervention, this is mainly done based
on human judgment, which naturally relies on the subjective perception of the process
instead of being based on objective facts.
in particular, the par system should analyze the past process executions, and cor-
relate alternative corrective actions with the likelihood of being effective; it should then
recommend the actions that are most likely to decrease risks. otherwise, the positive
occurrence of correctly monitoring a process and making an accurate prediction can be
nulliﬁed by an improper recovery or intervention. an organization will only proﬁt from
using a recommender system if the system is capable of making accurate decisions and
the organization is capable of making effective decisions on the basis of this. much
attention is being paid to making accurate decisions, speciﬁcally to the proper use of
data, measuring accuracy, etc. in this work, we show that the analysis of making effec-
tive decisions is just as important. both parts are essential ingredients of an overall
solution .
this paper reports on a ﬁeld experiment that we conducted within uwv , a dutch
governmental agency. among other things, uwv provides ﬁnancial support to dutch
residents that lose their job and seek a new employment. several subjects (hereafter of-
ten referred to as customers) receive more unemployment beneﬁts than the amount they
are entitled to. although this is eventually detected, it may take several months. using
the uwv’s terminology, a reclamation is created when this happens, i.e. a reclamation
event is raised when a reclamation is detected. to reclaim the amount of unlawfully
provided support is very hard, time-consuming, and, often unsuccessful. in this con-
text, an effective recommender system should be able to detect the customers who are
more likely to get a reclamation and provide operational support to prevent the provi-
sion of beneﬁts without entitlement.research at uwv has shown that the main causes
for reclamations can be attributed to the customer making a mistake when informing
uwv about income received next to their beneﬁts.
to follow up on this idea, we developed a predictor module that relies on machine-
learning techniques to monitor and identify the subjects who are more likely to receive
unlawful support. next, various possible interventions to prevent reclamations were
considered by uwv’s stakeholders. the intervention that was selected to be tested in a
ﬁeld experiment consists of sending a speciﬁc email to the subjects who were suspected
of being at higher risk. the results show that risky customers were detected rather well,
but no signiﬁcant reduction of the number of reclamations was observed. this indicates
that the intervention did not achieve the desired effect, which ultimately means that the
action was not effective in preventing reclamations. our ﬁndings show the importance
of conducting research not only on prediction but also on interventions. this is to ensure
that the par system will indeed achieve the improvements that it aims at, hence creating
process predictions that are followed by good recommendations.
the remainder of this paper is structured as follows. section 2 introduces the situ-
ation faced at uwv and section 3 shows which actions were taken, i.e., the buildingwhat if process predictions are not followed by good recommendations? 3
fig. 1. an example scenario of the potential activities that are related to the provision of the un-
employment beneﬁts for a customer for the months june, july and august (the year is irrelevant).
each row is related to the activities needed to handle an income form for the month of the ben-
eﬁts. each beneﬁts month takes several calendar months to be handled, e.g., the beneﬁts for the
month of june are handled from june until august.
of a par and the execution of a ﬁeld experiment. section 4 discusses the results from
the ﬁeld experiment and section 5 elaborates on the lessons learned from it. section 6
concludes the paper.
2 situation faced – the unemployment beneﬁts process at uwv
uwv is the social security institute of the netherlands and responsible for the imple-
mentation of a number of employee-related insurances. one of the processes that uwv
executes is the unemployment beneﬁts process. when residents in the netherlands be-
come unemployed, they need to ﬁle a request at uwv , which then decides if they are
entitled to beneﬁts. when requests are accepted, the customers receive monthly beneﬁts
until they ﬁnd a new job or until the maximum period for their entitlement is reached.
the unemployment beneﬁt payment process is bound by legal rules. customers and
employees of uwv are required to perform certain steps for each speciﬁc month (here-
after income month) in which customers have an entitlement. fig. 1 depicts a typical
scenario of a customer who receives beneﬁts, with the steps that are executed in each
calendar month. before a customer receives a payment of beneﬁts for an income month,
anincome form has to be sent to uwv . through this form customers specify whether
or not they received any kind of income next to their beneﬁts, and, if so, what amount.
the beneﬁts can be adjusted monthly as a function of any potential income, up to re-
ceiving no beneﬁts if the income exceeds the amount of beneﬁts to which the customer
is entitled.
fig. 1 clearly shows that, in october, when the reclamation is handled, two months
of unemployment beneﬁts have already been paid, possibly erroneously. although this
seems a limited amount (usually a few hundred euros) if one looks at a single customer,
it should be realized that this needs to be multiplied by tens of thousands of customers
in the same situation. uwv has on average 300,000 customers with unemployment
beneﬁts of whom each month on average 4% get a reclamation.
the main cause for reclamations lie with customers not correctly ﬁlling in the
amount of income earned next to their beneﬁts on the income form. the correct amount
can be obtained from the payslip. if the payslip is not yet received by the customer, they
will have to ﬁll in an estimate. however, even with a payslip it is not trivial to ﬁll in the
correct amount. the required amount is the social security wages , which is not equal4 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
to the gross salary and also is not equal to the salary after taxes. an other reason for
not correctly ﬁlling in the income form occurs when a customer is paid every 4-weeks,
instead of every month. in this case there is one month each year with two 4-weekly
payments. the second payment in the month is often forgotten. apart from the reasons
mentioned, there exist many more situations in which it can be hard to determine the
correct amount.
since the reclamations are caused by customers ﬁlling in income forms incorrectly,
the only thing that uwv can do is to try to prevent customers from making mistakes
ﬁlling in the income form. unfortunately, targeting all customers with unemployment
beneﬁts every month to prevent reclamations can become very expensive. furthermore,
uwv wants to limit communications to customers to only the necessary contact mo-
ments. otherwise, communication fatigue can set in with the customers, causing im-
portant messages of uwv to have less impact with the customers. only targeting cus-
tomers with a high chance of getting a reclamation reduces costs and should not in-
ﬂuence the effectiveness of messages of uwv . because of all these reasons, a recom-
mender system that could effectively identify customers with a high risk of getting a
reclamation would be really helpful for uwv . that recommender system needs to be
able to target risky customers and propose opportune interventions.
3 action taken – build par system and execute field
experiment
our approach for the development and test of a par system for uwv is illustrated
in fig. 2. the ﬁrst steps (step 1a and 1b) of the approach are to analyze and identify
the organizational issue. as described in section 2 the organizational issue at uwv is
related to reclamations.
the second step is to develop a recommender system, which consists of a predictor
module (step 2a) and a set of interventions (step 2b). the predictor module is needed
to identify the cases on which the interventions should be applied, namely the cases
with the highest risk to have reclamations. section 3.1 describes the predictor module
setup. together with the predictor module, an appropriate set of interventions needs
to be selected. interventions need to be determined in concert with stakeholders. only
by doing this together, interventions that have the support of the stakeholders can be
identiﬁed. support for the interventions is needed to also get support for the changes
necessary to implement the interventions in the process. at uwv several possible inter-
ventions were put forward, from which one was chosen (step 3). only one intervention
could be selected, due to the limited availability of resources at uwv to execute an
experiment. section 3.2 elaborates on the collecting of interventions and selection of
the intervention for the ﬁeld experiment.
the next step is to design a ﬁeld experiment (step 4). the ﬁeld experiment was set
up as an a/b test [6]. in an a/b test, one or more interventions are tested under the
same conditions, to ﬁnd the alternative that best delivers the desired effect. in our ﬁeld
experiment, risk type combined with the intervention can be tested in the natural setting
of the process environment. the objective of the ﬁeld experiment is to determine the
effect of applying an intervention for cases at a speciﬁc risk level, with respect to the
speciﬁc process metrics of interest, i.e. whether or not a customer gets a reclamation.
all other factors that can play a role in the ﬁeld experiment are controlled, as far as thiswhat if process predictions are not followed by good recommendations? 5
fig. 2. overview of the steps that make up the research method. these steps correspond to one
improvement cycle. the “i” is used as an abbreviation for “intervention”.
is possible in our business environment. under these conditions, the ﬁeld experiment
will show if a causal relation exists between the intervention and the change in the
values of the process metrics. section 3.3 describes the setup for the uwv study.
the results of the ﬁeld experiment are analyzed to determine if an effect can be de-
tected from applying the intervention (step 5). the desired effect is a reduction in the
number of customers with a reclamation. section 4.1 and section 4.2 contain respec-
tively the analysis of the intervention and the predictor module. if the intervention can
be identiﬁed as having an effect, then both the direction of the effect, i.e. whether the
intervention leads to better or worse performance, and the size of the effect need to be
calculated from the data. when an intervention has the desired effect, it can be selected
to become a regular part of the process. the intervention then needs to be implemented
in the process (step 6). interventions together with the predictor module from step 2a,
make up the par system. after the decision to implement an intervention it is necessary
to update the predictor module of the par system. changing the process also implies
that the situation under which the predictions are made has changed. some period of
time after the change takes effect, needs to be reserved to gather a new set of historic
process data on which the predictor module can be retrained.
the ﬁnal step (step 7) is the reﬂective phase in which the lessons learned from the
execution of the approach are discussed. within this research method, many choices
need to be made. for example, which organizational issue will be tackled and which
interventions will be tested. prior to making a choice, the research participants should be
aware of any assumptions or bias that could inﬂuence their choices. section 5 contains
the lessons learned for the uwv case.
3.1 building the predictor module
the prediction is based on training a predictor module which uses historical data. the
data-mining techniques logistic regression and ada boost were used to build the6 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
predictor module. they were tuned through hyper-parameter optimization [7]. to this
end, uwv historical data was split into a training set with 80% of the cases and a test set
with 20% of the cases. the models were trained through a 5-fold cross validation, using
different conﬁgurations of the algorithms parameters. the models trained with different
parameter conﬁgurations were tested on the second set with 20% of the cases and ranked
using the area under the roc curve (shortened as auc) [8]. the best scoring models
were selected for the predictions during the experiment.
the predictor module was implemented as a stand-alone application in python and
leveraged the sci-kit learn [9] library to access the data-mining functionality. for the
uwv case, the historical data was extracted from the company’s systems. it relates to
the execution of every activity for 73,153 customers who concluded the reception of
unemployment beneﬁts in the period from july 2015 until july 2017. space limitations
prevent us from providing the details of how the prediction module was built. details
are available in the technical report [10] that accompanies this submission.
3.2 collecting and selecting the interventions
after three brainstorm sessions, with 15 employees and 2 team managers of uwv ,
the choice of the intervention was made by the stakeholders. as mentioned earlier, the
choice of intervention was based on the experience and expectations of the stakeholders.
the aim of the intervention is to prevent customers from incorrectly ﬁlling the income
form. more speciﬁcally, to prevent the customer from ﬁlling in the wrong amount. the
sessions initially put forward three potential types of interventions. the types are de-
ﬁned based on the actors that are involved in the intervention (the customer, the uwv
employee, or the last employer):
1. the customer is supported in advance on how to ﬁll the income form;
2. the uwv employee veriﬁes the information provided by the customer in the in-
come form, and, if necessary, corrects it after contacting the customer;
3. the last employer of the uwv customer is asked to supply relevant information
more quickly, so as to be able to promptly verify the truthfulness of the information
provided by the customer in the income form;
an intervention can only be executed once a month, namely between two income forms
for two consecutive months. in the ﬁnal brainstorming session, out of the three interven-
tion types, the stakeholders ﬁnally opted for option 1 in the list above, i.e. supporting
the customer to correctly ﬁll the income form. stakeholders stated that, according to
their experience, their support with ﬁlling the form helps customers reduce the chance
of incurring in reclamations. as mentioned earlier, only one speciﬁc intervention was
selected for the experiment, due to the limited availability of resources at uwv .
the selected intervention entails pro-actively informing the customer about spe-
ciﬁc topics regarding the income form, which frequently lead to an incorrect amount.
these topics relate to the deﬁnition of social security wages, ﬁnancial unemployment
and receiving 4-weekly payments instead of monthly payments. the uwv employees
indicated that they found that most mistakes were made regarding these topics.
next to deciding the action, the medium through which the customer would be
informed, had to be determined. the options were: a physical letter, an email, or a
phone call by the uwv employee. in the spirit of keeping costs low, it was decided towhat if process predictions are not followed by good recommendations? 7
send the support information by email. an editorial employee of uwv designed the
exact phrasing. the email contained hyperlinks to web pages of the uwv website to
allow customers to obtain more insights into the support information provided in the
email itself. the customers to whom the email was sent were not informed about the
fact that they were targeted because they were expected to have a higher risk of getting
a reclamation. a tool used by uwv to send emails to large numbers of customers
at the same time provided functionality to check whether the email was received by
the recipient, namely without a bounce, as well as whether the email is opened by
the customer’s email client application. since the timing of sending the message can
inﬂuence the success of the action, it was decided to send it on the day preceding the last
working day of the calendar month in which the predictor module marked the customer
as risky. this ensured that the message could potentially be read by the customer before
ﬁlling in the income form for the subsequent month.
3.3 design and execution of the field experiment
the experiment aims to determine whether or not the use of the par system would
reduce the number of reclamations in the way it had been designed in terms of pre-
diction and intervention. speciﬁcally, we ﬁrst determined the number and the nature
of the customers who were monitored. then, the involved customers were split into
two groups: on one group the par system was applied, i.e. the experimental group, the
second group was handled without the par system, i.e. the control group.
we conducted the experiment with 86,850 cases, who were handled by the amster-
dam branch of uwv . these were customers currently receiving beneﬁts, and they are
different from the 73,153 cases who were used to train the predictor module. out of
the 86,850 cases, 35,812 were part of the experimental group. the experiment ran from
august 2017 until october 2017. on 30 august 2017, 28 september 2017 and 30 oc-
tober 2017 the intervention of sending an email was executed. the predictor was used
to compute the probability of having a reclamation for the 35,812 cases of the experi-
mental group. the probability was higher than 0.8 for 6,747 cases, and the intervention
was executed for those cases.
4 results achieved
the intervention did not have a preventive effect even though the risk was predicted
reasonably accurate. sections 4.1 and 4.2 describe the details of the results achieved.
4.1 the intervention did not have a preventive effect
fig. 3 shows the results of the ﬁeld experiment, where the black triangles illustrate
the percentage of reclamations observed in each group. the triangles at the left-most
stacked bar show that the number of reclamations did not signiﬁcantly decrease when
the system was used, i.e. from 4.0% without using the system to 3.8% while using the
system. the effectiveness of the system as a whole is therefore 0.2%.
the second bar from the left shows how the par system was used for the cus-
tomers: 6,747 cases were deemed risky and were emailed. out of these 6,747 cases,
4,065 received the emails with the links to access further information. the other 2,6828 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
fig. 3. the number of cases and percentage of cases having a reclamation for all groups. the
results show that risky customers are identiﬁed, but the intervention does not really help.
cases did not receive the email. as mentioned in section 3.2 the tool that uwv uses
for sending bulk email can detect whether an email is received and is opened, i.e. there
was no bounce. since there were almost no bounces, the cases that did not receive the
email, actually did not open the message in their email client. from the customers who
have received the email, only 294 actually clicked on the links and accessed uwv’s
web site. remarkably, among the customers who clicked the link, 10.9% of those had
a reclamation in the subsequent month: this percentage is more than 2.5 times the av-
erage. also, it is around 1.7 times of the frequency among the customers who received
the email but did not click the links.
we conducted a comparative analysis among the customers who did not receive the
email, those who received it but did not click the links and, ﬁnally, those who reached
the web site. the results of the comparative analysis are shown in fig. 4. the results
indicate that 76.5% of the customers who clicked the email’s links had an income next
to the beneﬁts. recall that it is possible to receive beneﬁts even when one is employed:
this is the situation when the income is reduced and the customer receives beneﬁts for
the difference. it is a reasonable result: mistakes are more frequent when ﬁlling the
income form is more complex (e.g., when there is some income, indeed). additional
distinguishing features of the customers who clicked on the email’s link are that 50.3%
of these customers have had a previous reclamation, as well as that these customers are
on average 3.5 years older, which is a statistically signiﬁcant difference.
the results even seem to suggest that emailing appears counterproductive or, at
least, that there was a positive correlation between exploring the additional information
provided and being involved in a reclamation in the subsequent month. to a smaller
extent, if compared with the average, a higher frequency of reclamations is observed
among the customers who received the email but did not click the links: 6.2% of recla-
mations versus a mean of 3.8-4%. a discussion on the possible reasons for these results
can be found in section 5. however, it is clear that the intervention did not achieve the
intended goal.what if process predictions are not followed by good recommendations? 9
fig. 4. comparison of characteristics of customers who did not receive the email, those who
received it but did not click the link and who accessed uwv’s web site through the email’s link.
4.2 the risk was predicted reasonably accurate
as already mentioned in section 1 and section 4.1, the analysis shows the experiment
did not lead to an improvement. to understand the cause, we analyzed whether this was
caused by inaccurate predictions or an ineffective intervention or both. in this section,
we analyze the actual quality of the predictor module. we use the so-called cumulative
lift curve [11] to assess the prediction model. this measure is chosen because of the
imbalance in the data as advised in [11]. as mentioned before in section 2, only 4% of
the customers are eventually involved in reclamations. in cases of unbalanced data sets
(such as between customers with and those without reclamations), precision and recall
are less suitable to assess the quality of predictors. furthermore, because of the low cost
of the intervention of sending an email, the presence of false negatives , here meaning
those customers with undetected reclamations during the subsequent month , is much
more severe than false positives , i.e. customers who are wrongly detected as going to
have reclamations during the subsequent month .
fig. 5. the cumulative lift curve shows that using the recommender system leads to a better
selection of cases than using a random selection of cases.10 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
fig. 5 shows the curve for the case study at uwv . the rationale is that, within
a set of x%of randomly selected customers, one expects to observe x%of the total
number of reclamations. this trend is shown as a dotted line in fig. 5. in our case, the
predictions are better than random. for example, the 10% of customers with the highest
risk of having a reclamation accounted for 19% of all reclamations, which is roughly
twice as what can be expected in a random sample.
in summary, although the prediction technique can certainly be improved, a con-
siderable prediction effectiveness can be observed (cf. section 3.1). however, as men-
tioned in section 4.1, the system as a whole did not bring a signiﬁcant improvement.
this leads us to conclude that the lack of a signiﬁcant effect should be mostly caused
by the ineffectiveness of the intervention. in section 5, we discuss this in more detail.
5 lessons learned
the experiment proved to be unsuccessful. on the positive side, the predictions were
reasonably accurate. however, the intervention to send an email to high risk customers
did not lead to a reduction in the number of reclamations. there even was a group of
customers who had twice as many reclamations as the average population. section 5.1
elaborates on the reasons why the intervention did not work. section 5.2 focuses on the
lesson learned, delineating how the research methodology needs to be updated.
5.1 why did the intervention not work?
one of the reasons why the intervention was not successful might be related to the
wrong timing of sending the email. a different moment within the month could have
been more appropriate. however, this does not explain why of the 6,747 cases selected
only 294 acted on the email by clicking the links. other reasons may be that the cus-
tomers might have found the email message unclear or that the links in the email body
pointed to confusing information on the uwv website. in the group of 294 cases who
clicked the links and who took notice of this information a reclamation actually oc-
curred 2.5 times as much.
also, the communication channel could be part of the cause. sending the message
by letter, or by actively calling the customer might have worked better. in fact, when
discussing reasons of the failure of the experiment, we heard several comments from
different stakeholders that they did not expect the failure because “after speaking to a
customer about how to ﬁll in the income form, almost no mistakes are made by that
customer” (quoted from a stakeholder). this illustrates how the subjective feelings can
be far from objective facts.
5.2 what should be done differently next time?
we certainly learned that the a/b testing is really beneﬁcial to assess the effectiveness
of interventions. the involvement of stakeholders and other process participants, in-
cluding, e.g., the uwv’s customers, is beneﬁcial towards achieving the goal. however,
the results did not achieve the expected results. we learned a number of lessons to adjust
our approach that we will put in place for the next round of the experiments:what if process predictions are not followed by good recommendations? 11
1. creating a predictor module requires the selection of independent features as inputs
to build the predictive model. from the reﬂection and the analysis of the reasons
that caused the failure of an intervention, one can derive interesting insights into
new features that should be incorporated when training the predictor. for instance,
the features presented in fig. 4 can be used to train a better predictor for the uwv
case. these features could be, e.g., a boolean feature whether a customer has in-
come next to the beneﬁts.
2. the insights discussed in the previous point, which can be derived from the anal-
ysis, can also be useful to putting forward potential interventions. for instance, an
intervention could be to perform a manual check of the income form when a cus-
tomer has had a reclamation in the previous month. this intervention example is
derived from the feature representing the number of executions of detect reclama-
tionas discussed in section 4.1.
3. before the selection of the interventions for the a/b test (step 3 in fig. 2), they
need to be pre-assessed. the intervention used in our experiment is about providing
information to the customers concerning speciﬁc topics related to ﬁlling the income
form. in fact, before running the experiments, we could have already checked on
the historical event data whether the reclamations were on average fewer when
providing information and support to ﬁll the income form. if this would had been
observed, we could prevent ourselves from running experiments destined to fail.
4. since a control group was compared with another group on which the system was
employed and the comparison is measured end-to-end, it is impossible to state the
reason of the failure of the intervention, beyond just observing it. for instance, we
should have used questionnaires to assess the reasons of the failure: the customers
that received the email should have been asked why they did not click on the links
or, even if clicked, still were mistaken. clearly, questionnaires are not applicable for
all kinds of interventions. different methods also have to be envisaged to acquire
the information needed to analyze the ineffectiveness of an intervention.
5. it is unlikely that the methodology in section 3 already provided satisfactory results
because of the methodology needs to be iterated in multiple cycles. in fact, this
ﬁnding is compliant with the principle of action research , which is based on idea
of continuous improvement cycles [12, 13].
6. the point above highlights the importance of having interaction cycles. however,
one cycle took a few months to be carried out. this is certainly inefﬁcient: the
whole cycle needs to be repeated at high speed and multiple interventions need to
be tested at each cycle. furthermore, if an intervention is clearly ineffective, the
corresponding testing needs to be stopped without waiting for the cycle to end.
all the lessons learned share one leitmotif: accurate predictions are crucial, but their
effect is nulliﬁed if it is not matched by effective recommendations, and effective recom-
mendations must be based on evidence from historical and/or experimental data .
6 conclusion
when building a process-aware recommender system, both the predictor module and
the recommender parts of the system must be effective in order for the whole system
to be effective. in our case, the predictor module was accurate enough. however, the12 marcus dees, massimiliano de leoni, wil m.p. van der aalst and hajo a. reijers
intervention did not have the desired effect. the lessons learned from the ﬁeld exper-
iment are translated into an updated research method. the updated approach asks for
high speed iterations with multiple interventions. systematic support will be needed for
each step of the approach to meet these requirements.
as future work, we plan to improve the predictor module to achieve better predic-
tions by using different techniques and leveraging on contextual information about the
customer and its history. our analysis showed that, e.g., the presence of some monetary
income next to the beneﬁts is strongly causally related to reclamations. as described,
we want to use evidence from the process executions, and insights from building the
predictor module, to select one or more interventions to be tested in a new experiment.
orthogonally to a new ﬁeld experiment, we aim to devise a new technique that
adaptively ﬁnds the best intervention based on the speciﬁc case. different cases might
require different interventions, and the choice of the best intervention should be auto-
matically derived from the historical facts recorded in the system’s event logs. in other
words, the system will rely on machine-learning techniques that (1) reason on past exe-
cutions to ﬁnd the interventions that have generally been more effective in the speciﬁc
cases, and (2) recommend accordingly.
references
1. conforti, r., de leoni, m., rosa, m.l., van der aalst, w.m., ter hofstede, a.h.: a recom-
mendation system for predicting risks across multiple business process instances. decision
support systems 69(2015) 1 – 19
2. schobel, j., reichert, m.: a predictive approach enabling process execution recommenda-
tions. in: advances in intelligent process-aware information systems - concepts, methods,
and technologies. springer international publishing (2017) 155–170
3. m ´arquez-chamorro, a.e., resinas, m., ruiz-cort ´es, a.: predictive monitoring of business
processes: a survey. ieee trans. services computing 11(6) (2018) 962–977
4. teinemaa, i., dumas, m., rosa, m.l., maggi, f.m.: outcome-oriented predictive process
monitoring: review and benchmark. corr abs/1707.06766 (2017)
5. lhannaoui, h., kabbaj, m.i., bakkoury, z.: towards an approach to improve business pro-
cess models using risk management techniques. in: 2013 8th international conference on
intelligent systems: theories and applications (sita). (05 2013) 1–8
6. kohavi, r., longbotham, r. in: online controlled experiments and a/b testing. springer
us, boston, ma (2017) 922–929
7. claesen, m., moor, b.d.: hyperparameter search in machine learning. corr
abs/1502.02127 (2015)
8. fawcett, t.: an introduction to roc analysis. pattern recogn. lett. 27(8) (2006) 861–874
9. pedregosa, f., varoquaux, g., gramfort, a., michel, v ., thirion, b., grisel, o., blondel, m.,
prettenhofer, p., weiss, r., dubourg, v ., vanderplas, j., passos, a., cournapeau, d., brucher,
m., perrot, m., duchesnay, e.: scikit-learn: machine learning in python. journal of machine
learning research 12(2011) 2825–2830
10. dees, m., de leoni, m., van der aalst, w.m.p., reijers, h.a.: what if process predictions
are not followed by good recommendations? arxiv e-prints (may 2019) arxiv:1905.10173
11. ling, c.x., li, c.: data mining for direct marketing: problems and solutions. in: proceedings
of the fourth international conference on knowledge discovery and data mining (kdd-
98), new york city, new york, usa, august 27-31, 1998. (1998) 73–79
12. cronholm, s., goldkuhl, g.: understanding the practices of action research. in: the 2nd
european conference on research methods in business and management. (2003)
13. rowell, l.l., riel, m.m., polush, e.y . in: deﬁning action research: on dialogic spaces
for constructing shared meanings. palgrave macmillan us, new york (2017) 85–101