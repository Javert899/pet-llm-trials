promise: coupling predictive process mining to
process discovery
vincenzo pasquadibiscegliea,, annalisa appicea,b, giovanna castellanoa,b,
wil van der aalstc
adepartment of informatics, universit a degli studi di bari aldo moro, via orabona, 4 -
70125 bari - italy
bconsorzio interuniversitario nazionale per l'informatica - cini, italy
tel.: +39-080-5443262
crwth aachen university, aachen, germany
abstract
process discovery, one of the main branches of process mining, aims to discover a
process model that accurately describes the underlying process captured within
the event data recorded in an event log. in general, process discovery algorithms
aim to return models describing the entire event log. however, this strategy
may lead to discover complex, incomprehensible process models concealing the
correct and/or relevant behavior of the underlying process. processing the entire
event log is no longer feasible when dealing with large amounts of events. in
this study, we propose the promise+method that rests on an abstraction
involving predictive process mining to generate an event log summary. this
summarization step may enable the discovery of simpler process models with
higher precision. experiments with several benchmark event logs and various
process discovery algorithms show the eectiveness of the proposed method.
keywords: process discovery, predictive process mining, deep learning,
abstraction, event log summarization
corresponding author
email addresses: vincenzo.pasquadibisceglie@uniba.it (vincenzo pasquadibisceglie),
annalisa.appice@uniba.it (annalisa appice), giovanna.castellano@uniba.it (giovanna
castellano), wvdaalst@pads.rwth-aachen.de (wil van der aalst)
preprint submitted to journal of l atex templates january 1, 20231. introduction
process-aware information systems (paiss) are software systems that are
being used increasingly by public and private organizations to manage and ex-
ecute operational processes involving people, applications and/or information
sources. examples of paiss are workow management systems, case-handling 5
systems and enterprise information systems. in principle, these systems are
driven by process models [17] that can be represented in a graphical language,
e.g., in a petri-net-like notation. in practice, most paiss do not use an ex-
plicit process model, i.e., there are implicit, emerging processes based on best
practices analysis. this is because processes are commonly invisible and of- 10
ten exist as abstract concepts, hence they are frequently dicult to materialize
[18]. in addition, even when processes are documented in some way, they are
often described using a variety of notations [18]. fortunately, more and more
detailed information about the execution of process instances (e.g., activities
being executed) are being recorded in the form of event logs [19]. these event 15
logs represent the key enabler for process discovery algorithms [18].
process discovery is a branch of process mining [19] that is concerned with
the automatic discovery of process models from event logs. existing process
discovery algorithms are formulated in process mining [19] and strike dierent
trade-os between the accuracy in capturing the behavior recorded in an event 20
log and the complexity of the derived process model. ensuring the quality of
discovered process models is one of the main issues of the current research in
process discovery. in fact, to be useful the process model should: (1) parse
the traces in the log, (2) parse traces that are not in the log but are likely to
belong to the process that produced the log and (3) not parse other traces. the 25
rst property is called tness , the second one generalization , and the third one
precision . on the other hand, the process model should be as simple as possible.
the simplicity property is usually quantied via complexity measures [2].
most of the process discovery algorithms tend to use the whole event log
for discovery. this makes no longer eective traditional process discovery al- 30
2gorithms in the emerging big data settings, where the event data may be too
big to be entirely processed with standard hardware in a reasonable time [51].
a straightforward solution to overcome this problem is to build summaries of
huge event logs by down-sizing the amount of event data to be processed with
a process discovery algorithm. to this purpose, various ltering [8, 38, 39, 44] 35
and sampling [40, 37, 16] methods have been proposed as preprocessing step
for process discovery. these methods apply extractive approaches to identify
important traces (sampling) or relevant excerpts from traces (ltering) in event
logs, and reproduce them verbatim as part of the log summary.
in this study, we explore how an abstraction-based approach can be eec- 40
tively used as an alternative to the extraction strategy performed by ltering
or sampling. in particular, the goal is to use an abstraction strategy to distill
the most important information from an event log, in order to produce a syn-
thesized version of the log useful for the process discovery task. this idea is
borrowed from the text summarization eld [15], where abstraction-based meth- 45
ods are proved to be commonly more eective than extraction-based methods
for realizing text summaries. when applied to event logs, traditional ltering
and sampling methods extract a subset of existing traces (or excerpts of exiting
traces) for inclusion in the summary output. opposed to these extraction-based
approaches, we propose an abstraction-based method that learns a generative 50
representation (abstraction) of an event log that can be helpful to capture im-
portant process information possibly spread across traces. this abstraction is
used for generating few \new" representative traces that comprise more exi-
ble activity sequences and provide a compact representation of the initial event
log content. these new traces are subsequently processed with process discov- 55
ery algorithms to better achieve the ideal balance between the quality and the
complexity of the process models nally discovered.
this idea goes in some way into the direction of [36], where clustering is used
to group traces, while cluster medoids are selected as representative, existing
traces to populate the log summary. however, dierently from [36], our method 60
creates a log summary by creating new representative traces, apart from extract-
3ing existing signicant ones. in particular, the key idea of our method, named
promise+(coupling predictive process miningto process discovery), is to
learn the generative abstraction of the event log by setting a predictive process
mining method. 65
predictive process mining is a recently emerged family of process mining
methods to predict the unfolding of running traces (e.g., the next activity) based
on the knowledge learned from a historical event log. with the recent boom
of deep learning, several accurate predictive process mining methods leveraging
deep neural networks have been proposed [47, 46, 7, 32, 35, 33, 34]. following 70
this research stream, we use a deep neural network with long short-term mem-
ory (lstm) layers [22] to process executed activities by taking into account the
sequential nature of events recorded in event logs. the trained lstm model is
used within a trace generation technique to produce new representative traces
by iteratively sampling from the network's output distribution, then feeding in 75
the sample as input at the next step. finally, we wrap a process discovery al-
gorithm in a forward trace selection strategy that selects the summary traces
that contribute to maximizing the quality of the process model discovered.
the paper is organized as follows. section 2 overviews recent advances of lit-
erature in process discovery and outlines the motivation of this study. section 3 80
reports preliminary concepts, while section 4 describes the proposed promise+
method. in section 5, we present an extensive empirical study that compares
promise+to sampling and ltering baselines taken from the recent literature
on process mining. finally, section 6 draws conclusions and sketches the future
work. 85
2. background and motivation
several process discovery algorithms (e.g., alpha miner [48], inductive miner
[25, 26], heuristic miner [50], fodina [49] and split miner [2]) have been pro-
posed in the recent process mining literature. although alpha miner [48] and
the basic inductive miner [25] have been originally designed to depict as much 90
4as possible behaviors seen in the event log into the process model, several pro-
cess discovery algorithms use ltering. for example, hybrid ilp miner [53],
heuristic miner [50], fodina [49] and split miner [2] have been designed to lter
infrequent behaviors within their internal data structure, in advance to discov-
ering a process model. also, the inductive miner family has introduced ltering 95
mechanisms to lter infrequent directly-follows dependencies [26]. heuristic
miner allows or-joins and or-splits on ltered directly-follows dependencies.
in addition, in the context of petri nets, researchers have been looking at the
region theory to construct a system model from a description of its behavior.
state-based regions have been used to construct a petri net from a transition 100
system as an intermediate representation [21] having lters integrated in the
transitions system generation. language-based regions have been also used to
construct a petri net from a prex-closed language [11]. in any case, all these
algorithms still load the whole event log to build the internal data structure
elaborated to discover the nal process models, although they start integrating 105
built-in ltering mechanisms operating on the internal data structure.
the preprocessing of event logs has recently gained attention as a useful
phase prior to application of process discovery algorithms. various cleaning
techniques are analyzed in [42] as a means to handle noise produced by the
presence of infrequent behaviors (outliers) in the event raw data. in fact, these 110
outliers may lead to discover process models exhibiting infrequent execution
paths that clutter the model. in particular, outliers may have a negative eect
on the precision of the discovered models, as well as on their complexity. to
this regard, in [8] the authors show that leaving outliers in event data may have
a detrimental eect on the quality of the process models produced by various 115
process discovery algorithms. to detect and remove infrequent process behaviors
in raw event data, various ltering methods [8, 38, 39] have been formulated in
recent literature. a ltering method for discovering more precise process models
in the presence of chaotic activities is also described in [44]. filtering methods
eectively reduce the size of traces elaborated by process discovery algorithms. 120
however, as discussed in [16], the time spent for applying ltering methods is
5sometimes longer than the time required for discovering a process model from
the initial event log. in addition, several ltering methods may have no accurate
control over the size of the reduced event log.
sampling methods reduce the number of traces in event logs. a random 125
trace-based sampling method is described in [3]. this method assumes that
traces have dierent behavior if they have dierent sets of directly follows re-
lations. a very recent study [37, 16] investigates the eectiveness of applying
sampling on event data prior to invoking process discovery algorithms, instead
of using all the available event data. the authors compare dierent biased sam- 130
pling strategies (frequency-based sampling, length-based sampling, similarity-
based sampling and structure-based sampling) and analyze their ability to im-
prove the scalability of the process discovery algorithm. finally, in [36] the
authors propose an iterative trace selection algorithm based on clustering and
conformance artifacts. each iterative phase rst performs a trace clustering 135
step with the k-medoids algorithm and the edit distance, in order to populate
a sublog with the cluster medoids (i.e., the traces of the original log that are
the closest to the cluster centroids). subsequently, it discovers a process model
from this sublog and computes the conformance artifacts of the original log on
the discovered process model, in order to identify deviating traces. the itera- 140
tive phase is repeated on the deviating traces until the quality of the discovered
process model improves. as a measure of the quality of a process model, the
authors of [36] adopt the f-measure of tness (computed as replay tness [4])
and precision (computed as alignment-based etc precision [1]). the experi-
ments illustrated in [36] prove that the use of cluster prototypes as input to the 145
process discovery algorithms often improves the f-measure of the discovered
process models outperforming both random and biased sampling strategies.
both ltering and sampling mechanisms reported above down-size event logs
by applying some extractive strategy. dierently, the method presented in this
study takes advantage of an abstraction strategy to generate new representative 150
traces potentially unobserved in the initial event logs. these new traces may
combine dierent activity sub-sequences also belonging to multiple traces result-
6ha;b;c;d;e;f;l;hi2000
ha;b;i;c;d;e;f;g;h i1000
ha;e;f;g;hi2000
ha;f;g;hi2000
ha;c;d;e;f;g;hi1500
ha;b;d;e;f;g;hi500
ha;c;d;e;f;hi500
ha;g;c;d;e;f;g;hi100
ha;g;g;hi50
(a)ha;b;c;d;e;f;g;hi
ha;c;d;e;f;g;hi
ha;e;f;g;hi
ha;f;g;hi
(b)ha;b;c;d;e;f;l;hi
ha;e;f;g;hi
ha;f;g;hi
ha;c;d;e;f;g;hi
ha;b;i;c;d;e;f;g;h i
(c)ha;b;c;d;e;f;l;hi
ha;c;d;e;f;g;hi
ha;f;g;hi
(d)
figure 1: a sample event log (a) and its summaries extracted with promise+(b), frequency-
based sampling [37, 16] (c), clustering [36] (d).
ing in more exible event log summaries for the process discovery. in particular,
the proposed trace generation approach uses an abstraction process to abstract
from the raw events recorded in the initial event logs and construct a predictive 155
process model that can be used to predict the next-activity of a running trace.
in this way, we take advantage of a predicting process mining pattern learned
from the original event log, in order to improve the process discovery step.
to better illustrate the potential of employing an abstraction-based strategy
in the event log summarization, let us consider the event log reported in fig- 160
ure 1a. the abstraction-based strategy underlying our method generates a new
traceha;b;c;d;e;f;g;hi(figure 1b), which does not belong to the original event
log, but stands out as a combination of subsequences coming from multiple origi-
nal traces (e.g.,ha;b;c;d;e;f;l;hi,ha;b;i;c;d;e;f; g;hiandha;b;d;e;f; g;hi).
we note that an eect of the abstraction-based strategy that has led to the 165
generation of this new trace is that the event lhas been dropped from the nal
log summary, while it has been kept in the log summaries extracted with both
sampling (figure 1c) and clustering (figure 1d). we compare the process
model discovered from the log summary extracted by our method promise+
(figure 2b) with the original process model (figure 2a) discovered from the 170
original event log and with the process models discovered from log summaries
7(a)
(b)
(c)
(d)
(e)
figure 2: petri net representation of (a) original process model; (b) process model discovered
with promise+; c) process model discovered with filtering; (d) process model discovered with
sampling; (e) process model discovered with clustering . process models are discovered
with inductive miner.
extracted by other strategies such as filtering (figure 2c), frequency-based
sampling [37, 16] (figure 2d) and clustering [36] (figure 2e). all the
process models have been discovered with inductive miner. note that we run
filtering on 50 dierent set-up of the embedded ltering parameter of the 175
process discovery algorithm and select the set-up that allows the discovery of
the process model achieving the highest f-measure of tness and precision. we
8runsampling by ranking all the variant-traces by frequency and selecting the
top-variants that allow the discovery of the process model achieving the highest
f-measure of tness and precision. we observe that the abstraction method 180
achieves the highest precision, without signicantly decreasing the tness. it
hassampling strategy as the runner-up in terms of f-measure of precision
and tness (as shown by conformance metrics of the compared process models
reported in table 1). in particular, promise+allows us to discover the pro-
cess model with the highest precision, while sampling allows us to discover 185
the process model with the highest tness. in the example log, this behavior
mainly depends on the fact that sampling (as well clustering ) keeps the
eventlin its log summary allowing the discovery of a process model that can
parse tracesha;b;c;d;e;f;l;hiof the original log. however, in this way, it also
parses traces like ha;f;l;hiandha;c;d;e;f;l;hithat are not in the original log. 190
therefore, dropping lfrom the log summary, promise+slightly loses in the
ability to parsing a few traces in the log, but gains in the ability of not parsing
other traces.
concerning the use of abstraction, we remark that abstraction-based strate-
gies are commonly investigated for text summarizing (see [15] for a survey), but 195
not yet for event log summarizing. to the best of our knowledge, our approach
is the rst attempt to check the feasibility of an abstraction-based strategy in
the event log summarization. on the other hand, various abstraction strategies
have already been explored in the process mining eld. for example, the au-
thors of [5] describe a means to form abstractions on patterns that capture the 200
manifestation of process model constructs commonly used in event logs. in par-
ticular, they present an algorithm for the trace transformation, which replaces
the repeated occurrences of the manifestation of a loop with an abstracted ac-
tivity entity that encodes the notion of a loop. in addition, the transformation
algorithm identies sub processes in traces and replaces them with abstract 205
entities. it also implements abstractions to deal with combinations of choice,
parallelism and loops. the authors of [45] introduce a supervised approach that
takes advantage of annotations with high-level interpretations of low-level events
9table 1: conformance metrics of the process models in fig. 2.
process model tness precision f-measure
original 1.00 0.46 0.63
promise+0.95 0.97 0.96
filtering 0.88 0.93 0.90
sampling 0.99 0.91 0.95
clustering 0.96 0.92 0.94
to map from ne-granular events to coarse-granular events. a taxonomy of the
various strategies to abstract the event log to a higher level of granularity has 210
been recently described in [54]. in addition, the authors of [20] use the notion of
log and model abstractions to illustrate a unifying overview of process discovery
techniques. in this view, discovery approaches operate on an abstraction of the
event log as a means to relate observed behavior to modeled behavior. in any
case, to the best of our knowledge, no previous study in the eld of process 215
mining has investigated the feasibility of an abstraction strategy that founds on
predictive process mining to generate the representative traces of an event log
and facilitate the process discovery task.
finally, we note that the task of trace generation has already received at-
tention in the process mining eld [30, 41]. however, these studies describe 220
approaches to generate entire event logs mainly through simulation of some
process models. the output of these trace generative approaches is a controlled
(big) event log that can be used to evaluate process mining algorithms in a com-
plete and predictable way. dierently, we describe here a method to generate a
few new traces from a (big) initial event log, in order to distill a compact version 225
of the most relevant information of the initial log for the process discovery task.
3. preliminaries
in this paper, we focus on sequences of activities, also called traces, that are
combined into event logs and next-activity classication functions that are used
in an abstraction strategy to generate event log summaries. 230
10multisets are commonly used to describe event logs where the same trace
may appear multiple times. an event log is a multiset of traces. each trace
describes the life-cycle of a particular case (i.e., a process instance) in terms
of the activities executed. in this simple denition of an event log, an event
refers to just an activity. let us consider some set of activities aua.= 235
ha1;a2;:::;anidenotes a sequence over aof lengthn=jj, where(i) =ai
for 1ijj.hd(;k) =ha1;a2;:::;akiwith 1knis the head of the
sequence consisting of the rst kelements.tl(;k) =hak+1;ak+2;:::;aniwith
1knis the tail of the sequence composed of the last jj kelements. the
selection(k+1) corresponds to the next activity of hd(;k). note that the next 240
activity ofhd(;n) is?where?denotes the end of the trace. for the sequence
=ha;b;c;d;e;f;g;hi,(2) =b,hd(;2) =ha;biandtl(;2) =hc;d;e;f;g;hi.
the next activity of hd(;2) isc, while the next activity of hd(;8) is?.hiis
the empty sequence.
sequences are used to represent traces in an event log. 12is the con- 245
catenation of two sequences. in addition, let adenote the set of all possible
sequences on a.b(a) denotes the set of all multisets over a. for a multiset
l2b(a),l() is the number of times a distinct variant-sequence appears
inl. for example, let us consider a set of activities a=fa;b;c;e;f;g;hg,
thenl1= [],l2= [ha;b;c;g;hi;ha;b;c;d;e;f;g;hi;ha;b;c;g;hi] andl3= 250
[ha;b;c;g;hi2;ha;b;c;d;e;f;g;hi] are multisets of b(a).l1is the empty mul-
tiset,l2andl3both consist of three sequences and l2=l3, i.e., the ordering
of sequences is irrelevant and a more compact notation may be used for re-
peating sequences. note that both l2andl3contain three sequences, but
two distinct variant-sequences (i.e., ha;b;c;g;hiandha;b;c;d;e;f;g;hi) as the 255
sequenceha;b;c;g;hiappears twice in both l2andl3.
denition 1 (trace, event log). letauabe a set of activities. a trace
2ais a sequence of activities. l2b(a)is an event log, i.e., a multiset of
traces.
we denote asjljthe cardinality (number of traces) of l. we denote as 260
11table 2: labeled head sequence multiset tlextracted from the log lreported in fig. 1a.
 a occurrences  a occurrences  a occurrences
hai b 3500 haei f 2000 habdei f 500
habi c 2000 haefi g 2000 habdefi g 500
habci d 2000 haefgi h 2000 habdefgi h 500
habcdi e 2000 haefghi ? 2000habdefghi ? 500
habcdei f 2000 hai f 2000 hacdefi h 500
habcdefi l 2000 hafi g 2000 hacdefhi ? 500
habcdefli h 2000 hafgi h 2000 hai g 150
habcdeflhi ? 2000hafghi ? 2000hagi c 100
habi i 1000 hai c 2000 hagci d 100
habii c 1000 haci d 2000 hagcdi e 100
habici d 1000 hacdi e 2000 hagcdei f 100
habicdi e 1000 hacdei f 2000 hagcdefi g 100
habicdei f 1000 hacdefi g 1500 hagcdefgi h 100
habicdefi g 1000 hacdefgi h 1500 hagcdefghi ? 100
habicdefgi h 1000 hacdefghi ? 1500hagi g 50
habicdefghi ? 1000habi d 500 haggi h 50
hai e 2000 habdi e 500 hagghi ? 50
maxlenlthe maximum length of a trace in l.
denition 2 (labeled head sequence multiset). letl2b(a)be an event
log.tl2b(aa)is the multiset of all the head sequences (samples) ex-
tracted from the traces of l. each head sequence is labeled with the next ac-
tivity (labels) associated to the head sequence in the corresponding trace so that 265
tl= [hd(;k);(k+ 1)j2l^1kjj].
let us consider the sample event log reported in figure 1a. the labeled
head sequence multiset extracted form this event log is reported in table 2.
denition 3 (next-activity classication model). a next-activity classi-
cation model fis a function f:ard7!a, wheredrepresents the number 270
of real-valued parameters in a model. the last darguments of the function are
later xed to create a hypothesis function from atoa.
denition 4 (next-activity classication hypothesis function). letfbe
a model with dreal-valued parameters. let 2rdbe a vector of dreal-valued
12parameters. a hypothesis hf;of the model fis a function: hf;:a7!a 275
such thathf;(x)f(x;).
denition 5 (cost function). lethf;be a hypothesis of the next-activity
classication model function f. the cost function of hf;is a function chf;:a
a7!rsuch thatchf;(x;y)measures the penalty of an incorrect classication
of the label ydone through hf;(x). 280
denition 6 (next-activity classication algorithm). lettlbe a labeled
head sequence multiset. the next-activity classication algorithm determines the
hypothesishf;that minimizes the cost function chf;, i.e., such that:
 = arg min
2rdx
(;a)2tlchf;(;a):
the hypothesis hf;depends on the model type and the labeled multiset. in
this study, following the deep learning approach, the model type is the network
architecture, i.e., the hypothesis is implicitly determined by the architecture
parameters. under the umbrella of deep learning, the long-short-term memory
(lstm) neural network [22] is a recurrent network that is suitable to process 285
sequences, such as those underlying a business process event log. the lstm
network uses cyclical connections among its processing units that enable the
classication of sequential data by using part of the output of a processing
unit for the processing of a new input. the information ows from a unit to
another unit with minimal variation, keeping certain aspects constant during 290
the processing of all inputs. this constant input keeps classications that are
coherent over long periods of time. this results in a long-term memory. a
common lstm unit accepts ct 1andht 1as state information from the prior
unrolled cell on the same layer, and xtas input from cells of the previous
layer. in turn, it passes ctandhtas new state information to the subsequent 295
unrolled cell on the same layer and also provides htas output to the next layer.
13specically, it performs the following computations:
ft=sigmoid 
wf[ht 1;xt] +bf
; (1)
it=sigmoid 
wi[ht 1;xt] +bi
; (2)
ct= tanh 
wc[ht 1;xt] +bc
; (3)
ct= (ftct 1) + (itct); (4)
ot=sigmoid 
wo[ht 1;xt] +bo
; (5)
ht=ottanh(ct); (6)
wherewandb(weights and biases) are the parameters of the network to be
learnt. eq. 1 represents the \forget gate" that determines, based on the inputs
xtandht 1, which part of the state to forget. eq. 2 represents the \input gate"
and determines which values of the state to update; some of the itwill be close
to zero, others close to one. eq. 3 computes new candidate values  c. eq. 2 and
3 determine how the inputs xtandht 1contribute to the updated cell state.
eq. 4 computes the new state ctby adding the information kept in ct 1after
forgetting (i.e., ftct 1) to the new information (i.e., itct). eq. 5 represents
the \output gate" that determines which values of the cell state are provided
as output to the following layer and subsequent unrolled cell. eq. 6 computes
the nal output htas the product between the value provided by the output
gate and the tanh of the cell state ct. the number of lstm processing units
on each layer is a hyper-parameter to x. the output of the lstm module is
nally fed into a softmax layer, in order to compute the nal output (i.e., the
next activity) from probabilities of dierent classes (activities) computed using
thesoftmax activation function:
yi=softmax (y)i=exp(yi)p
jexp(yj): (7)
the cross-entropy loss function, that measures the error between the ex-
pected label and the probability predicted by the neural network, is commonly 300
used as cost function to be optimized in classication tasks [31].
14several studies [47, 46, 7, 34] have recently proved that lstm neural network
architectures can learn accurate next-activity hypothesis functions hf;from
various event logs. in addition, the authors of [47] have recently shown that
repeatedly applying the function hf;learned by an lstm neural network, we 305
are able to make accurate longer-term classications that predict further ahead
than a single time step by accurately predicting the tail of a trace given its head.
denition 7 (trace tail prediction function). lethf;:a7!abe a next-
activity classication hypothesis function, 2abe a seed sequence, maxlenl 310
be the expected maximum trace length. h?
f;:a7!adenotes the function:
h?
f;() =8
>>>><
>>>>:hi if hf;() =?;
hi ifjj=maxlenl
hhf;()ih?
f;(hhf;()i)otherwise; (8)
that predicts the full continuation of a trace with head .
according to denition 7, we can generate a new trace 0=hh?
f;()i
by concatenating the head and the predicted tail h?
f;(). in principle, any
sequence2acan be used as a seed for this trace generation. however, in this 315
study, we generate new traces from a few seed sequences that appear as heads
in real traces of l. specically, we use a length-based criterion to determine the
seed sequences of the trace generation. let us consider hdl(l) =fhd(;l)j2
l^jjlg, that is, the set of head sequences of lwith length equal to l(with
l1), we consider all the head sequences of hdl(l) to prompt the generation 320
of traces to populate a log summary l.
denition 8 (event log summary). letl2b(a)be an event log, h?
f;:a7!
abe the trace tail prediction function that repeatedly applies the next-activity
classication hypothesis function hf;:a7!alearned from tl,hdl(l)be
the set of head sequences of lwith size equal to l. the log summary lis a set 325
of traces generated through h?
f;from each distinct head sequence 2hdl(l),
that is,l=fhh?
f;()ij2hdl(l)g.
15we highlight that predicting the completion of traces with head in distinct
sequences, we can generate distinct traces for l. as an example, let us con-
sider the event log lreported in figure 1a and the next-activity classication 330
hypothesis function hf;learned through an lstm neural network from the
labeled head sequence multiset tlreported in table 2. let us set l= 2. we pro-
cess the set of 2-sized head sequences hd 2(l) =fha;bi;ha;ci;ha;ei;ha;fiha;gig
as seeds for generating the traces of the log summary l=fha;b;c;d;e;f;g;hi,
ha;c;d;e;f;g;hi;ha;e;f;g;hi,ha;f;g;hi,ha;g;c;d;e;f;g;hig. note that, as re- 335
ported in denition 8, we generate the trace ha;b;c;d;e;f;g;hioflasha;bi
hc;d;e;f;g;hi, whereha;bi2hd 2(l) andhc;d;e;f;g;hiis the full continuation
that we predict through h?
f;for a trace with head ha;bi(i.e.,h?
f;(ha;bi) =
hc;d;e;f;g;hi). we similarly generate the traces ha;c;d;e;f;g;hi,ha;e;f;g;hi,
ha;f;g;hiandha;g;c;d;e;f;g;hioflby predicting the completion of traces 340
with headha;ci,ha;ei,ha;fiandha;gi, respectively.
further considerations concern the fact that, in the previous example, the
log summary lcomprises the trace ha;g;c;d;e;f;g;higenerated by predicting
the full continuation of a trace with the head ha;githat is infrequent in l. in
fact, the sequence ha;giappears as the head of only 150 traces out of 9650 traces 345
originally stored in l. this trace, if processed by a process discovery algorithm,
may lead to discovering process models exhibiting this infrequent execution path
that may clutter the model [42]. to handle the possible drawbacks of generating
traces that are abstractions of infrequent behaviors in the original log, a sam-
pling post-processing mechanism can be applied to lto lter-out generated 350
traces that may reduce the performance of process discovery algorithms.
denition 9 (summary event log sampling). letlbe a summary log of
l. we dene slas trace-based sample of lso thatsll.
16figure 3: schema of the promise+method.
4. abstraction-based event log summarizing method for process dis-
covery 355
in this section, we describe promise+. a schematic schematic view of our
approach is shown in figure 3. the method is composed of the following steps:
1.labeled head sequence multiset extraction . it takes as input the
initial event log land returns the labeled head sequence multiset tl.
2.next-activity classication. it takes as input tland learns the next- 360
activity classication hypothesis function hf;:a7!a.
3.event log summarization . it takes as input both landhf;and
generates the event log summary l.
4.process model discovery . it takes as input land wraps a process
discovery algorithm that is applied to the traces of l. in this step, the 365
traces oflthat may decrease the performance of the process discovery
algorithm are identied and ltered-out them.
the detailed description of each step is reported in the followings.
4.1. labeled head sequence multiset extraction
we transform lintotlthat, as presented in section 3, is composed of all the 370
possible labeled head sequences extracted from all the traces of l. each head
sequence is labeled with the next activity associated with the head sequence in
the corresponding trace.
17table 3: conguration of neural network hyperparameters.
parameters value
learning rate [0.00001, 0.01]
lstm unit size f8, 16, 32g
batch size [25, 210]
4.2. next-activity classication
starting from tland using an lstm neural network as model type, we 375
learnhf;:a7!ato classify the next activity of any sequence of activities
representing a running trace. as reported in section 3, this function will be
subsequently used as a trace summarizer of l.
we adopt an lstm neural network architecture with an embedding layer
that automatically learns a multi-dimensional real-valued representation of cat- 380
egorical activity sequences. the output of the embedding layer is fed into a re-
current neural network composed of two stacked lstm layers. the rst lstm
layer provides a sequence output to feed the second lstm layer. we conduct
the optimization phase of the hyper-parameters by using 20% of the training
set as the validation set. in particular, we perform hyper-parameters optimiza- 385
tion with smac [23].1table 3 reports the hyper-parameters optimized and the
corresponding range of possible values explored with smac. the training of
the network is accomplished by the backpropagation algorithm that performs
iterative backward passes to nd the optimal values of network weights based
on gradient descent. we use the cross-entropy loss function for the optimization 390
and perform the backpropagation training with early stopping to avoid overt-
ting. we stop the training process when there is no improvement of the loss on
the validation set for 20 consecutive epochs. to minimize the loss function, we
use the nadam optimizer. the maximum number of epochs is set to 200.
1https://automl.github.io/smac3/master/quickstart.html
184.3. event log summarization 395
oncehf;has been learned by training an lstm neural network from tl,
we use it to generate lthat is an event log summary of l. for this purpose,
we rst determine the set hdl(l) of the head sequences of lwith sizel. these
sequences are used as seeds for the trace generation. in fact, for each seed
sequence2hdl(l), we repeatedly use hf;to predict the tail of a new trace 400
havingas head (according to def. 7). the new trace is added to l. we
automatically choose las the minimum sequence length to select a number of
seed sequences greater than 1. as an example, let us consider the event log lin
figure 1a. if we consider l= 1, then we get one seed sequence, i.e., the sequence
hai, to prompt the generation of one trace for l. if we consider l= 2, then we 405
get ve distinct seed sequences, i.e., the sequences ha;bi,ha;ci,ha;ei,ha;fiand
ha;gi, to prompt the generation of ve distinct traces for l. according to the
considerations reported above, in the example, we automatically choose l= 2.
4.4. process model discovery
finally, we discover a process model from l. any process discovery algo-
rithm can be used in this step, although it is recommended to use algorithms
that generate sound process models [36]. the selected process discovery algo-
rithm is wrapped within an iterative trace selection process aiming to select the
most relevant traces of lthat contribute to discovering the process model that
better conforms to the initial event log l. to this aim, as in [36], we enclose
quality assessment metrics by measuring tness and precision. however, we may
integrate any other quality metrics. both metrics are computed on the traces
of the original event log l. as in [36], we consider the classical f-measure to
level tness and precision with equal weights as dened by:
f measure = 2fitnessprecision
fitness +precision(9)
410
we denote by slthe sample of lthat collects the traces contributing to
discover the process model that better conforms to the initial event log l. we
19populatesliteratively by exploring traces of laccording to a forward trace
selection procedure. starting from an empty slset, we iteratively test the
eect of moving a trace from ltoslby using the f-measure as a quality 415
criterion. at each iterative selection step, we identify the trace best2lthat
maximizes the f-measure of the process model that can be discovered from
sl[fbestg. the trace bestis denitely moved from ltoslif and only
if the f-measure of the process model discovered from sl[fbestgis greater
than the f-measure of process model discovered from sl. the forward trace 420
selection procedure stops when all the traces have been moved from ltosl
or the f-measure decreases (i.e., no improvement is obtained).
the pseudo-code of the process discovery step is shown in algorithm 1. note
that it performs a greedy search that may testjlj(jlj+1)
2process models, at
worst. the result of this search depends on both the quality measure selected to 425
evaluate the process model and the algorithm wrapped for the process discovery.
in principle, any quality criterion, as well any process discovery algorithm, may
be considered for this step.
final considerations concern the fact that the described process discovery
step may be adopted to explore any set of distinct variant-traces. therefore, 430
it can be also used to explore the distinct variant-traces of the original event
logl, as well as the subset of distinct variant-traces extracted from lwith a
sampling procedure (e.g., frequency-based sampling).
from this moment on, the baseline conguration that returns the process
model discovered from las output is denoted as promise . the upgrade con- 435
guration that returns the process model discovered from slas output is
denoted as promise+.
5. experiments
to evaluate the eectiveness of the proposed method, we have conducted a
broad range of experiments on several benchmark event logs. the main objective 440
of this experimental study is to investigate the performance of both promise
20algorithm 1: process discovery step
data:l(event log summary), l(original event log)
result:processmodel(process model)
1begin
2f= 0;
3sl=
4improvement =true
5 whilel6=andimprovement do
6fbest= 0
7 for2ldo
8 processmodel =process discovery(sl[fg)
9 f= f-measure( processmodel;l );
10 iff >fbestthen
11 fbest=f
12 best=
13 processmodel best=processmodel
14 iffbest>fthen
15 f=fbest
16 processmodel=processmodel best
17 sl=sl[fbestg
18 l=lnfbestg
19 else
20 improvement =false
21 returnprocessmodel
and its upgrade promise+in generating a compact version of event logs prior
to invoking process discovery algorithms. specically, we aim to answer the
following research questions:
q1: is the proposed method able to improve the quality and complexity of the 445
21discovered process models by also varying the process discovery algorithm
wrapped-in?
q2: how does the proposed method aect the time spent completing the dis-
covery process?
q3: does the proposed method outperform related ltering or sampling meth- 450
ods even when also related methods are coupled with the greedy search
for improving the quality measure selected for the evaluation?
q4: can the use of the proposed method aid the process discovery algorithm
in revealing relevant qualitative insights of the original process models?
the implementation of the log summarization method experimented in this 455
study is illustrated in section 5.1. the benchmark event logs considered in the
experiments and the experimental setting are presented in section 5.2. finally,
the analysis of the results is illustrated in section 5.3.
5.1. implementation details
we have implemented both promise andpromise+in python 3.6.9 { 64 460
bit version by using keras 2.3.13 library | a high-level neural network api
that adopts tensorflow 1.15.04 as the back-end. this implementation, that is
available via the public github repository,2is used to perform the experiments
illustrated in this study.
in the experiments illustrated in this study, the next-activity classication 465
hypothesis function is learned through an lstm neural network. the neural
network is trained on a pre-processed version of the original event log l. the
pre-processing step is performed to simplify the self-loop events that may appear
in each trace 2l. a self-loop is a sub-sequence of a trace where an activity is
repeated on nconsecutive events with n2. a self-loop with nrepetitions is 470
called an-repetition. every n-repetition of the same event with a 2-repetition
2https://github.com/vinspdb/promise
22by performing an activity dropping operation. this simplication is introduced
to reduce the risk that the lstm is led to learn a next-activity classication
hypothesis function predicting an innite loop on a specic activity. in addition,
as an lstm neural network architecture trained for sequence classication takes 475
equal-length sequences as input, we use the padding technique in combination
with a windowing mechanism [34], in order to standardize dierent sequence
lengths and obtain labeled samples with xed size. the combination of padding
and windowing uses a length equal to w, in order to standardize dierent se-
quence lengths and obtain xed sized sequences. according to this mechanism, 480
dummy events are added to sequences with lengths less than w, while the most
recentwactivities are kept into sequences with lengths greater than w. we
setw= 4 in this experimental study.
in addition, we have used three state-of-the-art process discovery algorithms:
inductive miner [25] and hybrid ilp miner [52] (ver 6-10.154) imported from 485
prom63, split miner [2] imported from apromore4. we have integrated the
implementation of the replay fitness [4], the align-etconformance [1], the
token-based repair generalization [6] and the petri net properties from pm4py,
in order to compute the tness, precision, generalization and model size (num-
ber of transitions, places, and arcs) of the discovered process models, respec- 490
tively. finally, we have used the java plug-in of show petri-net metrics [24]
from prom6, in order to compute the extended cardoso index that measures
the complexity of a process model by its complex structures, i.e., xor, or and
and components. the subprocess module5present in python 3.6.9 is adopted,
in order to run the java plug-in of inductive miner, hybrid ilp miner, split 495
miner and show petri-net metrics through the python code by creating new
processes.
3https://www.promtools.org/doku.php?id=prom610
4https://apromore.org/platform/tools/
5https://docs.python.org/3/library/subprocess.html
23table 4: event logs description: number of activity classes, traces, events, variants and
directly-follows (df) relations.
event log # activities # traces # events # variants # df relations
bpic2012 [12] 23 13087 164506 4336 138
bpic2018 insp [14] 15 5485 197717 3190 67
bpic2019 [13] 42 251734 1595923 11973 538
hospital [29] 18 100000 451359 1020 143
road [27] 11 150370 561470 231 70
sepsis [28] 16 1050 15214 846 115
5.2. experimental setting
5.2.1. event logs and process discovery algorithms
we have used six real-life event logs provided by the 4tu centre for re- 500
search. table 4 reports the characteristics of these event logs. these logs record
executions of business processes in healthcare, nance and trac management.
they are heterogeneous in the number of activity classes (from 11 to 42), num-
ber of traces (from 1050 to 251734), i number of events (from 15214 to 1595932),
number of variants (from 231 to 11973) and number of distinct direct ow re- 505
lations (from 67 to 537). we have considered these event logs to explore to
what extent the method improves the performance of various process discovery
algorithms. we have used the following process discovery algorithms: hybrid
ilp miner [52] (ilp), inductive miner [25] (imi) and split miner [2] (sm).
5.2.2. evaluation metrics 510
we have analyzed the eectiveness of the proposed method (as well as of the
related methods) by evaluating both the complexity and quality of the process
models nally discovered, as well as the eciency of the learning process.
to evaluate the complexity, we have used the model size and the extended
cardoso index. the model size is measured as the number of transitions, number 515
of arcs and number of places. the extended cardoso index [24] is based on the
presence of certain splits and joins in the syntactical process denition. in fact,
it counts the various splits (xor, or, and and) and give each of them a
24certain penalty (e.g., the penalty for a place pis the number of subsets of places
reachable from a place p). 520
to evaluate the quality, we have considered the following metrics that are
commonly used in process mining literature: tness, precision, f-measure of
tness and precision, generalization. in this paper, we refer to tness as the
replay tness [4], to precision as the alignment-based etc precision [1] and
to generalization as the token-based repair generalization [6]. in particular, 525
the replay tness is computed for alignments by returning the percentage of
traces that are completely t, along with a tness value that is calculated as
the average of the tness values of the single traces. the etc precision is
computed by replaying (whether possible) the dierent prexes of the log on
the model. at the reached marking, the set of transitions that are enabled in 530
the process model is compared with the set of activities that follow the prex.
the more the sets are dierent, the lower the precision value is. the more the
sets are similar, the higher the precision is.the etc precision measure is also
adopted in the evaluation of the cluster-based sampling procedure described in
[37] due to its high performance. however, this precision method works only 535
if the replay of the prex on the process model works, otherwise the prex is
not considered for the computation of precision. this is a limit of the metric
to be taken into account for the analysis of precision results. on the other
hand, although various precision measures are formulated in the process mining
eld, the recent study of [43] have concluded that none of the existing precision 540
measures consistently quantify precision. finally, the generalization is measured
performing a token-based replay operation and computing 1  avgt(s
1
freq(t)),
whereavgtis the average of the inner value over all the transitions and freq(t)
is the frequency of tafter the replay. all these metrics have been computed on
the original log. 545
finally, to measure the eciency, we have analyzed the computation time
spent in minutes to complete each step of the method (i.e., \labeled head se-
quence multiset extraction", \next-activity classication", \event log summa-
25rization" and \process model discovery"). the computation times have been
collected running the experiments on intel(r) core(tm) i7-9700 cpu, geforce 550
rtx 2080 gpu, 32gb ram memory, windows 10 home.
note that any other metrics may be used for the evaluation of the quality
and complexity of process models. however, the metrics selected in this study
have been commonly used in various process mining studies [38, 37, 16] for the
evaluation of sampling and ltering approaches. 555
5.2.3. compared methods
in these experiments, we have run both the baseline conguration of the pro-
posed method| promise |and its upgrade| promise+. as an initial baseline
we have considered original that performs the process discovery algorithm on
the original event log without any summarizing mechanism enabled (neither 560
sampling nor ltering). in addition, we have considered the frequency-based
sampling method sampling [16] coupled with the f-measure analysis as a re-
lated method. for sampling, we have sorted the distinct variant-traces of the
original log by frequency and started with the sample composed of the top-
frequent variant-trace. we have added top-frequent variants one-by-one to this 565
sample until the f-measure of the process model discovered with the sample
increases. as an additional related method, we have evaluated the iterative
clustering-based sampling method ( clustering ) described in [36]. note that
alsoclustering method uses f-measure to extract a sample of trace medoids
(selected through an iterative clustering step performed on the original log), 570
which allows the method to improve the f-measure of the process model nally
discovered. note that, as in [36], to elaborate event log summaries, determined
through promise ,promise+,sampling andclustering , we have applied
the process discovery algorithms by disabling the built-in ltering mechanisms
of adopted the process discovery algorithms. 575
on the other hand, ltering is a prominent approach that various process
mining algorithms (comprising inductive miner, hybrid ilp miner and split
miner) implement to handle large amounts of events reducing the complexity of
26the process models, while improving their quality. so, for the completeness of
the experiments, we have also applied the process discovery algorithms to the 580
entire event logs by testing the eect of built-in ltering mechanisms in each
tested process discovery algorithm ( filtering ), respectively. experiments with
ltering mechanisms have been conducted by selecting the best internal lter-
ing threshold set-up on 50 dierent congurations. again, the tested ltering
congurations have been evaluated with respect to the f-measure of the pro- 585
cess model discovered with each conguration. the conguration achieving the
highest f-measure is, nally, selected for the comparative study.
note that, according to the description reported above, all the methods of
this experimental study have been compared coupled with the search of the pro-
cess model that improves the f-measure. this search may be equally conducted 590
by considering any other process model metric.
5.3. results and discussion
in this section, we illustrate how the experimental results collected in the
evaluation study allow us to address the formulated research questions.
5.3.1. q1 and q2 595
we start analysing the overall performance of promise andpromise+in
terms of complexity and quality of process models discovered, as well as time
spent completing the discovery process. this analysis is done to quantify the
eect of the log summarization performed by coupling the proposed abstraction-
based strategy ( promise ) to the removal of the abstraction-generated traces 600
that may decrease the f-measure of the nal process model discovered ( promise+).
so, as a baseline of this initial evaluation we consider original that discovers
process models from initial logs by wrapping the process discovery algorithms
with neither the built-in ltering mechanisms nor the sampling pre-processing
step. this analysis is conducted to explore how the use of an abstraction-based 605
strategy in log summarizing may be an opportunity to improve the perfor-
mance of a process discovery algorithm independently of the possible use of
27table 5: comparison of the process models produced by the original ,promise and
promise+approaches in terms of complexity metrics and varying the process discovery
algorithm - hybrid ilp miner (ilp), inductive miner (imi) and split miner (sm).
conguration metric bpic2012 bpic2018 insp bpic2019 hospital road sepsis
ilporiginal generalization 0.98 0.96 0.92 0.92 0.99 0.92
model size 33x28x426 26x19x324 61x46x1550 29x22x440 16x15x150 31x20x470
cardoso 163 142 561 120 67 209
promise generalization 0.99 0.99 0.98 0.98 0.99 0.97
model size 18x23x130 9x9x21 19x16x88 13x10x100 11x10x37 12x12x68
cardoso 63 27 32 33 17 34
promise+generalization 0.99 0.99 0.98 1.00 1.00 0.97
model size 17x21x108 9x9x21 8x6x16 9x9x21 8x8x16 12x15x60
cardoso 52 27 8 11 8 30
imioriginal generalization 0.98 0.93 0.91 0.94 0.99 0.90
model size 66x37x140 35x20x72 74x31x154 48x25x98 28x25x70 49x38x114
cardoso 52 27 44 37 27 49
promise generalization 0.98 0.99 0.98 0.93 0.99 0.96
model size 37x26x76 12x12x26 35x25x74 15x6x30 11x7x22 24x25x62
cardoso 34 13 31 8 8 28
promise+generalization 0.99 0.99 0.97 0.99 1.00 0.97
model size 23x17x46 12x12x26 15x10x30 9x7x18 9x7x18 22x18x48
cardoso 22 13 12 7 8 22
smoriginal generalization 0.83 0.80 0.69 0.71 0.82 0.72
model size model size 84x31x168 568x80x1136 175x37x350 69x24x138 142x35x284
cardoso cardoso 84 567 174 69 142
promise generalization 0.97 0.99 0.97 0.93 0.99 0.89
model size model size 11x9x22 33x20x66 18x9x36 12x8x24 36x19x72
cardoso cardoso 10 30 12 9 9
promise+generalization 0.99 0.99 0.97 0.99 0.99 0.92
model size model size 10x9x20 27x19x54 11x9x22 10x8x20 27x18x54
cardoso cardoso 10 23 10 9 26
well-known extraction-based strategies (e.g., ltering or sampling). in any case,
we also compare promise+to the pipelines with sampling-based pre-processing
(sampling andclustering ) and the pipeline with ltering ( filtering ) in 610
the analysis of q3.
tables 5 and 6 show the complexity and quality metrics measured on the pro-
cess models discovered with promise ,promise+andoriginal . the analysis
of the complexity metric values shows that both promise andpromise+allow
us to discover process models that are simpler than the baseline process models 615
discovered with original . moreover, we note that the simpler process mod-
els discovered with both promise andpromise+, as expected, always gain
in precision by diminishing the number of unobserved process behaviors (other
traces not recorded in the log) that may be inappropriately covered with the
process models discovered with original . on the other hand, both promise 620
28table 6: comparison of the process models produced by the original ,promise and
promise+approaches in terms of quality metrics and varying the process discovery algorithm
- hybrid ilp miner (ilp), inductive miner (imi) and split miner (sm).
conguration metric bpic2012 bpic2018 insp bpic2019 hospital road sepsis
ilporiginal tness 1.00 1.00 1.00 1.00 1.00 1.00
precision 0.12 0.13 0.36 0.39 0.53 0.20
f-measure 0.21 0.22 0.53 0.57 0.69 0.34
promise tness 0.77 0.47 0.72 0.84 0.92 0.77
precision 0.87 0.95 0.90 0.56 0.93 0.73
f-measure 0.82 0.63 0.80 0.67 0.92 0.75
promise+tness 0.77 0.47 0.78 0.95 0.91 0.75
precision 0.88 0.95 1.00 0.99 1.00 0.86
f-measure 0.82 0.63 0.88 0.97 0.95 0.80
imioriginal tness 1.00 1.00 1.00 1.00 1.00 1.00
precision 0.14 0.15 0.26 0.56 0.63 0.29
f-measure 0.25 0.26 0.41 0.72 0.77 0.34
promise tness 0.86 0.71 0.80 0.82 0.95 0.89
precision 0.75 1.00 0.70 0.54 0.93 0.55
f-measure 0.80 0.83 0.75 0.65 0.94 0.68
promise+tness 0.82 0.71 0.81 0.90 0.94 0.84
precision 0.88 1.00 1.00 1.00 1.00 0.81
f-measure 0.85 0.83 0.90 0.95 0.97 0.82
smoriginal tness 0.98 0.97 1.00 1.00 1.00 0.99
precision 0.46 0.18 0.50 0.75 0.92 0.26
f-measure 0.63 0.31 0.67 0.86 0.96 0.41
promise tness 0.84 0.66 0.78 0.87 0.95 0.67
precision 0.90 0.67 1.00 0.93 0.93 0.89
f-measure 0.87 0.66 0.88 0.90 0.94 0.76
promise+tness 0.84 0.71 0.81 0.94 0.94 0.71
precision 0.91 1.00 1.00 1.00 1.00 0.95
f-measure 0.87 0.83 0.90 0.97 0.97 0.81
andpromise+undergo a slight decrease in tness by discovering process mod-
els that may fail in parsing a few traces recorded in the initial event logs. this
is commonly accepted, whereas the precision value increases signicantly [9].
from this point of view, the f-measure of precision and tness shows that both
promise andpromise+may achieve a better trade-o between precision and 625
tness in the process models discovered than original . the analysis of the gen-
eralization values further supports these conclusions. in fact, both promise
andpromise+provide generalization values that are greater than (or equal
to) the generalization values measured with original . both promise and
promise+always enable the discovery of process models that decrease the 630
risk of covering traces that are not in the initial event logs, but are likely to
29belong to the process that produced the logs.
further considerations concern the evaluation of how promise+can ac-
tually improve promise in terms quality and complexity of process models
discovered. promise+always outperforms promise by discovering process 635
models that achieve the lower model size and extended cardoso index, as well
as the higher f-measure of tness and precision, and the higher generalization.
finally, figure 4 shows the computation time spent completing the various
steps of the compared congurations. these results reveal that both promise
andpromise+spend most of their computation time both training the lstm 640
during the \next-activity classication" step and discovering the nal process
model during the \process model discovery" step. the computation time spent
preparing the training data set during the \labeled head sequence multiset ex-
traction" step, as well as generating the new traces during the \event log sum-
marization" step is negligible. as expected, promise+spends more computa- 645
tion time than promise completing the \process model discovery" step. this
is due to the greedy search performed in promise+for removing traces that,
generated during the \event log summarization" step, may decrease the quality
of the process model discovered. final considerations concern the comparison
withoriginal . this conguration spends all its computation time complet- 650
ing the \process model discovery" step. so, comparing the cumulative time of
original ,promise andpromise+, we note that promise andpromise+
are more time-consuming than original . on the other hand, repeating the
conclusions drawn above, the additional time spent for the summarization in
bothpromise andpromise+allows us to discover a better process model. 655
all the conclusions illustrated above are equally drawn independently on the
process discovery algorithm and the event log tested.
5.3.2. q3
we have compared the process models discovered with promise+to the
models discovered with the filtering ,sampling andclustering methods. 660
all these methods someway take advantage of the f-measure information in the
30figure 4: computation time spent in minutes completing the \labeled head sequence multiset
extraction", \next-activity classication", \event log summarization" and \process model
discovery" steps.
discovery of the nal process model. figures 5 and 6 compare the values of the f-
measure and extended cardoso index measured with promise+,filtering ,
sampling andclustering methods, respectively. to rank the compared
methods, we statistically test whether the improvement of both f-measure and 665
extended cardoso index of the process models discovered with promise+is
signicant over the various event logs. to this aim, we have used friedman's
test [10]. this is a non-parametric test that is commonly used to compare
multiple methods over multiple event logs. it compares the average ranks of
the approaches, so that the best performing approach gets the rank of 1. the 670
second best gets rank 2. the null-hypothesis states that all the methods are
equivalent. under this hypothesis, the ranks of compared methods should be
equal. in this study, we reject the null hypothesis with p-value0:05. as the
null-hypothesis has been rejected, that is, no method has been singled out, we
have used a post-hoc test|the nemenyi test|for pairwise comparisons [10]. 675
the results of this test are reported in figures 7a and 7b for hybrid ilp
miner (ilp), figures 7c and 7d for inductive miner, (imi) and figures 7e and
31figure 5: f-measure of precision and tness: promise+vs related methods ( original ,
filtering ,sampling andclustering ) by varying the process discovery algorithm among
hybrid ilp miner (ilp), inductive miner (imi) and split miner (sm).
7f for split miner (sm). they show that promise+enables the discovery of
the process models that commonly achieve the highest f-measure by having
filtering as runner-up with ilp, while sampling as runner-up with imi 680
and sm. on the other hand, sampling commonly enables the discovery of
the simplest process models independently of the process discovery algorithm.
so, based upon the previous considerations, sampling is the most relevant
competitor of promise+in this study. in particular, sampling works better
thanpromise+in event logs with traces distributed according to the pareto 685
distribution (a large portion of log traces is held by a small fraction of top-
frequent variants). for example, sampling works better than promise+
in hospital, where sampling with imi selects the top-six frequent variants
that cover 87.38% of traces in the event log (see figure 8a). on the other
hand, promise+works better than sampling in event logs that disregard 690
the pareto distribution (the majority of traces in the log is spanned on a high
number of top-frequent variants). for example, this happens in bpic2018 insp
(see figure 8b).
in general, this analysis shows that the proposed abstraction-based strat-
32figure 6: extended cardoso index: promise+vs related methods ( original ,filtering ,
sampling andclustering ) by varying the process discovery algorithm among hybrid ilp
miner(ilp), inductive miner (imi) and split miner (sm).
(a)ilp{ f-measure
 (b)ilp{ extended cardoso
(c)imi{ f-measure
 (d)imi { extended cardoso
(e)sm{ f-measure
 (f)sm{ extended cardoso
figure 7: nemenyi test of f-measure and extended cardoso index on process models discov-
ered using hybrid ilp miner (ilp) (a-b), inductive miner (imi) (c-d) and split miner (sm)
(e-f) with both promise+and the related methods ( original ,filtering and cluster-
ing). groups of methods that are not signicantly dierent (at p0:05) are connected.
33(a) hospital
 (b) bpic2018 insp
figure 8: frequency distribution (axis y) of 20 - top frequent variant-traces (axis x) in
hospital (figure 8a) and bpic2018 insp (figure 8b).
egy may be an eective alternative to the extraction-based strategies already 695
formulated in the process discovery eld. in this regard, the experiments show
that it can summarize the event log by improving the performance of various
process discovery algorithms. in any case, there is no summarization method
that systematically enables the discovery of the simpler process models with the
highest quality in all the event logs. in fact, the performance of the compared 700
methods also depends on the characteristics of the event logs (in addition to the
peculiarities of the process discovery algorithms).
5.3.3. q4
we complete this study by giving a qualitative understanding of the outcome
of the process models discovered with filtering ,sampling ,clustering 705
andpromise+. as an example, let us consider the bpic2018 insp event log.
figures 9a, 9b, 9c and 9d show the process models discovered with filtering ,
sampling ,clustering andpromise+from bpic2018 insp using inductive
miner (imi). in accordance with the conclusions already drawn by analyzing
the complexity metrics measured with these methods (q3), the process model 710
discovered with filtering is the most complex, while the process models dis-
covered with both sampling andpromise+are the simplest. on the other
hand, the process models discovered with both sampling andpromise+mea-
sure the highest f-measure with a precision equal to 1 and a tness equal to
0.67 and 0.71, respectively. the higher tness of promise+is achieved thanks 715
34(a)filtering - precision=0.63, tness=0.80, f-measure=0.70, model size=43 3192, ex-
tended cardoso index = 42.
(b)sampling - precision=1.00, tness=0.67, f-measure=0.80, model size=8 816, ex-
tended cardoso index = 8.
(c)clustering - precision=0.63, tness=0.96, f-measure=0.76, model size=21 1848,
extended cardoso index = 23.
(d)promise+- precision=1.00, tness=0.71, f-measure=0.83, model size=12 1226, ex-
tended cardoso index = 13.
figure 9: comparison of process models discovered by imi with dierent sampling methods
on bpic2018 insp.
to the ability of capturing the behavior comprising the sub-sequence of activi-
ties "prepare external" and "abort external". this subsequence also appears in
the process model discovered with clustering (that achieves tness equal to
0.96), but disappear in the process model discovered with sampling .
356. conclusion 720
in this paper, we have presented promise+{ a method that generates event
log summaries by using an abstraction-based summarization strategy. this
strategy leverages a next-activity classication function learned by training a
deep neural network architecture composed of lstm modules. the log sum-
mary is then used for the process discovery. experimental results on dierent 725
benchmark event logs show that the proposed method provides synthesized logs
that enable the discovery of process models with high quality according to the
f-measure metric. in particular, the results indicate that promise+is able
to generate proper new traces (that may not appear in the original event log)
and the resulting log summaries enable process discovery algorithms to return 730
process models with a good balance between quality measures. in any case, the
comparison with the extraction-based counterpart (i.e., sampling ) highlights
that both approaches are competitive to improve the quality of discovered pro-
cess models according to the f-measure metric. so, as future work we plan to
explore log summarization approaches to other event logs with specic domain 735
knowledge, to better identify which log characteristics contribute more to the
success of an extraction or abstraction strategy.
another important question to address is the selection of the seed sequences
for the trace generation in our method. in this study, we use a length-based cri-
terion to select the initial seeds. however, this may lead to select seed sequences 740
that are the head of an infrequent behavior in the event log. we have handled
this issue through a process discovery step that performs a greedy search to
remove traces generated for the summary that may be outliers. in any case, al-
ternative criteria may be studied to determine seeds by exploring properties like
frequency, similarity or structure. the impact of these properties on ranking 745
trace-variants has been recently explored in [16] for sampling.
also, experimental results have shown that the proposed method provides
a good balance between quality and complexity in the process models nally
discovered. one main advantage of the method is that the discovered pro-
36cess models are not only precise, but also simple and consequently, easier to 750
understand and explainable. another advantage is that the process model is
discovered from event data generated with a next-activity predictive model. so
it may also be seen as a way to provide an easy-to-interpret, graphical explana-
tion of the expected behavior of the predictive model even when it is a black-box
model learned by deep neural networks (such as lstms). hence, a direction for 755
future works would be to validate how explainable the resulting process models
are for end-users. indeed the level of explainability of a process model may have
a signicant impact on its usability. hence our research can be considered as a
step towards adding explanations to business process models, paving the way to
create algorithms capable to discover process models with desirable properties 760
of explainability and usability.
finally, this study opens the door also for dierent research directions. for
example, besides process discovery, the proposed summarization method could
be designed for conformance checking.
7. acknowledgment 765
the research of vincenzo pasquadibisceglie is funded by pon ri 2014-2020
- big data analytics for process improvement in organizational development
- cup h94f18000270006.
references
[1] adriansyah, a., munoz-gama, j., carmona, j., van dongen, b. f., & w. 770
van der aalst (2015). measuring precision of modeled behavior. inf. syst.
e bus. manag. ,13, 37{67.
[2] augusto, a., conforti, r., dumas, m., la rosa, m., & polyvyanyy, a.
(2019). split miner: automated discovery of accurate and simple business
process models from event logs. knowl. inf. syst. ,59, 251{284. 775
37[3] bauer, m., senderovich, a., gal, a., grunske, l., & weidlich, m. (2018).
how much event data is enough? a statistical framework for process discov-
ery. in j. krogstie, & h. a. reijers (eds.), advanced information systems
engineering (pp. 239{256). cham: springer international publishing.
[4] berti, a., & aalst, w. (2019). reviving token-based replay: increas- 780
ing speed while improving diagnostics. in proceedings of the interna-
tional workshop on algorithms and theories for the analysis of event
data (ataed 2019) (pp. 87{103). volume 2371 of ceur workshop pro-
ceedings .
[5] bose, r. p. j. c., & w. van der aalst (2009). abstractions in process 785
mining: a taxonomy of patterns. in u. dayal et al. (ed.), business process
management, 7th international conference, bpm 2009, proceedings (pp.
159{175). springer volume 5701 of lncs .
[6] buijs, j., dongen, van, b., & aalst, van der, w. (2014). quality dimensions
in process discovery : the importance of tness, precision, generalization 790
and simplicity. international journal of cooperative information systems ,
23, 1440001/1{39. doi: 10.1142/s0218843014400012 .
[7] camargo, m., dumas, m., & rojas, o. g. (2019). learning accurate lstm
models of business processes. in t. t. hildebrandt et al. (ed.), business
process management - 17th international conference, bpm 2019, proceed- 795
ings (pp. 286{302). springer volume 11675 of lncs .
[8] conforti, r., rosa, m. l., & t. hofstede, a. h. m. (2017). filtering out
infrequent behavior from business process event logs. ieee transactions
on knowledge and data engineering ,29, 300{314.
[9] de weerdt, j., de backer, m., vanthienen, j., & baesens, b. (2011). a 800
robust f-measure for evaluating discovered process models. in 2011 ieee
symposium on computational intelligence and data mining (cidm) (pp.
148{155).
38[10] dem sar, j. (2006). statistical comparisons of classiers over multiple data
sets. j. mach. learn. res. ,7, 1{30. 805
[11] van derwerf, j. m. e. m., van dongen, b. f., hurkens, c. a. j., & sere-
brenik, a. (2009). process discovery using integer linear programming.
fundam. inf. ,94, 387{412.
[12] van dongen, b. (2012). bpi challenge 2012. doi: 10.4121/uuid:
3926db30-f712-4394-aebc-75976070e91f . 810
[13] van dongen, b. (2019). bpi challenge 2019. doi: 10.4121/uuid:
d06aff4b-79f0-45e6-8ec8-e19730c248f1 .
[14] van dongen, b., & borchert, f. f. (2018). bpi challenge 2018. doi: 10.
4121/uuid:3301445f-95e8-4ff0-98a4-901f1f204972 .
[15] el-kassas, w. s., salama, c. r., rafea, a. a., & mohamed, h. k. (2021). 815
automatic text summarization: a comprehensive survey. expert systems
with applications ,165, 113679.
[16] fani sani, m., van zelst, s., & w. van der aalst (2021). the impact
of biased sampling of event logs on the performance of process discovery.
computing , (pp. 1{20). 820
[17] w. van der aalst (2009). process-aware information systems: lessons to
be learned from process mining. in k. jensen, & w. van der aalst (eds.),
transactions on petri nets and other models of concurrency ii: special
issue on concurrency in process-aware information systems (pp. 1{26).
berlin, heidelberg: springer berlin heidelberg. 825
[18] w. van der aalst (2010). process discovery: capturing the invisible. ieee
computational intelligence magazine ,5, 28{41.
[19] w. van der aalst (2016). process mining - data science in action, second
edition . springer.
39[20] w. van der aalst (2018). process discovery from event data: relating 830
models and logs through abstractions. wiley interdiscip. rev. data min.
knowl. discov. ,8.
[21] w. van der aalst, rubin, v. a., verbeek, h. m. w., van dongen, b. f.,
kindler, e., & g unther, c. w. (2010). process mining: a two-step approach
to balance between undertting and overtting. softw. syst. model. ,9, 87{ 835
111.
[22] hochreiter, s., & schmidhuber, j. (1997). long short-term memory. neural
computation ,9, 1735{1780.
[23] hutter, f., hoos, h. h., & leyton-brown, k. (2011). sequential model-
based optimization for general algorithm conguration. in c. a. c. coello 840
(ed.), learning and intelligent optimization - 5th international confer-
ence, lion 2011, selected papers (pp. 507{523). springer volume 6683 of
lncs .
[24] lassen, k. b., & w. van der aalst (2009). complexity metrics for workow
nets. information and software technology ,51, 610{626. 845
[25] leemans, s. j. j., fahland, d., & van der aalst, w. (2013). discovering
block-structured process models from event logs - a constructive approach.
in j.-m. colom, & j. desel (eds.), application and theory of petri nets
and concurrency (pp. 311{329). berlin, heidelberg: springer berlin hei-
delberg. 850
[26] leemans, s. j. j., fahland, d., & van der aalst, w. (2013). discovering
block-structured process models from event logs containing infrequent be-
haviour. in n. lohmann et al. (ed.), business process management work-
shops - bpm 2013 international workshops, revised papers (pp. 66{78).
springer volume 171 of lnbip . 855
[27] de leoni, m. m., & mannhardt, f. (2015). road trac ne management
process. doi: 10.4121/uuid:270fd440-1057-4fb9-89a9-b699b47990f5 .
40[28] mannhardt, f. (2016). sepsis cases - event log. doi: 10.4121/uuid:
915d2bfb-7e84-49ad-a286-dc35f063a460 .
[29] mannhardt, f. (2017). hospital billing - event log. doi: 10.4121/uuid: 860
76c46b83-c930-4798-a1c9-4be94dfeb741 .
[30] mitsyuk, a. a., shugurov, i. s., kalenkova, a. a., & w. van der aalst
(2017). generating event logs for high-level process models. simulation
modelling practice and theory ,74, 1{16.
[31] murphy, k. p. (2012). machine learning: a probabilistic perspective . mit 865
press.
[32] pasquadibisceglie, v., appice, a., castellano, g., & malerba, d. (2019).
using convolutional neural networks for predictive process analytics. in
2019 international conference on process mining (icpm) (pp. 129{136).
[33] pasquadibisceglie, v., appice, a., castellano, g., & malerba, d. (2020). 870
predictive process mining meets computer vision. in d. fahland et al. (ed.),
business process management forum - bpm forum 2020, proceedings (pp.
176{192). springer volume 392 of lnbip .
[34] pasquadibisceglie, v., appice, a., castellano, g., & malerba, d. (2021).
a multi-view deep learning approach for predictive business process moni- 875
toring. ieee transactions on services computing (2021) , .
[35] pasquadibisceglie, v., appice, a., castellano, g., malerba, d., & mod-
ugno, g. (2020). orange: outcome-oriented predictive process monitor-
ing based on image encoding and cnns. ieee access ,8, 184073{184086.
[36] sani, m. f., boltenhagen, m., & w. van der aalst (2020). prototype 880
selection using clustering and conformance metrics for process discovery. in
a. del-r o-ortega et al. (ed.), business process management workshops
- bpm 2020 international workshops, revised selected papers (pp. 281{
294). springer volume 397 of lnbip .
41[37] sani, m. f., van zelst, s. j., & van der aalst, w. (2020). improving the 885
performance of process discovery algorithms by instance selection. comput.
sci. inf. syst. ,17, 927{958.
[38] sani, m. f., van zelst, s. j., & w. van der aalst (2017). improving process
discovery results by ltering outliers using conditional behavioural prob-
abilities. in e. teniente, & m. weidlich (eds.), business process man- 890
agement workshops - bpm 2017 international workshops, revised papers
(pp. 216{229). springer volume 308 of lnbip .
[39] sani, m. f., van zelst, s. j., & w. van der aalst (2018). applying sequence
mining for outlier detection in process mining. in h. panetto et al. (ed.),
on the move to meaningful internet systems. otm 2018 conferences, 895
proceedings, part ii (pp. 98{116). springer volume 11230 of lncs .
[40] sani, m. f., van zelst, s. j., & w. van der aalst (2019). the impact of event
log subset selection on the performance of process discovery algorithms. in
t. welzer et al. (ed.), new trends in databases and information systems,
adbis 2019 short papers, proceedings (pp. 391{404). springer volume 900
1064 of communications in computer and information science .
[41] skydanienko, v., francescomarino, c. d., ghidini, c., & maggi, f. m.
(2018). a tool for generating event logs from multi-perspective declare
models. in w. van der aalst et al. (ed.), proceedings of the dissertation
award, demonstration, and industrial track at bpm 2018 (pp. 111{115). 905
volume 2196 of ceur workshop proceedings .
[42] suriadi, s., andrews, r., ter hofstede, a., & wynn, m. (2017). event log
imperfection patterns for process mining: towards a systematic approach
to cleaning event logs. information systems ,64, 132{150.
[43] tax, n., lu, x., sidorova, n., fahland, d., & w. van der aalst (2018). 910
the imprecisions of precision measures in process mining. information
processing letters ,135, 1{8.
42[44] tax, n., sidorova, n., & van der aalst, w. (2019). discovering more precise
process models from event logs by ltering out chaotic activities. j. intell.
inf. syst. ,52, 107{139. 915
[45] tax, n., sidorova, n., haakma, r., & w. van der aalst (2018). event ab-
straction for process mining using supervised learning techniques. in y. bi
et al. (ed.), proceedings of sai intelligent systems conference (intellisys)
2016 (pp. 251{269). springer international publishing.
[46] tax, n., teinemaa, i., & van zelst, s. j. (2020). an interdisciplinary com- 920
parison of sequence modeling methods for next-element prediction. soft-
ware and systems modeling , (pp. 1619{1374).
[47] tax, n., verenich, i., la rosa, m., & dumas, m. (2017). predictive busi-
ness process monitoring with lstm neural networks. in e. dubois, &
k. pohl (eds.), international conference on advanced information sys- 925
tems engineering (pp. 477{492). springer.
[48] van der aalst, w., weijters, t., & maruster, l. (2004). workow min-
ing: discovering process models from event logs. ieee transactions on
knowledge and data engineering ,16, 1128{1142.
[49] vanden broucke, s. k., & de weerdt, j. (2017). fodina: a robust and 930
exible heuristic process discovery technique. decision support systems ,
100, 109{118. smart business process management.
[50] weijters, a., & ribeiro, j. (2011). flexible heuristics miner (fhm). (pp.
310{317).
[51] van zelst, s., van dongen, b., & van der aalst, w. (2018). event stream- 935
based process discovery using abstract representations. knowl. inf. syst. ,
54, 407{435.
[52] van zelst, s. j., van dongen, b. f., & w. van der aalst (2015). avoiding
over-tting in ilp-based process discovery. in h. r. motahari-nezhad et
43al. (ed.), business process management (pp. 163{171). cham: springer 940
international publishing.
[53] van zelst, s. j., van dongen, b. f., w. van der aalst, & verbeek, h.
m. w. (2018). discovering workow nets using integer linear programming.
computing ,100, 529{556.
[54] van zelst, s. j., mannhardt, f., de leoni, m., & koschmider, a. (2020). 945
event abstraction in process mining: literature review and taxonomy.
granular computing , (pp. 1{18).
44