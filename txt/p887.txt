heuristic approaches for generating local process models
through log projections
tax, n.; sidorova, n.; van der aalst, w.m.p.; haakma, r.
published in:
proceedings of ieee symposium on computational intelligence and data mining, december 6-9, 2016, athens,
greece
doi:
10.1109/ssci.2016.7849948
published: 01/01/2017
document version
accepted manuscript including changes made at the peer-review stage
please check the document version of this publication:
‚Ä¢ a submitted manuscript is the author's version of the article upon submission and before peer-review. there can be important differences
between the submitted version and the official published version of record. people interested in the research are advised to contact the
author for the final version of the publication, or visit the doi to the publisher's website.
‚Ä¢ the final author version and the galley proof are versions of the publication after peer review.
‚Ä¢ the final published version features the final layout of the paper including the volume, issue and page numbers.
link to publication
citation for published version (apa):
tax, n., sidorova, n., van der aalst, w. m. p., & haakma, r. (2017). heuristic approaches for generating local
process models through log projections. in proceedings of ieee symposium on computational intelligence and
data mining, december 6-9, 2016, athens, greece (pp. 1-8). piscataway: institute of electrical and electronics
engineers (ieee). doi: 10.1109/ssci.2016.7849948
general rights
copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright owners
and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
            ‚Ä¢ users may download and print one copy of any publication from the public portal for the purpose of private study or research.
            ‚Ä¢ you may not further distribute the material or use it for any profit-making activity or commercial gain
            ‚Ä¢ you may freely distribute the url identifying the publication in the public portal ?
take down policy
if you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately
and investigate your claim.
download date: 14. jan. 2018heuristic approaches for generating local process
models through log projections
niek tax, natalia sidorova, wil m. p. van der aalst
department of mathematics and computer science
eindhoven university of technology
p.o. box 513, 5600mb eindhoven, the netherlands
email:fn.tax,n.sidorova,w.m.p.v.d.aalst g@tue.nlreinder haakma
philips research
prof. holstlaan 4, 5665 aa eindhoven
the netherlands
email: reinder.haakma@philips.com
abstract ‚Äîlocal process model (lpm) discovery is focused
on the mining of a set of process models where each model
describes the behavior represented in the event log only partially,
i.e. subsets of possible events are taken into account to create so-
called local process models. often such smaller models provide
valuable insights into the behavior of the process, especially when
no adequate and comprehensible single overall process model
exists that is able to describe the traces of the process from start
to end. the practical application of lpm discovery is however
hindered by computational issues in the case of logs with many
activities (problems may already occur when there are more than
17 unique activities). in this paper, we explore three heuristics to
discover subsets of activities that lead to useful log projections
with the goal of speeding up lpm discovery considerably
while still Ô¨Ånding high-quality lpms. we found that a markov
clustering approach to create projection sets results in the largest
improvement of execution time, with discovered lpms still being
better than with the use of randomly generated activity sets of the
same size. another heuristic, based on log entropy, yields a more
moderate speedup, but enables the discovery of higher quality
lpms. the third heuristic, based on the relative information gain,
shows unstable performance: for some data sets the speedup and
lpm quality are higher than with the log entropy based method,
while for other data sets there is no speedup at all.
i. i ntroduction
process mining is an area of research that combines methods
and techniques from computational intelligence, data mining,
and process analysis to extract novel insights from event
data [ 1]. one of the most prominent tasks within the process
mining Ô¨Åeld is process discovery , where the goal is to discover
a process model that accurately describes some sequences
‚Äì called traces ‚Äì of event data from start to end. in some
application domains a high degree of variability can be found
in such traces. an example of such a domain is human behavior
[2], with events registered e.g. in smart homes, by wearables,
or manually in so called lifelogs. another application domain,
where the variability in the process is often large is the medical
workÔ¨Çows of patient care [3].
existing process discovery algorithms (e.g. [ 4], [5]) often fail
to generate insightful models on such event logs and generate
process models in which any sequence of events is allowed,
often referred to as the Ô¨Çower model . more insight in such
event logs can often be obtained using declarative process
discovery algorithms (e.g. [ 6], [7]). declarative models describe
the behavior through a set of constraints on activities (eventtypes), e.g. binary constraint ‚Äúeach activity ais always followed
by activityb‚Äù. they primarily focus on binary constrains, with
a limited set of non-binary extensions by branching, like ‚Äúeach
activityais always followed by activity bor activityc‚Äù. richer
models of unstructured processes can be discovered using local
process model (lpm) discovery [ 8]. lpms describe frequent
behavioural patterns in a process modeling notation (e.g. petri
nets, bpmn, uml activity diagrams), allowing each pattern
to have full expressive power of the respective process model
notation. lpms allow for complex relations between activities.
fig. 1 shows some of the lpms discovered on the log
extracted from mimic-ii [ 9], a medical database containing
147461 logged medical procedure events from 1734 activities
for28280 patients collected between 2001 and 2008 from
multiple intensive care units (icus) in the usa. the Ô¨Årst lpm
indicates that the placement of continuous invasive mechanical
ventilation on a patient is 3638 out of 8291 times followed by
either arterial catheterization or by one or more instances of
infusion of concentrated nutrition. the second lpm shows that
the placement of invasive mechanical ventilation frequently
co-occurs with the insertion of an endotracheal tube. the
third lpm shows that a sequence of one or more hemodialysis
events is always followed by the placement of a venous catheter
for renal dialysis, and from the numbers we can deduce that
there are on average almost 4hemodialysis events before one
placement of a venous catheter. the fourth lpm in fig. 1
shows that all placements of a continuous invasive mechanical
ventilation are eventually followed by the placement of a
non-invasive mechanical ventilation. however, on average 8
continuous invasive mechanical ventilations have been applied
before the placement of a non-invasive mechanical ventilation.
the lpms in fig. 1 cannot be discovered using the brute-
force approach described in [ 8]. there are 1734 activities in
the log, yielding an incredible number of models. the computa-
tional complexity of lpm discovery grows combinatorially with
the number of activities in the event log, hindering the practical
application to many real life event logs. however, lpms can be
discovered on a projection on a subset of the activities in the log,
as long as the set of activities projected on contains at least the
activities in the lpm. the top lpm in fig. 1 can for example
be discovered using a projection on just the activities contin-
uous invasive mech ,arterial catheterization ,arxiv:1610.02876v1  [cs.lg]  10 oct 2016continuous
inv asive
mech
3638/8291arterial
catheter-
ization
1028/2478ext infus
conc
nutrition
2710/5543
continuous
inv asive
mech
6080/8291
insert en-
dotracheal
tube 6080/6432
hemodia-
lysis
1314/1314
venous cath
for renal
di339/700
continuous
inv asive
mech
8291/8291
non-inv asive
mechanical
790/2193
fig. 1. local process models discovered using projection set discovery on
the mimic ii medical procedure data set
andext infus conc nutrition . in this paper, we explore
and compare three heuristic approaches to lpm mining, using
intelligently chosen subsets of the activities. by applying lpm
discovery to an event log projected on the subsets of activities
that are discovered with the heuristic approaches the computa-
tional time of discovery decreases considerably. for example,
the lpms in fig. 1 were discovered in 12seconds using the
markov clustering approach that we will discuss in section iv.
we start by introducing basic concepts and notations in
section ii. in section iii we discuss quality criteria and rankings
of local process models. in section iv we introduce the
three heuristic approaches to discovery of projection sets. we
introduce an experimental setup for evaluation of heuristics
and describe Ô¨Åve real-life event logs used for evaluation in
section v. section vi contains the results of the experiments
and their discussion. we discuss related work in section vii
and conclude this paper in section viii.
ii. p reliminaries
in this section we introduce concepts used in later sections
of this paper.
xdenotes the set of all sequences over a set xand
=ha1;a2;:::;ania sequence of length n;hiis the empty
sequence and 12is the concatenation of sequences 1;2.
in the context of process logs, we assume the set of all
process activities lto be given. an eventein an event
log is the occurrence of an activity e2l. we call a
sequence of events 2
latrace . an event logl2n
l
is a Ô¨Ånite multiset of traces. for example, the event log
l= [ha;b;ci2;hb;a;ci3]consists of 2 occurrences of traceha;b;ciand three occurrences of trace hb;a;ci. a projection
of sequence 2xon a subset x0xis denoted by
x0. for example,ha;b;c;a;b;cifa;cg=ha;c;a;ci. we also
lift the projection operator to multi-sets of sequences. for
example, [ha;b;c;a;b;ci3;ha;c;a;d;ci2;ha;c;d;ci4]fa;cg=
[ha;c;a;ci5;ha;c;ci4].
in the context of process mining, commonly used statistics
over the log are related to the activities directly follow-
ing/preceding each other in the traces of the log [ 1]. the
directly follows ratio dfr(a;b;l )for activities (a;b)in logl
is the ratio of occurrences of athat are directly followed by
bin event log lto the total number of the occurrences of a
inl. the directly precedes ratio of(a;b)in logl, denoted
dpr(a;b;l ), is the ratio of occurrences of adirectly preceded
by abto the total number of the occurrences of ainl.
assuming an arbitrary but Ô¨Åxed order over the activities of
l,dpr(a;l)anddfr(a;l)represent respectively the vectors
of values dpr(a;b;l ),dfr(a;b;l )for allb2l.
petri nets is a process modeling formalism frequently used
in process modeling and process mining. a petri net is a
directed bipartite graph consisting of places (depicted as circles)
and transitions (depicted as rectangles), connected by arcs.
transitions represent activities, while places represent the
enabling conditions of transitions. labels are assigned to
transitions to indicate the type of activity that they model.
a special label is used to represent invisible transitions
(depicted as black rectangles), which are only used for routing
purposes and not recorded in the execution log.
deÔ¨Ånition 1 (labeled petri net): alabeled petri net n=
hp;t;f; m;`iis a tuple where pis a Ô¨Ånite set of places,
tis a Ô¨Ånite set of transitions such that p\t=;,f
(pt)[(tp)is a set of directed arcs, called the Ô¨Çow
relation, mis a Ô¨Ånite set of labels representing activities,
with =2mbeing a label representing invisible events, and
`:t!m[fgis a labeling function that assigns a label
to each transition.
for a node n2(p[t)we usenandnto denote the
set of input and output nodes of n. a state of a petri net is
deÔ¨Åned by its markingm2npbeing a multiset of places.
a marking is graphically denoted by putting m(p)tokens on
each placep2p. a pair (n;m )is called a marked petri net.
state changes occur through transition Ô¨Årings. a transition t
is enabled (can Ô¨Åre) in a given marking mif each input place
p2tcontains at least one token. once a transition Ô¨Åres, one
token is removed from each input place of tand one token
is added to each output place of t, leading to a new marking.
a Ô¨Åring of a transition tleading from marking mto marking
m0is denoted as mt !m0.m1 !m2indicates that m2
can be reached from m1through Ô¨Åring sequence 2t.
often, it is useful to consider a petri net in combination
with an initial marking and a set of possible Ô¨Ånal markings.
this allows us to deÔ¨Åne the language accepted by the petri net
as a set of Ô¨Ånite sequences of activities.
deÔ¨Ånition 2 (accepting petri net): anaccepting petri net
is a triple apn = (n;m 0;mf), wherenis a labeled petri
net,m02npis its initial marking, and mfnpis its setof possible Ô¨Ånal markings, such that 8m1;m22mfm16m2.
a sequence 2
mis a trace of an accepting petri net
apn if there exists a Ô¨Åring sequence m0 !mfsuch that
mf2mf,2tand`() =. the language l(apn )
is the set of all its traces, which can be inÔ¨Ånite when apn
contains one or more loops.
the four models in fig. 1 are accepting petri nets. places
that belong to the initial marking contain a token and
places belonging to a Ô¨Ånal marking are simply marked
as , since the nets in the Ô¨Ågure have a single Ô¨Ånal
marking. the language of the accepting petri net at the
bottom consists of all the sequences starting with one
of more continuous invasive mech followed by
non-invasive mechanical .
iii. l ocal process models and their rankings
local process model (lpm) discovery generates a set of
lpms that each individually describe some frequent behavior
in the event log. the quality of an lpm ln is calculated
using segmentation of traces from a log into sequences
0112:::kk, withi2l(ln)andi=2l(ln), such
that the number of events in 1:::kis maximized: the higher
the number of events in segments, the larger the share of
traces of the log explained by the lpm. the ranking of local
process models is based on a weighted average over Ô¨Åve
quality criteria in a zero to one range, as described in [8]:
support the frequency of the behavior described by the lpm
in the event log, i.e. the number of trace segments in the
log that Ô¨Åt l(ln).
conÔ¨Ådence the share of events of the activities described by
the lpm that abide to the behavior described by the lpm,
i.e., are in a i2l(ln)segment.
language Ô¨Åt the ratio of traces from l(ln)that were
observed at least once in the event log to jl(ln)j
(bounded up to a certain length for models with loops).
determinism this metric reÔ¨Çects the average number of
enabled transitions in each marking of the lpm reached
while replaying the event log on the lpm. the intuition
behind this is that a model with a higher degree of non-
determinism captures less information about the ordering
of events.
coverage the frequency of the activities described in the
lpm in the event log.
in [8] we developed an incremental procedure for building
lpms, starting from models with two activities, and
recursively extending them to more activities. the support
and the determinism quality dimensions can be used there for
pruning thanks to their monotonicity with respect to model
extensions, resulting in speedup of lpm discovery because
of a smaller search space of lpms.
iv. d iscovering logprojection sets
local process models (lpms) only contain a subset of
the activities of the log and each lpm can in principle be
discovered on any projection of the log containing the activitiesused in this lpm. in this section we describe three heuristics
for discovery of projection sets ‚Äì subsets of activities to be
used for projecting the log. each heuristic described takes an
event log as input and produces a set of projection sets. these
projection sets could potentially be overlapping, which is a
desired property as interesting patterns might exist within a set
of activitiesfa;b;c;dg, as well as within a set of activities
fa;b;c;eg, and discovering on both subsets individually is
faster than discovering once on fa;b;c;d;eg. none of the
projection sets in this set is a subset of another projection set
to avoid double work, as each lpm that can be discovered on
a certain projection set can also be discovered on a superset
of that same projection set.
a. an approach based on markov clustering
markov clustering [ 10], [11] is a fast and scalable clustering
algorithm for graphs that is based on simulation of Ô¨Çow in
graphs. the main intuition behind markov clustering is that,
while performing a random walk on a graph, the likelihood
of transitioning between two members of the same cluster
is higher than the likelihood of transitioning between two
nodes that are in different clusters. markov clustering takes
as input a markov matrix, i.e. a matrix that describes the
transition probabilities in a markov chain. we generate a
matrixmthat represents the connectedness of two activities
by using the directly-follows and directly-precedes ratios:
mi;j=p
dpr(i;j;l )2+dfr(j;i;l )2, using the l2norm of
the directly precedes ratio and the directly follows ratio. a
markov matrix m0is obtained by normalizing mrow-wise.
the intuition behind using both dpranddfrcombined instead
of either of the two is that it can be both of importance that
activityiis often followed by jand thatjis often preceded
byi; if either of the two is true than there is apparently some
relationi!j. applying markov clustering to m0results in a
set of clusters of activities, where we use each activity cluster
as a projection set. markov clustering simulates a random
walk over a graph by alternating expansion , taking the power
of this matrix, and inÔ¨Çation , taking the entrywise power of
this matrix. the clusters generated by markov clustering can
overlap, which is a desired property in projection set discovery.
the inÔ¨Çation parameter of the markov clustering algorithm is
known to be the main parameter in determining the granularity
of the clustering obtained [11] with markov clustering.
b. log entropy based approach
another approach to generate projections sets is to form
groups of activities such that the categorical probability
distributions over activities preceding or following an activity
in the projected logs are peaked, i.e. far away from the discrete
uniform distribution. when after an occurrence of activity a
observing each other type of activity as the next event in the
projected log is equally likely, the activities can be considered
to be independent of each other. if, on the other hand, the
occurrence of activity aconveys information on the next event
and e.g. makes it very likely that some other activity bis goingto be observed next, the activities in the projection set are
likely to be somehow related.
we use the standard entropy function over cate-
gorical probability distribution xoverjxjelements:
h(x) =p
x2x xlog2(x). we calculate the total entropy
ent(l)of the log statistics in original event log las:
ent(l) =x
a2l(h(dfr(a;l)) +h(dpr(a;l))):
we choose an entropy ratio threshold parameter rto indicate
the maximum entropy of the log statistics of a log projection.
we start from a set of elementary log projection sets, s1=
ffagja2lg. we proceed iteratively as follows: we deÔ¨Åne
si+1=fa[bja2s0
i;b2s1:b*agand select those pro-
jection sets of si+1that lead to the log projections whose total
entropy does not exceed the threshold deÔ¨Åned by parameter r:
s0
i+1=faja2si+1^ent(la)rent(l)g. the procedure
stops ifs0
i+1=;orsi+1= l. since lpms that can be
discovered using some projection set can also be discovered
using a superset of this projection set, projection sets that are
subsets of other projection sets are removed from the Ô¨Ånal
result:sÔ¨Ånal=faj9s0
i: (a2s0
i^(8s0
j;8b2s0
j:a6b))g.
c. maximal relative information gain based approach
a more local perspective on entropy-based projection set
discovery would be to compare a projection set with the
projection set of the previous time step, instead of the original
log. we add an activity to a projection set when adding
it strongly decreases the entropy of at least one of the
categorical probability distributions over following or preceding
activities, even when the entropy of other categorical probability
distributions might increase. the intuition behind this is that any
decrease in entropy of a categorical probability distribution over
following or preceding activities indicates that some pattern is
getting stronger by adding that activity and that a lpm could
potentially be found.
we deÔ¨Åne the maximal relative information gain (mrig)
of projection set aover projection set b, withab,
on event log l,mrig (a;b;l ), as the maximal relative
information gain over all the log statistics on l, that is
the ratio of bits for encoding the log statistic that is most
decreased by growing the projection set:
mrig (a;b;l ) = maxa2bmaxfh(dfr(a;lb)) h(dfr(a;la))
h(dfr(a;lb));
h(dpr(a;lb)) h(dpr(a;la))
h(dpr(a;lb))g:
we choose a threshold parameter rto indicate the minimum
value of mrig for considering a projection set as potentially
interesting. like in the log entropy based approach, we start
from the set of elementary log projection sets, s1=ffagja2
lgands0
1=s1. we proceed iteratively deÔ¨Åning si+1=
fa[bja2s0
i;b2s1:b6agands0
i+1=faja2
si+1^9b2s0
i:mrig (a;b;l )> rg, Ô¨Åltering out those
projection sets where adding an extra activity to the projection
set leads to an insufÔ¨Åcient decrease in the number bits of
entropy needed to encode the log statistics. the procedure
stops ifs0
i+1=;orsi+1= l. again, projection setsthat are subsets of other projection sets are Ô¨Ånally removed:
sÔ¨Ånal=faj9s0
i:a2s0
i^(8s0
j;8b2s0
j:a6b)g.
v. e xperimental setup
fig. 2 gives an overview of the methodology for evaluation
of projection set discovery methods. we assess their quality
via the comparison of the quality of the local process models
(lpms) discovered using the projection sets that they generate
with the quality of the lpms discovered on the same log
without the use of projections. first, we apply lpm discovery
to the original, unprojected, event log l, resulting in the ‚Äúideal‚Äù
topkof lpms. then we apply one of the projection set
discovery methods on event log l, resulting in a set qof
projection sets. on each of the projected event logs in lqwe
apply lpm discovery, resulting in jqjklpms. we select
from themkunique best lpms in terms of weighted average
over the quality criteria. we compare the ‚Äúideal‚Äù set of lpms
with the set discovered using projections. theoretically, they
can coincide, or be equally good, if for every ‚Äúideal‚Äù lpm
there is a projection set containing its activities. if it is not the
case, some of the best scoring models will be missing and we
compare how close are the scores of the models present in the
set to the scores of the ‚Äúideal‚Äù lpms.
even with the loss of quality in lpm, projections could be a
good option to opt for, since they allow to signiÔ¨Åcantly improve
the time performance and make lpm discovery possible for
logs with many different activities. we evaluate how our
projection discovery methods perform compared to random
projections as follows: we create a set q0of random projection
sets, consisting ofjqjprojection sets where for each projection
setq2qwe create a random projection set q0lof the
same size as q:jq0j=jqj. we apply lpm discovery on each of
the projected event logs in lq0and selectkbest lpms from
the discovered ones. the projection discovery method works
well if the lpms from top kscore closer to the ‚Äúideal‚Äù top k
than the lpms generated based on the random projection sets.
to obtain statistically relevant results, we create the random
projection sets ten times.
a. metrics for comparison of local process model rankings
we deÔ¨Åne the recall, denoted recall@k , as the fraction of
lpms from the ‚Äúideal‚Äù top kthat are also discovered with
the use of projections. a more nuanced way of comparison
is to look at the weighted average scores of the lpms in the
‚Äúideal‚Äù ranking. the intuition behind this is that not being able
to Ô¨Ånd lpms from the ‚Äúideal‚Äù top kis less severe in case the
alternatively found lpms are also of good quality, i.e. just
below the top k. furthermore, missing the best-scoring lpms
is more severe than missing the lower scoring lpms. for
this reason, we use normalized discounted cumulative gain
(ndcg@k) [ 12], [13], one of the most widely used metrics
for evaluation of a ranking with an ‚Äúideal‚Äù ranking in the Ô¨Åeld
of information retrieval [ 14], that gives more weight to the
top of the rankings than to the lower parts of the rankings.
discounted cumulative gain (dcg) aggregates the relevant
scores of the individual lpms in the ranking of lpms in such afig. 2. evaluation methodology for projection set discovery approaches
way that the graded relevance is reduced in the logarithmic pro-
portion to the position of the result; due to that, more weight is
put on the top of the ranking than on the lower parts of the rank-
ing. dcg is formally deÔ¨Åned as: dcg @k=pk
i=12reli 1
log2(i+1),
wherereliis deÔ¨Åned as the relevance, which is the weighted
average of the model‚Äôs scores on the quality dimensions for
the lpm on position iin the ranking. normalized discounted
cumulative gain (ndcg) is obtained by dividing the dcg
value by the theoretical maximum of the discounted cumulative
gain value, which is called ideal discounted cumulative gain
(idcg). the idcg value is the dcg value obtained from the
ground truth ranking on the lpms discovered on the original,
non-projected log, since all local process models that can be
discovered from projected event logs can also be discovered
from the original event log. normalized discounted cumulative
gain (ndcg) is deÔ¨Åned as:
ndcg @k=dcg @k
idcg @k.
b. data sets for projection discovery experiments
we perform the evaluation on Ô¨Åve different data sets using
the methodology described above. all of the data sets used
originate from the human behavior logging domain, with four
data sets consisting of activities of daily life (adl) and one
consisting of activities performed by an employee in a working
environment. event logs from the human behavior domain are
generally too unstructured to allow for discovery of informative
process models with process discovery techniques, while lpm
discovery allows to discover some relations between activities
at a local level that are not discoverable with usual process
mining techniques. table i gives an overview of main event log
characteristics used in the evaluation. the event logs used for
in the experiments have limited number of activities (at most
14), which allows us to determine the ground truth ranking
of lpms, that is, the ranking of lpms obtained by lpm
discovery on the full log without using projections. however,table i
an overview of the data sets used in the evaluation experiment
data set # of activities # of cases # of events
bpi ‚Äô12 resource 10939 14 49 1682
bruno 14 57 553
chad subject 1900010 10 26 238
ordonez a 12 15 409
van kasteren 14 23 1285
log projections also enable discovery of lpms on datasets with
many more activities, such as the lpms in fig. 1 that were
discovered on a log with 1734 activities. for the van kasteren
data set [ 15] we show and discuss the ranking of lpms for
which we aim to speed up discovery through projections. for
each data set we identiÔ¨Åed a support threshold used for pruning
that allows us to run lpm discovery on the unprojected event
log within reasonable time (max. 10 minutes on a 4-core 2.4
ghz cpu). this value depends on the number of activities as
well as the length of the traces within the event logs, i.e. more
activities and/or longer traces result in a need for a higher
support threshold to Ô¨Ånish the experiment within the time limit.
1) bpic ‚Äô12 resource 10939 data set: the business
process intelligence challenge (bpic)‚Äô12 data set originates
from a personal loan application process in a global Ô¨Ånancial
institution. we transformed the event log to obtain traces of
daily activities of one speciÔ¨Åc employee. we set the pruning
parameter to 0:675for this data set.
2) bruno data set: the bruno et al. [ 16] data set is a public
collection of labeled wrist-worn accelerometer recordings. the
data set is composed of fourteen types of adl events performed
by sixteen volunteers. we set the pruning parameter to 0:6for
this data set.
3) chad data set: the chad database [ 17] consists of
22 exposure and time-use studies that have been consolidated
in a consistent format. in total the database contains 54000
individual days of human behavior, from which we extract
an event log from a randomly chosen study subject such that
each case represents a day. for this data set we set the support
pruning parameter to 0:4.
4) ordonez a data set: the ordonez [ 18] data set consists
of adl events that are performed by two users in their own
homes, recorded through smart home sensors. we use an event
log obtained from sensor events of subject a, with each case
representing a day. we set the pruning parameter to 0:6.
5) van kasteren data set: this data set is a smart home
environment event log described by van kasteren et al. [ 15].
it consists of multidimensional time series data, where each
dimension represents the binary state of an in-home sensor.
these sensors include motion sensors, open/close sensors, and
power sensors (discretized to on/off states). we transform the
multidimensional sensor time series into events by considering
the change points of sensor states as events. we create cases
by grouping events by day, with a cut-off point at midnight.
we set the pruning parameter to 0:675for this data set.
fig. 3 shows the Ô¨Årst Ô¨Åve elements of the local process
model ranking discovered on one of the data sets (van kasteren
[15]). the 5thlpm shows that roughly half of the times that(1)toilet Ô¨Çush
87/143
hall-toilet
door 154/191hall-bathroom
door 146/252weighted average 0.7384
support 0.6840
conÔ¨Ådence 0.6508
language Ô¨Åt 5 1.0000
determinism 0.8165
coverage 0.4560
(2)hall-bathroom
door 147/252toilet Ô¨Çush
85/143
hall-toilet
door 145/191weighted average 0.7315
support 0.6843
conÔ¨Ådence 0.6364
language Ô¨Åt 5 1.0000
determinism 0.8160
coverage 0.4560
(3)toilet Ô¨Çush
93/143
hall-bedroom
door 93/116hall-bathroom
door 136/252weighted average 0.7248
support 0.6809
conÔ¨Ådence 0.6468
language Ô¨Åt 5 1.0000
determinism 0.7759
coverage 0.3977
(4)hall-bathroom
door 147/252hall-toilet
door 56/191
toilet Ô¨Çush
121/143weighted average 0.6645
support 0.6843
conÔ¨Ådence 0.4757
language Ô¨Åt 5 1.0000
determinism 0.6884
coverage 0.4560(5)hall-
bathroom
door 122/252plates
cupboard
30/63
toilet Ô¨Çush
92/143weighted average 0.6415
support 0.6760
conÔ¨Ådence 0.5245
language Ô¨Åt 5 1.0000
determinism 0.6666
coverage 0.3564
fig. 3. example top 5 local process models discovered from the van kasteren data set.
thehall-bathroom door is opened, either toilet Ô¨Çush orplates
cupboard are observed afterwards. the bathroom in this house
contains both a toilet and a shower, but showering events are
not observed as the shower does not contain a sensor. it is likely
thathall-bathroom events that are followed by plates cupboard
events represent a morning shower after which the subject pro-
ceeds his morning routine with breakfast. the ordering of lpms
matches the weighted average of the lpm quality criteria.
vi. r esults & d iscussion
fig. 4 shows the results of the evaluation using the Ô¨Åve eval-
uation data sets and table ii shows the speedup of projection-
based lpm discovery. the speedup mainly depends on the
size of the projection sets. note that we do not compare the
computation time of the discovered and the randomly generated
projections, as both consist of equally sized projections. the
time needed for discovering the projection set is included in the
computation time shown in table ii, however, the projection set
discovery time is negligible compared to the time needed for
lpm discovery. each dark gray bar in fig. 4 represents the per-
formance on the measure indicated by the column of the projec-
tion set discovery method indicated by the row on the dataset in-
dicated below. each gray bar indicates the average performance
over ten random projection sets of the same size as the discov-
ered projection set belonging to the dark gray bar to its left. the
error bars indicate the standard error (se), indicating the uncer-
tainty of the mean performance of the random projection sets.our Ô¨Årst observation is that all three projection set discovery
methods are better than random projections on almost all
data sets. the only exception is the markov-based projection
set discovery approach in case of the bruno data set, which
performs worse than random projections, with none of the
lpms discovered on the original data set being found on
the projection sets. the markov clustering based approach to
projection set discovery achieves the highest speedup on three
of the Ô¨Åve data sets, and second highest speedup on the other
two data sets. the high speedup indicates that the projections
generated by markov clustering are relatively small, which also
explains the quality loss of the lpms with regard to the ground
truth which is typically larger than with the other projection set
discovery methods. the size of the clusters created with markov
clustering can be inÔ¨Çuenced through its inÔ¨Çation parameter,
which we set to 1:5. we found lower values to result in all
activities being clustered into one single cluster, meaning that
the only projection created is the original log itself.
the entropy based approach shows a higher gain than the
markov based approach on the ndcg@ f5,10,20gmetrics for
all data set, when compared to random projections of the same
size. however, the obtained speedup of lpm discovery with
the entropy based approach is lower than speedup with the
markov based approach on all but one data set.
the mrig based approach shows signiÔ¨Åcant improvements
on all metrics on the bpi‚Äô12 and bruno data sets. on these
two data sets it also results in the second highest and highestrecall ndcg@5 ndcg@10 ndcg@20
0.000.250.500.751.00
0.000.250.500.751.00
0.000.250.500.751.00markov entropy mrig
bpi'12 resource 10939bruno
chad 1600010ordonez a
van kasteren
bpi'12 resource 10939bruno
chad 1600010ordonez a
van kasteren
bpi'12 resource 10939bruno
chad 1600010ordonez a
van kasteren
bpi'12 resource 10939bruno
chad 1600010ordonez a
van kasterenvalueorigin
discovered
randomfig. 4. performance of the three projection set discovery methods on the six data sets on the four metrics
table ii
local process model discovery speedup obtained with each
projection set discovery method on the evaluation data sets
data set speedup markov speedup entropy speedup mrig
bpi‚Äô12 resource 10939 42.9 5.6 6.9
bruno 222.9 111.5 336.1
chad subjection 1900010 1060.1 1041.2 1
ordonez a 7.3 33.7 1.1
van kasteren 111.2 83.5 96.7
speedup respectively. however, on the chad data and the
ordonez data the discovered projection sets consist of a single
projection that contains almost all log activities, resulting
consequently in no speedup with close to perfect recall scores.
vii. r elated work
the task of discovering projections plays an important
role within the area of decomposed process discovery and
conformance checking. decomposed process discovery aims at
partitioning the activities in the event log such that after apply-
ing process discovery to each partition of the events, the start-
to-end process model can be constructed by stitching togetherthe process models discovered on the individual partitions. in
[19] an approach was introduced to decompose process mining
by using a maximal decomposition of a causal dependency
graph, where the activities associated with each edge in the
causal dependency graph end up in one cluster. hompes et al.
[20] describe an approach to make more coarse-grained activity
clusters by recombining the clusters of the maximal decompo-
sition by balancing three quality criteria: cohesion, coupling,
and balance. van der aalst and verbeek [ 21] introduced a
decomposed process mining approach based on passages . a
passage is a pair of two non-empty sets of activities (x,y) such
that the set of direct successors of x is y and the set of direct
predecessors of y is x. munoz-gama et al. [ 22] proposed a
decomposed conformance checking approach that discovers
clusters of activities based on identifying single-entry single-
exit (sese) blocks in a petri net model of the process. a sese
block in a petri net is a set of edges that has exactly two bound-
ary nodes: one entry and one exit. clustering activities using
this approach assumes availability of a structured process model
describing process instances from the beginning to the end,
which is different from the application area of lpm discovery.carmona et al. [ 23] describe an approach to generate overlap-
ping sets of activities from a causal dependency graph and uses
it to speed up region theory based petri net synthesis from a
transitions system representation of the event log. the activities
are grouped together in such a way that the synthesized petri
nets can be combined into a single petri net generating all
the traces of the log. in a more recent paper carmona [ 24]
describes an approach to discover a set of projections from an
event log based on principle component analysis (pca).
all the projection discovery methods mentioned above
aim at the discovery of models that can be combined into
a single process model. in the context of lpm discovery,
we are not interested in combining process models into one
single process model; instead, each lpm is assumed to convey
interesting information about a relationship between activities
itself. projection methods in the context of decomposed process
mining all aim to minimize overlap between projections, leaving
only the overlap needed for combining the individual process
models into one. in our case, overlap between clusters is often
desired, as interesting patterns might exist within a set of
activitiesfa;b;c;dg, as well as within a set of activities
fa;b;c;eg, and discovering on both subsets individually is
faster than discovering once on fa;b;c;d;eg. the three
projection set discovery methods introduced in this paper
exploit this and aim for overlapping projection sets.
the episode miner [ 25] is a related technique to lpm
discovery, which discovers patterns that are less expressive (i.e.
limited to partial orders), but is computationally less expensive.
while the need for heuristic techniques to speed up episode
mining is limited compared to lpm discovery, in principle
the three heuristics described to speedup lpm discovery could
also be used to speedup the discovery of frequent episodes
viii. c onclusion & f uture work
we explored three different heuristics for the discovery of
projection sets for speeding up local process model (lpm)
discovery. these heuristics enable the discovery of lpms from
event logs where it is computationally not feasible to discover
lpms from the full set of activities in the log. all three of them
produce better than random projections on a variety of data sets.
projection discovery based on markov clustering leads to the
highest speedup, while higher quality lpms can be discovered
using a projection discovery based on log statistics entropy.
the maximal relative information gain based approach to pro-
jection discovery shows unstable performance with the highest
gain in lpm quality over random projections on some event
logs, while not being able to discover any projection smaller
than the complete set of activities on some other event logs. in
fact, we would like to explore event log properties that can serve
as a predictor for the relative performance of these methods.
references
[1]w. m. p. van der aalst, process mining: data science in action . springer,
2016.
[2]n. tax, e. alasgarov, n. sidorova, and r. haakma, ‚Äúon generation of
time-based label reÔ¨Ånements,‚Äù in proceedings of the 25th international
workshop on concurrency, speciÔ¨Åcation and programming . ceur-
ws.org, 2016, pp. 25‚Äì36.[3]r. s. mans, m. h. schonenberg, m. song, w. m. p. van der aalst,
and p. j. m. bakker, ‚Äúapplication of process mining in healthcare ‚Äì a
case study in a dutch hospital,‚Äù in international joint conference on
biomedical engineering systems and technologies . springer, 2008, pp.
425‚Äì438.
[4]s. j. j. leemans, d. fahland, and w. m. p. van der aalst, ‚Äúdiscovering
block-structured process models from event logs-a constructive approach,‚Äù
ininternational conference on applications and theory of petri nets
and concurrency . springer berlin heidelberg, 2013, pp. 311‚Äì329.
[5]j. m. e. m. van der werf, b. f. van dongen, c. a. j. hurkens, and
a. serebrenik, ‚Äúprocess discovery using integer linear programming,‚Äù
ininternational conference on applications and theory of petri nets .
springer, 2008, pp. 368‚Äì387.
[6]f. m. maggi, a. j. mooij, and w. m. p. van der aalst, ‚Äúuser-guided
discovery of declarative process models,‚Äù in computational intelligence
and data mining, 2011 ieee symposium on . ieee, 2011, pp. 192‚Äì199.
[7]s. sch ¬®onig, c. cabanillas, s. jablonski, and j. mendling, ‚Äúmining the
organisational perspective in agile business processes,‚Äù in international
conference on enterprise, business-process and information systems
modeling . springer, 2015, pp. 37‚Äì52.
[8]n. tax, n. sidorova, r. haakma, and w. m. p. van der aalst, ‚Äúmining
local process models,‚Äù arxiv:1606.06066 , 2016.
[9]m. saeed, c. lieu, g. raber, and r. g. mark, ‚Äúmimic ii: a massive
temporal icu patient database to support research in intelligent patient
monitoring,‚Äù in computers in cardiology, 2002 . ieee, 2002, pp. 641‚Äì
644.
[10] s. van dongen, ‚Äúgraph clustering via a discrete uncoupling process,‚Äù
siam journal on matrix analysis and applications , vol. 30, no. 1, pp.
121‚Äì141, 2008.
[11] ‚Äî‚Äî, ‚Äúgraph clustering by Ô¨Çow simulation,‚Äù ph.d. dissertation, univer-
sity of utrecht, utrecht, may 2000.
[12] k. j¬®arvelin and j. kek ¬®al¬®ainen, ‚Äúcumulated gain-based evaluation of ir
techniques,‚Äù acm transactions on information systems , vol. 20, no. 4,
pp. 422‚Äì446, 2002.
[13] c. burges, t. shaked, e. renshaw, a. lazier, m. deeds, n. hamilton, and
g. hullender, ‚Äúlearning to rank using gradient descent,‚Äù in proceedings
of the 22nd international conference on machine learning . acm,
2005, pp. 89‚Äì96.
[14] n. tax, s. bockting, and d. hiemstra, ‚Äúa cross-benchmark comparison
of 87 learning to rank methods,‚Äù information processing & management ,
vol. 51, no. 6, pp. 757‚Äì772, 2015.
[15] t. van kasteren, a. noulas, g. englebienne, and b. kr ¬®ose, ‚Äúaccurate
activity recognition in a home setting,‚Äù in proceedings of the 10th
international conference on ubiquitous computing . acm, 2008, pp.
1‚Äì9.
[16] b. bruno, f. mastrogiovanni, a. sgorbissa, t. vernazza, and r. zaccaria,
‚Äúanalysis of human behavior recognition algorithms based on acceleration
data,‚Äù in robotics and automation, 2013 ieee international conference
on. ieee, 2013, pp. 1602‚Äì1607.
[17] t. mccurdy, g. glen, l. smith, and y . lakkadi, ‚Äúthe national exposure
research laboratory‚Äôs consolidated human activity database,‚Äù journal of
exposure analysis and environmental epidemiology , vol. 10, no. 6, pp.
566‚Äì578, 2000.
[18] f. j. ord ¬¥onez, p. de toledo, and a. sanchis, ‚Äúactivity recognition using
hybrid generative/discriminative models on home environments using
binary sensors,‚Äù sensors , vol. 13, no. 5, pp. 5460‚Äì5477, 2013.
[19] w. m. p. van der aalst, ‚Äúdecomposing petri nets for process mining: a
generic approach,‚Äù distributed and parallel databases , vol. 31, no. 4,
pp. 471‚Äì507, 2013.
[20] b. f. a. hompes, h. m. w. verbeek, and w. m. p. van der aalst,
‚Äúfinding suitable activity clusters for decomposed process discovery,‚Äù
indata-driven process discovery and analysis . springer, 2014, pp.
32‚Äì57.
[21] w. m. p. van der aalst and h. m. w. verbeek, ‚Äúprocess discovery and
conformance checking using passages,‚Äù fundamenta informaticae , vol.
131, no. 1, pp. 103‚Äì138, 2014.
[22] j. munoz-gama, j. carmona, and w. m. p. van der aalst, ‚Äúsingle-entry
single-exit decomposed conformance checking,‚Äù information systems ,
vol. 46, pp. 102‚Äì122, 2014.
[23] j. carmona, j. cortadella, and m. kishinevsky, ‚Äúdivide-and-conquer
strategies for process mining,‚Äù in business process management .
springer, 2009, pp. 327‚Äì343.[24] j. carmona, ‚Äúprojection approaches to process mining using region-based
techniques,‚Äù data mining and knowledge discovery , vol. 24, no. 1, pp.
218‚Äì246, 2012.
[25] m. leemans and w. m. p. van der aalst, ‚Äúdiscovery of frequent episodes
in event logs,‚Äù in data-driven process discovery and analysis . springer,
2014, pp. 1‚Äì31.