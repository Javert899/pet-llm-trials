process mining and simulation: a match made in hea ven!
wil m.p. van der aalst
process and data science (informatik 9)
rwth aachen university, d-52056 aachen, germany
wvdaalst@pads.rwth-aachen.de
springsim-scsc, 2018 july 9-12, bordeaux, france; c⃝2018 society for modeling & simulation international (scs)abstract
event data are collected everywhere: in logistics, manufacturing, finance, healthcare, e-learning, e-
government, and many other domains. the events found in these domains typically refer to activities
executed by resources at particular times and for particular cases . process mining provides the means to
discover the real processes, to detect deviations from normative processes, and to analyze bottlenecks and
waste from such events. however, process mining tends to be backward-looking . fortunately, simulation
can be used to explore different design alternatives and to anticipate future performance problems. this
keynote paper discusses the link between both types of analysis and elaborates on the challenges process
discovery techniques are facing. quality notions such as recall ,precision , and generalization are discussed.
rather than introducing a specific process discovery or conformance checking algorithm, the paper provides
acomprehensive set of conformance propositions . these conformance propositions serve two purposes: (1)
introducing the essence of process mining by discussing the relation between event logs and process models,
and (2) discussing possible requirements for the quantification of quality notions related to recall, precision,
and generalization.
keywords: process mining, simulation, process discovery, conformance checking.
1 introduction
discrete-event simulation (des)—simply referred to as simulation in this paper—is a widely used approach
toplay-out process models (law and kelton 1982, kleijnen and groenendaal 1992, aalst and stahl 2011,
aalst 2015). based on the rules defined by the simulation model, events are generated. each event occurs
at a particular instant in time and marks a change of state in the system. states enable new events. a
simulation run describes one of the possibly many ways in which the model can be played-out. random-
number generators are used to resolve choices when the model allows for different paths (e.g. xor-splits)
and to sample durations from predefined probability distributions to determine waiting and service times.
through simulation experiments various “what if” questions can be answered and redesign alternatives can
be compared with respect to key performance indicators. however, making a good simulation model may
be very time consuming, and it may be outdated by the time it is ready.
process mining is fundamentally different as it starts from observed behavior rather than modeled behavior
(aalst 2016). event data , recorded in a so-called event log , describe what really happened. each event
refers to an activity that occurred at a particular point in time for a particular case. the chronological
ordering of events for a particular case yields a trace , i.e., a sequence of activities executed for that case.
one such trace is similar to a simulation run; it is only one example of possibly many different behaviors.
process discovery aims to play-in process models, i.e., the example traces found in the event log are used tovan der aalst
construct a process model. other process mining techniques replay event data on a process model to check
conformance and to analyze bottlenecks. next, to providing insights into the real behavior of a system
or process, these techniques help to understand and improve performance and compliance problems. the
discovered process models may even be used to predict the trajectories of running cases (e.g., the remaining
flow time) assuming that the process does not change. whereas simulation focuses on play-out , process
mining focuses on play-in andreplay . however, both use or generate event data (e.g., simulation logs) and
process models. therefore, they nicely complement each other.
in the remainder, we assume that the reader has a good understanding of discrete-event simulation (des)
approaches. the focus in the first part of the paper will be on the relation between simulation and process
mining. we will show that together they form “a match made in heaven!”. process mining can be used
to make better simulation models and to compare simulation runs with actual behaviors. simulation can be
used to make process mining more forward-looking and explore different process changes.
the second part of the paper proposes a comprehensive set of 21 conformance propositions . each proposi-
tion describes a property of some quality measure that compares an event log and a process model. consider
for example recall, i.e., the ability to replay the behavior seen in the event log (sometimes called fitness).
when a model and log have a high recall, most of the observed behavior in the log can be explained by the
model. a possible proposition is that recall should be monotonic in terms of the model behavior, i.e., when
the model allows for more behavior, recall can only improve. similarly, precision cannot improve by adding
behavior to the model that was never observed, and generalization cannot improve by removing behavior
from the model. by providing a comprehensive set of conformance propositions, we aim to stimulate the
reader to think about the relationship between observed behavior (event data) and modeled behavior (pro-
cess models), thus revealing the essence of process mining. moreover, the conformance propositions also
reveal limitations of existing approaches that aim to quantify this relation.
finally, we add probabilities to event logs and discuss conformance in the setting where the real process
(i.e., the “ground truth”) is known. this reveals the foundational challenges that need to be addressed by
process discovery techniques. the idealized setting with a known “ground truth” can be used as a reality
check for process mining research. moreover, it further illustrates process mining concepts for readers with
a simulation background.
the remainder of this paper is organized as follows. section 2 discusses synergies between process mining
and simulation. section 3 introduces basic process mining concepts assuming a rather simplistic setting.
this is used to present the 21 conformance propositions. these are interesting for the development of new
recall, precision, and generalization measures. section 4 describes a more informative setting where the real
stochastic process and desired process model are known. section 5 concludes the paper.
2 relating process mining and simulation
simulation has been used to analyze operational processes since the uptake of simula and related languages
in the 1960-ties (dahl and nygaard 1966). through simulation experiments various “what if” questions can
be answered and redesign alternatives can be compared with respect to key performance indicators (law and
kelton 1982, kleijnen and groenendaal 1992, aalst and stahl 2011, aalst 2015). despite these forward-
looking capabilities, rich history, and powerful simulation tools, the real-life application of simulation is
limited. there are two main reasons for this. first of all, it is typically very time-consuming to build a
good simulation model. second, it is not easy to let a simulation model mimic reality. therefore, simulation
results can always be questioned: “it is just a modeled reality”. therefore, organizations are resorting
increasingly to evidence-based approaches.van der aalst
process models
(descriptive /
simulatable )
simulation
performance 
analysisinformation 
systems /real-life 
processes
event logs
(simulated )
performance 
diagnostics
discovery
event logs
(real)
performance 
diagnostics
process models
(descriptive /
non-simulatable )
conformance 
analysisinformation 
systems /real-life 
processes
event logs
(real)
conformance 
diagnostics
process models
(normative /
non-simulatable )
process models
(descriptive /
simulatable )
(a) scenario 1: classical use of simulation
(b) scenario 2: process discovery followed by replay to reveal performance 
and to create a process model that can be simulated
(c) scenario 3: conformance checking to compare observed behavior and modeled behavior
figure 1: three analysis scenarios: (a) classical model-driven simulation, (b) discovery and performance
analysis, and (c) conformance checking based on a normative model.
process mining is a more recent development where operational processes are analyzed based on the event
data they generate (aalst 2016). currently, there are over 25 commercial tools supporting process mining
(e.g., disco, celonis, processgold, qpr, minit, and myinvenio). prom is the leading open-source process
mining tool, serving as the de facto standard for the academic world. using these tools processes can be
constructed automatically based on event data. by replaying event data on process models (discovered or
hand-made), one can answer compliance and performance questions. since it is always possible to drill-
down and show the actual event data, results become undeniable. however, the “achilles heel of process
mining” is the fact that it is backward-looking . process mining can be used to diagnose problems (e.g.,
bottlenecks or non-compliance) and predict the paths taken by running process instances (i.e., cases), but it
cannot be used to answer “what if” questions and explore radical redesigns.
given the above, it is very natural to combine process mining and simulation. figure 1 shows three analysis
scenarios. simulation starts from a process model and produces behavior and performance diagnostics
(figure 1(a)). simulations can be used to generate event logs recording the simulated behavior. figure 1(b)
shows a process mining scenario where event data generated by some process or information system are
used to discover a descriptive process model. by replaying the event data on the discovered process model,
it is possible to analyze bottlenecks and add the temporal and stochastic behavior to the model. the result is
a process model that can be used to simulate the process. figure 1(c) shows a process mining scenario where
there is a normative process model. by replaying the real behavior on the normative model, it is possible to
diagnose deviations.
a deeper look at figure 1 reveals possible interfaces between process mining and simulation: (1) the
event logs generated through simulation can be analyzed using process mining techniques and (2) the
process models generated by process mining can be used as input for simulation. the xes standard
(www.xes-standard.org) can be used to exchange event logs between process mining and simulation tools.
it is more difficult to exchange simulation models (cf. the attempted wfmc standard bpsim). note that
prom provides exports to a limited number of simulation tools (e.g., cpn tools).
as discussed in (aalst 2015, rozinat, wynn, aalst, hofstede, and fidge 2009) more advanced scenarios are
possible. one of these scenarios is sketched in figure 2. first, a simulation model is learned from event data.
in the discovery step, a descriptive model is learned which is enriched into a simulation model by replaying
the event log (adding probability distributions). at any point in time, the current state of the process can
be loaded from the information system. then a so-called short-term simulation can be performed. the key
idea is to start all simulation runs from the current state and focus on transient behavior. this way a “fast-van der aalst
states
performance 
analysisinformation 
systems /real-life 
processes
discovery
event logs
(real)
process models
(descriptive /
non-simulatable )
process models
(descriptive /
simulatable )
simulation
event logs
(simulated )
performance 
analysis
performance 
diagnostics
figure 2: a more advanced scenario using a combination of simulation and process mining. using a simula-
tion model learned through process mining and an initialization using actual state information, it is possible
to explore different “futures” (fast-forward capability).
forward button” into the future is provided (aalst 2015, rozinat, wynn, aalst, hofstede, and fidge 2009).
for transient analysis the focus is on the initial part of future behavior, i.e., starting from the initial state
the “near future” is explored. while for steady-state analysis the initial state is irrelevant and the simulation
can be started without any cases in progress, this type of simulation relies on state information and a tight
coupling between the information system and the simulation model. this is facilitated by process mining.
figure 2 illustrates that new types of analysis are enabled by combining process mining and simulation. it
is particularly interesting that the difference between interpreting simulated behavior and real behavior is
fading . this facilitates data-driven exploration of “ist” and “soll” processes.
3 basic process mining notions explained through conformance
propositions
process mining techniques focus on the relationship between observed behavior and modeled behavior.
therefore, we first formalize event logs (i.e., observed behavior) and process models (i.e., modeled behav-
ior). to do this, we consider a very simple setting where we only focus on the control-flow, i.e., sequences
of activities. then, we discuss various quality notions and provide a set of conformance propositions.
3.1 event logs and process models
the starting point for process mining is an event log. each event in such a log refers to an activity possibly
executed by a resource at a particular time and for a particular case. an event may have many more at-
tributes, e.g., transactional information, costs, customer, location, and unit. here, we focus on control-flow.
therefore, we only consider activity labels and the ordering of events within cases.
definition 1 (traces) .ais the universe of activities . atracet∈ a∗is a sequence of activities. t=a∗is
the universe of traces.
a trace t=⟨a, b, a, b, a, b, c ⟩ ∈ t refers to 7 events belonging to the same case (i.e., |t|= 7). the events
occurred in the order indicated, starting with aand ending with c. an event log refers to a collection of cases
each represented by a trace.
definition 2 (event log) .l=b(t)is the universe of event logs. an event log l∈ l is a multiset of
observed traces. el={t∈l}is the set of traces appearing in l∈ l.van der aalst
a c
b d
c
da
b(a) a petri net model (with start and end transitions )
(b) a bpmn model allowing for the same behaviorm1
start
startend
endm2
figure 3: two process models m1andm2having the same behavior: em1=em2={⟨a, b, c⟩,
⟨a, b, d⟩,⟨b, a, c⟩,⟨b, a, d⟩}.m1is a petri net with a special start activity (occurs once at the beginning)
and a special end activity to signal the end of the trace. m2is a bpmn (business process model and
notation) model.
an event log is a multiset of traces. event log l= [⟨a, b, c⟩5,⟨b, a, d⟩3,⟨a, b, d⟩2]refers to 10 cases (i.e., |l|=
10). five cases are represented by the trace ⟨a, b, c⟩, three cases are represented by the trace ⟨b, a, d⟩, and two
cases are represented by the trace ⟨a, b, d⟩.l(t)is the number of times tappears in l, e.g., l(⟨b, a, d⟩) = 3 .
we assume that the usual operators are defined for multisets. l1⊎l2is the union of two multisets, |l|is the
number of elements, and l1\l2is the difference. l1≤l2means that l1is contained in l2. for example,
[a2, b]≤[a2, b2, c], but[a2, b3]̸≤[a2, b2, c2]and[a2, b2, c]̸≤[a3, b3]. next, we define process models
where we distinguish between representation andbehavior using the same simplistic setting (only control-
flow).
definition 3 (process model) .mis the set of all process models. a process model m∈ m allows for the
set of traces denoted by em⊆ t.
m∈ m could be represented in different modeling languages, e.g., petri nets, bpmn models, uml activity
diagrams, automata, and process trees. here we abstract from the actual representation and focus on the
behavior allowed by the model. em⊆ t denotes the set of traces possible according to the model. figure 3
shows two process models that have the same behavior: em1=em2={⟨a, b, c⟩,⟨a, b, d⟩,⟨b, a, c⟩,⟨b, a, d⟩}.
definition 4 (complement) .em= (t \em)is the complement of the set of traces allowed by model m∈ m .
el= (t \el)is the complement of the traces appearing in log l∈ l.
emis the set traces not allowed by the model. note that emandempartition the universe of traces tand may
both contain infinitely many traces.
a discovery algorithm takes an event log as input and returns a process model. for example, m1andm2
in figure 3 could have been discovered based on event log l= [⟨a, b, c⟩5,⟨b, a, d⟩3,⟨a, b, d⟩2]. ideally,
the process model captures the (dominant) behavior observed, but also generalizes without becoming too
imprecise. for example, m1andm2allow for trace t=⟨b, a, c⟩although this was never observed.
definition 5 (discovery algorithm) .a discovery algorithm can be described as a function disc∈ l → m
mapping event logs onto process models.van der aalst
we abstract from concrete discovery algorithms. over 100 discovery algorithms have been proposed in
literature (aalst 2016). merely as a reference to explain basic notions, we define three simple, but extreme,
algorithms: disc ofit,disc ufit, and disc nfit. letl∈ l be a log. disc ofit(l) =mosuch that emo=elproduces
an overfitting model that allows only for the behavior seen in the log. disc ufit(l) =musuch that emu=t
produces an underfitting model that allows for any behavior. disc nfit(l) =mnsuch that emn=elproduces a
non-fitting model that allows for all behavior notseen in the log.
3.2 quality dimensions
since process mining focuses on the relationship between observed behavior and modeled behavior, it is
important to quantify the relation between a log l∈ l and model m∈ m . when learning a process model
from event data, there is a trade-off between the following four quality dimensions (aalst 2016): (1) recall :
the discovered model should allow for the behavior seen in the event log (avoiding “non-fitting” behavior),
(2)precision : the discovered model should not allow for behavior completely unrelated to what was seen
in the event log (avoiding “underfitting”), (3) generalization : the discovered model should generalize the
example behavior seen in the event log (avoiding “overfitting”), and (4) simplicity : the discovered model
should be as simple as possible. the simplicity dimension refers to occam’s razor: “one should not in-
crease, beyond what is necessary, the number of entities required to explain anything”. in the context of
process mining, this is often operationalized by quantifying the complexity of the model (number of nodes,
number of arcs, understandability, etc.). we do not consider the simplicity dimension in this paper, since we
focus on behavior and abstract from the actual model representation. recall is often referred to as fitness
in process mining literature. sometimes fitness refers to a combination of the four quality dimensions. to
avoid later confusion, we use the term recall commonly used in pattern recognition, information retrieval,
and (binary) classification.
in the remainder, we assume the existence of three functions: rec(),prec(),gen(). all three take a log and
model as input and return a value between 0 and 1. the higher the value, the better. in process mining
literature one can find many proposals for such functions. here, we do not describe specific functions, but
discuss their (desired) properties.
definition 6 (recall) .arecall measure rec∈ l × m → [0,1]aims to quantify the fraction of observed
behavior that is allowed by the model.
definition 7 (precision) .aprecision measure prec∈ l × m → [0,1]aims to quantify the fraction of
behavior allowed by the model that was actually observed.
definition 8 (generalization) .ageneralization measure gen∈ l × m → [0,1]aims to quantify the
likelihood that new unseen cases will fit the model.
assume event log l= [⟨a, c⟩5,⟨a, b, b, c ⟩3,⟨a, b, b, b, b, b, c ⟩]containing 29 events relating to 9 cases. mo=
disc ofit(l) ={⟨a, c⟩,⟨a, b, b, c ⟩,⟨a, b, b, b, b, b, c ⟩}allows only for the behavior seen in the log. mu=
disc ufit(l) =tallows for any behavior. mn=disc nfit(l) =t \ {⟨ a, c⟩,⟨a, b, b, c ⟩,⟨a, b, b, b, b, b, c ⟩}
allows for any behavior notseen in the log. the recall of models moandmuis good because these models
allow for all traces in event log l. the recall of mnis poor because none of the observed traces is allowed.
the precision of mois good because all behavior allowed has been observed. the precision of muandmn
is poor because most of the behavior allowed by these models was never observed. the generalization of
muis good because it will also allow for unseen traces (e.g., ⟨a, b, c⟩and⟨a, b, b, b, c ⟩). the generalization
ofmoandmnis poor because these models are unlikely to allow for a new trace generated by the process
that also generated l. table 1 summarizes the anticipated values for the different quality measures.van der aalst
table 1: expected values for event log l= [⟨a, c⟩5,⟨a, b, b, c ⟩3,⟨a, b, b, b, b, b, c ⟩].
model recall precision generalization
mo={⟨a, c⟩,⟨a, b, b, c ⟩,⟨a, b, b, b, b, b, c ⟩} good good poor
mu=t good poor good
mn=t \ {⟨ a, c⟩,⟨a, b, b, c ⟩,⟨a, b, b, b, b, b, c ⟩} poor poor poor
to explain the difference between recall and generalization, consider the event logs l1= [⟨a, c⟩5,⟨a, b, b, c ⟩3,
⟨a, b, b, b, b, b, c ⟩],l2= [⟨a, c⟩10,⟨a, b, b, c ⟩6,⟨a, b, b, b, b, b, c ⟩2], and l100= [⟨a, c⟩500,⟨a, b, b, c ⟩300,
⟨a, b, b, b, b, b, c ⟩100]. note that l2is a duplication of the original event log. in l100all traces appear 100
times as frequent as in l1. note that mo(overfitting model), mu(underfitting model), and mn(non-fitting
model) are the same for these three event logs (i.e., the models listed in table 1). it seems reasonable to
assume that given a model m, the recall and precision values will not be radically different for l1,l2, and
l100. however, generalization will be influenced by the absolute frequencies of traces in the event log even
when the distribution does not change. obviously, gen(l100, mo)>gen(l1, mo). after observing 900 cases
exhibiting only three unique traces, it is more likely that case 901 will again follow one of these three traces.
after observing only 9 cases, this is less certain and it seems reasonable to think that for example trace
⟨a, b, c⟩is still possible.
in the remainder, we use a “closed world assumption” with respect to the set of activities. we assume that
ais limited to the activities that have been observed. hence, models like mu=disc ufit(l)andmn=
disc nfit(l)only refer to observed activities. it makes no sense to reason about activities that were never
observed (although the propositions do not depend on this).
3.3 conformance propositions
many recall measures have been proposed in literature (aalst 2016, rozinat and aalst 2008, medeiros,
weijters, and aalst 2007, aalst, adriansyah, and dongen 2012). in recent years, also several precision
measures have been proposed (adriansyah, munoz-gama, carmona, dongen, and aalst 2015, tax, lu,
sidorova, fahland, and aalst 2018). only few generalization measures have been proposed (aalst, adrian-
syah, and dongen 2012). the goal of this paper is not to present new measures, but to define properties for
such quality measures. through this, we hope to facilitate a better understanding of process discovery and
conformance checking. moreover, these properties may help to choose an existing measure or to define new
ones.
in the remainder, we provide 21 conformance propositions . the merriam-webster dictionary defines the
noun proposition as “an expression in language or signs of something that can be believed, doubted, or
denied or is either true or false”. most of the conformance propositions have broad support from the com-
munity, i.e., there is consensus that these propositions should hold. these are marked with a “ +”. for
example, the first two propositions are commonly accepted; the computation of a quality measure should be
deterministic ( detpro+) and only depend on behavioral aspects ( behpro+). the latter is a design choice.
we deliberately exclude simplicity notions. more controversial propositions are marked with a “ 0” (rather
than a “ +”).
proposition 1 (detpro+).rec(),prec(),gen()are deterministic functions, i.e., rec(l, m),prec(l, m),
gen(l, m)are fully determined by l∈ l andm∈ m .
proposition 2 (behpro+).for any l∈ l andm1, m2∈ m such that em1=em2:rec(l, m 1) = rec(l, m 2),
prec(l, m 1) = prec(l, m 2), and gen(l, m 1) = gen(l, m 2), i.e., the measures are fully determined by the
behavior observed and the behavior allowed by the model (representation does not matter).van der aalst
3.4 recall propositions
first, we consider a few recall propositions .rec∈ l×m → [0,1]aims to quantify the fraction of observed
behavior that is allowed by the model.
proposition 3 (recpro1+).for any l∈ landm1, m2∈ m such that em1⊆em2:rec(l, m 1)≤rec(l, m 2).
proposition recpro1+states that extending the model to allow for more behavior can never result in a
lower recall. similarly, it cannot be the case that adding fitting behavior to the event logs, lowers recall
(recpro2+). adding non-fitting behavior to the log, cannot improve recall ( recpro3+).
proposition 4 (recpro2+).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
rec(l1, m)≤rec(l2, m).
proposition 5 (recpro3+).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
rec(l1, m)≥rec(l2, m).
for any natural number k:lk(t) =k·l(t), e.g., if l= [⟨a, b⟩3,⟨c⟩2], then l4= [⟨a, b⟩12,⟨c⟩8]. we use
this notation to enlarge event logs without changing the distribution. one could argue that this should not
influence recall ( recpro40), e.g., rec([⟨a, b⟩3,⟨c⟩2], m) = rec([⟨a, b⟩12,⟨c⟩8], m). however, unlike the
previous propositions, this requirement is debatable as is indicated by the “ 0” tag.
proposition 6 (recpro40).for any l∈ l,m∈ m , and k≥1:rec(lk, m) = rec(l, m).
finally, we provide a proposition stating that recall should be 1 if all traces in the log fit the model
(recpro5+). as a result, the empty log has recall 1 for any model.
proposition 7 (recpro5+).for any l∈ l andm∈ m such that el⊆em:rec(l, m) = 1 .
based on this proposition, rec(l,disc ofit(l)) = rec(l,disc ufit(l)) = 1 for any log l.
3.5 precision propositions
precision ( prec∈ l × m → [0,1]) aims to quantify the fraction of behavior allowed by the model that was
actually observed. in (tax, lu, sidorova, fahland, and aalst 2018) several precision axioms were intro-
duced. these partly overlap with the propositions below (but more are added and some are strengthened).
precpro1+states that removing behavior from a model that does not happen in the event log cannot lead to
a lower precision. adding fitting traces to the event log can also not lower precision ( precpro2+). however,
adding non-fitting traces to the event log should not change precision ( precpro30).
proposition 8 (precpro1+).for any l∈ l andm1, m2∈ m such that em1⊆em2andel∩(em2\em1) =∅:
prec(l, m 1)≥prec(l, m 2).
proposition 9 (precpro2+).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
prec(l1, m)≤prec(l2, m).
proposition 10 (precpro30).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
prec(l1, m) = prec(l2, m).
one could also argue that duplicating the event log should not influence precision because the distribution
remains the same ( precpro40), e.g., prec([⟨a, b⟩20,⟨c⟩20], m) = prec([⟨a, b⟩40,⟨c⟩40], m).
proposition 11 (precpro40).for any l∈ l,m∈ m , and k≥1:prec(lk, m) = prec(l, m).van der aalst
if the model allows for the behavior observed and nothing more, precision should be maximal ( precpro5+).
one could also argue that if all modeled behavior was observed, precision should also be 1 ( precpro60).
the latter proposition is debatable, because it implies that the non-fitting behavior cannot influence perfect
precision. consider for example extreme cases where the model covers just a small fraction of all observed
behavior (or even more extreme situations like em=∅).
proposition 12 (precpro5+).for any l∈ l andm∈ m such that em=el:prec(l, m) = 1 .
proposition 13 (precpro60).for any l∈ l andm∈ m such that em⊆el:prec(l, m) = 1 .
based on proposition precpro5+orprecpro60,rec(l,disc ofit(l)) = 1 for any log l.
3.6 generalization propositions
generalization ( gen∈ l × m → [0,1]) aims to quantify the likelihood that new unseen cases will fit the
model. this conformance dimension is a bit different than the other two dimensions, because it reasons
about future unseen cases (i.e., not yet in the event log). if the recall is good and the log is complete with
lots of repeating behavior, then future cases will most likely fit the model. analogous to recall, model
extensions cannot lower generalization ( genpro1+), extending the log with fitting behavior cannot lower
generalization ( genpro2+), and extending the log with non-fitting behavior cannot improve generalization
(genpro3+).
proposition 14 (genpro1+).for any l∈ l andm1, m2∈ m such that em1⊆em2:gen(l, m 1)≤
gen(l, m 2).
proposition 15 (genpro2+).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
gen(l1, m)≤gen(l2, m).
proposition 16 (genpro3+).for any l1, l2, l3∈ l andm∈ m such that l2=l1⊎l3andel3⊆em:
gen(l1, m)≥gen(l2, m).
duplicating the event log does not necessarily influence recall and precision. according to propositions
recpro40andprecpro40this should have no effect on recall and precision. however, making the event log
more redundant, should have an effect on generalization. for fitting logs, adding redundancy without chang-
ing the distribution can only improve generalization ( genpro4+). for non-fitting logs, adding redundancy
without changing the distribution can only lower generalization ( genpro5+). note that genpro4+and
genpro5+are special cases of genpro60andgenpro70.genpro60andgenpro70consider logs where
some traces are fitting and others are not. for a log where more than half of the traces is fitting, duplication
can only improve generalization ( genpro60). for a log where more than half of the traces is non-fitting,
duplication can only lower generalization ( genpro60).
proposition 17 (genpro4+).for any l∈ l,m∈ m , and k≥1such that el⊆em:gen(lk, m)≥
gen(l, m).
proposition 18 (genpro5+).for any l∈ l,m∈ m , and k≥1such that el⊆em:gen(lk, m)≤
gen(l, m).
proposition 19 (genpro60).for any l∈ l ,m∈ m , and k≥1such that |[t∈l|t∈em]| ≥
|[t∈l|t̸∈em]|:gen(lk, m)≥gen(l, m).
proposition 20 (genpro70).for any l∈ l ,m∈ m , and k≥1such that |[t∈l|t∈em]| ≤
|[t∈l|t̸∈em]|:gen(lk, m)≤gen(l, m).van der aalst
when the model allows for any behavior, clearly the next case will also be fitting ( genpro80). nevertheless,
it is marked as controversial because the proposition would also need to hold for an empty event log.
proposition 21 (genpro80).for any l∈ l andm∈ m such that em=t:gen(l, m) = 1 .
this concludes this first inventory of conformance propositions. as mentioned, their purpose is twofold.
first of all, they help to understand the goals and challenges of process discovery. second, they provide a
checklist for existing and future conformance measures. as demonstrated in (tax, lu, sidorova, fahland,
and aalst 2018) for precision, most of the existing approaches violate seemingly obvious requirements. this
justifies the formulation of the above 21 conformance propositions.
4 towards a more realistic setting
the propositions presented in section 3 are based on a very simple setting where we know nothing about
the underlying process and only see the event log. to provide another view on process discovery, we now
provide a different setting where each trace has a likelihood. a stochastic process model is composed of a
set of traces and a trace likelihood function. the focus is still on control-flow, but we add probabilities to
show that “behavior is not binary”.
definition 9 (trace likelihood function) .π =t → [0,1]is the set of all trace likelihood functions .π∈π
assigns a likelihood π(t)to any trace t∈ t such thatp
t∈tπ(t) = 1 .
note that the number of possible traces may be infinite, but it is easy to see that the universe of traces tis
countable if ais finite (order by the length of the trace and then in lexicographical order). for pragmatic
reasons, we can also restrict tto traces of a maximal length (e.g., the maximal length seen in any log). for
any set of traces x⊆ t ,π(x) =p
t∈xπ(t)is the probability that a run of the process πyields a trace
inx. we assume that traces are sampled independently. when considering time this is not realistic (due to
queueing), but for the control-flow (routing of a case through the model) this is a common assumption.
definition 10 (stochastic process model) .s=m × πis the set of all stochastic process models . a
stochastic process model s= (m, π)∈ scombines a model mand trace likelihood function π.
the relation between mandπins= (m, π)∈ s is not very strict. sometimes we will require em⊆ {t∈
t |π(t)>0}(all modeled traces are possible), {t∈ t | π(t)>0} ⊆em(model allows for all traces
possible), or even em={t∈ t | π(t)>0}. model mmay be an abstraction of the real process and leave
out unlikely behavior. due to simplification, the model may also allow for traces that cannot happen (e.g.,
particular interleavings or loops).
next, we assume a highly idealized setting where we have a real stochastic process model sr= (mr, πr)∈
sand a discovered or manually designed stochastic process model sd= (md, πd)∈ s (also see (aalst
2013)). πris the real underlying stochastic process and mris the process we would like to obtain. as
mentioned, mrmay abstract from unlikely behavior or include behavior that is impossible. sd= (md, πd)
is the model that was discovered (or obtained in some other way) and we would like to compare it with the
real stochastic process model sr= (mr, πr). although sdcould have been hand-made, we just refer to it as
the discovered model.
this idealized setting leads to very natural notions of precision and recall. for example, πr(emr)is the
coverage of mraccording to πr,πr(emd)is the coverage of mdaccording to πr,πd(emr)is the coverage of
mraccording to πd, andπd(emd)is the coverage of mdaccording to πd.van der aalst
definition 11 (precision and recall) .letsr= (mr, πr), sd= (md, πd)∈ s be two stochastic process
models (real and discovered).
rec(sr, sd) =πr(emd∩emr)
πr(emr)prec(sr, sd) =πd(emd∩emr)
πd(emd)(1)
to compute recall , we compare the traces allowed by both models ( emd∩emr) with the traces allowed by the
real model ( emr).πris used to quantify the fractions of traces. to compute precision , we compare the traces
allowed by both models ( emd∩emr) with the traces allowed by the discovered model ( emd).πdis used to
quantify the fractions of traces. note that it does not make any sense to use πrwhen computing precision.
the behavior in emd\emrwill be unlikely according to πr. hence, precision based on πrwould always yield
a high value even when the discovered model allows for lots of additional behavior.
definition 11 computes recall ( rec(sr, sd)) and precision ( prec(sr, sd)) by making the assumption that the
real process is known. in real-life settings, this is not realistic. however, the definitions can be used when
the ground truth is known (e.g., in process mining challenges and to evaluate discovery algorithms).
ifsr= (mr, πr)is unknown, we can try to approximate it by a log-based estimate of the real process. of
course, this is very a crude approximation when only a fraction of the possible traces has been observed.
definition 12 (log-based precision and recall) .letl∈ landsl= (ml, πl), sd= (md, πd)∈ ssuch that
πl(t) =l(t)
|l|fort∈ t andeml=el.
rec(sl, sd) =πl(emd∩eml)
πl(eml)prec(sl, sd) =πd(emd∩eml)
πd(emd)(2)
it is interesting to investigate under which conditions the above precision and recall metrics satisfy the
different propositions. moreover, assuming that πrandmrare known may help to understand the interplay
between generalization on the one hand and recall and precision on the other hand. see (aalst 2018) for an
analysis of a few simple conformance measures based on the propositions while including probabilities.
the above formulas also reveal the importance of the trace likelihood function. any process model that has
loops will allow for infinitely many traces. therefore, we cannot reason about fractions of traces. moreover,
the likelihood of traces is very different. consider the process that just does a’s and after each astops with
a 0.5 probability. hence, trace ⟨a⟩has a likelihood of 0.5. trace ⟨a, a, a, a, a ⟩has a likelihood of 0.03125
showing that it makes no sense to count traces without using some trace likelihood function.
5 outlook
in this paper, we discussed the relation between simulation and process mining. as shown, they comple-
ment each other well. process mining can be made more forward-looking by using simulation. moreover,
process mining can breathe new life into simulation research . on the one hand, process mining can be
used to automate parts of the modeling process and create much better fact-based simulation models. on
the other hand, it provides ways to view the real processes and the simulated process in a unified manner.
to introduce process mining, we followed a rather unusual approach. instead of describing existing pro-
cess mining algorithms, we reasoned about quality criteria using so-called conformance propositions . by
providing 21 conformance propositions, we revealed the challenges that process discovery techniques are
facing. moreover, we related modeled behavior and real behavior (event logs or ground-truth models) in a
novel way. this could serve as a basis for revisiting recall, precision, and generalization measures.van der aalst
references
aalst, w. van der 2013. “mediating between modeled and observed behavior: the quest for the “right”
process”. in ieee international conference on research challenges in information science (rcis
2013) , pp. 31–43, ieee computing society.
aalst, w. van der 2015. “business process simulation survival guide”. in handbook on business process
management 1 , edited by j. vom brocke and m. rosemann, international handbooks on information
systems, pp. 337–370, springer-verlag, berlin.
aalst, w. van der 2016. process mining: data science in action . springer-verlag, berlin.
aalst, w. van der 2018. “relating process models and event logs: 21 conformance propositions”. in
algorithms & theories for the analysis of event data (ataed 2018) . ceur workshop proceedings.
bratislava, slovakia, june 2018.
aalst, w. van der, a. adriansyah, and b. van dongen. 2012. “replaying history on process models for con-
formance checking and performance analysis”. wires data mining and knowledge discovery vol. 2
(2), pp. 182–192.
aalst, w. van der, and c. stahl. 2011. modeling business processes: a petri net oriented approach . mit
press, cambridge, ma.
adriansyah, a., j. munoz-gama, j. carmona, b. van dongen, and w. van der aalst. 2015. “measuring
precision of modeled behavior”. information systems and e-business management vol. 13 (1), pp.
37–67.
dahl, o., and k. nygaard. 1966, sept. “simula: an algol based simulation language”. communica-
tions of the acm vol. 1, pp. 671–678.
kleijnen, j., and w. groenendaal. 1992. simulation: a statistical perspective . john wiley and sons, new
york.
law, a., and d. kelton. 1982. simulation modeling and analysis . mcgraw-hill, new york.
medeiros, a., a. weijters, and w. van der aalst. 2007. “genetic process mining: an experimental evalua-
tion”. data mining and knowledge discovery vol. 14 (2), pp. 245–304.
rozinat, a., and w. van der aalst. 2008. “conformance checking of processes based on monitoring real
behavior”. information systems vol. 33 (1), pp. 64–95.
rozinat, a., m. wynn, w. van der aalst, a. ter hofstede, and c. fidge. 2009. “workflow simulation for
operational decision support”. data and knowledge engineering vol. 68 (9), pp. 834–850.
tax, n., x. lu, n. sidorova, d. fahland, and w. van der aalst. 2018. “the imprecisions of precision
measures in process mining”. information processing letters vol. 135, pp. 1–8.
author biography
wil m.p. v an der aalst is a professor at rwth aachen university where he chairs the process and
data science (pads) group. his research interests lie in data science, process science, process mining,
business process management, data mining, process discovery, conformance checking, and simulation.
his website is www.vdaalst.com.