evaluating conformance measures in process mining
using conformance propositions
(extended version)
anja f. syring1, niek tax2, and wil m.p. van der aalst1;2;3
1process and data science (informatik 9),
rwth aachen university, d-52056 aachen, germany
2architecture of information systems,
eindhoven university of technology, eindhoven, the netherlands
3fraunhofer institute for applied information technology fit,
sankt augustin, germany
abstract. process mining sheds new light on the relationship between process
models and real-life processes. process discovery can be used to learn process
models from event logs. conformance checking is concerned with quantifying
thequality of a business process model in relation to event data that was logged
during the execution of the business process. there exist different categories of
conformance measures. recall , also called ﬁtness, is concerned with quantify-
ing how much of the behavior that was observed in the event log ﬁts the process
model. precision is concerned with quantifying how much behavior a process
model allows for that was never observed in the event log. generalization is con-
cerned with quantifying how well a process model generalizes to behavior that is
possible in the business process but was never observed in the event log. many
recall, precision, and generalization measures have been developed throughout
the years, but they are often deﬁned in an ad-hoc manner without formally deﬁn-
ing the desired properties up front. to address these problems, we formulate 21
conformance propositions and we use these propositions to evaluate current and
existing conformance measures. the goal is to trigger a discussion by clearly for-
mulating the challenges and requirements (rather than proposing new measures).
additionally, this paper serves as an overview of the conformance checking mea-
sures that are available in the process mining area.
keywords: process miningconformance checking evaluation measures
1 introduction
process mining [1] is a fast growing discipline that focuses on the analysis of event data
that is logged during the execution of a business process. events in such an event log
contain information on what was done, by whom, for whom, where, when, etc. such
event data are often readily available from information systems that support the execu-
tion of the business process, such as erp, crm, or bpm systems. process discovery ,
the task of automatically generating a process model that accurately describes a business
process based on such event data, plays a prominent role in process mining. throughoutarxiv:1909.02393v1  [cs.ai]  30 aug 2019the years, many process discovery algorithms have been developed, producing process
models in various forms, such as petri nets, process trees, and bpmn.
event logs are often incomplete, i.e., they only contain a sample of all possible be-
havior in the business process. this not only makes process discovery challenging; it is
also difﬁcult to assess the quality of the process model in relation to the log. process
discovery algorithms take an event log as input and aim to output a process model that
satisﬁes certain properties, which are often referred to as the four quality dimensions [1]
of process mining: (1) recall : the discovered model should allow for the behavior seen
in the event log (avoiding “non-ﬁtting” behavior), (2) precision : the discovered model
should not allow for behavior completely unrelated to what was seen in the event log
(avoiding “underﬁtting”), (3) generalization : the discovered model should generalize
the example behavior seen in the event log (avoiding “overﬁtting”), and (4) simplicity :
the discovered model should not be unnecessarily complex. the simplicity dimension
refers to occam’s razor: “one should not increase, beyond what is necessary, the num-
ber of entities required to explain anything”. in the context of process mining, this is
often operationalized by quantifying the complexity of the model (number of nodes,
number of arcs, understandability, etc.). we do notconsider the simplicity dimension
in this paper, since we focus on behavior and abstract from the actual model repre-
sentation. recall is often referred to as ﬁtness in process mining literature. sometimes
ﬁtness refers to a combination of the four quality dimensions. to avoid later confusion,
we use the term recall which is commonly used in pattern recognition, information re-
trieval, and (binary) classiﬁcation. many conformance measures have been proposed
throughout the years, e.g., [1,3,5,12,13,14,15,24,25,27,31,32].
so far it remains an open question whether existing measures for recall, precision,
and generalization measure what they are aiming to measure. this motivates the need
for a formal framework for conformance measures. users of existing conformance mea-
sures should be aware of seemingly obvious quality issues of existing approaches and
researchers and developers that aim to create new measures should be clear on what
conformance characteristics they aim to support. to address this open question, this
paper evaluates state-of-the-art conformance measures based on 21 propositions intro-
duced in [2].
the remainder is organized as follows. section 2 discusses related work. section 3
introduces basic concepts and notations. the rest of the paper is split into two parts
where the ﬁrst one discusses the topics of recall and precision (section 4) and the sec-
ond part is dedicated to generalization (section 5). in both parts, we introduce the corre-
sponding conformance propositions and provide an overview of existing conformance
measures. furthermore, we discuss our ﬁndings of validating existing these measures
on the propositions. additionally, section 4 demonstrates the importance of the propo-
sitions on several baseline conformance measures, while section 5 includes a discussion
about the different points of view on generalization. section 6 concludes the paper.
2 related work
in early years, when process mining started to gain in popularity and the community
around it grew, many process discovery algorithms were developed. but at that timethere was no standard method to evaluate the results of these algorithms and to compare
them to the performance of other algorithms. based on this, rozinat et al. [28] called
on the process mining community to develop a standard framework to evaluate process
discovery algorithms. this led to a variety of ﬁtness/recall, precision, generalization
and simplicity notions [1]. these notions can be quantiﬁed in different ways and there
are often trade-offs between the different quality dimensions. as shown using generic
algorithms assigning weights to the different quality dimensions [10], one quickly gets
degenerate models when leaving out one or two dimensions. for example, it is very easy
to create a simple model with perfect recall (i.e., all observed behavior ﬁts perfectly)
that has poor precision and provides no insights.
throughout the years, several conformance measures have been developed for each
quality dimension. however, it is unclear whether these measures actually measure what
they are supposed to. an initial step to address the need for a framework to evaluate
conformance measures was made in [29]. five so-called axioms for precision measures
were deﬁned that characterize the desired properties of such measures. additionally,
[29] showed that none of the existing precision measures satisﬁed all of the formu-
lated axioms. in comparison to [29] janssenswillen et al. [19] did not rely on qualitative
criteria, but quantitatively compared existing recall, precision and generalization mea-
sures under the aspect of feasibility, validity and sensitivity. the results showed that
all recall and precision measures tend to behave in a similar way, while generalization
measures seemed to differ greatly from each other. in [2] van der aalst made a follow-
up step to [29] by formalizing recall and generalization in addition to precision and
by extending the precision requirements, resulting in a list of 21 conformance propo-
sitions. furthermore, [2] showed the importance of probabilistic conformance mea-
sures that also take into account trace probabilities in process models. beyond that,
[29] and [2] motivated the process mining community to develop new precision mea-
sures, taking the axioms and propositions as a design criterion, resulting in the measures
among others the measures that are proposed in [26] and in [7]. using the 21 propo-
sitions of [2] we evaluate state-of-the-art recall (e.g. [4,26,3,16,23,27,33]), precision
(e.g. [3,16,17,23,13,26,27,30]) and generalization (e.g. [3,13,16]) measures.
this paper uses the mainstream view that there are at least four quality dimensions:
ﬁtness/recall, precision, generalization, and simplicity [1]. we deliberately do not con-
sider simplicity, since we focus on behavior only (i.e., not the model representation).
moreover, we treat generalization separately. in a controlled experiment one can assume
the existence of a so-called “system model”. this model can be simulated to create a
synthetic event log used for discovery. in this setting, conformance checking can be re-
duced to measuring the similarity between the discovered model and the system model
[9,20]. in terms of the well-known confusion matrix, one can then reason about true
positives, false positives, true negatives, and false negatives. however, without a system
model and just an event log, it is not possible to ﬁnd false positives (traces possible in
the model but not in reality). hence, precision cannot be determined in the traditional
way. janssenswillen and depaire [18] conclude in their evaluation of state-of-the-art
conformance measures that none of the existing approaches reliably measures this sim-
ilarity. however, in this paper, we follow the traditional view on the quality dimensions
and exclude the concept of the system from our work.whereas there are many ﬁtness/recall and precision measures there are fewer gen-
eralization measures. generalization deals with future cases that were not yet observed.
there is no consensus on how to deﬁne generalization and in [19] it was shown that
there is no agreement between existing generalization metrics. therefore, we cover gen-
eralization in a separate section (section 5). however, as discussed in [1] and demon-
strated through experimentation [10], one cannot leave out the generalization dimen-
sion. the model that simply enumerates all the traces in the log has perfect ﬁtness/recall
and precision. however, event logs cannot be assumed to be complete, thus proving that
a generalization dimension is needed.
3 preliminaries
amultiset over a setxis a function b:x!nwhich we write as [aw1
1;aw2
2;:::;awnn]
where for all i2[1;n]we haveai2xandwi2n.b(x)denotes the set of
all multisets over set x. for example, [a3;b;c2]is a multiset over set x=fa;b;cg
that contains three aelements, one belement and two celements.jbjis the num-
ber of elements in multiset bandb(x)denotes the number of xelements in b.
b1]b2is the sum of two multisets: (b1]b2)(x) =b1(x) +b2(x): b1nb2is
the difference containing all elements from b1that do not occur in b2. thus, (b1n
b2)(x) = maxfb1(x) b2(x);0g.b1\b2is the intersection of two multisets.
hence, (b1\b2)(x) = minfb1(x);b2(x)g.[x2bjb(x)]is the multiset of all
elements in bthat satisfy some condition b.b1b2denotes that b1is contained in
b2, e.g., [a2;b][a2;b2;c], but[a2;b3]6[a2;b2;c2]and[a2;b2;c]6[a3;b3].
process mining techniques focus on the relationship between observed behavior and
modeled behavior. therefore, we ﬁrst formalize event logs (i.e., observed behavior) and
process models (i.e., modeled behavior). to do this, we consider a very simple setting
where we only focus on the control-ﬂow, i.e., sequences of activities.
3.1 event logs
the starting point for process mining is an event log. each event in such a log refers to
anactivity possibly executed by a resource at a particular time and for a particular case.
an event may have many more attributes, e.g., transactional information, costs, cus-
tomer, location, and unit. here, we focus on control-ﬂow. therefore, we only consider
activity labels and the ordering of events within cases.
deﬁnition 1 (traces). ais the universe of activities . atracet2ais a sequence of
activities.t=ais the universe of traces.
tracet=ha;b;c;d;airefers to 5 events belonging to the same case (i.e., jtj= 5).
an event log is a collection of cases each represented by a trace.
deﬁnition 2 (event log). l=b(t)is the universe of event logs. an event logl2l
is a ﬁnite multiset of observed traces. (l) =ft2lgt is the set of traces appearing
inl2l.(l) =tn(l)is the complement of the set of non-observed traces.
event logl= [ha;b;ci5;hb;a;di3;ha;b;di2]refers to 10 cases (i.e., jlj= 10 ).
five cases are represented by the trace ha;b;ci, three cases are represented by the trace
hb;a;di, and two cases are represented by the trace ha;b;di. hence,l(ha;b;di) = 2 .a c
b d
c
da
b(a) a petri net model (with start and end transitions )
(b) a bpmn model allowing for the same behaviorm1
start
startend
endm2trace
abc
bac
abcdc
bacdcdcdcdc
(c) example logl12fig. 1: two process models m1andm2allowing for the same set of traces ( (m1) =
(m2)) with an example log l12(c).
3.2 process models
the behavior of a process model mis simply the set of traces allowed by m. in our
deﬁnition, we will abstract from the actual representation (e.g. petri nets or bpmn).
deﬁnition 3 (process model). mis the set of process models. a process model m2
mallows for a set of traces (m)t .(m) =t n(m)is the complement of the
set of traces allowed by model m2m .
a process model m2m may abstract from the real process and leave out unlikely
behavior. furthermore, this abstraction can result in (m)allowing for traces that can-
not happen (e.g., particular interleavings or loops).
we distinguish between representation andbehavior of a model. process model
m2m can be represented using a plethora of modeling languages, e.g., petri nets,
bpmn models, uml activity diagrams, automata, and process trees. here, we abstract
from the actual representation and focus on behavioral characteristics (m)t.
figure 1 (a) and (b) show two process models that have the same behavior: (m1) =
(m2) =fha;b;ci;ha;c;bi;ha;b;c;d;ci;hb;a;c;d;ci;:::g. figure 1(c) shows a pos-
sible event log generated by one of these models l12= [ha;b;ci3;hb;a;ci5;
ha;b;c;d;ci2;hb;a;c;d;c;d;c;d;c;d;c i2].
the behavior (m)of a process model m2m can be of inﬁnite size. we use
figure 1 to illustrate this. there is a “race” between aandb. afteraandb, activityc
will occur. then there is a probability that the process ends or dcan occur. let ta;k=
ha;bi(hc;di)khcibe the trace that starts with aand wheredis executedktimes.
tb;k=hb;ai(hc;di)khciis the trace that starts with band wheredis executedk
times.(m1) =(m2) =s
k0fta;k;tb;kg. some examples are given in figure 1(c).
since any log contains only a ﬁnite number of traces, one can never observe all
traces possible in m1orm2.a c
b d
start endfig. 2: a process model m3discovered based on log l3 =
[ha;b;ci5;hb;a;di3;ha;b;di2].
3.3 process discovery
a discovery algorithm takes an event log as input and returns a process model. for
example, the model m3in figure 2 could have been discovered based on event log
l3= [ha;b;ci5;hb;a;di3;ha;b;di2]. ideally, the process model should capture the
(dominant) behavior observed but it should also generalize without becoming too im-
precise. for example, the model allows for trace t=hb;a;cialthough this was never
observed.
deﬁnition 4 (discovery algorithm). a discovery algorithm can be described as a
function disc2l!m mapping event logs onto process models.
we abstract from concrete discovery algorithms. over 100 discovery algorithms
have been proposed in literature [1]. merely as a reference to explain basic notions, we
deﬁne three simple, but extreme, algorithms: disc ot,disc ut, and disc nt. letl2l
be a log. disc ot(l) =mosuch that(mo) =(l)produces an overﬁtting model that
allows only for the behavior seen in the log. disc ut(l) =musuch that(mu) =t
produces an underﬁtting model that allows for any behavior. disc nt(l) =mnsuch that
(mn) =(l)produces a non-ﬁtting model that allows for all behavior notseen in the
log.
4 recall and precision
many recall measures have been proposed in literature [1,3,5,12,13,14,15,24,25,27,31,32].
in recent years, also several precision measures have been proposed [6,29]. only few
generalization measures have been proposed [3]. the goal of this paper is to evaluate
these quality measures. to achieve this, in the following the propositions introduced in
[2] are applied to existing conformance measures.
the notion of recall and precision are well established in the process mining com-
munity. deﬁnitions are in place and there is an agreement on what these two measures
are supposed to measure. however, this is not the case for generalization. there exist
different points of view on what generalization is supposed to measure. depending on
these, existing generalization measures might greatly differ from each other.
to account for the different levels of maturity in recall, precision and generaliza-
tion and to address the controversy in the generalization area, the following section
will solely handle recall and precision while section 5 focuses on generalization. bothsections establish baseline measures, introduce the corresponding propositions of [2],
present existing conformance measures and evaluate them using the propositions.
4.1 baseline recall and precision measures
we assume the existence of two functions: rec()andprec()respectively denoting recall
and precision. both take a log and model as input and return a value between 0 and 1.
the higher the value, the better.
deﬁnition 5 (recall). arecall measure rec2lm! [0;1]aims to quantify the
fraction of observed behavior that is allowed by the model.
deﬁnition 6 (precision). aprecision measure prec2lm! [0;1]aims to quantify
the fraction of behavior allowed by the model that was actually observed.
if we ignore frequencies of traces, we can simply count fractions of traces yielding
the following two simple measures.
deﬁnition 7 (trace-based l2m precision and recall). letl2l andm2m be
an event log and a process model. trace-based l2m precision and recall are deﬁned as
follows:
rectb(l;m) =j(l)\(m)j
j(l)jprectb(l;m) =j(l)\(m)j
j(m)j(1)
sincej(l)jis bounded by the size of the log, rectb(l;m)is well-deﬁned. however,
prectb(l;m)is undeﬁned when (m)is unbounded (e.g., in case of loops).
one can argue, that the frequency of traces should be taken into account when eval-
uating conformance which yields the following measure. note that it is not possible
to deﬁne frequency-based precision based on a process model that does not deﬁne the
probability of its traces. since probabilities are speciﬁcally excluded from the scope of
this paper, the following approach only deﬁnes frequency-based recall.
deﬁnition 8 (frequency-based l2m recall). letl2l andm2m be an event log
and a process model. frequency-based l2m recall is deﬁned as follows:
recfb(l;m) =j[t2ljt2(m)]j
jlj(2)
4.2 a collection of conformance propositions
in [2], 21 conformance propositions covering the different conformance dimensions
(except simplicity) were given. in this section, we focus on the general, recall and pre-
cision propositions introduced in [2]. we discuss the generalization propositions sepa-
rately, because they reason about unseen cases not yet recorded in the event log. most
of the conformance propositions have broad support from the community, i.e., there
is broad consensus that these propositions should hold. these are marked with a “ +”.
more controversial propositions are marked with a “ 0” (rather than a “ +”).general propositions the ﬁrst two propositions are commonly accepted; the com-
putation of a quality measure should be deterministic ( detpro+) and only depend on
behavioral aspects ( behpro+). the latter is a design choice. we deliberately exclude
simplicity notions.
proposition 1 (detpro+).rec(),prec(),gen()are deterministic functions, i.e., the
measures rec(l;m),prec(l;m),gen(l;m)are fully determined by l2l andm2m .
proposition 2 (behpro+).for anyl2 l andm1;m22 m such that(m1) =
(m2):rec(l;m1) = rec(l;m2),prec(l;m1) = prec(l;m2), and gen(l;m1) =
gen(l;m2), i.e., the measures are fully determined by the behavior observed and the
behavior described by the model (representation does not matter).
recall propositions in this subsection, we consider a few recall propositions .rec2
lm! [0;1]aims to quantify the fraction of observed behavior that is allowed by
the model. proposition recpro1+states that extending the model to allow for more
behavior can never result in a lower recall. from the deﬁnition follows, that this propo-
sition implies behpro+. recall measures violating behpro+also violate recpro1+
which is demonstrated as follows:
for two models m1;m2with(m1) =(m2)it follows from recpro1+that
rec(l;m1)rec(l;m2)because(m1)(m2). from recpro1+follows that
rec(l;m2)rec(l;m1)because(m2)(m1). combined, rec(l;m2)rec(l;m1)
andrec(l;m1)rec(l;m2)gives rec(l;m2) = rec(l;m1), thus, recall measures that
fulﬁll recpro1+are fully determined by the behavior observed and the behavior de-
scribed by the model, i.e., representation does not matter.
proposition 3 (recpro1+).for anyl2 l andm1;m22 m such that(m1)
(m2):rec(l;m1)rec(l;m2).
similarly to recpro1+, it cannot be the case that adding ﬁtting behavior to the event
logs, lowers recall ( recpro2+).
proposition 4 (recpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):rec(l1;m)rec(l2;m).
similarly to recpro2+, one can argue that adding non-ﬁtting behavior to event logs
should not be able to increase recall ( recpro30). however, one could also argue that
recall should not be measured on a trace-level, but should instead distinguish between
non-ﬁtting traces by measuring the degree in which a non-ﬁtting trace is still ﬁtting.
therefore, unlike the previous propositions, this requirement is debatable as is indicated
by the “ 0” tag.
proposition 5 (recpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):rec(l1;m)rec(l2;m).
for anyk2n:lk(t) =kl(t), e.g., ifl= [ha;bi3;hci2], thenl4= [ha;bi12;hci8].
we use this notation to enlarge event logs without changing the original distribution.
one could argue that this should not inﬂuence recall ( recpro40), e.g., rec([ha;bi3;hci2];m) = rec([ha;bi12;hci8];m). on the other hand, larger logs can provide more
conﬁdence that the log is indeed a representative sample of the possible behavior. there-
fore, it is debatable whether the size of the event log should have inﬂuence on recall as
indicated by the “ 0” tag.
proposition 6 (recpro40).for anyl2 l ,m2 m , andk1:rec(lk;m) =
rec(l;m).
finally, we provide a proposition stating that recall should be 1 if all traces in the
log ﬁt the model ( recpro5+). as a result, the empty log has recall 1 for any model.
based on this proposition, rec(l;disc ot(l)) = rec(l;disc ut(l)) = 1 for any logl.
proposition 7 (recpro5+).for anyl2 l andm2 m such that(l)(m):
rec(l;m) = 1 .
precision propositions precision ( prec2lm! [0;1]) aims to quantify the frac-
tion of behavior allowed by the model that was actually observed. initial work in the
area of checking requirements of conformance checking measures started with [29],
where ﬁve axioms for precision measures were introduced. the precision propositions
that we state below partly overlap with these axioms, but some have been added and
some have been strengthened. axiom 1 of [29] speciﬁes detpro+for the case of pre-
cision, while we have generalized it to the recall and generalization dimension. fur-
thermore, behpro+generalizes axiom 4 of [29] from its initial focus on precision to
also cover recall and generalization. precpro1+states that removing behavior from a
model that does not happen in the event log cannot lead to a lower precision. from the
deﬁnition follows, that this proposition implies behpro+. precision measures violat-
ingbehpro+also violate precpro1+. adding ﬁtting traces to the event log can also
not lower precision ( precpro2+). however, adding non-ﬁtting traces to the event log
should not change precision ( precpro30).
proposition 8 (precpro1+).for anyl2l andm1;m22m such that(m1)
(m2)and(l)\((m2)n(m1)) =;:prec(l;m1)prec(l;m2).
this proposition captures the same idea as axiom 2 in [29], but it is more general.
axiom 2 only put this requirement on precision when (l)(m1), while precpro1+
also concerns the situation where this does not hold.
proposition 9 (precpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):prec(l1;m)prec(l2;m).
this proposition is identical to axiom 5 in [29].
proposition 10 (precpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):prec(l1;m) =prec(l2;m).
one could also argue that duplicating the event log should not inﬂuence precision
because the distribution remains the same ( precpro40), e.g., prec([ha;bi20;hci20];m) =
prec([ha;bi40;hci40];m). similar to ( recpro30) and ( recpro40), the equivalents on
the precision side are tagged with “0”.proposition 11 (precpro40).for anyl2l,m2m , andk1:prec(lk;m) =
prec(l;m).
if the model allows for the behavior observed and nothing more, precision should be
maximal ( precpro5+). one could also argue that if all modeled behavior was observed,
precision should also be 1 ( precpro60). the latter proposition is debatable because it
implies that the non-ﬁtting behavior cannot inﬂuence perfect precision, as indicated by
the “0” tag. consider for example extreme cases where the model covers just a small
fraction of all observed behavior (or even more extreme situations like (m) =;).
according to precpro5+andprecpro60,rec(l;disc ot(l)) = 1 for any logl.
proposition 12 (precpro5+).for anyl2l andm2m such that(m) =(l):
prec(l;m) = 1 .
proposition 13 (precpro60).for anyl2l andm2m such that(m)(l):
prec(l;m) = 1 .
4.3 evaluation of baseline conformance measures
to illustrate the presented propositions and justify their formulation, we evaluate the
conformance measures deﬁned as baselines in section 4.1. note that these 3 baseline
measures were introduced to provide simple examples that can be used to discuss the
propositions. we conduct this evaluation under the assumption that l6= [ ] ,(m)6=;
andhi62(m). table 5 in appendix a.1 summarizes the evaluation.
general propositions. based on the deﬁnition of rectbandrecfbit is clear that all
measures can be fully determined by the log and the model. consequently, detpro+
hold for these two baseline conformance measures. however, prectbis undeﬁned
when(m)is unbound and, therefore, non-deterministic.
the behavior of the model is deﬁned as sets of traces (m), which abstracts from the
representation of the process model itself. therefore, all recall and precision baseline
conformance measures fulﬁll behpro+.
recall propositions. considering measure rectb, it is obvious that recpro1+holds
if(m1)(m2), because the intersection between (m2)and(l)will always
be equal or bigger to the intersection of (m1)and(l). the recpro2+proposi-
tion holds for rectb, ifl2=l1]l3and(l3)(m), because the additional ﬁt-
ting behavior is added to the nominator as well as the denominator of the formula:
j(l1)\(m)j+j(l3)j)=(j(l1)j+j(l3)j. this can never decrease recall. further-
more, recpro30propositions holds for rectbsince adding unﬁtting behavior cannot
increase the intersection between traces of the model and the log if l2=l1]l3and
(l3)(m). consequently, only the denominator of the formula grows, which de-
creases recall. similarly, we can show that these two proposition hold for recfb.
duplication of the event log cannot affect rectb, since it is deﬁned based on the set
of traces and not the multiset. the proposition also holds for recfbsince nominator and
denominator of the formula will grow in proportion. hence, recpro40holds for bothbaseline measures. considering rectb,recpro5+holds, since (l)\(m) =(l)if
(l)(m)and consequentlyj(l)\(m)j=j(l)j=j(l)j=j(l)j= 1. the same
conclusions can be drawn for recfb.
precision propositions. consider proposition precpro1+together with prectb. the
proposition holds, since removing behavior from the model that does not happen in the
event log will not affect the intersection between the traces of the model and the log:
(l)\(m2) =(l)\(m1)if(m1)(m2)and(l)\((m2)n(m1)) =;.
at the same time the denominator of the formula decreases, which can never decrease
precision itself. precpro2+also holds for prectb, since the ﬁtting behavior increases
the intersection between traces of the model and the log, while the denominator of
the formula stays the same. furthermore, precpro30holds for prectb, since unﬁtting
behavior cannot affect the intersection between traces of the model and the log.
duplication of the event log cannot affect prectb, since it is deﬁned based on the
set of traces and not the multiset, i.e. precpro40holds.
considering prectb,precpro5+holds, since (l)\(m) =(m)if(m) =(l)
and consequentlyj(l)\(m)j=j(m)j=j(m)j=j(m)j= 1. similarly, precpro60
holds for prectb.
4.4 existing recall measures
the previous evaluation of the simple baseline measures shows that the recall measures
fulﬁll all propositions and the baseline precision measure only violates one proposition.
however, the work presented in [29] demonstrated for precision, that most of the ex-
isting approaches violate seemingly obvious requirements. this is surprising compared
to the results of our simple baseline measure. inspired by [29], this paper takes a broad
look at existing conformance measures with respect to the previously presented propo-
sitions. in the following section, existing recall and precision measures are introduced,
before they will be evaluated in section 4.6.
causal footprint recall ( reca).van der aalst et al. [4] introduce the concept of the
footprint matrix, which captures the relations between the different activities in the log.
the technique relies on the principle that if activity ais followed by bbutbis never fol-
lowed bya, then there is a causal dependency between aandb. the log can be described
using four different relations types. in [1] it is stated that a footprint matrix can also be
derived for a process model by generating a complete event log from it. recall can be
measured by counting the mismatches between both matrices. note that this approach
assumes an event log which is complete with respect to the directly follows relations.
token replay recall ( recb).token replay measures recall by replaying the log on
the model and counting mismatches in the form of missing and remaining tokens. this
approach was proposed by rozinat and van der aalst [27]. during replay, four types of
tokens are distinguished: pthe number of produced tokens,cthe number of consumed
tokens,mthe number of missing tokens that had to be added because a transition wasnot enabled during replay and rthe number of remaining tokens that are left in the
model after replay. in the beginning, a token is produced in the initial place. similarly,
the approach ends by consuming a token from the ﬁnal place. the more missing and
remaining tokens are counted during replay the lower recall: recb=1
2(1 m
c) +
1
2(1 r
p)note that the approach assumes a relaxed sound workﬂow net, but it allows
for duplicate and silent transitions.
alignment recall ( recc).another approach to determine recall was proposed by van
der aalst et al. [3]. it calculates recall based on alignments, which detect process de-
viations by mapping the steps taken in the event log to the ones of the process model.
this map can contain three types of steps (so-called moves): synchronous moves when
event log and model agree, logmoves if the event was recorded in the event log but
should not have happened according to the model and model moves if the event should
have happened according to the model but did not in the event log. the approach uses
a function that assigns costs to log moves and model moves. this function is used to
compute the optimal alignment for each trace in the log (i.e. the alignment with the least
cost associated).
to compute recall, the total alignment cost of the log is normalized with respect to
the cost of the worst-case scenario where there are only moves in the log and in the
model but never together. note, that the approach assumes an accepting petri net with
an initial and ﬁnal state. however, it allows for duplicate and silent transitions in the
process model.
behavioral recall ( recd).goedertier et al. [16] deﬁne recall according to its deﬁ-
nition in the data mining ﬁeld using true positive (tp) and false negative (fn) coun-
ters.tp(l;m)denotes the number of true positives, i.e., the number of events in the
log that can be parsed correctly in model mby ﬁring a corresponding enabled tran-
sition.fn(l;m)denotes the number of false negatives, i.e., the number of events in
the log for which the corresponding transition that was needed to mimic the event was
not enabled and needed to be force-ﬁred. the recall measure is deﬁned as follows:
recd(l;m) =tp(l;m)
tp(l;m)+fn(l;m).
projected recall ( rece).leemans et al. [23] developed a conformance checking ap-
proach that is also able to handle big event logs. this is achieved by projecting the event
log as well as the model on all possible subsets of activities of size k. the behavior of
a projected log and projected model is translated into the minimal deterministic ﬁnite
automata (dfa)4. recall is calculated by checking the fraction of the behavior that
is allowed for by the minimal log-automaton that is also allowed for by the minimal
model-automaton for each projection and by averaging the recall over each projection.
continued parsing measure ( recf).this continued parsing measure was developed
in the context of the heuristic miner by weijters et al. [33]. it abstracts from the rep-
resentation of the process model by translating the petri net into a causal matrix. this
4every regular language has a unique minimal dfa according to the myhill–nerode theorem.matrix deﬁnes input and output expressions for each activity, which describe possible
in- and output behavior. when replaying the event log on the causal matrix, one has to
check whether the corresponding input and output expressions are activated and there-
fore enable the execution of the activity. to calculate the continued parsing measure
the number of events ein the event log, as well as the number of missing activated in-
put expressions mand remaining activated output expressions rare counted. note, that
the approach allows for silent transitions in the process model but excludes duplicate
transitions.
eigenvalue recall ( recg).polyvyanyy et al. [26] introduce a framework for the deﬁ-
nition of language quotients that guarantee several properties similar to the propositions
introduced in [2]. to illustrate this framework, they apply it in the process mining con-
text and deﬁne a recall measure. hereby they rely on the relation between the language
of a deterministic ﬁnite automaton (dfa) that describes the behavior of the model and
the language of the log. in principle, recall is deﬁned as in deﬁnition 7. however, the
measure is undeﬁned if the language of the model or the log are inﬁnite. therefore,
instead of using the cardinality of the languages and their intersection, the measure
computes their corresponding eigenvalues and sets them in relation. to compute these
eigenvalues, the languages have to be irreducible. since this is not the case for the
language of event logs, polyvyanyy et al. [26] introduce a short-circuit measure over
languages and proved that it is a deterministic measure over any arbitrary regular lan-
guage.
4.5 existing precision measures
soundness ( prech).the notion of soundness as deﬁned by greco et al. [17] states
that a model is precise if all possible enactments of the process have been observed in
the event log. therefore, it divides the number of unique traces in the log compliant
with the process model by the number of unique traces through the model. note, that
this approach assumes the process model in the shape of a workﬂow net. furthermore,
it is equivalent to the baseline precision measure prectb.
simple behavioral appropriateness ( preci).rozinat and van der aalst [27] intro-
duce simple behavioral appropriateness to measure the precision of process models.
the approach assumes that imprecise models enable a lot of transitions during replay.
therefore, the approach computes the mean number of enabled transitions xifor each
unique trace iand puts it in relation to the visible transitions tvin the process model.
note, that the approach assumes a sound workﬂow net. however, it allows for duplicate
and silent transitions in the process model.
advanced behavioral appropriateness ( precj).in the same paper, rozinat and van
der aalst [27] deﬁne advanced behavioral appropriateness. this approach abstracts
from the process model by describing the relation between activities of both the log and
model with respect to whether these activities follow and/or precede each other. herebythey differentiate between never ,sometimes andalways precede/follow relations. to
calculate precision the set of sometimes followed relations of the log sl
fand the model
sm
fare considered, as well as their sometimes precedes relations sl
pandsm
p. the frac-
tion of sometimes follows/precedes relations of the model which are also observed by
the event log deﬁnes precision. note, that the approach assumes a sound workﬂow net.
however, it allows for duplicate and silent transitions in the process model.
etc-one/etc-rep ( preck) and etc-all ( precl).munoz-gama and carmona [25]
introduced a precision measure which constructs an automaton that reﬂects the states
of the model which are visited by the event log. for each state, it is evaluated whether
there are activities which were allowed by the process model but not observed by the
event log. these activities are added to the automaton as so-called escaping edges. since
this approach is not able to handle unﬁtting behavior, [6] and [3] extended the approach
with a preprocessing step that aligned the log to the model before the construction of the
automaton. since it is possible that traces result in multiple optimal alignments, there
are three variations of the precision measure. one can randomly pick one alignment
and construct the alignment automaton based on it (etc-one), select a representative
set of multiple alignments (etc-rep) or use all optimal alignments (etc-all). for each
variation, [3] deﬁnes an approach that assigns appropriate weights to the edges of the
automaton. precision is then computed by comparing for each state of the automaton,
the weighted number of non-escaping edges to the total number of edges.
behavioral speciﬁcity ( precm) and behavioral precision ( precn).goedertier et al.
[16] introduced a precision measure based on the concept of negative events that is de-
ﬁned based on the concept of a confusion matrix as used in the data mining ﬁeld. in
this confusion matrix, the induced negative events are considered to be the ground truth
and the process model is considered to be a prediction machine that predicts whether
an event can or cannot occur. a negative event expresses that at a certain position in
a trace, a particular event cannot occur. to induce the negative events into an event
log, the traces are split in subsequences of length k. for each event ein the trace, it is
checked whether another event encould be a negative event. therefore the approach
searches whether the set of subsequences contains a similar sequence to the one pre-
cedinge. if no matching sequence is found that contains enat the current position of e,
enis recorded as a negative event of e. to check conformance the log, that was induced
with negative events, is replayed on the model.
for both measures, the log that was induced with negative events is replayed on the
model. speciﬁcity and precision are measured according to their data mining deﬁnition
using true positive (tp), false positive (fp) and true negative (tn) counts.
goedertier et al. [16] ( precm) deﬁned behavioral speciﬁcity precision as
precm(l;m) =tn(l;m)
tn(l;m)+fp(l;m), i.e., the ratio of the induced negative events that
were also disallowed by m. more recently, de weerdt et al. [32] gave an inverse deﬁni-
tion, called behavioral precision ( precn), as the ratio of behavior that is allowed by m
that does not conﬂict an induced negative event, i.e. precn(l;m) =tp(l;m)
tp(l;m)+fp(l;m).weighted negative event precision ( preco).van den broucke et al. [30] proposed
an improvement to the approach of goedertier et al. [16], which assigns weights to
negative events. these weights indicate the conﬁdence of the negative events actually
being negative. to compute the weight, the approach takes the sequence preceding event
eand searches for the matching subsequences in the event log. all events that have never
followed such a subsequence are identiﬁed as negative events for eand their weight is
computed based on the length of the matching subsequence. to calculate precision the
enhanced log is replayed on the model, similar to the approach introduced in [32].
however, instead of increasing the counters by 1 they are increased by the weight of
the negative event. furthermore, van den broucke et al. [30] also introduced a modiﬁed
trace replay procedure which ﬁnds the best ﬁtting ﬁring sequence of transitions, taking
force ﬁring of transitions as well as paths enabled by silent transitions into account.
projected precision ( precp).along with projected recall ( rece) leemans et al. [23]
introduce projected precision. to compute precision, the approach creates a dfa which
describes the conjunction of the behavior of the model and the event log. the num-
ber of outgoing edges of dfa (mja)and the conjunctive automaton dfac (l;m;a )
are compared. precision is calculated for each subset of size kand averaged over the
number of subsets.
anti-alignment precision ( precq).van dongen et al. [13] propose a conformance
checking approach based on anti-alignments. an anti-alignment is a run of a model
which differs from all the traces in a log. the principle of the approach assumes that a
very precise model only allows for the observed traces and nothing more. if one trace
is removed from the log, it becomes the anti-alignment for the remaining log.
therefore, trace-based precision computes an anti-alignment for each trace in the
log. then the distance dbetween the anti-alignment and the trace is computed. this
is summed up for each trace and averaged over the number of traces in the log. the
more precise a model, the lower the distance. however, the anti-alignment used for
trace-based precision is limited by the length of the removed trace jj. therefore, log-
based precision uses an anti-alignment between the model and the complete log which
has a length which is much greater than the traces observed in the log. anti-alignment
precision is the weighted combination of trace-based and log-based anti-alignment pre-
cision. note, that the approach allows for duplicate and silent transitions in the process
model.
eigenvalue precision ( precr).polyvyanyy et al. [26] also deﬁne a precision measure
along with the eigenvalue recall ( recg). for precision, they rely on the relation between
the language of a deterministic ﬁnite automaton (dfa) that describes the behavior of
the model and the language of the log. to overcome the problems arising with inﬁnite
languages of the model or log, they compute their corresponding eigenvalues and set
them in relation. to compute these eigenvalues, the languages have to be irreducible.
since this is not the case for the language of event logs, polyvyanyy et al. [26] introduce
a short-circuit measure over languages and proof that it is a deterministic measure over
any arbitrary regular language.table 1: overview of the recall propositions that hold for the existing measures (under
the assumption that l6= [ ] ,(m)6=;andhi62(m)):pmeans that the proposition
holds for any log and model and means that the proposition does not always hold.
proposition name recarecbreccrecdrecerecfrecg
1 detpro+pppp
2 behpro+ppppp
3 recpro1+pppp
4 recpro2+ppppppp
5 recpro30p
6 recpro40ppppppp
7 recpro5+ppppp
4.6 evaluation of existing recall and precision measures
several of the existing precision measures are not able to handle non-ﬁtting behavior
and remove it by aligning the log to the model. we use a baseline approach for the
alignment, which results in a deterministic event log: lis the original event log, which
is aligned in a deterministic manner. the resulting event log l0corresponds to unique
paths through the model. we use l0to evaluate the propositions.
evaluation of existing recall measures the previously presented recall measures
are evaluated using the corresponding propositions. the results of the evaluation are
displayed in table 1. to ensure the readability of this paper, only the most interesting
ﬁndings of the evaluation are addressed in the following section. for full details refer
to appendix a.2.
ac b
d ef
fig. 3: a process model m4.
the evaluation of the causal footprint recall measure (reca) showed that it is deter-
ministic and solely relies on the behavior of the process model. however, the measure
violates several propositions such as recpro 1+,recpro 30, and recpro 5+. these vio-
lations are caused by the fact that recall records every difference between the footprint
of the log and the model. behavior that is described by the model but is not observed in
the event log has an impact on recall, although deﬁnition 5 states otherwise. to illus-
trate this, consider m4in figure 3, event log l4= [ha;b;c;d;e;fi;ha;b;d;c;e;fi]and
recpro 5+. the traces in l4perfectly ﬁt process model m4. the footprint of l4is showntable 2: the causal footprints of m4(a),l4(b). mismatching relations are marked in red.
(a)
a b c d e f
a#!#!# #
b #! jj jj #
c# #jj jj !
d jj jj #!#
e#jj jj  #!
f# # # #(b)
a b c d e f
a#!# # # #
b #!! # #
c# #jj ! #
d# jj #!#
e# #  #!
f# # # # #
in table 2 (b). comparing it to the footprint of m4in table 2 (a) shows mismatches
althoughl4is perfectly ﬁtting. these mismatches are caused by the fact that the log
does not show all possible behavior of the model and, therefore, the footprint cannot
completely detect the parallelism of the model. consequently 10 of 36 relations of the
footprint represent mismatches: reca(l4;m4) = 1 10
36= 0:726= 1. van der aalst
mentions in [1] that checking conformance using causal footprints is only meaningful
if the log is complete in term of directly followed relations. moreover, the measure also
includes precision and generalization aspects, next to recall.
in comparison, recall based on token replay (recb) depends on the path taken
through the model. due to duplicate activities and silent transitions, multiple paths
through a model can be taken when replaying a single trace. different paths can lead
to different numbers of produced, consumed, missing and remaining tokens. therefore,
the approach is neither deterministic nor independent from the structure of the process
model and, consequently, violates recpro 1+. the continued parsing measure (recf)
builds on a similar replay principle as token-based replay and also violates detpro+.
however, the approach translates the process model into a causal matrix and is therefore
independent of its structure.
table 1 also shows that most measures violate recpro 30. this is caused by the fact,
that we deﬁne non-ﬁtting behavior in this paper on a trace level: traces either ﬁt the
model or they do not. however, the evaluated approaches measure non-ﬁtting behavior
on an event level. a trace consists of ﬁtting and non-ﬁtting events. in cases where the
log contains traces with a large number of deviating events, recall can be improved by
adding non-ﬁtting traces which contain several ﬁtting and only a few deviating events.
to illustrate this, consider token replay (recb), process model m5in figure 4, l5=
[ha;b;f;g ]andl6=l5][ha;d;e;f;gi]. the logl5is not perfectly ﬁtting and replaying
it on the model results in 6 produced and 6 consumed tokens, as well as 1 missing and
1 remaining token. recb(l5;m5) =1
2(1 1
6) +1
2(1 1
6) = 0:833. event logl6was
created by adding non-ﬁtting behavior to l5. replayingl6onm5results inp=c= 13 ,
r=m= 2andrecb(l7;m6) =1
2(1 2
13)+1
2(1 2
13) = 0:846. hence, the additional
unﬁtting trace results in proportionally more ﬁtting events than deviating ones which
improves recall: recb(l6;m6)<recb(l7;m6).
to overcome the problems arising with the differences between trace-based and
event-based ﬁtness, one could alter the deﬁnition of recpro 30by requiring, that theacb
d ef
hgfig. 4: petri net m5
table 3: overview of the precision propositions that hold for the existing measures
(under the assumption that l6= [ ] ,(m)6=;andhi 62(m)):pmeans that the
proposition holds for any log and model and means that the proposition does not
always hold.
prop. name prechpreciprecjpreckpreclprecmprecnprecoprecpprecqprecr
1 detpro+pppp
2 behpro+pppp
8precpro1+ppp
9precpro2+ppp
10 precpro30pp
11 precpro40ppppppppppp
12 precpro5+pppppppp
13 precpro60pppppppp
initial logl1only contains ﬁtting behavior ( (l1)(m)). however, to stay within the
scope of this paper, we decide to use the propositions as deﬁned in [2] and keep this
suggestion for future work.
evaluation of existing precision measures the previously presented precision mea-
sures are evaluated using the corresponding propositions. the results of the evaluation
are displayed in table 3. to ensure the readability of this paper, only the most interest-
ing ﬁndings of the evaluation are addressed in the following section. for full details, we
refer to appendix a.3.
the evaluation showed that several measures violate the determinism detpro+
proposition. for example, the soundness measure ( prech) solely relies on the number
of unique paths of the model j(m)jand unique traces in the log that comply with the
process modelj(l)\(m)j. hence, precision is not deﬁned if the model has inﬁnite
possible paths. additionally to detpro+, behavioral speciﬁcity ( recm) and behavioral
precision (recn) also violate behpro+. if during the replay of the trace duplicate or
silent transitions are encountered, the approach explored which of the available transi-
tions enables the next event in the trace. if no solution is found, one of the transitions
is randomly ﬁred, which can lead to different recall values for traces with the same
behavior.a
db
gc
e
f(a)
 (b)
fig. 5: petri net m6(a) and the alignment automaton describing the state space of =
ha;b;c;gi(b)
table 3 shows that simple behavioral appropriateness ( preci) violates all but one of
the propositions. one of the reason is that it relies on the average number of enabled
transitions during replay. even when the model allows for all exactly observed behavior
(and nothing more), precision is not maximal when the model is not strictly sequential.
advanced behavioral appropriateness ( precj) overcomes these problems by relying on
follow relations. however, it is not deterministic and depends on the structure of the
process model.
the results presented in [29] show that etc precision ( preckandprecl), weighted
negative event precision ( preco) and projected precision ( precp) violate precpro1+.
additionally, all remaining measures aside from anti-alignment precision ( precq) and
eigenvalue precision ( precr) violate the proposition. the proposition states that remov-
ing behavior from a model that does not happen in the event log cannot lower precision.
consider, projected precision ( precp) and a model with a length-one-loop. we remove
behavior from the model by restricting the model to only execute the looping activity
twice. this changes the dfa of the model since future behavior now depends on how
often the looping activity was executed: the dfa contains different states for each exe-
cution. if these states show a low local precision, overall precision decreases. further-
more, [29] showed that etc precision ( preckandprecl), projected precision ( precp)
and anti-alignment precision ( precq) also violate precpro2+.
in general, looking at table 3 shows that all precision measures, except for sound-
ness (prech) and eigenvalue precision ( precr) violate precpro30, which states that
adding unﬁtting behavior to the event log should not change precision. however, for
example, all variations of the etc-measure ( preck,precl) align the log before con-
structing the alignment automaton. unﬁtting behavior can be aligned to a trace that
was not seen in the log before and introduce new states to the automaton. consider
process model m6, together with trace =ha;b;c;giand its alignment automaton dis-
played in figure 5. adding the unﬁtting trace ha;d;gicould result in the aligned trace
ha;d;e;giorha;d;f;gi. both aligned traces introduce new states into the alignment
automaton, alter the weights assigned to each state and, consequently, change preci-
sion. weighted negative precision ( preco) also violates this proposition. the measure
accounts for the number of negative events that actually could ﬁre during trace replay
(fp). these false positives are caused by behavior that is shown in the model but not
observed in the log. as explained in the context of recpro 30, although the trace is notﬁtting when considered as a whole, certain parts of the trace can ﬁt the model. these
parts can possibly represent the previously missing behavior in the event log that leads
to the wrong classiﬁcation of negative events. adding these traces will, therefore, lead
to a decrease in false positives and changes precision. fp(l1;m)> fp (l2;m)and
tp(l1;m)
(tp(l1;m)+fp(l1;m))<tp(l2;m)
(tp(l2;m)+fp(l2;m)).
table 3 shows that preci,preckandpreclviolate proposition precpro60, which
states that if all modeled behavior was observed, precision should be maximal and un-
ﬁtting behavior cannot effect precision. precionly reports maximal precision if the
model is strictly sequential and both etc measures ( preckandprecl) can run into
problems with models containing silent or duplicate transitions.
the etc (preck,precl) and anti-alignment measures ( precq) form a special
group of measures as they are unable to handle unﬁtting behavior without pre-processing
unﬁtting traces and aligning them to the process model. accordingly, we evaluate the
conformance measure based on this aligned log. the evaluation of precpro30and the
etc measure ( preck,precl) is an example of the alignment of the log resulting in a vi-
olation. however, there are also cases where the proposition only holds because of this
alignment. consider, for example, anti-alignment precision ( precq) and proposition
precpro60. by deﬁnition, an anti-alignment will always ﬁt the model. consequently,
when computing the distance between the unﬁtting trace and the anti-alignment it will
never be minimal. however, after aligning the log, it exactly contains the modeled be-
havior, precision is maximal and the proposition holds.
5 generalization
generalization is a challenging concept to deﬁne, in contrast to recall and precision. as
a result, there are different viewpoints within the process mining community on what
generalization precisely means. the main reason for this is, that generalization needs to
reason about behavior that was not observed in the event log and establish its relation
to the model.
the need for a generalization dimension stems from the fact that, given a log, a
model can be ﬁtting and precise, but be overﬁtting. the algorithm that simply creates
a modelmsuch that(m) =ft2lgis useless because it is simply enumerating
the event log. consider an unknown process. assume we observe the ﬁrst four traces
l1= [ha;b;ci;hb;a;ci;ha;b;di;hb;a;di]. based on this we may construct the model
m3in figure 2 with (m3) =fha;b;ci;hb;a;ci;ha;b;di;hb;a;dig. this model al-
lows for all the traces in the event log and nothing more. however, because the real
underlying process in unknown, this model may be overﬁtting event log l1. based on
just four example traces we cannot be conﬁdent that the model m3in figure 2 will be
able to explain future behavior of the process. the next trace may as well be ha;cior
ha;b;b;ci. now assume that we observe the same process for a longer time and consider
the ﬁrst 100 traces (including the initial four): l2= [ha;b;ci25;hb;a;ci25;ha;b;di25;
hb;a;di25]. after observing 100 traces, we are more conﬁdent that model m3in fig-
ure 2 is the right model. intuitively, the probability that the next case will have a trace not
allowed bym3gets smaller. now assume that we observe the same process for an even
longer time and obtain the event log l2= [ha;b;ci53789;hb;a;ci48976;ha;b;di64543;hb;a;di53789]. although we do not know the underlying process, intuitively, the prob-
ability that the next case will have a trace not allowed by m3is close to 0. this simple
example shows that recall and precision are not enough for conformance checking. we
need a generalization notion to address the risk of overﬁtting example data.
it is difﬁcult to reason about generalization because this refers to unseen cases. van
der aalst et al. [3] was the ﬁrst to quantify generalization. in [3], each event is seen as
an observation of an activity ain some state s. suppose that state sis visitedntimes
and thatwis the number of different activities observed in this state. suppose that n
is very large and wis very small, then it is unlikely that a new event visiting this state
will correspond to an activity not seen before in this state. however, if nandware of
the same order of magnitude, then it is more likely that a new event visiting state swill
correspond to an activity not seen before in this state. this reasoning is used to provide
a generalization metric. this estimate can be derived under the bayesian assumption
that there is an unknown number of possible activities in state sand that probability
distribution over these activities follows a multinomial distribution.
it is not easy to develop an approach that accurately measures generalization. there-
fore, some authors deﬁne generalization using the notion of a “system” (i.e., a model
of the real underlying process). the system refers to the real behavior of the underlying
process that the model tries to capture. this can also include the context of the process
such as the organization or rules. for example, employees of a company might excep-
tionally be allowed to deviate from the deﬁned process model in certain situations [20].
in this view, system ﬁtness measures the fraction of the behavior of the system that is
captured by the model and system precision measures how much of the behavior of
the model is part of the system. buijs et al. [11] link this view to the traditional un-
derstanding of generalization. they state that both system ﬁtness and system precision
are difﬁcult to obtain under the assumption that the system is unknown. therefore,
state-of-the-art discovery algorithms assume that the process model discovered from an
event log does not contain behavior outside of the system. in other words, they assume
system precision to be 1. given this assumption, system ﬁtness can be seen as general-
ization [11]. janssenswillen et al. [20] agree that in this comparison between the system
and the model, especially the system ﬁtness, in fact is what deﬁnes generalization. fur-
thermore, janssenswillen and depaire [18] demonstrated the differences between the
traditional and the system-based view on conformance checking by showing that state-
of-the-art conformance measures cannot reliably assess the similarity between a process
model and the underlying system.
although capturing the unobserved behavior by assuming a model of the system is
a theoretically elegant solution, practical applicability of this solution is hindered by the
fact that is often impossible to retrieve full knowledge about the system itself. further-
more, [2] showed the importance of trace probabilities in process models. to accurately
represent reality, the system would also need to include probabilities for each of its
traces. however, to date, there is only one conformance measure that can actually sup-
port probabilistic process models [22]. this approach uses the earth movers’ distance
which measures the effort to transform the distributions of traces of the event log into
the distribution of traces of the model.some people would argue that one should use cross-validation (e.g., k-fold check-
ing). however, this is a very different setting. cross validation aims to estimate the
quality of a discovery approach and not the quality of a given model given an event log.
of course, one could produce multiple process models using fragments of the event log
and compare them. however, such forms of cross-validation evaluate the quality of the
discovery technique and are unrelated to generalization.
for these reasons, we deﬁne generalization in the traditional sense.
deﬁnition 9 (generalization). ageneralization measure gen2lm! [0;1]aims
to quantify the probability that new unseen cases will ﬁt the model.5
this deﬁnition assumes that a process generates a stream of newly executed cases.
the more traces that are ﬁtting and the more redundancy there is in the event, the more
certain one can be that the next case will have a trace that ﬁts the model. note that
we deliberately do not formalize the notion of probability, since in real-life we cannot
know the real process. also phenomena like concept drift and contextual factors make
it unrealistic to reason about probabilities in a formal sense.
based on this deﬁnition, we present a set of propositions. note that we do not claim
our set of propositions to be complete and invite other researchers who represent a
different viewpoint on generalization to contribute to the discussion.
5.1 generalization propositions
generalization ( gen2lm! [0;1]) aims to quantify the probability that new un-
seen cases will ﬁt the model. this conformance dimension is a bit different than the two
previously discussed conformance dimensions because it reasons about future unseen
cases (i.e., not yet in the event log). if the recall is good and the log is complete with
lots of repeating behavior, then future cases will most likely ﬁt the model. analogous
to recall, model extensions cannot lower generalization ( genpro1+), extending the log
with ﬁtting behavior cannot lower generalization ( genpro2+), and extending the log
with non-ﬁtting behavior cannot improve generalization ( genpro30).
proposition 14 (genpro1+).for anyl2l andm1;m22m such that(m1)
(m2):gen(l;m1)gen(l;m2).
similar to recall, this proposition implies behpro+. generalization measures vio-
lating behpro+also violate genpro1+.
proposition 15 (genpro2+).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):gen(l1;m)gen(l2;m).
proposition 16 (genpro30).for anyl1;l2;l32l andm2m such thatl2=l1]l3
and(l3)(m):gen(l1;m)gen(l2;m).
5note that the term “probability” is used here in an informal manner. since we only have exam-
ple observations and no knowledge of the underlying (possibly changing) process, we cannot
compute such a probability. of course, unseen cases can have traces that have been observed
before.duplicating the event log does not necessarily inﬂuence recall and precision. ac-
cording to propositions recpro40andprecpro40this should have no effect on recall
and precision. however, making the event log more redundant, should have an effect
on generalization. for ﬁtting logs, adding redundancy without changing the distribution
can only improve generalization ( genpro4+). for non-ﬁtting logs, adding redundancy
without changing the distribution can only lower generalization ( genpro5+). note that
genpro4+andgenpro5+are special cases of genpro60andgenpro70.genpro60
andgenpro70consider logs where some traces are ﬁtting and others are not. for a log
where more than half of the traces is ﬁtting, duplication can only improve generaliza-
tion ( genpro60). for a log where more than half of the traces is non-ﬁtting, duplication
can only lower generalization ( genpro70).
proposition 17 (genpro4+).for anyl2l,m2m , andk1such that(l)
(m):gen(lk;m)gen(l;m).
proposition 18 (genpro5+).for anyl2l,m2m , andk1such that(l)
(m):gen(lk;m)gen(l;m).
proposition 19 (genpro60).for anyl2l,m2m , andk1such that most traces
are ﬁtting (j[t2ljt2(m)]jj[t2ljt62(m)]j):gen(lk;m)gen(l;m).
proposition 20 (genpro70).for anyl2l,m2m , andk1such that most traces
are non-ﬁtting (j[t2ljt2(m)]jj[t2ljt62(m)]j):gen(lk;m)gen(l;m).
when the model allows for any behavior, clearly the next case will also be ﬁtting
(genpro80). nevertheless, it is marked as controversial because the proposition would
also need to hold for an empty event log.
proposition 21 (genpro80).for anyl2 l andm2 m such that(m) =t:
gen(l;m) = 1 .
5.2 existing generalization measures
the following sections introduce several state-of-the-art generalization measures, be-
fore they will be evaluated using the corresponding propositions.
alignment generalization ( gens).van der aalst et al. [3] also introduce a measure
for generalization. this approach considers each occurrence of a given event eas ob-
servation of an activity in some state s. the approach is parameterized by a statem
function that maps events onto states in which they occurred. for each event ethat oc-
curred in state sthe approach counts how many different activities wwere observed in
that state. furthermore, it counts the number of visits nto this state. generalization is
high ifnis very large and wis small, since in that case, it is unlikely that a new trace
will correspond to unseen behavior in that state.table 4: an overview of the generalization propositions that hold for the measures:
(assumingl6= [ ] ,(m)6=;andhi62(m)):pmeans that the proposition holds for
any log and model and means that the proposition does not always hold.
proposition name gensgentgenu
1 detpro+pp
2 behpro+p
14 genpro1+
15 genpro2+
16 genpro30
17 genpro4+ppp
18 genpro5+pp
19 genpro60ppp
20 genpro70pp
21 genpro80p
weighted negative event generalization ( gent).aside from improving the approach
of goedertier et al. [16] van den broucke et al. [30] also developed a generalization mea-
sure based on weighted negative events. it deﬁnes allowed generalizations agwhich
represent events, that could be replayed without errors and conﬁrm that the model is
general and disallowed generalizations dg which are generalization events, that could
not be replayed correctly. if during replay a negative event eis encountered that actu-
ally was enabled the agvalue is increased by 1 weight (e). similarly, if a negative
event is not enabled the dg value is increased by 1 weight (e). the more disallowed
generalizations are encountered during log replay the lower generalization.
anti-alignment generalization ( genu).van dongen et al. [13] also introduce an
anti-alignment generalization and build on the principle that with a generalizing model,
newly seen behavior will introduce new paths between the states of the model, how-
ever no new states themselves. therefore, they deﬁne a recovery distance drecwhich
measures the maximum distance between the states visited by the log and the states
visited by the anti-alignment . a perfectly generalizing model according to van don-
gen et al. [13] has the maximum distance to the anti-alignment with minimal recovery
distance. similar to recall they deﬁne trace-based and log-based generalization. finally,
anti-alignment generalization is the weighted combination of trace-based and log-based
anti-alignment generalization.
5.3 evaluation of existing generalization measures
the previously presented generalization measures are evaluated using the correspond-
ing propositions. the results of the evaluation are displayed in table 4. to improve
the readability of this paper, only the most interesting ﬁndings of the evaluation are
addressed in the following section. for full details refer to appendix a.4.table 4 displays that alignment based generalization ( gens) violates several propo-
sitions. generalization is not deﬁned if there are unﬁtting traces since they cannot be
mapped to states of the process model. therefore, unﬁtting event logs should be aligned
to ﬁt to the model before calculating generalization. aligning a non-ﬁtting log and du-
plicating it will result in more visits to each state visited by the log. therefore, adding
non-ﬁtting behavior increases generalization and violates the propositions genpro30,
genpro5+andgenpro70.
in comparison, weighted negative event generalization ( gent) is robust against the
duplication of the event log, even if it contains non-ﬁtting behavior. however, this mea-
sure violates detpro+,behpro+,genpro1+,genpro2+andgenpro30, which states
that extending the log with non-ﬁtting behavior cannot improve generalization. how-
ever, in this approach, negative events are assigned a weight which indicates how certain
the log is about these events being negative ones. even though the added behavior is
non-ﬁtting it might still provide evidence for certain negative events and therefore in-
crease their weight. if these events are then not enabled during log replay the value for
disallowed generalizations (dg) decreases dg(l1;m)< dg (l2;m)and generaliza-
tion improves:ag(l1;m)
ag(l1;m)+dg(l1;m)<ag(l2;m)
ag(l2;m)+dg(l2;m).
table 4 shows that anti-alignment generalization ( genu) violates several propo-
sitions. the approach considers markings of the process models as the basis for the
generalization computation which violates the behavioral proposition. furthermore, the
measure cannot handle if the model displays behavior that has not been observed in
the event log. if the unobserved model behavior and therefore also the anti-alignment
introduced a lot of new states which were not visited by the event log, the value of the re-
covery distance increases and generalization is lowered. this clashes with propositions
genpro1+andgenpro8+. finally, the approach also excludes unﬁtting behavior from
its scope. only after aligning the event log, generalization can be computed. as a result,
the measure fulﬁlls genpro5+,genpro60andgenpro70, but violates genpro30.
6 conclusion
with the process mining ﬁeld maturing and more commercial tools becoming available
[21], there is an urgent need to have a set of agreed-upon measures to determine the
quality of discovered processes models. we have revisited the 21 conformance propo-
sitions introduced in [2] and illustrated their relevance by applying them to baseline
measures. furthermore, we used the propositions to evaluate currently existing confor-
mance measures. this evaluation uncovers large differences between existing confor-
mance measures and the properties that they possess in relation to the propositions. it
is surprising that seemingly obvious requirements are not met by today’s conformance
measures. however, there are also measures that do meet all the propositions.
it is important to note that we do not consider the set of propositions to be complete.
instead, we consider them to be an initial step to start the discussion on what properties
are to be desired from conformance measures, and we encourage others to contribute to
this discussion. moreover, we motivate researchers to use the conformance propositions
as design criteria for the development of novel conformance measures.one relevant direction of future work is in the area of conformance propositions that
have a more ﬁne-grained focus than the trace-level, i.e., that distinguish between almost
ﬁtting andcompletely non-ﬁtting behavior. another relevant area of future work is in the
direction of probabilistic conformance measures, which take into account branching
probabilities in models, and their desired properties.
acknowledgements we thank the alexander von humboldt (avh) stiftung for sup-
porting our research.
references
1. w.m.p. van der aalst. process mining: data science in action . springer-verlag, berlin,
2016.
2. w.m.p. van der aalst. relating process models and event logs: 21 conformance propo-
sitions. in w.m.p. van der aalst, r. bergenthum, and j. carmona, editors, workshop on
algorithms & theories for the analysis of event data (ataed 2018) , pages 56–74. ceur
workshop proceedings, 2018.
3. w.m.p. van der aalst, a. adriansyah, and b. van dongen. replaying history on process
models for conformance checking and performance analysis. wires data mining and
knowledge discovery , 2(2):182–192, 2012.
4. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining: discovering
process models from event logs. ieee transactions on knowledge and data engineering ,
16(9):1128–1142, 2004.
5. a. adriansyah, b. van dongen, and w.m.p. van der aalst. conformance checking using
cost-based fitness analysis. in c.h. chi and p. johnson, editors, ieee international en-
terprise computing conference (edoc 2011) , pages 55–64. ieee computer society, 2011.
6. a. adriansyah, j. munoz-gama, j. carmona, b. f. van dongen, and w. m. p. van der aalst.
alignment based precision checking. in m. la rosa and p. soffer, editors, business process
management workshops , pages 137–149. springer, 2013.
7. a. augusto, a. armas-cervantes, r. conforti, m. dumas, m. la rosa, and d. reissner.
abstract-and-compare: a family of scalable precision measures for automated process dis-
covery. in m. weske, m. montali, i. weber, and j. vom brocke, editors, proceedings of the
international conference on business process management , pages 158–175, cham, 2018.
springer international publishing.
8. s.k.l.m. vanden broucke, j. de weerdt, j. vanthienen, and b. baesens. on replaying pro-
cess execution traces containing positive and negative events. technical report, ku leuven-
faculty of economics and business, 2013.
9. j.c.a.m. buijs. flexible evolutionary algorithms for mining structured process models . phd
thesis, department of mathematics and computer science, 2014.
10. j.c.a.m. buijs, b.f. van dongen, and w.m.p. van der aalst. on the role of fitness, pre-
cision, generalization and simplicity in process discovery. in r. meersman, s. rinderle,
p. dadam, and x. zhou, editors, otm federated conferences, 20th international confer-
ence on cooperative information systems (coopis 2012) , volume 7565 of lecture notes in
computer science , pages 305–322. springer-verlag, berlin, 2012.
11. j.c.a.m. buijs, b.f. van dongen, and w.m.p. van der aalst. quality dimensions in process
discovery: the importance of fitness, precision, generalization and simplicity. interna-
tional journal of cooperative information systems , 23(1):1–39, 2014.12. j. carmona, b. van dongen, a. solti, and m. weidlich. conformance checking: relating
processes and models . springer-verlag, berlin, 2018.
13. b.f. van dongen, j. carmona, and t. chatain. a uniﬁed approach for measuring precision
and generalization based on anti-alignments. in m. la rosa, p. loos, and o. pastor, editors,
international conference on business process management (bpm 2016) , volume 9850 of
lecture notes in computer science , pages 39–56. springer-verlag, berlin, 2016.
14. b.f. van dongen, j. carmona, t. chatain, and f. taymouri. aligning modeled and observed
behavior: a compromise between computation complexity and quality. in e. dubois and
k. pohl, editors, international conference on advanced information systems engineering
(caise 2017) , volume 10253 of lecture notes in computer science , pages 94–109. springer-
verlag, berlin, 2017.
15. l. garcia-banuelos, n. van beest, m. dumas, m. la rosa, and w. mertens. complete and
interpretable conformance checking of business processes. ieee transactions on software
engineering , 44(3):262–290, 2018.
16. s. goedertier, d. martens, j. vanthienen, and b. baesens. robust process discovery with
artiﬁcial negative events. journal of machine learning research , 10:1305–1340, 2009.
17. g. greco, a. guzzo, l. pontieri, and d. sacc `a. discovering expressive process models by
clustering log traces. ieee transaction on knowledge and data engineering , 18(8):1010–
1027, 2006.
18. g. janssenswillen and b. depaire. towards conﬁrmatory process discovery: making asser-
tions about the underlying system. business & information systems engineering , dec 2018.
19. g. janssenswillen, n. donders, t. jouck, and b. depaire. a comparative study of existing
quality measures for process discovery. information systems , 50(1):2:1–2:45, 2017.
20. g. janssenswillen, t. jouck, m. creemers, and b. depaire. measuring the quality of mod-
els with respect to the underlying system: an empirical study. in m. la rosa, p. loos,
and o. pastor, editors, business process management , pages 73–89, cham, 2016. springer
international publishing.
21. m. kerremans. gartner market guide for process mining, research note g00353970. www.
gartner.com , 2018.
22. s. leemans, a. syring, and w.m.p. van der aalst. earth movers’ stochastic conformance
checking. in t. hildebrandt, b.f. van dongen, m. r ¨oglinger, and j. mendling, editors,
business process management forum (bpm forum 2019) , volume 360 of lecture notes in
business information processing , pages 1–16. springer-verlag, berlin, 2019.
23. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. scalable process discovery and
conformance checking. software and systems modeling , 17(2):599–631, 2018.
24. f. mannhardt, m. de leoni, h.a. reijers, and w.m.p. van der aalst. balanced multi-
perspective checking of process conformance. computing , 98(4):407–437, 2016.
25. j. munoz-gama and j. carmona. a fresh look at precision in process conformance. in
r. hull, j. mendling, and s. tai, editors, business process management (bpm 2010) , volume
6336 of lecture notes in computer science , pages 211–226. springer-verlag, berlin, 2010.
26. a. polyvyanyy, a. solti, m. weidlich, c. di ciccio, and j. mendling. behavioural quotients
for precision and recall in process mining. technical report, university of melbourne, 2018.
27. a. rozinat and w.m.p. van der aalst. conformance checking of processes based on moni-
toring real behavior. information systems , 33(1):64–95, 2008.
28. a. rozinat, a.k. alves de medeiros, c.w. g ¨unther, a.j.m.m. weijters, and w.m.p. van der
aalst. the need for a process mining evaluation framework in research and practice. in
m. castellanos, j. mendling, and b. weber, editors, informal proceedings of the interna-
tional workshop on business process intelligence (bpi 2007) , pages 73–78. qut, brisbane,
australia, 2007.
29. n. tax, x. lu, n. sidorova, d. fahland, and w.m.p. van der aalst. the imprecisions of
precision measures in process mining. information processing letters , 135:1–8, 2018.30. s. k. l. m. vanden broucke, j. de weerdt, j. vanthienen, and b. baesens. determining
process model precision and generalization with weighted artiﬁcial negative events. ieee
transactions on knowledge and data engineering , 26(8):1877–1889, aug 2014.
31. j. de weerdt, m. de backer, j. vanthienen, and b. baesens. a multi-dimensional quality
assessment of state-of-the-art process discovery algorithms using real-life event logs.
information systems , 37(7):654–676, 2012.
32. j. de weerdt, m. de backer, j. vanthienen, and b. baesens. a robust f-measure for eval-
uating discovered process models. in n. chawla, i. king, and a. sperduti, editors, ieee
symposium on computational intelligence and data mining (cidm 2011) , pages 148–155,
paris, france, april 2011. ieee.
33. a.j.m.m. weijters, w.m.p. van der aalst, and a.k. alves de medeiros. process mining
with the heuristics miner-algorithm. beta working paper series, wp 166, eindhoven
university of technology, eindhoven, 2006.
a appendix
a.1 evaluation results of the baseline conformance measures
table 5: overview of the conformance propositions that hold for the three baseline
measures (under the assumption that l6= [ ] ,(m)6=;andhi62(m)):pmeans that
the proposition holds for any log and model and means that the proposition does not
always hold.
proposition name rectbrecfbprecfb
1 detpro+pp
2 behpro+ppp
3 recpro1+pp
4 recpro2+pp
5 recpro30pp
6 recpro40pp
7 recpro5+pp
8 precpro1+p
9 precpro2+p
10 precpro30p
11 precpro40p
12 precpro5+p
13 precpro60pa.2 detailed results of the recall measure evaluation
proposition detpro+
causal footprint recall ( reca).proposition holds.
reasoning. the causal footprint fully describes the log as well as the model in terms of
directly followed relations. by comparing the footprints of the model and the log recall
can be determined.
token replay recall ( recb).proposition does not hold.
reasoning. this technique depends on the path taken through the model. due to dupli-
cate activities and silent transitions, multiple paths through a model can be taken when
replaying a single trace. different paths can lead to different numbers of produced,
consumed, missing and remaining tokens and recall is not deterministic.
alignment recall ( recc).proposition holds.
reasoning. when computing alignments the algorithm searches per trace for the opti-
mal alignment: the alignment with the least cost associated to it. there may be multiple
alignments, but these all have the same cost. recall is computed based on this cost.
therefore, given a log, a model and a cost function the recall computation is determin-
istic.
behavioral recall ( recd).proposition does not hold.
reasoning. if duplicate or silent transitions are encountered during replay of the traces,
which were enhanced with negative events, it is explored which of the available transi-
tions enables the next event in the trace. if no solution is found one of the transitions
is randomly ﬁred, which can lead to different recall values for traces with the same
behavior.
projected recall ( rece).proposition holds.
reasoning. this technique splits log and model into subsets and calculates how many
traces in the sub-log can be replayed on the corresponding sub-model, which is rep-
resented as a deterministic ﬁnite automaton. the sub-logs and models are created by
projection on a subset of activities which is a deterministic process. therefore, the com-
putation of the average recall value over all subsets is also deterministic.
continued parsing measure ( recf).proposition does not hold.
reasoning. the continued parsing measure translates the behavior of the process model
into a causal matrix. this translation is not deﬁned if the model contains duplicate or
silent transitions. consequently, the continued parsing measure is not deﬁned for these
models, which violates this proposition.
eigenvalue recall ( recg).proposition holds.
reasoning. the measure compares the languages of the model and the language of the
process model. these have to be irreducible, to compute their eigenvalue. since the
language of an event log is not irreducible, polyvyanyy et al. [26] introduce a short-
circuit measure over languages and proof that it is a deterministic measure over any
arbitrary regular language.proposition 2 behpro+
causal footprint recall ( reca).proposition holds.
reasoning. the causal footprint completely describes the log as well as the model in
terms of directly followed relations and therefore does not depend on the representation
of the model.
token replay recall ( recb).proposition does not hold.
reasoning. due to duplicate activities and silent transitions, one can think of models
with the same behavior but a different structure. it is also possible to have implicit places
that do not change the behavior but do inﬂuence the number of produced, consumed,
missing and remaining tokens. for example, if a place often has missing tokens, then
duplicating this place will lead to even more missing tokens (also relatively). moreover,
nondeterminism during replay can lead to a difference in the replay path and therefore
in different numbers of produced, consumed, missing and remaining tokens and shows
that token replay-based recall depends on the representation of the model.
alignment recall ( recc).proposition holds.
reasoning. silent transitions are used for routing behavior of the petri net and are not
linked to the actual behavior of the process. during alignment computation, there is no
cost associated with silent transitions. also, the places do not play a role (e.g., implicit
places have no effect). therefore, the structure of the model itself has no inﬂuence on
the alignment computation and two different models expressing the same behavior will
result in the same recall measures.
behavioral recall ( recd).proposition does not hold.
reasoning. if duplicate or silent transitions are encountered during the replay of the
traces, which were enhanced with negative events on the model, it is explored which of
the available transitions enables the next event in the trace. if no solution is found, one
of the transitions is randomly ﬁred, which can lead to different recall values for a trace
on two behaviorally equivalent but structurally different models.
projected recall ( rece).proposition holds.
reasoning. this technique translates the event log as well as the process model into
deterministic ﬁnite automata before computing recall. therefore, it is independent of
the representation of the model itself.
continued parsing measure ( recf).proposition holds.
reasoning. the continued parsing measure translates the possible behavior into so-
called input expressions and output expressions, which describe possible behavior be-
fore and after the execution of each activity. therefore, it abstracts from the structure of
the process model.
eigenvalue recall ( recg).proposition holds.
reasoning. the approach computes recall based on the languages of the event log and
the language of the process model. this abstracts from the representation of the process
model and, therefore, the proposition holds.ac b
d ef
gtrace
abdef
adbef
abcdef
abdcef
(b) example log l7
(a) petri net m 4  and its extension  m 5  which includes the dotted transitionfig. 6: petri net m4and its extension m5which includes the dotted transition (b), as
well as example log l7.
table 6: the causal footprints of m4(a),l7(b) andm5(c). mismatching relations are
marked in red.
(a)
a b c d e f
a#!#!# #
b #! jj jj #
c# #jj jj !
d jj jj #!#
e#jj jj  #!
f# # # #(b)
a b c d e f
a#!#!# #
b #! jj ! #
c# #jj ! #
d jj jj #!#
e#   #!
f# # # # #(c)
a b c d e f g
a#!#!# #!
b #! jj jj #jj
c# #jj jj ! jj
d jj jj #!# #
e#jj jj  #!#
f# # # # 
g jj jj # # #
proposition 3 recpro1+
causal footprint recall ( reca).proposition does not hold.
reasoning. recall is calculated by dividing the number of relations where log and
model differ by the total number of relations. when adding behavior to the model while
keeping the log as is, the causal footprint of the model changes while the causal footprint
of the log stays the same. this may introduce more differences between both footprints
and therefore lowers recall. process model m4, its extension m5in figure 6 (a) and
event logl7= [ha;d;b;e;fi;ha;b;d;e;fi;ha;b;c;d;e;fi;ha;b;d;c;e;fi]illustrate
this. the corresponding causal footprints are displayed in table 6. since the log l7
does not contain activity g, when computing recall between l7andm5, we assume
that all activities show a #-relation with g. computing recall based on the footprints
results in areca(l7;m4) = 1 6
36= 0:83andreca(l7;m5) = 1 14
49= 0:71. the
proposition is violated since reca(l7;m4)< reca(l7;m5). van der aalst mentions in
[1] that checking conformance using causal footprints is only meaningful if the log is
complete in term of directly followed relations. furthermore, the approach intended to
cover precision, generalization and recall in one conformance value.
token replay recall ( recb).proposition does not hold.
reasoning. behpro+does not hold, which implies that recpro1+does not hold.alignment recall ( recc).proposition holds.
reasoning. note that the model extension only adds behavior to the model and does not
restrict it further. during alignment computation, this means that either the initial align-
ments are computed or that the additional behavior resulted in an optimal alignment
with even lower cost (i.e. alignments with less log/model moves). therefore, recall of
the extended model cannot be lower than the value calculated for the initial model.
behavioral recall ( recd).proposition does not hold.
reasoning. behpro+does not hold, which implies that recpro1+does not hold.
projected recall ( rece).proposition holds.
reasoning. based on the deﬁnition of projected recall it can only be lowered if fewer
traces of the log can be replayed on the model. this is only possible if the model exten-
sion also restricts parts of its behavior. hence, by purely extending the model the num-
ber of ﬁtting traces can only be increased: j[t2ljajt2dfa (m1ja)]j 
j[t2ljajt2dfa (m2ja)]jif(m1)(m2). as a result, recall cannot be lowered
and the proposition holds.
continued parsing measure ( recf).proposition holds.
reasoning. note that the model extension only adds behavior to the model and does
not restrict it further. when replaying the log on the extended causal matrix the num-
ber of missing and remaining activated expressions stays the same or decreases which
consequently cannot lower recall.
eigenvalue recall ( recg).proposition holds.
reasoning. trivially, adding behavior to the model can only increase the intersection
between the language of the log and the model.: l(l)\l(m1)l(l)\l(m2)if
(m1)(m2). polyvyanyy et al. [26] proved in lemma 5.6 that the short-circuit
measure based on eigenvalue is increasing, i.e. thateig(l(l)\l(m1))
eig(l(l))eig(l(l)\l(m2))
eig(l(l)).
proposition 4 recpro2+
causal footprint recall ( reca).proposition holds.
reasoning. adding ﬁtting behavior to the event log either does not change the footprint
of the log because no new relations were observed or it changes the corresponding
causal footprint in a way that more of its relations match the causal footprint of the
model. the only three options are, that a #-relation changes to !,!becomesjjor 
becomesjj. hence the differences between the two footprints are minimized and recall
improved.
token replay recall ( recb).proposition holds.
reasoning. adding ﬁtting behavior to the event log means that these traces can be
replayed on the model without any problems. here we assume that if a trace is perfectly
replayable it will also be replayed perfectly. in case of duplicate and silent transitions
this does not need to be the case. consider two very long branches in the process modelallowing for the same behavior. only at the end, they have differences. this may lead
to the situation where initially the wrong branch was chosen. in this paper, we make
the assumption that ﬁtting behavior is replayed correctly. hence, adding ﬁtting traces
results in more produced and consumed tokens without additional missing or remaining
tokens (c1<c2andp1<p2). therefore, recall can only be improved by adding ﬁtting
behavior.1
2(1 m
c1) +1
2(1 r
p1)1
2(1 m
c2) +1
2(1 r
p2).
alignment recall ( recc).proposition holds.
reasoning. fitting behavior results in a perfect alignment which only consists of syn-
chronous moves. consequently, this alignment has no costs assigned and adding it to
the existing log cannot lower recall.
behavioral recall ( recd).proposition holds.
reasoning. for ﬁtting log l3,fn(l3;m) = 0 andtp(l3;m)is proportional to the size
ofl3. forl2=l1]l3, we havetp(l2;m) =tp(l1;m)+tp(l3;m), andfn(l2;m) =
fn(l1;m)+fn(l3;m) =fn(l1;m). therefore,recd(l2;m) =tp(l1;m)+tp(l3;m)
fn(l1;m)
and sincerecd(l1;m) =tp(l1;m)
fn(l1;m), we haverecd(l2;m)recd(l1;m).
projected recall ( rece).proposition holds.
reasoning. based on the deﬁnition of projected recall it can only be lowered if fewer
traces of the log can be replayed on the model. fitting traces can be replayed and
recall cannot be lowered by adding them to the log. j[t2l1jajt2dfa (mja)]j 
j[t2l2jajt2dfa (mja)]jifl2=l1]l3and(l3)(m). averaging recall over all
subsets of a given length also does not inﬂuence this.
continued parsing measure ( recf).proposition holds.
reasoning. adding ﬁtting behavior to the event log means that these traces can be re-
played on the causal matrix without any problems. here we assume, similar to recb,
that if a trace is perfectly replayable it will also be replayed perfectly. hence ﬁtting be-
havior does not yield additional missing or remaining activated expressions. therefore
recall can only be improved.
eigenvalue recall ( recg).proposition holds.
reasoning. trivially, adding ﬁtting behavior to the event log can only increase the
intersection between the language of the log and the model, i.e., l(l1)\l(m)l(l2)\
l(m)ifl2=l1]l3and(l3)(m). polyvyanyy et al. [26] proved in lemma 5.6 that
the short-circuit measure based on eigenvalue is increasing, and therefore it also holds
thateig(l(l1)\l(m))
eig(l(l1))eig(l(l2)\l(m))
eig(l(l2)).
proposition 5 recpro30
causal footprint recall ( reca).proposition does not hold.
reasoning. the causal footprint technique deﬁnes ﬁtting and non-ﬁtting behavior on an
event level, while the proposition states that a trace is either ﬁtting or non-ﬁtting. the
added non-ﬁtting behavior could consist of multiple ﬁtting and one non-ﬁtting event.table 7: the causal footprints of l8. the differences to the footprint of m4in figure 6
are marked in red
a b c d e f
a#!#!! #
b #! jj jj !
c# #jj jj #
d jj jj #!#
e jj jj  #!
f# # # #
acb
d ef
hg
fig. 7: petri net m6
the ﬁtting events could actually decrease the differences between the log and the model
while the single non-ﬁtting event introduces an additional difference. the added non-
ﬁtting trace in total introduce more similarities than differences and therefore improve
recall. beyond that, it is possible that the other traces in the log already resulted in the
appropriate causal relations and the non-ﬁtting event does not change anything.
to illustrate that, consider m4andl7in figure 6. we extend l4with four clearly
unﬁtting traces: l8=l7][ha;d;b;fi;ha;e;b;fi;ha;e;ci;ha;b;c;ei]. the footprint of
l8is displayed in table 7. it shows that adding the unﬁtting traces reveals the parallelism
between the activities d;eandb, and decreases the differences to the footprint of m4
in figure 6 (a). consequently, reca(l8;m4) = 1 4
36= 0:88andreca(l7;m4)<
reca(l8;m4).
token replay recall ( recb).proposition does not hold.
reasoning. in the recall formula used during token replay, the number of produced and
consumed tokens is in the denominator, while the number of missing and remaining
token is in the nominator. if we add a very long non-ﬁtting trace to the event log, which
yields a lot of produced and consumed token but only a few missing and remaining
token, recall is improved. for long cases with only a few deviations, the approach gives
too much weight to the ﬁtting part of the trace.
to illustrate this consider process model m6of figure 7 and log l9= [ha;b;f;g ].
this log is not perfectly ﬁtting and therefore results in 6 produced and 6 consumed
tokens, as well as 1 missing and 1 remaining token. recb(l9;m6) =1
2(1 1
6)+1
2(1 
1
6) = 0:833. we extend the log l9with non-ﬁtting behavior: l10=l9][ha;d;e;f;gi].
replayingl10onm6results inp=c= 13 ,r=m= 2 andrecb(l10;m6) =
1
2(1 2
13) +1
2(1 2
13) = 0:846. hence,recb(l9;m6)<recb(l10;m6).alignment recall ( recc).proposition does not hold.
reasoning. consider model m4in figure 6 and event log l11= [hf;ai], which result
in costsfcost (l;m ) = 6 and worst-case cost movel(l) = 2 andmovem(m) = 6 .
consequently, recall is recc(l11;m4) = 1 6
2+6= 0:25. we add a non-ﬁtting trace
to the logl12=l11]ha;b;c;d;ei, which shows less deviations than l11. this re-
sults in an additional cost of 1 and additional worst-case costs of 5 + 6 . this leads to
recc(l12;m4) = 1 7
(2+5)+26= 0:64. hence, adding a non-ﬁtting trace with fewer
deviations improves recall recc(l11;m4)< recc(l12;m4)and violates the proposi-
tion.
behavioral recall ( recd).proposition does not hold.
reasoning. in the recall formula used during token replay, the number correctly re-
played events (tp) is in the denominator as well as in the nominator, while the number
of transitions that were forced to ﬁre although they were not enabled (fn) is only in the
denominator. if we now add a very long non-ﬁtting trace to the event log, which consists
of a large number of correctly replayed events but only a few force ﬁrings, recall is im-
proved. furthermore, remaining tokens during replay are not considered in the formula
and cannot lower recall, although these tokens are clear indications for unﬁtting traces.
considerm6of figure 7 and log l9= [ha;b;f;g ]. after replaying the log on the
model there are 3 recorded true positive events and 1 recorded false negative events.
this results in recd(l9;m6) =3
3+1= 0:75. we extend the log l9with non-ﬁtting
behavior:l10=l9][ha;d;e;f;gi]. replaying l10onm6results in 7 recorded true
positive events and 2 recorded false negative events. consequently recd(l10;m6) =
7
7+2= 0:77andrecd(l9;m6)<recd(l10;m6).
projected recall ( rece).proposition does not hold.
reasoning. according to this approach, there can be traces that are ﬁtting most of the
automata and traces that are ﬁtting only a few automata. the counter-example of recc
illustrates this and shows that the proposition does not hold. projecting the model and
both logs onfha;bigresults in recall values for both logs of rece(l9jha;bi;m6jha;bi) =
0
1andrece(l10jha;bi;m6jha;bi) =1
2. the additional non-ﬁtting trace in l10similarly ﬁts
the other projected dfas of size 2, except for the ones containing f. however, event
logl9ﬁts none of the projected dfas of size 2. therefore adding non-ﬁtting traces that
ﬁt most projected automata can increase the aggregated recall.
continued parsing measure ( recf).proposition does not hold.
reasoning. in the recall formula, the number of events ein the log is present in the
denominator as well as in the nominator while the number of missing mand remain-
ing activated expressions ris subtracted from the nominator. similar to token replay
recall (b), adding a very long non-ﬁtting trace to the event log which introduces a large
number of events but only a few missing and remaining activated expressions, improves
recall.
eigenvalue recall ( recg).proposition holds.
reasoning. trivially, unﬁtting behavior to the event log cannot change the intersectionbetween the language of the log and the model and consequently also not their eigen-
value. however, the language of the log might increase which lowers recall. l(l1)\
l(m) =l(l2)\l(m)ifl2=l1]l3and(l3)(m). polyvyanyy et al. [26]
proved in lemma 5.6 that the short-circuit measure based on eigenvalue is increasing,
and therefore it also holds thateig(l(l1)\l(m))
eig(l(l1))eig(l(l2)\l(m))
eig(l(l2)).
proposition 6 recpro40
causal footprint recall ( reca).proposition holds.
reasoning. the causal footprint does not account for frequencies of traces and events.
therefore, multiplying the log has no inﬂuence on the causal footprint and therefore
recall does not change.
token replay recall ( recb).proposition holds.
reasoning. multiplying the log ktimes will equally increase the number of produced,
consumed, missing and remaining token. their ratios stay the same and recall does not
change.1
2(1 km
kc)+1
2(1 kr
kp) =1
2(1 m
c)+1
2(1 r
p),recb(lk;m) =recb(l;m).
alignment recall ( recc).proposition holds.
reasoning. multiplying the event log ktimes has no inﬂuence on recall since the for-
mula accounts for trace frequency in denominator and nominator. the ratio of replay
cost and cost of the worst case scenario stays the same and recall does not change.
1 kfcost (l;m)
kmove l(l)+kjljmove m(m)= 1 fcost (l;m)
move l(l)+jljmove m(m),recc(lk;m) =
recc(l;m).
behavioral recall ( recd).proposition holds.
reasoning. multiplying the log ktimes will equally increase the number of correctly
replayed events and force ﬁred transitions.
for anyl2 l,recd(lk;m) =ktp(l;m)
ktp(l;m)+kfn(l;m)=ktp(l;m)
k(tp(l;m)+fn(l;m))=
tp(l;m)
(tp(l;m)+fn(l;m))=recd(l;m).
projected recall ( rece).proposition holds.
reasoning. multiplying the log will equally increase the total number of traces and ﬁt-
ting traces. their ratio stays the same and recall does not change.
kj[t2ljajt2dfa (mja)]j
(kjljaj)=j[t2ljajt2dfa (mja)]j
jljaj,rece(lk;m) =rece(l;m). averaging
over all subsets of a given length also does not inﬂuence this.
continued parsing measure ( recf).proposition holds.
reasoning. multiplying the log will equally increase the number of parsed events, miss-
ing and remaining activated expressions. their ratio stays the same and recall does not
change.1
2k(e m)
ke+1
2k(e r)
ke=1
2(e m)
e+1
2(e r)
e,recf(lk;m) =recf(l;m).
eigenvalue recall (g.) proposition holds.
reasoning. eigenvalue recall is deﬁned purely on the language of the log and the model
and it does not take into account trace frequencies in the log, therefore, this proposition
holds.table 8: the causal footprints of l13. mismatches with the footprint of m4are marked
in red.
a b c d e f
a#!# # # #
b #!! # #
c# #jj ! #
d# jj #!#
e# #  #!
f# # # # #
proposition 7 recpro5+
causal footprint recall ( reca).proposition does not hold.
reasoning. the recall measure based on causal footprints compares the behavior in
both directions. if the model has additional behavior that is not present in the log, even
in the case where all traces in the log ﬁt the model, the footprint comparison will show
the difference and recall will not be maximal.
to illustrate this, consider m4in figure 6. we compute recall for m4andl13=
[ha;b;c;d;e;fi;ha;b;d;c;e;fi]. the traces in l13perfectly ﬁt process model m4. the
footprint ofl13is shown in table 8. comparing it to the footprint of m4in table 6 (a)
shows mismatches although l13is perfectly ﬁtting. these mismatches are caused by the
fact that the log does not show all possible paths of the model and therefore the footprint
cannot completely detect the parallelism of the model. consequently, reca(l13;m4) =
1 10
36= 0:726= 1even though (l)(m).
van der aalst mentions in [1] that checking conformance using causal footprints is
only meaningful if the log is complete in term of directly followed relations.
token replay recall ( recb).proposition holds.
reasoning. there will be no missing and remaining tokens if all traces in the log ﬁt
the model. hence, recall is maximal, if (l)(m).1
2(1 0
p) +1
2(1 0
c= 1. note,
that again we make the assumption that perfectly ﬁtting behavior is replayed perfectly.
due to the nondeterministic nature of replay in the presence of silent and duplicate
transition, this is not guaranteed.
alignment recall ( recc).proposition holds.
reasoning. the alignments only consist of synchronous moves if all traces in the log ﬁt
the model. consequently, the alignment costs fcost (l;m )are 0 and recall is maximal.
recc= 1 fcost (l;m)
(move l(l)+jljmove m(m))= 1 0
(move l(l)+jljmove m(m))= 1, if
(l)(m).
behavioral recall ( recd).proposition holds.
reasoning. if all traces in log lﬁt modelm, thenfn(l;m) = 0 . as a result, recd(l;m) =
tp(l;m)
tp(l;m)+fn(l;m)=tp(l;m)
tp(l;m)= 1.projected recall ( rece).proposition holds.
reasoning. if all traces in the log ﬁt the model, the number of correctly replayed traces
equals the number of traces in the log j[t2ljajt2dfa (mja)]j=jljaj, if(l)
(m)and recall is maximal. the approach also deﬁnes that recall is maximal if the log
is empty. [23].
continued parsing measure ( recf).proposition does not hold.
reasoning. flower models consisting of one place that connects to all transitions do
not have a ﬁnal place. translating this model into a causal matrix will cause that there
is no activity with an empty output expression. hence, after replaying the ﬁtting log,
there will always be remaining activated output expressions and recall is not maximal.
eigenvalue recall (g.) proposition holds.
reasoning. proven in corollary 5.15 of [26].
a.3 detailed results of the precision measure evaluation
proposition 1 detpro+
soundness ( prech).proposition does not hold.
reasoning. the formula divides the unique traces observed in the log by the unique
paths through the model. if the model contains loops there are inﬁnitely many unique
paths and precision is not deﬁned.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. shown to be non-deterministic in [29] and was already stressed in the orig-
inal paper [27] that introduced the measure.
advanced behavioral appropriateness ( precj).proposition does not hold.
reasoning. shown to be undeﬁned for some combinations of logs and models in [29].
note, that implementation of the approach in the process mining tool prom6deﬁnes
precision for these combinations and is, therefore, deterministic. however, in this paper,
we only consider the approach as formally deﬁned in the paper [27].
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. for the construction of the state space and its escaping edges, the aligned
log is used. in the case of multiple optimal alignments, one (etc-one) or a set of rep-
resentative alignments (etc-rep) is used to construct the state space. during regular
conformance checking based on alignments, all optimal alignments are equal. how-
ever different alignments can lead to different escaping edges and therefore to different
precision measures.
consider process m7in figure 8 along with event log l14= [ha;gi]. it is clear that
the log does not ﬁt the process model and after aligning log and model there are three
possible aligned traces: 1=ha;b;c;gi,2=ha;d;e;giand3=ha;d;f;gi. the
6http://www.promtools.orga
db
gc
e
ffig. 8: petri net m7
ɛ a abcg ab abca b c gd
(a)
ɛ a adeg ad adea d e gb f
(b)
ɛ a abcg ab abca b c gd (c)
ɛ a adeg ad adea d e gb f
(d)
fig. 9: two alignment automata describing the state space of 1=ha;b;c;gi(a) and
2=ha;d;e;gi(b).
etc-one approach randomly picks one of the traces and construct the corresponding
alignment automaton. the automata of 1and2in figure 9 show the different escap-
ing edges that result from both traces. as a result, precision is different for these two
aligned traces: preck(1;m7) =4
5= 0:8andpreck(2;m7) =4
6= 0:67.
etc-all (precl).proposition holds.
reasoning. for etc-all all optimal alignments are used. which leads to a complete
state space and a deterministic precision measure.
behavioral speciﬁcity ( precm).proposition does not hold.
reasoning. if during the replay of the trace duplicate or silent transitions are encoun-
tered, the approach explored which of the available transitions enables the next event
in the trace. if no solution is found, one of the transitions is randomly ﬁred, which can
lead to different recall values for traces with the same behavior.
furthermore, to balance the proportion of negative and positive events in the log, the
algorithm induces the log with the calculated negative events based on a probabilistic
parameter. only if this parameter is set to 1 all negative events are added to the log.
hence the recall measure is non-deterministic for parameter settings smaller than 1.finally, it is possible that the negative event induction algorithm does not induce
any negative events. this, for example, happens when the algorithm assesses that all
activity types in the log are in parallel. when there are no negative events found, it fol-
lows from the deﬁnition that precision is0
0and thus undeﬁned.
behavioral precision ( precn).proposition does not hold.
reasoning. precnuses the same non-deterministic replay procedure and the same neg-
ative event induction approach (possibly also non-deterministic, depending on parame-
ter settings) as precm.
precndoes not have the same problem as precmwith regards to being undeﬁned
when there are no negative events, as this measure additionally has the number of true
positives in the formula.
weighted negative event precision ( preco).proposition does not hold.
reasoning. precouses a non-deterministic replay procedure, which is detailed in [8].
therefore, the precision calculation is non-deterministic.
projected precision ( precp).proposition holds.
reasoning. this technique projects the behaviors of the log and the model onto subsets
of activities and compares their deterministic ﬁnite automata to calculate precision. the
sub-logs and models are created by projection on a subset of activities which is a de-
terministic process. moreover, the process of creating a deterministic automaton is also
deterministic. there is a unique dfa which has the minimum number of states, called
the minimal automaton. therefore, the computation of the average precision value over
all subsets is also deterministic.
anti-alignment precision ( precq).proposition holds.
reasoning. precision is computed based on the maximal anti-alignment. even if there
are multiple maximal anti-alignments, the distance will always be maximal and, there-
fore, precision is deterministic. note, that we assume in case of non-ﬁtting behavior
that the log is ﬁrst aligned before evaluating this proposition.
eigenvalue precision ( precr).proposition holds.
reasoning. the measure compares the languages of the model and the language of the
process model. these have to be irreducible, to compute their eigenvalue. since the
language of an event log is not irreducible, polyvyanyy et al. [26] introduce a short-
circuit measure over languages and proof that it is a deterministic measure over any
arbitrary regular language.
proposition 2 behpro+
soundness ( prech).proposition holds.
reasoning. the behavior of the model is deﬁned as sets of traces (m), which abstracts
from the representation of the process model itself.a
b
c(a)
c b c (b)
fig. 10: two process models m8(a) andm9(b) that show the same behavior but differ-
ent representations.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. a counter-example to axiom 4 (as introduced in [29]), which is equivalent
tobehpro+, was shown in [29].
advanced behavioral appropriateness ( precj).proposition does not hold.
reasoning. a counter-example to axiom 4 (as introduced in [29]), which is equivalent
tobehpro+, was shown to hold in [29].
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. a counter-example to axiom 4 (as introduced in [29]), which is equivalent
to behpro+, was shown in [29].
etc-all (precl).proposition does not hold.
reasoning. this technique depends on the path taken through the model to examine the
visited states and its escaping edges. one can think of petri nets with the same behavior
but described by different paths through the net which then also results in different
escaping edges and hence different precision. the two models shown in figure 10 prove
this case.
behavioral speciﬁcity ( precm).proposition does not hold.
reasoning. if duplicate or silent transitions are encountered while replaying a trace, the
approach checks if one of the available transitions enables the next event in the trace.
whether this is the case can depend on the structure of the model.
behavioral precision ( precn).proposition does not hold.
reasoning. forprecm,behpro+did not hold because of its replay procedure. precn
uses the same replay procedure as precm.
weighted negative event precision ( preco).proposition does not hold.
reasoning. like withprecmandprecnthe outcome of the replay procedure can be
impacted by duplicate transitions and by silent transitions. therefore, this proposition
does not hold.projected precision ( precp).proposition holds.
reasoning. this technique translates the event log as well as the process model into
deterministic ﬁnite automata before computing recall (recall that the minimal determin-
istic automaton is unique due to the myhill–nerode theorem). therefore, it is indepen-
dent of the representation of the model itself.
anti-alignment precision ( precq).proposition holds.
reasoning. the authors deﬁne an anti-alignment as a run of a model which differs
sufﬁciently from the observed traces in a log. this anti-alignment is solely constructed
based on the possible behavior of the process model and the observed behavior of the
log. it is independent of the structure of the net.
eigenvalue precision ( precr).proposition holds.
reasoning. the approach calculates precision based on the languages of the model and
the language of the process model. this abstracts from the representation of the process
model and, consequently, the proposition holds.
proposition 8 precpro1+
soundness ( prech).proposition holds.
reasoning. this proposition holds since, removing behavior from the model that does
not happen in the event log decreases the set of traces allowed by the model j(m1)j
j(m2)j, while the set of traces of the event log complying with the model stays the
samej(l)\(m1)j=j(l)\(m2)j.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. behpro+does not hold, which implies that precpro1+does not hold.
advanced behavioral appropriateness ( precj).proposition does not hold.
reasoning. behpro+does not hold, which implies that precpro1+does not hold.
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. a counter-example to axiom 2 (as introduced in [29]) was presented in [29].
since precpro1+is a generalization of axiom 2, the same counter-example shows that
precpro1+does not hold. furthermore, behpro+does not hold, which implies that
precpro1+does not hold.
etc-all (precl).proposition does not hold.
reasoning. a counter-example to axiom 2 (as introduced in [29]) was presented in [29].
since precpro1+is a generalization of axiom 2, this implies that precpro1+does not
hold. furthermore, behpro+does not hold, which implies that precpro1+does not
hold.
behavioral speciﬁcity ( precm).proposition does not hold.
reasoning. behpro+does not hold, which implies that precpro1+does not hold.behavioral precision ( precn).proposition does not hold.
reasoning. behpro+does not hold, which implies that precpro1+does not hold.
weighted negative event precision ( preco).proposition does not hold.
reasoning. a counter-example to axiom 2 (as introduced in [29]) was presented in [29].
since precpro1+is a generalization of axiom 2, this implies that precpro1+does not
hold. furthermore, behpro+does not hold, which implies that precpro1+does not
hold.
projected precision ( precp).proposition does not hold.
reasoning. a counter-example to axiom 2 (as introduced in [29]) was presented in
[29]. to illustrate this, the paper considers a model with a length-one-loop and its more
precise corresponding model that unrolled the loop up to two executions. the dfa of
the unrolled model will contain more states since the future allowed behavior depends
on the number of executions of the looping activity, while the dfa of the initial model
will contain only one state for this activity. this can cause that the unrolled model is
considered less precise which violates the proposition.
anti-alignment precision ( precq).proposition holds.
reasoning. the behavior of the model that is not observed in the log will become the
anti-alignment between the log and the model. the distance between the log and the
anti-alignment is big which leads to low precision. if this behavior is removed from the
model an anti-alignment closer to the log is found which leads to a higher precision.
eigenvalue precision ( precr).proposition holds.
reasoning. proven in lemma 5.6 of [26].
proposition 9 precpro2+
soundness ( prech).proposition holds.
reasoning. adding ﬁtting behavior to the event log can lead to additional unique
process executions that comply with the process model: j(l1)\(m)jj(m)j
j(l2)j. this cannot lower precision according to the deﬁnition of soundness.
j(l1)\(m)j
j(m)jj(l2)\(m)j
j(m)j, ifl2=l1]l3.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. this approach does not consider whether the behavior of the log ﬁts the
model or not, but it focuses on the average number of enabled transitions during log
replay. it is possible that the additional behavior enables a large number of transitions,
this increases the average count and thereby lowers precision.
advanced behavioral appropriateness ( precj).proposition holds.
reasoning. adding ﬁtting behavior to the log can only increase the intersection be-
tween the follow relations of the log and the model.sl1
f\sm
fsl2
f\sm
fandsl1
p\sm
psl2
p\sm
p, ifl2=l1]l3and(l3)(m). hence by adding ﬁtting
behavior to the log precision can only be increased.ɛ ssa sab
sa b
sbb
sbaa2 21 1
1 1sabccd
1
sabcee1
sbacc1
dsbacee1(a)
ɛ ssa sab
sa b
sbb
sbaa3 32 2
1 1sabccd
2
sabcee1
sbacc1
dsbacee1sabcd sabcdcc1
sabcdcdd1e
sabcdcdcc1
sabcd
cdcdd1e
sabcd
cdcdcc1
sabcdc
dcdcdd1e
sabcd
cdcdcdcc1
sabcdc
dcdcdcdd1e
sabcdcd
cdcdcdcc1
sabcdcd
cdcdcdcdd1e
sabcdcd
cdcdcdcdcc1sabcdcdcd
cdcdcdcee1d
(b)
fig. 11: two alignment automata describing the state space of m1andl15(a) and the
state space of m1andl10(b).
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. a counter-example to axiom 5 (as introduced in [29]) was presented in [29].
since precpro2+is a generalization of axiom 5, this implies that precpro2+does not
hold.
etc-all (precl).proposition does not hold.
reasoning. the conclusions drawn in [29] for etc-one can also be transferred to
etc-all. when adding ﬁtting behavior it is possible that the new trace visits states
that introduce a lot of new escaping edges. the increase in escaping edges is big-
ger than the increase in non-escaping ones which lowers precision. consider process
modelm1in figure 1 (a) and the event log l15= [hs;a;b;c;ei;hs;b;a;c;ei]and its
extension with a ﬁtting trace l16=l15][hs;a;b;c;d;c;d;c;d;c;d;c;d;c;e i]. note,
that the “start” and “end” activities of m1are abbreviated to “s” and “e”. the corre-
sponding automata in figure 11 show that the additional ﬁtting trace adds additional
states and escaping edges. this decreases precision: precl(l15;m1) =12
14= 0:857and
precl(l16;m1) =31
37= 0:838.
behavioral speciﬁcity ( precm).proposition does not hold.
reasoning. this proposition does not hold when the additional ﬁtting trace introduces
proportionally more negative events that could actually ﬁre (fp) than correctly identi-
ﬁed negative events (tn). to illustrate this, consider process model m10in figure 12
and event log l16= [ha;b;b;di;ha;b;c;di]. table 9 shows the negative events calcu-
lated for the log. we assume a window size that equals the longest trace in the event
log and we generate the negative events with probability 1. after replaying the log on
the process model we record fp(l16;m10) = 10 andtn(l16;m10) = 8 . hence,
tn(l16;m10)
tn(l16;m10)+fp(l16;m10)=12
22= 0:545. we extend the log with ﬁtting trace l10,
i.e.,l17=l16][ha;b;c;b;b;b;b;b;d i]. the negative events calculated for l17areab
d
cfig. 12: process model m10
table 9: the traces of l16with the corresponding negative events.
abbd
:b:a:a:a
:c:c:d:b
:d:d:cabcd
:b:a:a:a
:c:c:d:b
:d:d:c
displayed in table 10 and replaying it on m10results infp(l17;m10) = 31 and
tn(l17;m10) = 23 . consequently,tn(l17;m10)
tn(l17;m10)+fp(l17;m10)=23
54= 0:426. although
l11is ﬁtting, it introduces more negative events that are actually enabled during replay.
therefore,precm(l16;m10)>precm(l17;m10), which violates the proposition.
behavioral precision ( precn).proposition does not hold.
reasoning. the counter-example of precmand can also be used to show that this
proposition is violated. during replay of l16andl17we also count the positive events
that can be correctly replayed (tp). this results in tp(l16;m10) = 8 and
tp(l16;m10) = 17 . when we calculate precision, we obtain precn(l16;m10) =
tp(l16;m10)
tp(l16;m10)+fp(l16;m10)=8
8+10= 0:44andprecn(l17;m10) =17
17+31= 0:35. the
additional ﬁtting trace lowers precision: precn(l16;m10)>precn(l17;m10).
weighted negative event precision ( preco).proposition does not hold.
reasoning. consider the same counter-example as that we provided for precm. nega-
tive events are weighted by the size of the longest matching window of events. for the
long repetition of b-events inha;b;c;b;b;b;b;b;d i, the negative events for a,candd
have weight 2due to the 2 consecutive b’s in trace ha;b;b;di. since the negative events
inl3that caused the precision to go up when l3was added to l1have above average
weight, the weighting does not invalidate the counter-example.
projected precision ( precp).proposition does not hold.
reasoning. a counter-example to axiom 5 (as introduced in [29]) was presented in [29].
since precpro2+is a generalization of axiom 5, this implies that precpro2+does not
hold.table 10: the traces of l17with the corresponding negative events.
abbd
:b:a:a:a
:c:c:d:b
:d:d:cabcd
:b:a:a:a
:c:c:d:b
:d:d:cabcbbbbbd
:b:a:a:a:a:a:a:a:a
:c:c:d:c:c:c:c:c:b
:d:d:d:d:d:d:d:c
anti-alignment precision ( precq).proposition does not hold.
reasoning. a counter-example to axiom 5 (as introduced in [29]) was presented in [29].
since precpro2+is a generalization of axiom 5, this implies that precpro2+does not
hold.
eigenvalue precision ( precr).proposition holds.
reasoning. proven in lemma 5.6 of [26].
proposition 10 precpro30
soundness ( prech).proposition holds.
reasoning. adding non-ﬁtting behavior to the event log cannot lead to additional pro-
cess executions, which comply with the process model. hence, it cannot lower pre-
cision according to the deﬁnition of soundness. j(l1)\(m)j=j(l2)\(m)j, if
l2=l1]l3and(l3)(m).
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. adding non-ﬁtting behavior to the event log might change the average num-
ber of enabled transitions during log replay and therefore precision. note that this sce-
nario was not considered by the approach since the authors assume a ﬁtting log [27].
advanced behavioral appropriateness ( precj).proposition does not hold.
reasoning. this approach records relations between activities and compares these re-
lations between the model and the log. hence, it does not consider entire traces to be
ﬁtting or non-ﬁtting but reﬁnes it to an activity level. therefore, it is possible that non-
ﬁtting traces contain ﬁtting events that improve precision. for example, the non-ﬁtting
traces change a never follows relation of the event log to a sometimes follows relation
that matches the process model. consequently, precision increases and violates this
proposition.
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. before the alignment automaton is constructed the log is aligned to en-
sure that the traces ﬁt the model. adding non-ﬁtting behavior can possibly lead to new
alignments that lead to new escaping edges and change precision.
consider model m7in figure 8 and alignment automata in figure 9 (a) correspond-
ing to traceha;b;c;gi. adding the unﬁtting trace ha;d;giresults in the aligned trace
ha;d;e;giorha;d;f;gi. either of the two aligned traces introduces new states into
the alignment automaton. additionally, the new trace alters the weights of each state.
therefore, even though both automata contain one escaping edge, precision changes.etc-all (precl).proposition does not hold.
reasoning. the counter-example presented for preckalso shows that this proposi-
tion does not hold for this approach. the unﬁtting trace ha;d;giresults in the aligned
traceha;d;e;giorha;d;f;gi. this variant of etc precision uses both of the two
aligned traces to construct the alignment automaton. this introduces new states, alters
the weights of the states, removes the escaping edge and changes precision.
behavioral speciﬁcity ( precm).proposition does not hold.
reasoning. negative events describe behavior that was not allowed during process ex-
ecution. they are constructed based on the behavior observed in the event log. from
this point of view, non-ﬁtting behavior is behavior that should have been described by
negative events, but since it was observed in the event log, the algorithm does not de-
ﬁne it as negative events anymore. hence adding non-ﬁtting behavior l3to event log l1
decreases the number of correctly identiﬁed negative events (tn) in the traces of l1.
furthermore, this measure accounts for the number of negative events that actually
could ﬁre during trace replay (fp). these false positives are caused by the fact that
behavior is shown in the model but not observed in the log. although the trace is not
ﬁtting when considered as a whole, certain parts of the trace can ﬁt the model, and
these parts can represent the previously missing behavior in the event log that leads
to the wrong classiﬁcation of negative events. adding these non-ﬁtting traces l3can,
therefore, lead to a decrease in false positives in the traces of l1and changes precision.
behavioral precision ( precn).proposition does not hold.
reasoning. as shown in the reasoning for precm, adding non-ﬁtting traces l3to a
ﬁtting logl1can decrease the number of false positives fp in the negatives events that
were generated for the traces of l1.
weighted negative event precision ( preco).proposition does not hold.
reasoning. as shown in the reasoning for precm, adding non-ﬁtting traces l3to a
ﬁtting logl1can decrease the number of false positives fp in the negatives events that
were generated for the traces of l1. weighing the negative events does not change this.
projected precision ( precp).proposition does not hold.
reasoning. since projected precision calculates precision based on several projected
sub-models and sub-logs, it is possible that unﬁtting behavior ﬁts some of these sub-
models locally. to illustrate this consider a model with the language (m11) =fha;bi;
hc;digandl18= [ha;bi]. it is clear, that the model is not perfectly precise since trace
hc;diis not observed in the event log. hence, if we project our model and log on fc;dg,
precision will be 0 for this projection.
we extend the log with an unﬁtting trace l19=l18][hc;d;ai]. projecting l19on
fc;dgresults in a precision value of 1. since this approach aggregates the precision
over several projections, it is clear, that unﬁtting behavior can improve precision.
anti-alignment precision ( precq).proposition does not hold.
reasoning. by deﬁnition anti-alignments always ﬁt the model. consequently, there
will always be a distance between a non-ﬁtting trace and an anti-alignment. addingnon-ﬁtting behavior to the event log will, therefore, change precision. the proposition
does not require l1to be ﬁtting. therefore it could be the case that l1has a trace that has
a higher distance to behavior that is allowed by mthan what can be found amongst the
traces ofl3. note that this scenario is not considered by the approach since the authors
assume a ﬁtting log [13]. however, also after aligning the log, it might still change
precision by resulting in alignments that were not contained in the initial log.
eigenvalue precision ( precr).proposition holds.
reasoning. the measure is deﬁned aseig((m)\(l))
eig((m)). as(m)\(l)does not change
when adding non-ﬁtting traces to l, neither does the measure change.
proposition 11 precpro40
soundness ( prech).proposition holds.
reasoning. since the approach only considers unique model executions, duplicating
the log has no effect on precision.
simple behavioral appropriateness ( preci).proposition holds.
reasoning. since the number of traces niis present in the denominator as well as the
nominator of the formula duplication has no effect on precision.pk
i=1ni(jtvj xi)
(jtvj 1)pk
i=1ni. here
we assume that if a trace is perfectly replayable it will also be replayed perfectly.
advanced behavioral appropriateness ( precj).proposition holds.
reasoning. the sometimes follows relations will not change by duplicating the event
log. hence, the result is unaffected.
etc-one/etc-rep ( preck).proposition holds.
reasoning. the weight of escaping and non-escaping edges is calculated based on the
trace frequency. however, since the distribution of the traces does not change the weight
of both edge types grows proportionally and precision does not change.
etc-all (precl).proposition holds.
reasoning. the weight of escaping and non-escaping edges is calculated based on the
trace frequency. however, since the distribution of the traces does not change the weight
of both edge types grows proportionally and precision does not change.
behavioral speciﬁcity ( precm).proposition holds.
reasoning. multiplying the event log ktimes leads to a proportional increase in true
negatives and false positives. consequently, precision does not change.
ktn(l;m)
k(tn(l;m)+fp(l;m)=tn(l;m)
tn(l;m)+fp(l;m), henceprecm(lk;m) =precm(l;m).
behavioral precision ( precn).proposition holds.
reasoning. multiplying the event log ktimes leads to a proportional increase in true
positives and false positives. consequently, precision does not change.
ktp(l;m)
k(tp(l;m)+fp(l;m))=tp(l;m)
tp(l;m)+fp(l;m), henceprecn(lk;m) =precn(l;m).weighted negative event precision ( preco).proposition holds.
reasoning. multiplying the event log ktimes leads to a proportional increase in true
negatives and false positives. consequently precision does not change.
ktn(l;m)
k(tn(l;m)+fp(l;m)=tn(l;m)
tn(l;m)+fp(l;m), henceprecm(lk;m) =precm(l;m).
projected precision ( precp).proposition holds.
reasoning. the precision measure does not consider trace frequency and therefore will
not be changed by duplicating the event log.
anti-alignment precision ( precq).proposition holds.
reasoning. this approach sums for each trace in the log the distance between anti-
alignment and trace. this sum is averaged over the number of traces in the log and
consequently, duplication of the log will not change precision.
eigenvalue precision (r.) proposition holds.
reasoning. eigenvalue precision is deﬁned purely on the language of the log and the
model and it does not take into account trace frequencies in the log, therefore, this
proposition holds.
proposition 12 precpro5+
soundness ( prech).proposition holds.
reasoning. if the model allows for the behavior observed and nothing more each,
unique process execution corresponds to a unique path through the model (l) =(m).
therefore precision is maximal: j(l)j=j(m)j= 1.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. this approach only considers strictly sequential models to be perfectly pre-
cise. if the model has choices, loops or concurrency, then, multiple transitions might be
enabled during replay even if the model only allows only for the observed behavior. as
a result, precision is not maximal.
advanced behavioral appropriateness ( precj).proposition holds.
reasoning. if the model allows for only the behavior observed and nothing more, the
set of sometimes follows/precedes relations of the model are equal to the ones of the
event log.sl
f\sm
f=sm
fandsl
p\sm
p=sm
p, if(l) =(m). consequently precision
is maximal.
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. consider a model with a choice between two a-labeled transitions and a
trace[hai]. when constructing the alignment automaton, there will be an escaping edge
for the other a-labeled transition. similar problems may arise with silent transitions.
etc-all (precl).proposition does not hold.
reasoning. the counter-example for preckshows that also preclviolates this propo-
sition.behavioral speciﬁcity ( precm).proposition holds.
reasoning. when the model allows for only observed behavior (i.e., in l), then fp (l;m) =
0, as false positives are caused by the fact that behavior is shown in the model but not
observed in the log. therefore, (l) =(m))precm(l;m) =tn(l;m)
tn(l;m)+fp(l;m)=
tn(l;m)
tn(l;m)+0= 1.
behavioral precision ( precn).proposition holds.
reasoning. when the model allows for only observed behavior (i.e., in l), then fp (l;m) =
0, as false positives are caused by the fact that behavior is shown in the model but not
observed in the log. therefore, (l) =(m))precn(l;m) =tp(l;m)
tp(l;m)+fp(l;m)=
tp(l;m)
tp(l;m)+0= 1.
weighted negative event precision ( preco).proposition holds.
reasoning. see the reasoning for precnabove. the weighing of negative events doesn’t
change the fact that false positives cannot occur when (l) =(m), therefore the same
reasoning applies to preco.
projected precision ( precp).proposition holds.
reasoning. if the model allows for only the behavior observed and nothing more, the
two automata describing the behavior of the log and the model are exactly the same:
dfa(mja) =dfa(lja) =dfac (l;m;a ), if(l) =(m). hence, precision is maxi-
mal.
anti-alignment precision ( precq).proposition holds.
reasoning. if the model allows for the behavior observed and nothing more, each anti-
alignment will exactly match its corresponding trace. consequently, the distance be-
tween the log and the anti-alignment is minimal and precision maximal.
eigenvalue precision ( precr).proposition holds.
reasoning. proven in corollary 5.15 of [26].
proposition 13 precpro60
soundness ( prech).proposition holds.
reasoning. if the log contains non-ﬁtting behavior and all modeled behavior was ob-
served, the set of traces in the event log complying with the process model equals the
paths through the model. j(l)\(m)j=j(m)jand precision is maximal.
simple behavioral appropriateness ( preci).proposition does not hold.
reasoning. this approach only considers strictly sequential models to be perfectly pre-
cise. if the model contains choices, concurrency, loops, etc., multiple transitions may be
enabled, lowering precision. moreover, the approach assumes all behavior to be ﬁtting.
hence, behavior that is observed and not modeled is likely to lead to problems.advanced behavioral appropriateness ( precj).proposition holds.
reasoning. non-ﬁtting behavior cannot affect the follow relations of the process model.
furthermore, it does not inﬂuence the sometimes follows/precedes relations of the event
log if all the modeled behavior was observed. hence, the sets of sometimes follows/precedes
relations of the log and the model are equal to each other. sl
f\sm
f=sm
fand
sl
p\sm
p=sm
pand, therefore, precision is maximal.
etc-one/etc-rep ( preck).proposition does not hold.
reasoning. the counter-example from precpro5+also shows that precpro60is vio-
lated.
etc-all (precl).proposition does not hold.
reasoning. the counter-example from precpro5+also shows that precpro60is vio-
lated.
behavioral speciﬁcity ( precm).proposition holds.
reasoning. when the model allows for only observed behavior (i.e., in l), then fp (l;m) =
0, as false positives are caused by the fact that behavior is shown in the model but not
observed in the log. therefore, (m)(l))precm(l;m) =tn(l;m)
tn(l;m)+fp(l;m)=
tn(l;m)
tn(l;m)+0= 1.
behavioral precision ( precn).proposition holds.
reasoning. when the model allows for only observed behavior (i.e., in l), then fp (l;m) =
0, as false positives are caused by the fact that behavior is shown in the model but not
observed in the log. therefore, (m)(l))precm(l;m) =tp(l;m)
tp(l;m)+fp(l;m)=
tn(l;m)
tn(l;m)+0= 1.
weighted negative event precision ( preco).proposition holds.
reasoning. see the reasoning for precnabove. the weighing of negative events doesn’t
change the fact that false positives cannot occur when (m)(l), therefore the same
reasoning applies to preco.
projected precision ( precp).proposition holds.
reasoning. if the all modeled behavior is observed, the automaton describing the model
and the conjunctive automaton of the model and the log are exactly the same. dfac (s;m;a )n
dfa(mja) =;, if(m)(l). hence, precision is maximal. furthermore, the au-
thors deﬁne that precision is 1 if the model is empty [23].
anti-alignment precision ( precq).proposition holds.
reasoning. by deﬁnition, an anti-alignment will always ﬁt the model. consequently,
when computing the distance between the unﬁtting trace and the anti-alignment, it will
never be minimal. however note, that scenarios with unﬁtting behavior were not con-
sidered by the approach since the authors assume a ﬁtting log. after aligning the log, it
contains exactly the modeled behavior (l) =(m)and precision is maximal.eigenvalue precision ( precr).proposition holds.
reasoning. corollary 5.15 of [26] proves that precr(l;m) = 1 when(m) =(l).
from the deﬁnition of the precision measure (i.e., precr(l;m) =eig((m)\(l))
eig((m))) it fol-
lows that the proposition holds, as the numerator eig((m)\(l))is equal toeig((m))
when(m)(l).
a.4 generalization
proposition 1 detpro+
alignment generalization ( gens).proposition holds.
reasoning. generalization is calculated based on the states visited by the process. the
approach counts how often each state is visited ( n) and how many different activities
were observed in this state ( w). these two numbers can be obtained from the model and
the log at all times. hence generalization is deterministic. note, that we assume in case
of non-ﬁtting behavior that the log is ﬁrst aligned before evaluating this proposition.
weighted negative event generalization ( gent).proposition does not hold.
reasoning. if duplicate or silent transitions are encountered during the replay of the
trace, which was enhanced with negative events, the approach explores which of the
available transitions enables the next event in the trace. if no solution is found’, one of
the transitions is randomly ﬁred. hence precision also depends on the representation of
the model.
anti-alignment generalization ( genu).proposition holds.
reasoning. precision is computed based on the maximal anti-alignment. even if there
are multiple maximal anti-alignments the distance will always be maximal and, there-
fore, precision is deterministic. note, that we assume in case of non-ﬁtting behavior
that the log is ﬁrst aligned before evaluating this proposition.
proposition 2 behpro+
alignment generalization ( gens).proposition holds.
reasoning. the approach abstracts from the concrete representation of the process
models. a key element is the function statemwhich is a parameter of the approach
and maps each event onto the state in which the event occurred. this function only uses
behavioral properties. hence, the proposition holds.
weighted negative event generalization ( gent).proposition does not hold.
reasoning. if duplicate or silent transitions are encountered while replaying a trace, the
approach checks if one of the available transitions enables the next event in the trace.
whether this is the case can depend on the structure of the model.anti-alignment generalization ( genu).proposition does not hold.
reasoning. this approach deﬁnes a so-called recovery distance which measures the
distance between the states of the anti-alignment and the states visited by the log. it
deﬁnes a state as a marking of the petri net. one can think of two petri nets with the
same behavior but different markings based on their structure. the two process models
presented in figure 10 can be used as examples. therefore generalization depends on
the representation of the process model.
proposition 14 genpro1+
alignment generalization ( gens).proposition does not hold.
reasoning. the approach does not allow for unﬁtting behavior and therefore aligns the
log with the process model. these aligned traces might visit states that have already
been observed by the ﬁtting behavior, which increases the number of visits nto these
states and improves generalization. the extension of the model such that (m1)
(m2)might cause this previously unﬁtting behavior to ﬁt m2and aligning the log is
not necessary anymore. however, these “missing” aligned traces cause a decrease in
the number of visits nto each state of the previously aligned trace and generalization
decreases.
weighted negative event generalization ( gent).proposition does not hold.
reasoning. behpro+does not hold, which implies that genpro1+does not hold
anti-alignment generalization ( genu).proposition does not hold.
reasoning. behpro+does not hold, which implies that genpro1+does not hold.
proposition 15 genpro2+
alignment generalization ( gens).proposition does not hold.
reasoning. according to the deﬁnition of generalization, it is possible that additional
ﬁtting behavior in the event log decreases generalization if the additional traces intro-
duce new unique events to the log and pnew (w;n) = 1 . hence, the new traces raise the
number of unique activities ( w) in stateswhile the number of times swas visited by
the event log stays low ( n).
weighted negative event generalization ( gent).proposition does not hold.
reasoning. the ﬁtting behavior can lead to the generation of additional negative events.
if these negative events are correctly identiﬁed, they increase the value of disallowed
generalizations (dg). ag(l1;m) =ag(l2;m)anddg(l1;m)< dg (l2;m)which
decreases generalizationag(l1;m)
ag(l1;m)+dg(l1;m)>ag(l2;m)
ag(l2;m)+dg(l2;m).
anti-alignment generalization ( genu).proposition does not hold.
reasoning. the approach deﬁnes the perfectly generalizing model as a model with a
maximal anti-alignment distance dand minimal recovery distance drec. the newly ob-
served behavior of the general model should introduce new paths between states butno new states [13]. however, if the model is very imprecise and with a lot of differ-
ent states, it is possible that the added traces visit very different states than the anti-
alignment, generalization will be low for these traces. consequently, the average gen-
eralization over all traces decreases.
proposition 16 genpro30
alignment generalization ( gens).proposition does not hold.
reasoning. according to this approach, generalization is not deﬁned if there are unﬁt-
ting traces, since unﬁtting behavior cannot be mapped to states of the process model.
note that unﬁtting behavior was intentionally excluded from the approach and the au-
thors state that unﬁtting event logs should be preprocessed to ﬁt to the model. however,
after the log is aligned, the added traces might improve generalization by increasing n
the times how often certain states are visited while executing events that have already
been observed by the ﬁtting traces in the log.
weighted negative event generalization (s). proposition does not hold.
reasoning. in this approach, negative events are assigned a weight which indicates
how certain the log is about these events being negative ones. even though the added
behavior is non-ﬁtting it might still provide evidence for certain negative events and
therefore increase their weight. if these events are not enabled during log replay the
value for disallowed generalizations (dg) decreases dg(l1;m)> dg (l2;m)and
generalization improves:ag(l;m)
ag(l;m)+dg(l1;m)<ag(l;m)
ag(l;m)+dg(l2;m).
anti-alignment generalization (r). proposition does not hold.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since unﬁtting behavior cannot be mapped to states of the process model.
note that the authors exclude unﬁtting behavior from this approach and state that unﬁt-
ting event logs should be preprocessed to ﬁt to the model. but after aligning the event
log, it might be the case that the added and aligned traces report a big distance to the
anti-alignment without introducing new states, which increases generalization.
proposition 17 genpro4+
alignment generalization ( gens).proposition holds.
reasoning. multiplying a ﬁtting log ktimes will result in more visits nto each state
while the number of different activities observed wstays the same and generalization
increases.
weighted negative event generalization ( gent).proposition holds.
reasoning. multiplying the log ktimes will proportionally increase the number of al-
lowed and disallowed generalizations and therefore not change generalization:
kag(l;m)
k(ag(l;m)+dg(l;m))=ag(l;m)
ag(l;m)+dg(l;m),gent(lk;m) =gent(l;m).anti-alignment generalization ( genu).proposition holds.
reasoning. this approach sums for each trace in the log the trace-based generalization.
this sum is averaged over the number of traces in the log and consequently, duplication
of the log will not change precision.
proposition 18 genpro5+
alignment generalization ( gens).proposition does not hold.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces, since they cannot be mapped to states of the process model. note that
unﬁtting behavior was intentionally excluded from the approach and the authors state
that unﬁtting event logs should be preprocessed to ﬁt the model. however, after the
log is aligned the added traces might improve generalization by increasing nthe times
how often certain states are visited while being aligned to traces that have already been
observed by the other traces in the log. hence, generalization increases.
weighted negative event generalization ( gent).proposition holds.
reasoning. multiplying the log ktimes will proportionally increase the number of al-
lowed and disallowed generalizations and therefore not change generalization:
kag(l;m)
k(ag(l;m)+dg(l;m))=ag(l;m)
ag(l;m)+dg(l;m),gent(lk;m) =gent(l;m).
anti-alignment generalization ( genu).proposition holds.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since unﬁtting behavior cannot be mapped to states of the process model.
note, that the authors exclude unﬁtting behavior from this approach and state that un-
ﬁtting event logs should be preprocessed to ﬁt the model. therefore we evaluate this
proposition after the event log was aligned. duplicating the aligned log will not change
generalization since the sum of trace-generalization is averaged over the number of
traces in the log.
proposition 19 genpro60
alignment generalization ( gens).proposition holds.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since they cannot be mapped to states of the process model. note that
unﬁtting behavior was intentionally excluded from the approach and the authors state
that unﬁtting event logs should be preprocessed to ﬁt the model. duplicating the aligned
log will result in more visits to each state visited by the log. generalization increases.
weighted negative event generalization ( gent).proposition holds.
reasoning. multiplying the log ktimes will proportionally increase the number of al-
lowed and disallowed generalizations and therefore not change generalization:
kag(l;m)
k(ag(l;m)+dg(l;m))=ag(l;m)
ag(l;m)+dg(l;m),gent(lk;m) =gent(l;m).anti-alignment generalization ( genu).proposition holds.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since unﬁtting behavior cannot be mapped to states of the process model.
note that the authors exclude unﬁtting behavior from this approach and state that un-
ﬁtting event logs should be preprocessed to ﬁt the model. duplicating the aligned log
will not change generalization since the sum of trace-generalization is averaged over
the number of traces in the log.
proposition 20 genpro70
alignment generalization ( gens).proposition does not hold.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since they cannot be mapped to states of the process model. note that
unﬁtting behavior was intentionally excluded from the approach and the authors state
that unﬁtting event logs should be preprocessed to ﬁt the model. duplicating an aligned
log will result in more visits to each state. hence, generalization increases and violates
the proposition.
weighted negative event generalization ( gent).proposition holds.
reasoning. multiplying the log ktimes will proportionally increase the number of al-
lowed and disallowed generalizations and therefore not change generalization:
kag(l;m)
k(ag(l;m)+dg(l;m))=ag(l;m)
ag(l;m)+dg(l;m),gent(lk;m) =gent(l;m).
anti-alignment generalization ( genu).proposition holds.
reasoning. according to this approach, generalization is not deﬁned if there are un-
ﬁtting traces since unﬁtting behavior cannot be mapped to states of the process model.
note that the authors exclude unﬁtting behavior from this approach and state that un-
ﬁtting event logs should be preprocessed to ﬁt the model. duplicating the aligned log
will not change generalization since the sum of trace-generalization is averaged over
the number of traces in the log.
proposition 21 genpro80
alignment generalization ( gens).proposition does not hold.
reasoning. according to the deﬁnition, generalization can never become 1. it only
approaches 1. consider a model allowing for just (m) =haiand the logl= [harak].
the log visits the state k-times and observes one activity w= 1 in this state. the
functionpnew =1(1+1)
k(k 1)will approach 0 as kincreases but never actually be 0. hence,
gens(l;m) = 1 pnew (1;k)approaches 1, but will never be precisely 1.
weighted negative event generalization ( gent).proposition holds.
reasoning. if the model allows for any behavior, it does not contain any negative behav-
ior which is not allowed. hence the algorithm cannot ﬁnd negative events, which are not
enabled during replay (dg) and generalization will be maximal.
genr=ag(l;m)=(ag(l;m) +dg(l;m)) =ag(l;m)=(ag(l;m) + 0) = 1 .a b
a cfig. 13: a process model that allows for any behavior while displaying different states.
anti-alignment generalization ( genu).proposition does not hold.
reasoning. assume a model that allows for any behavior because of silent transition,
loops and duplicate transitions. the distance between the log and the anti-alignment
is maximal. however, due to the duplicate transitions which are connected to separate
places the recovery distance is not minimal. consequently, generalization would not
be maximal which violates the proposition. figure 13 is an example of such a process
model.