quality and reliability engineering international
qual. reliab. engng. int. (2008)
published online in wiley interscience (www.i nterscience.wiley.com). doi: 10.1002/qre.937
research softreliability:an
interdisciplinaryapproach
withauser–systemfocus
a. koca∗,†, m. funk, e. karapanos, a. rozinat, w. m. p. van der aalst, h. corporaal,
j. b. o. s. martens, p. h. a. van der putten, a. j. m. m. weijters and a. c. brombacher
eindhoven university of technology ,p .o. box 513 ,nl-5600 mb eindhoven ,the netherlands
a recent trend in technologica l innovation is towards the development of increasingly
multifunctional and complex products to be used within rich socio-cultural contexts
such as the high-end ofﬁce, the digital home, and professional or personal healthcare.one important consequence of the development of strongly innovative products isa growing market uncertainty regarding ‘if’, ‘how’, and ‘when’ users can and willadopt such products. often, it is not even clear to what extent these products are
understood andinteracted with in the intended manner. the mentioned problems have
already become an evident concern in the ﬁeld, where there is a signiﬁcant rise in the
numbers of seemingly sound products being complained about , signaling a lack of soft
reliability . in this paper, we position soft reliability as a growing and critical industrial
problem, whose solution requires new academic expertise from various disciplines.
we illustrate potential root causes for soft reliability problems, such as discrepancy
between the perceptions of users and designers. we discuss the necessary approach to
effectively capture subjective feedback data from actual users, e.g. when they contact
call centers. furthermore, we present a novel observation and analysis approach that
enables insight into actua l product usage, and outline opportunities for combining
such objective data with the subjective feedback provided by users. copyright ©2008
john wiley & sons, ltd.
key words: soft reliability; customer-perceive d quality; new product development; information ﬂow
analysis; process mining; business process design
1. introduction
rapid technological advancements, as well as prevailing social trends (such as an aging population and
a stressed job market), create and promote new opportunities for incorporating technology within our
environments in an innovative manner. this is especially observable in recently developed products
∗correspondence to: a. koca, eindhoven university of technol ogy, p.o. box 513, nl-5600 mb eindhoven, the netherlands.
†e-mail: a.koca@tue.nl
contract /grant sponsor: dutch ministry of economic affairs
copyright q2008 john wiley & sons, ltd.a. koca et al.
0102030405060
1975 1980 1985 1990 1995 2000 2005percentage no fault found
y ear
figure 1. rapid growth of ‘no fault found’ cases, where returned products seem to function correctly, in
modern high-volume consumer electronics1
that display enhanced multi-functionality in complex socio-cultural settings, such as the high-end ofﬁce,
the digital home, and professional or personal healthcare. the downside is a growing market uncertainty asto ‘if’, ‘how’, and ‘when’ users can and will adopt such products: many innovatively designed products,
especially where ambient intelligence is involved, go so far as suggesting substantial changes in the everyday
ﬂow of people’s lives; however, it is often highly uncertain to what extent intended users are able to
understand andinteract with such products in the intended manner, not to mention the difﬁculty in getting
concrete ideas about how willing people are to accept (or embrace) such products. in fact, negligence ofsuch issues has already become an evident concern in the ﬁeld, where a signiﬁcant rise in the numbers of
seemingly sound products being complained about can be observed (figure 1), signaling a crisis of lacking
soft reliability
‡1–5.
the relevance of soft reliability is expected to grow with increasing product complexity, and hence with
increasing product state spaces. in control engineering, the notion of a state space is used to characterize
the behavior of a system as a set of input ,output ,a n d state variables (i.e. the state of the system can be
represented as a vector within that state space). owing to the exponentially increasing computer hardware
capabilities (cf. moore’s law), the state spaces of digital electronic devices are growing faster and faster.this effect is further ampliﬁed by the merging of technologies, increasing networking capabilities, and by
the fact that products nowadays offer a high degree of adaptability to accommodate many diverse user
contexts and preferences
§. therefore, dynamically and sometimes unintentionally growing state spaces
naturally imply an increase in uncertainty about the real ﬁeld performance (e.g. compatibility in various use-
contexts) of products. accordingly, product speciﬁca tions can no longer cover entire state spaces, and hence
ﬁeld incidents attributable to speciﬁcation omissions (i.e. due to unexplored system boundaries) become
more frequent than before. on the whole, soft reliability issues observed in the ﬁeld for many consumer
electronics devices have already started to overtake the numbers of hard reliability problems arising fromtypical speciﬁcations violations
6,7. this trend conﬁrms that the recently emerging issue of soft reliability is
a serious problem today, and will be even more so in the future, unless the necessary know-how to tackle it
is developed.
soft reliability research is concerned with (i) understanding the sources of and then (ii) managing (i.e. fore-
casting, avoiding, tolerating, and remedying) user dissatisfaction with respect to the user-expected product
capabilities, aesthetics, performance and usability. more speciﬁcally, it deals with analyzing user–system
‡note that softreliability should not be confused with software reliability.
§adaptability of a product may lead to situati ons where two instances of the same product appear and behave in very different manners,
e.g. hand-held computers equipped with differ ent softwares and used for entirely diffe rent purposes (a mobile navigator, a mobile
phone, etc.).
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
interactions that potentially reveal causes of soft failures (an exact deﬁnition of soft failures is given in the
following section), thus devising a set of theories, models, methods, and tools to ultimately enable theireffective and efﬁcient handling in an industrial context. therefore, it is natural to investigate soft reliability
from two complementary perspectives: first, the product-oriented perspective focuses on user–system inter-
action and tries to identify soft reliability problems as they manifest themselves when product characteristics,
which are consequences of explicit or implicit design decisions, are put to test in a realistic context of
use. second, the process-oriented perspective focuses on the business processes (before, during, and after
product development), where the root causes of soft reliability problems, such as inadequate communication
of feedback or lack of user-centeredness, need to be established. the focus of this paper is on soft failures
at the user–system level (i.e. the product-oriented perspective). in particular, we will report several early
ﬁndings from an ongoing multidisciplinary collaboration that addresses different aspects of soft reliability,
characterized by a range of questions and proposed approaches:
(a) how is it possible to account for non-instrumental quality values (e.g. emotional, societal) of a targeted
audience early on in the design of products?
(b) how can unexpected user or system behavior be captured and understood from instrumental and
reported data obtained from real usage? how can such information be fed back to the product devel-
opment processes?
(c) how can observation units within products help in gathering objective usage data that are speciﬁcally
oriented toward extracting soft-reliability-related information?
(d) how can logged data from ﬁeld-feedback processes and from product usage processes be processed
and presented in such a way that information contributing to our understanding of soft reliability
becomes evident?
the paper is organized as follows: section 2 provides necessary background on soft reliability. section 3
discusses an experiment that reveals potential root causes of soft reliability problems by evidencing a gap
between designers’ views on users’ perceptions and actual users’ perceptions (a). section 4 elaborates onthe necessary approach to adopt in collecting and processing subjective feedback data from actual users (b).
section 5 describes a novel approach that can be used to trace and analyze user behavior during actual
product usage, i.e. the collection and processing of objective usage data (c and d). section 6 concludes the
paper and identiﬁes directions for future work (e.g. possibilities to combine the objective and the subjective
data to obtain a more complete view on the overall usage process).
2. background on soft reliability
formerly, failures encountered in products were mostly technical component failures of hardware or soft-
ware that displayed explicit violations of product speciﬁcations8, and hence which could be linked back
to product implementation or manufacturing processes during development. however, within the current,
rapidly evolving market circumstances, new classes of failures that cannot be traced back to product imple-
mentation or manufacturing processes are increasingly being observed in the ﬁeld. at present, these additionalclasses of failures (attributable to speciﬁcations omissions, usability /learnability problems, customer misun-
derstandings, or speciﬁc use context) are not identiﬁable and manageable within existing industrial quality
and reliability information ﬂows
1,3,5,9. this is related to the fact that, currently deployed quality and relia-
bility information ﬂows exist mostly between ‘product aftercare’ activities and the ‘product implementation-
manufacturing’ activities of new product development (npd) processes; but hardly ever between ‘productaftercare’ activities and the front-end ‘product design development’ activities of npd processes. accord-
ingly, the structure and content of the data transferred via these ﬂows are not compatible with the nature
of the new classes of failures. the end result is many ‘product assistance’ calls at call centers, ‘no faultfound’ labeled products at service /repair centers, returned products at dealers that function well, and to top
it all, disconﬁrmed expectations of unhappy customers yielding damaged brand image of companies. the
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
realization of this trend (figure 1) was the starting point for making a distinction between hard versus soft
reliability concerns1,2,4,7.
the distinction we make between hard failures and soft failures is as follows7.hard failures are product
failures where the product is incapable of performing its functions, as listed in its technical speciﬁcations,
without the intervention of authorized technical support for recovery by means of repair or replacement
of parts. on the other hand, soft failures are product failures where the product, despite being capable of
performing its functions as listed in its technical speciﬁ cations, still necessitates professional intervention
for recovery (but not repair), through instructions or information, from an unexpected state of user–product
interaction¶.
previously mentioned npd, the outcome of which establishes new products in the market, is an interdis-
ciplinary domain; and so is soft reliability, which is involved therein. in the npd context, soft reliability
speciﬁcally concerns (i) product ‘quality and reliability’, which is largely determined by (ii) ‘customers in
the market’, that is
(i) ‘quality and reliability’ of a product is concerned both with the hard engineering of it to meet technical
product speciﬁcations, and the soft design of it (including the organizational design decisions to
include /exclude certain functionalities or use of materials) to meet user requirements. thus, the design
of the product is concerned with the user in terms of her socio-cultural context and hence related
expectations. generally speaking, hard engineering is about making the product right, whereas soft
design is about making the right product (with the required functionalities, ease of use, appeal, etc.).
(ii) getting to know the ‘customers in the market’ requires getting to know ‘customer segments’ in
an ‘evolving market’, both of which relate to the marketing domain. knowledge of these aspects
contributes to an improved identiﬁcation of customer needs and expectations, and to a better posi-
tioning of new products within a dynamic market. in addition to these presales concerns, marketingalso involves handling post-sales /aftercare services, such as minimizing customer dissatisfaction,
which requires knowledge of consumer complaint behavior and the key points of what is referred to
as customer relationship management (crm).
a good understanding of all related ﬁelds is necessary in order to be able to manage soft reliability, which
currently presents a growing, uncontrollable problem in npd. the ﬁndings of soft reliability research areintended to be beneﬁcial: (i) for the user, in that the confusion due to the discrepancy between expectations
and real product capabilities is minimized; (ii) for the manufacturer, in that product investments are made in
the correct and relevant areas (i.e. not for product functionalities that users do not seem to be interested in);
and (iii) for the impetus of technological innovations and their acceptance by society, in that the convenience
offered is perceived as greater than its burdens. consequently, it is relevant for both academia and industry:while expanding the scope of the current quality and reliability approaches to systematically cover user-
centered failures, industry is offered efﬁcient and effective ways to cope with an increasingly threatening
problem.
3. where are soft failures rooted?
the emergence of soft reliability has shifted the emphasis of product quality and reliability from engineering
back to (the preceding) design of the product. recent research in soft reliability pointed to the crucial
importance of the initial concept design phase in minimizing the occurrence of soft failures3. as usually there
is not much effective communication between the front-end concept design activities of most npd processes
¶for instance, applying an in-house developed software patch as an end- user, provided proper resources (e.g. instructions and information),
is also considered a recovery case and not a repair case as the actual repair is done in-house. hence, it is a case of a soft failure,
conforming to the deﬁnitions.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
figure 2. two-dimensional visualization of dissimilarities between designers’ and users’ perceptions and
hierarchical clustering (minimum variance)
and, in the end, the users of the resulting products, more research is needed to explore the compatibility of
the perceptions that are in effect at both ends.
based on the number of interviews with stakeholders in the concept design practices in a multinational
printing company, we observed that it is often difﬁcult to trace back the reasons that motivated certaindesign decisions. the questions raised then were on what basis are design decisions made? can designers
really foresee user preferences? to answer these questions, we conducted an experiment where designers’
views on users’ perceptions of a product were compared to actual users’ perceptions
10. eleven employees
of the multinational printing equipment manufacturer, all involved in the concept design phase of the
touchtoprint/bardblproduct, as well as eleven potential users that had no prior experience with the product,
were interviewed using the repertory grid technique11. during the interview, participants (i.e. designers
and users) were ﬁrst asked to compare the product to ﬁve competitor products. this process resulted in a
list of attributes (e.g. modern–traditional and fast–slow) that participants used to differentiate between theproducts. subsequently, each participant was asked, in a product-rating procedure, to compare all products
on his /her own elicited attributes as well as on overall measures of preference anddissimilarity
10.a sa
result, the repertory grid process yielded three kinds of data: dissimilarity ,preference ,a n d attribute data.
to explore the discrepancies between designers’ and users’ perceptions, we deﬁned the perceptual distance
dijbetween participants iand jbased on dot-product correlations rijof the kdissimilarity scores ( 1).
derived distances were then visualized in two dimensions (figure 2) using the mds tool xgms12.t h e
two-dimensional visualization was judged as adequate (stress value s=0.18). hierarchical clustering (with
minimum variance) revealed two main clusters that can be further subdivided into ﬁve relatively homoge-neous participant groups. groups 3 and 4 consist entirely of users, whereas groups 1, 2, and 5 consist mostly
of designers. identiﬁcation of the designers revealed that group 1 consists mostly of technically-oriented
designers, whereas group 2 consists mostly of user-oriented designers
d
ij=1−r2
ij,rij=/summationtext
kdi(k)·dj(k)
/radicalbig
/summationtext
kd2
i(k)·d2
j(k)(1)
where di(k)is the dissimilarity score given by subject ionkth product comparison ( ksums over all product
comparisons).
to acquire richer insight into the ways in which desi gners and users differed, we focused speciﬁcally
on a comparison between two of the six products: touchtoprint andbadge . figure 3(a) illustrates the
reasons supporting preference for touchtoprint over badge, as it shows the number of attributes that are
/bardbltouchtoprint is an alternative to other user identiﬁcation mechanisms such as pin-code or badge.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
touchtoprint over badge
(positive attributes)
024681012
emotionalnumber of attributesdesigners (4)
users (8)touchtoprint over badge 
(negative attributes)
0246810number of attributesdesigners (4)
users (8)importance rankings of reliability attributes
01234
12 3 4 5 6 7
importance rankingnumber of subjectsdesigners (2)
users (7)
effectiveness efficiency emotional effectiveness efficiency
(a) (b) (c)
figure 3. attributes (a) positively ranked and (b) negatively ranked when touchtoprint is preferred (along with 95% exact
conﬁdence intervals). (c) importance rankings for reliability attributes for designers and users
positively ranked when touchtoprint is preferred. although users’ most frequent reason for preference of
touchtoprint was related to emotional attributes, for designers it was efﬁciency attributes. all attributes in
the effectiveness category were related to security. touchtoprint was perceived as more secure than badge
by both designers and users. users’ most frequent negative concerns, shown in figure 3(b), were related toreliability (ﬁve out of the seven effectiveness attributes had to do with reliability). this is also evident in
figure 3(c) where we can observe that only two designers expressed reliability concerns and ranked them
as the sixth and seventh most important attributes, whereas ﬁve users ranked reliability within their threemost important concerns. hence, although most users preferred touchtoprint, they had some concerns that
can potentially turn into failures to motivate use.
the experiment seemed to corroborate our initial hypothesis: designers could not predict users’ responses
to the product that they were designing. although designers seemed to predict the different concerns that
people would have with the product, they highly mispredicted the importance that users attach to eachconcern. further, the position and background of each de signer seemed to impact their sensitivity to users’
concerns. while market experts and usability engineers seemed to have higher sensitivity to users’ concerns
due to the high exposure to users’ respons es, designers with technical roles (i.e. receiving less exposure to
potential users) seemed to have higher misprediction rates.
the results of this experiment can be read from two different, but in our view complementary, perspectives.
on the one hand, they point to the importance of user involvement in the early design phases, for the
products’ acceptance in the market. this view brings the requirement of iterative andscalable product
evaluation practices within the npd process. on the other hand, it brings awareness of designers’ inabilityto foresee real users’ preferences and behavior. the latter implies the need for rapid prototyping practices
to enhance the ability to learn from the ﬁeld. we believe that both tracks need to be pursued for managing
soft failures in highly uncertain markets.
4. in what forms are soft failures revealed?
soft failures crop up due to a combination of factors. from a high-level managerial perspective, the root cause
can be seen as the conﬂict between the perspectives of th e developer, manager, and the user, which is surfaced
by the rapidly changing market conditions: products with growing degrees of innovation are developed
and put out on the global market in decreasing time periods to meet the more demanding expectations
(e.g. with respect to extended warranty periods) of a variety of customers . in this setting, (i) the developer
focuses on implementing the most recent technological advancements in a product correctly; (ii) the manager
focuses on launching a reliable and proﬁtable product on the global market at a speed exceeding that of
the competitors, with the concern of maintaining customer satisfaction before and after sales; and (iii) theuser focuses on fulﬁlling her expectations about the product she chooses to invest money in. the results of
the experiment presented in section 3 is a ﬁne example of the discrepancy between the designer’s and the
user’s views.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
from a slightly lower-level design perspective, the roots of soft failures can naturally be attributed to
speciﬁc decisions taken during the concept design of a product. however, consistently attributing each soft
failure to a particular cause requires a means that enables a deeper level of analysis. first and foremost, such
an analysis requires a clear understanding of the fundamental nature of soft failures, which in turn requires
a thorough knowledge about the failed state during user–system interaction, because the actual revelation of
soft failures occurs only during use of the product in its speciﬁc use context. then, all identiﬁable properties
of soft failures need to be synthesized in a structured way to relate observable failure symptoms to their
respective diagnosis .
in the case of hard failures, due to their well-documented reference point of technical speciﬁcations and
hence solely product-centered manifestation, it is comparatively simple to identify them and trace theirfault→error→failure chains. however, this is not the case with soft failures, due to their negated stance
with respect to technical speciﬁcations (i.e. they are not covered by technical speciﬁcations) and also their
user-centered manifestation. this difﬁculty, with the human factor involved, has already been expressed in
the wide-ranging related literature (e.g. systems engineering and software engineering), and the need for
developing methodologies for systematic identiﬁcation and improvement of user–system interaction failureshas been explicitly underlined
13.
partly as an attempt to address the above-mentioned necessities and difﬁculties, we developed a failure
classiﬁcation model6. this model acts as the means to analyze soft failures by taking into account both
the actual (vs the ‘assumed’) product capabilities and the phases of use, speciﬁcally accounting for the
use context. by phases of use we mean the phases that need to be accomplished consecutively by the
user in order to ensure that the product gets successfully communicated. these phases, as deﬁned inreference
14, are (i) appropriate commercial product exposure and communication, (ii) creating user aware-
ness for the (many) features and functions the product has, (iii) making these functions appealing for
the user in order to motivate use, (iv) making the product intuitive to use, to ensure (v) acceptance of
the product through satisfaction, and (vi) adoption of the product to the user’s daily life, for extended
use. additionally, the analysis offered by our model is designed to ﬁt the conventional failure mecha-nism framework that complies with the ‘fundamental chain’ of fault →error→failure (i.e. cause →state→
event)
13,15,16. this approach is adopted to render our model compliant with the widely acknowledged funda-
mental concepts and taxonomy of dependable computing13. consequently, we highlight the parallelism
between failure mechanisms in computing sciences and in npd, and hence encourage the development of
respective fault forecasting, avoidance, tolerance, remedy techniques for soft reliability, which is part of our
ongoing work.
to date, the industrial applicability of our model has been tested by classifying data from different
feedback sources (i.e. call centers, service centers, internet forums and user tests) regarding various innovativeelectronics products of multinational companies, and our ﬁndings reveal promising improvements as to
the effective capture of classes of soft failures. an earlier pilot study, the details of which can be found
in reference
5, is summarized in what follows: in figure 4, the classiﬁcation results of 2321 calls to a
call center, done in the call center itself using their own classiﬁcation model (left), are compared with the
classiﬁcation results of the same 2321 calls, using an earlier version of our model (right). note that the
company’s own model is overpopulated with many categories, which makes it impracticable to be used
correctly and consistently by the human classiﬁers (i.e. call center agents). in figure 5, the classiﬁcation of
1368 calls to another call center, done in the call center itself using their own classiﬁcation model (left),is compared with the classiﬁcation of the same 1368 calls, using the same earlier version of our model
(right). note that the company’s own model has the problem that there is only one category, namely ‘product
assistance’, which is being used. the problems encountered in both cases stem from ineffective ways ofclassifying failure data, which has been conﬁrmed by the respective companies who are currently in the
process of exploring improved approaches based on our recommendations.
to conclude, earlier on, we stated that the actual revelation of soft failures occurs only during use of the
product in its speciﬁc use context (i.e. in contrast to simulated failures at premeditated user tests). therefore,
actual subjective user feedback about the failed interaction is crucial to identify any mismatch between
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
0%5%10%15%20%25%
0%5%10%15%20%25%
(a) (b)
figure 4. percentages of call reason categories: (a) ﬁrst company’s own model and (b) our failure classiﬁcation model.
the large number of categories in (a) render the company’s model impracticable to be used correctly and consistently by
the call center agents, making the classiﬁcation less useful
0%10%20%30%40%50%60%70%80%90%100%
0%5%10%15%20%25%30%35%40%
(a) (b)
figure 5. percentages of call reason categories: (a) second company’s own model and (b) our failure classiﬁcation
model. a highly used category in the company-owned model, ‘product assistance’, makes it impossible to gain
deeper insight into the actual call reasons
(latent or explicit) user expectations and actual product speciﬁcations. however, the empirical data presented
show that current industrial systems to collect user feedback from the ﬁeld miss soft reliability altogether,
contributing to increasing numbers of ‘no fault found’ cases. to this point, ﬁeld feedback has remained an
untapped resource of invaluable information to be utilized during npd processes. as each contact with a
customer is an opportunity to discover the ‘basic’, ‘performance’, and ‘excitement’ needs17of (potential)
users, we work towards operationalizing our generic failure classiﬁcation model for effective gathering ofﬁeld feedback to facilitate dynamic improvement of soft reliability.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
5. how can soft failures be traced?
the successful classiﬁcation of soft reliability probl ems reported in the ﬁeld (cf. section 4) is necessary to
effectively handle customer dissatisfaction and al so to enable a feedback process to the development team
for improving products. however, the available data sources from service centers, call centers, the internet
forums, and dealers (e.g. about returned products) are typically very limited, i.e. they lack important context
information that is needed to track down the root cause of a problem, leading to many ‘no fault found’ cases.furthermore, due to a gap between designers’ and users’ perceptions of a product (cf. section 3), problems
often appear in user–system interaction, as users may behave in ways that designers had not anticipated.
therefore, we suggest that objective information about real user actions on products can help to bridge this
gap. being able to reproduce experienced problems should make it easier to devise solutions that effectively
remove such problems. for this, usage data need to be collected directly by the product and processed in a
structured and automatic manner. this may be applied in different scenarios that require different levels of
data collection and transparency:
•during sessions of early user involvement, e.g. experiments with prototypes in an early phase of the
npd process;
•as part of the post-sales service process, e.g. as a remote diagnosis instrument while the user is in
contact with an agent in the call center; or
•in the context of a more general (voluntary) feedback process, e.g. regular usage reports from motivated
customers, stimulated by monetary incentives.
in what follows, we ﬁrst present a ﬂexible usage observation framework that can be used to support all
these scenarios. data collection can be speciﬁed by the actual stakeholders involved in product developmentand the observation framework automatically collects the data from product instances in use. furthermore,
we give an outlook on possible approaches to evaluate these data in a structured and automated form.
5.1. observation approach
in principle, by observing users interacting with a product, two kinds of data can be captured: subjective data
about usability and objective data about actual usage patterns. customarily, such data are only collected in
special testing environments. a drawback is that simulated settings in usability labs may bias the outcome
of product research as, for instance, users will feel supervised and thus act differently than in real situations.
in order to get more reliable and objective usage information, we propose an approach to make products
self-observing and let them capture usage within the user’s habitual environment.
the objective is to get usage information directly from within a product; however, what is mostly provided
by products is data in the form of various, rather simple, events. what is missing is semantic coherence
and meaning. examples of such simple events are key presses, user-interface events, and performancesamples of system components. information at this level cannot directly be used for further analysis, as for
that semantically rich information is required. to deal with this, a semantic layer has been established by
combining the provided low-level product events to complex events (cf. reference
18). these events capture
higher-level processes that are triggered in the product and therefore support understanding of the actual
usage. in addition, it enables further (i.e. automatic) analysis as we will show in the second half of thissection.
the basic idea of self-observing products is to equip commercial digital electronics products with an
additional internal software component. this observation component can sense information, process it, and
transmit captured data ﬁnally to a central instance. so-called hooks are places in the hardware or software
where events can be observed, which are suitable as sources of observation data. information about the
event or just the fact that the event occurred forms a simple event and is routed to an event processing
component that is responsible for the proper handling of product events. here, more complex events are
constructed by correlating low-level product events from different sources spatially and temporally. thoseaggregates are described in the form of complex events annotated with semantic information provided by
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
figure 6. system overview: d’puis and prom
respective stakeholders. thus, the system generates meaningful information that can be analyzed and reused
seamlessly. a small example will be provided later.
the approach outlined above has been realized as the d’puis framework that has been used in a consumer
electronics product that is connected to the internet19. this framework enables one to create a large network
of distributed self-observing products. product instances in use (figure 6) are ﬁrst instructed what to observe;
then, during the actual data collection, they send their information up to the global observation unit. thisglobal unit stores the collected information in a database, which can subsequently be accessed by the
prom import
∗∗20tool in order to convert the data for automatic analysis.
regarding the architecture and development effort to build a system that is self-observing, it is obvious
that a mismatch exists between interests of various npd stakeholders, e.g. people who are interested in
usage problems and people who have to build the product. a possible stakeholder involved in npd is, for
example, a knowledge engineer, who is responsible for evaluating the feedback from the ﬁeld, to predict
and measure the product quality and to provide information that can lead to improvements for future
product generations. domain experts such as the knowledge engineer are precisely those people who willbe interested in eventually analyzing the usage data collected in the product. however, as they should not
be expected to program the observation logic themselves, they would need to be able to specify exactly to
software engineers what data should be observed when.
the resulting communication overhead and inﬂexibility can be avoided by our d’puis framework in two
ways. first, our d’puis framework deploys a visual approach for programming (figure 6), which enables
various types of domain experts without deep technical knowledge of the product architecture to specify
complex events that will provide the basis for the analysis. second, on the developer side, only the hooks
need to be speciﬁed at design time. as soon as they are implemented, complex events can be speciﬁed viathe visual editor, which results in a remote change of observation in the products. this has the advantage
that observation logic can be modiﬁed at any point in time without changing the product itself.
the architectural and development-wise bottleneck of product observation is clearly the integration into
products and the connection to product hooks that supply the required usage information. therefore, the
most efﬁcient solution is to keep the number and assignment of product hooks stable and emphasize the
ﬂexibility of routing and assignment of complex events.
figure 7 depicts a screenshot of the visual editor, which helps to deﬁne what shall be observed in the form
of product events and how these events will be processed. it is a simpliﬁed way to program the networkof distributed observation units. technical matter is hidden and simpliﬁed as building blocks such as event
sources, processing nodes, and a global event sink (triangular box in figure 7) that directly routes incoming
data to a central database. the editor represents the ﬂexibility to dynamically change how complex events
∗∗software and documentation (including source code) are freely available at http://promimport.sf.net.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
figure 7. visual editor showing the observation speciﬁcation for the example scenario in the text
table i. experimental data coming from observed products using observation modules, which have been instrumented
with the above-mentioned observation speciﬁcation (cf. visual editor in figure 7)
user
 id task task data timestamp
1 parental off 2007-08-31 17:01:012 parental off 2007-08-31 17:01:231 open tv guide 2007-08-31 17:02:063 open tv guide 2007-08-31 17:02:074 open tv guide 2007-08-31 17:02:152 open tv guide 2007-08-31 17:02:553 menu select record 10 p.m. news 2007-08-31 17:03:024 shortcut select record 10 p.m. news 2007-08-31 17:03:041 menu select record 10 p.m. news 2007-08-31 17:03:34
4 parental off 2007-08-31 17:05:23
2 shortcut select record 10 p.m. news 2007-08-31 17:06:373 parental off 2007-08-31 17:06:41
are constructed and how they are routed within the observation system. this allows stakeholders to deﬁne
their own complex events that are later identiﬁed and can be mined in an efﬁcient manner.
as a working example, we present the case of a hard-disk recording device that allows one to record pre-
selected tv programs. in our scenario, the users are instructed to record the 10 p.m. news, initially restricted
by parental control, which simply blocks recording after 9 p.m. in order to achieve her task, the user has to
disable the parental control, open the tv guide, and record the program. the observation speciﬁcation used
in the example (figure 7) shows that four complex events (‘parental off’, ‘shortcut select record’, ‘menuselect record’ and ‘open tv guide’) are generated by combining simple events from ﬁve hooks in the product
(rectangular boxes). the combination of information can be ﬁltering or the correlation of concurrent events
with a boolean ‘and’ operation. whenever the ‘shortcut select record’ or ‘menu select record’ event occurs,the parental control settings hook (‘user
settings
 3
item4’) is automatically triggered, i.e. the current state
of those settings is retrieved and ﬁltered. only if the state then evaluates to ‘off’ the ‘parental control off’
complex event is generated.
table i shows an excerpt of logged data, which was created using the observation speciﬁcation. complex
events combining several data sources result in log data that are stored in a very simple format (e.g. user,task, [data], and time). four users performed the experiment task almost simultaneously as the ‘timestamp’
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
data indicate. the ‘task’ column in the log data shows that the routes leading to the export node in the
observation speciﬁcation (the triangle in figure 7) directly determine the contents of the resulting logdata. this short excerpt of example data represents four slightly different approaches to perform the task,
pointing at the complexity of usage data coming directly from products. the example incorporates only a
few hooks, but potentially many more simple events are suitable for observation, for instance, system status
information, various input device events, or even contextual data. this emphasizes the need for elaborate
analysis tools that enable the detection of the different processes from within hundreds of thousands of suchlog entries in real-life cases. however, the storage a nd access to collected usage information inside products
are standardized. therefore, the whole workﬂow from data collection, aggregation, and /or ﬁltering to the
analysis can be automated.
5.2. data analysis
with the storage of usage data in elementary (simple events) and aggregate (complex events) form in the
database of the observation system, relevant information can be directly evaluated. for example, simple
frequency measures (e.g. ‘how often was the recording started via the menu rather than the shortcut?’) canbe easily extracted. however, to gain deeper insight into the user–system interaction, further analysis of the
users’ behavior is needed. in the remainder of this section, we outline how process mining can help to ﬁll
the gap between raw log data and knowledge about the usage process.
process mining is a relatively young ﬁeld of log-data analysis techniques that have been successfully
applied to many real-life logs from, e.g. hospitals, banks, municipalities, etc. (cf. references
21,22for
example). the basic idea of process mining is to discover, monitor, and improve real processes (in contrast
to assumed processes) by extracting knowledge from event logs . today, many of the activities occurring in
processes are either supported or monitored by information systems, such as enterprise resource planning,workﬂow management, or crm systems. however, process mining is not limited to information systems
and can also be used to monitor other operational processes or systems, e.g. complex x-ray machines,
high-end copiers, web services, careﬂows in hospitals. the common denominator in the various applicationsof process mining is that there is a notion of a process and that the occurrences of activities are recorded
in so-called event logs . assuming that we are able to log events, a wide range of process mining techniques
comes into reach: we can use process mining to (1) discover new models (e.g. constructing a petri net that
is able to reproduce the observed behavior), (2) check the conformance of a model by checking whether the
modeled behavior matches the observed behavior, and (3) extend an existing model by projecting informa-
tion extracted from the logs onto some initial model (e.g. show bottlenecks in a process model by analyzing
the event log).
to enable the application of process mining techniques to the product logs recorded by the d’puis
framework as shown in figure 6, we ﬁrst need to convert the collected log data to the mining xml
(mxml
††) f o r m a t ,w h i c hi su s e db yp r o m‡‡23, our process mining tool. therefore, we developed the
d’puis plug-in of the prom import framework, which facilitates log conversion tasks. figure 8 depicts a
fragment of the mxml log for the hard-disk recording example from table i. one can see that a process (here
the experimental process as a whole) contains several process instances (the different users participating in
the experiment), which in turn consist of a number of audit trail entries (the events or process steps that are
logged). each event carries a name (e.g. ‘parental off’) and a type(here always ‘complete’), and potentially
atime stamp ,aperformer (not used here), and additional data (e.g. ‘10 p.m. news’).
an important area in the ﬁeld of process mining is ‘control-ﬂow discovery’. the goal of control-ﬂow
discovery is to automatically construct a process model showing the causal dependencies between activities
in the process. this is not only challenging but also interesting as it immediately provides an overview
about the actual ﬂow of the process. many different approaches have been proposed in the literature
††the mxml schema deﬁnition and further information on process min ing research can be found in our website: www.processmining.org.
‡‡software and documentation (including source code) are freely available at http://prom.sf.net/.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
figure 8. fragment of mxml log for consumer test example in table i
figure 9. discovered process model (a) in prom and (b) conceptional visualization
(cf. reference21for further references). for example, figure 9(a) depicts the result of the genetic miner in
prom 4.2 based on the event log from figure 8 in the form of a petri net model. the same process is depicted
in figure 9(b) in a simpler notation for illustration purposes: the and and xor nodes are routing nodes
(activating or synchronizing branches), whereas the other, rectangular nodes form the steps, or activities, in
the recording experiment process. one can see that in the beginning of the process two parallel branches arestarted (both activated by the top-most and node). note that parallel activities are not causally related and
can be executed in any order. the left branch only contains the task ‘parental off’. the right branch requires
to ﬁrst complete the task ‘open tv guide’ before either ‘menu select record’ or ‘shortcut select record’ canbe performed. both branches are required to ﬁnish before the process is completed (synchronized by the
bottom and node). note that all the four scenarios from table i are incorporated in the discovered process
model. their variations are captured by the choice (xor) and the detected parallelism (and).
although the control-ﬂow perspective is a very important view on a process, there are also other interesting
perspectives, such as the organizational perspective, data, and time. for example, figure 10 depicts twoscreenshots of analysis plug-ins that make use of the time information in the log: figure 10(a) visualizes
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
figure 10. screenshots of further analysis plug-ins in prom: (a) sequence diagrams and (b) performance analysis
the observed patterns as a sequence diagram (similar t o a uml sequence diagram) and provides statistics
on their frequency and duration; figure 10(b) highlights bottlenecks in the discovered process model based
on an evaluation of the timestamps in the log (most time was spent between ‘open tv guide’ and either‘menu select record’ or ‘shortcut select record’, which might hint at a usability problem). note that there
are many more mining, analysis, and visualization tools available in prom, and further research is needed
to determine which of them are most relevant to gain insight into soft failures. for example, we mightwant to compare the actual user behavior in such a cons umer test scenario with the expected interactions.
conformance checking techniques
24can then be used to visualize and measure potential deviations.
note that the automatic construction of process models for task analysis is not new25.h o w e v e r ,t h e
techniques described in reference25are very basic as they, for example, do not consider parallelism. overall,
process mining techniques seem very suitable for the analysis of product logs as recorded by the d’puisframework. furthermore, with the prom (and prom import ) framework a powerful tool set is available, which
can also be easily extended to, e.g. address certain domain-speciﬁc needs.
6. conclusions
in the past, both the quality and the reliability of a product were mainly determined by the product’s compli-
ance with its technical speciﬁcations. customer satisfaction came in secondary, as a nice-to-have dimension
of quality. due to the conditions of today’s competitive global market, customer satisfaction through ensuring
user-perceived quality and reliability has become the foremost concern. accordingly, products need to be
developed to comply not only with their technical speciﬁcations but also with explicit customer requirements
and implicit expectations. this shift of focus in the assessment of quality and reliability of products makes it
important to distinguish between hard reliability and soft reliability. furthermore, the lack of soft reliability
in modern products has led to increasing numbers of ﬁeld cases, where products that in fact technically
function well according to their speciﬁcations are being returned or being sought compensation for.
meeting the customer’s requirements and expectations poses a number of challenges. firstly, it is evident
that customers are becoming more and more quality demanding, backed by their increasing span of options
on the global market. irrespective of the underlying system complexity, customers seek for easy- and
delightful-to-use products that do not induce learning costs. secondly, whereas precisely deﬁning customer
requirements and expectations is crucial while developing a new product; these are rather vague at design
time, and also continuously changing. therefore, it is not trivial, if possible at all, to deﬁne a consistent set
of requirements that is complete and unambiguous, especially in a highly dynamic context. thirdly, owing
to many competitors in the global arena, there is a strong pressure on time to market, which enforces pacingup of the development activities of products.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
we argue that to address these challenges, and hence improve the soft reliability of products, we need
a user-centered approach to product development. there are many users with diverse proﬁles, culturalbackgrounds, and past experiences, and the aim is to design products to suit them in their various social and
physical contexts of use. without properly taking the se parameters into account, acceptance and eventual
adoption of a product cannot be achieved. to alleviate the problem of vague and uncertain customer require-
ments, users must be involved early in the development process of a product. moreover, user feedback at all
times (i.e. both before and after sales) should be captured and integrated back in the development processto be able to account for diverse use proﬁles. finally, to address the problem of the ever-shortening time to
market, automatic data collection and knowledge extraction mechanisms are needed.
in this paper, we illustrated that there is the need for early user involvement due to a natural gap between
the perceptions of designers and users. then, we reported on current limitations in user-feedback collection
processes and demonstrated that they can be overcome by utilizing our failure classiﬁcation model. finally,
we presented our usage observation and analysis approach, which enables insights into actual product usage.
note that the collection of both subjective (e.g. user perceptions and feedback )and objective (e.g. product
usage )information are needed to cope with soft failures. expertise from different disciplines is needed to
obtain a holistic view of the problem at hand.
soft reliability is important. hence, solutions need to be devised, which help companies to tackle the
increasing numbers of ‘no fault found’ cases. we currently apply our approach in several industrial casestudies. furthermore, we explore ways to semantically enhance the collected data in order to investigate the
opportunities provided by semantics-aware analysis techniques (cf. reference
26). finally, the availability
of both subjective and objective data about the very same usage process opens up new and interestingpossibilities for combining the knowledge of the various domain experts.
acknowledgements
this work is being carried out as part of the ‘managing soft reliability in strongly innovative product creation
processes’ project (http://s oftreliability.org/), sponsored by the dutch ministry of economic affairs, under the
iop-ipcr program.
references
1. brombacher ac, sander pc, sonnemans pjm, rouvroye jl. managing product re liability in business processes ‘under
pressure’. reliability engineering & system safety 2005; 88(2):137–146.
2. geudens whjm, sonnemans pjm, petkova vt, brombacher ac . soft reliability, a new class of problems for innovative
products: ‘how to approach them’. fifty-ﬁrst annual reli ability and main tainability symposium , alexandria, va,
u.s.a., 2005.
3. ouden den e, lu y, sonnemans pjm, brom bacher ac. quality and reliability probl ems from a consumer’s perspective:
an increasing problem overlooked by businesses? quality and reliability engi neering international 2006; 22(7):
821–838.
4. koca a, lu y, brombacher ac, hartmann jh. towards estab lishing foundations for new cla sses of reliability problems
concerning strongly innovative products. third ieee international conference on management of innovation and
technology . ieee press: singapore, 2006.
5. koca a, schouwenaar ajm, brombacher ac. field-feedback in innovative product development: a comparison of two
industrial approaches. fourteenth international product development management conference . eiasm: porto, 2007.
6. koca a, schouwenaar ajm, brombacher ac. analysis of user-centered failure mechanisms in new product
development for quality improvement. fourteenth international annual euroma conference . eiasm: ankara, 2007.
7. koca a, brombacher ac. extracting ‘broken expectations’ from call center records: why and how. chi’08 extended
abstracts on human factors in computing systems . acm press: florence, 2008.
8. pecht mg, ramappan v . are components still the major pr oblem: a review of electronic syste m and device ﬁeld
failure returns. ieee transactions on components ,hybrids ,and manufacturing technology 1992; 16(6):1060–1064.
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
9. ouden den e. development of a design analysis model for consumer complaints. doctoral dissertation , eindhoven
university of technology, eindhoven, 2006.
10. karapanos e, martens j-b. on the discrepancies between de signers’ and users’ perceptions as antecedents of failures
in motivating use. international conference on interfaces and human computer interaction . iadis: lisbon, 2007.
11. hassenzahl m, wessler r. capturing design space from a user perspective: the repertory grid technique revisited.
international journal of human–computer interaction 2000; 12(3–4):441–459.
12. martens j-b. multidimensional modeling of image quality. proceedings of the ieee software 2002; 90(1):133–153.
13. avizienis a, laprie j-c, randell b, landwehr c. basic concepts and taxonomy of dependable and secure, computing.
ieee transactions on dependable and secure computing 2004; 1(1):11–33.
14. bouwmeester den k, bosma e. phases of use: a means t o identify factors that inﬂ uence product utilization. chi’06
extended abstracts on human factors in computing systems . acm press: montreal, canada, 2006.
15. bsi london. reliab ility of systems, e quipment and components. guide to a ssessment of reliability of systems
containing software. british st andard 5760-8 :1998 . bsi london, editor, london, 1998; 5.
16. randell b, koutny m. failures :their deﬁnition ,modeling and analysis (lecture notes in computer science ,
vol. 4711). springer: berlin, 2007; 260.
17. kano n, seraku n, takashi f, tsuji s. attractive quality and must-be quality. the journal of japanese society for
quality control 1984; 14(2):39–48.
18. luckham dc. the power of events :an introduction to complex event processing in distributed enterprise systems .
addison-wesley longman publishing co., inc.: boston, ma, u.s.a., 2001.
19. funk m, putten pvd, corporaal h. speciﬁcation for user modeling with self-observing systems. first international
conference on advances in computer–human interaction , saint luce, martinique, 2008.
20. g¨ unther cw, aalst wmpvd. a generic import framework for process event logs. business process management
workshops ,workshop on business process intelligence (bpi 2006 ). springer: vienna, austria, 2006.
21. aalst wmpvd, reijers ha, weijters ajmm, dongen bfv , medeiros akad, song m, verbeek hmw. business
process mining: an industrial application. information systems 2007; 32(5):713–732.
22. rozinat a, jong ismd, g¨ unther cw, aalst wmpvd. process mining of test processes: a case study. beta working
paper series , 2007.
23. aalst wmpvd, dongen bfv, g¨ unther cw, mans rs, medeiros akad, rozinat a, rubin v , song m, verbeek hmw,
weijters ajmm. prom 4.0: co mprehensive support for real process analysis. international conference on applications
and theory of petri nets and other models of concurrency (icatpn 2007 ), siedcle, poland, springer, 2007.
24. rozinat a, aalst wmpvd. conformance checking of processes based on monitoring real behavior. information systems
2007; 33(1):64–95.
25. rauterberg m. amme: an automatic mental model evalua tion to analyze user behavior traced in a ﬁnite, discrete
state space. ergonomics 1993; 36(11):1369–1380.
26. medeiros akad, pedrinaci c, aalst wmpvd, domingue j, song m, rozinat a, norton b, cabral l. an outlook on
semantic business process mining and monitoring. otm 2007 workshops . springer: berlin, 2007.
authors’ biographies
a. koca received her bsc and msc degrees in computer engineering from bilkent university, ankara,
turkey. both her msc thesis and her research at the information technology research institute (itri),
university of brighton, england, concerned the ﬁeld of computational linguistics. currently, her research is
on soft reliability and has strong connections with the industry, in the context of her phd studies ongoing inthe business process design group at the faculty of industrial design, eindhoven university of technology.
m. funk graduated from aachen university of technology in 2006. he is currently a phd candidate in the
electronic systems group at the faculty of electrical engineering of eindhoven university of technology
and visiting researcher at philips consumer lifestyle, eindhoven. his research interests include modeling
of self-observing systems, model-driven engineering, and design methodologies.
e. karapanos received a bsc in physics from the university of patras, greece, and an msc in human-
computer interaction from university college london, u.k. he is currently a phd candidate in the user-centered engineering group at the faculty of industrial design of eindhoven university of technology. his
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qresoft reliability: an interdisciplinary approach
research tries to understand diversity, and speciﬁcally that induced by time, in users’ subjective judgments,
in the ﬁeld of user experience.
a. rozinat received her bsc and msc degrees in software engineering from the hasso plattner-institute
(hpi), university of potsdam, germany. she is currently a phd candidate in the information systems
group at the faculty of technology management of the eindhoven university of technology. her research
interests include process mining, data mining, business process management, evaluation techniques, simu-lation, and case studies.
w. m. p. van der aalst is a full-time professor of mathematics and computer science at the eindhoven
university of technology (tu /e). currently he is also an adjunct professor at queensland university of
technology (qut) working within the bpm group there. his research interests include workﬂow manage-
ment, process mining, petri nets, business process m anagement, process modeling, and process analysis.
wil van der aalst has published more than 80 journal papers, 12 books (as author or editor), 200 refereed
conference publications, and 25 book chapters. many of his papers are highly cited and his ideas haveinﬂuenced researchers, software developers, and standardization committees working on process support.
h. corporaal has gained an msc in theoretical physics from the university of groningen, and a phd in
electrical engineering, in the area of computer architecture, from delft university of technology. corporaal
has been teaching at several schools for higher education, has been associate professor at the delft university
of technology in the ﬁeld of computer architecture and code generation, had a joint professor appointmentat the national university of singapore, and has been a scientiﬁc director of the joined nus-tue design
technology institute. he has also been department head and chief scientist within the desics (design
technology for integrated information and communication systems) division at imec, leuven (belgium).currently, corporaal is a professor in embedded system architectures at the eindhoven university of
technology (tu /e) in the netherlands. he has co-authored over 200 journal and conference papers in the
(multi-)processor architecture and embedded system design area. furthermore, he invented a new class of
vliw architectures, the transport triggered architectures, which is used in several commercial products.
his current research projects are on the predictable design of soft and hard real-time embedded systems.
j. b. o. s. martens studied at the state university of gent, belgium, where he received his electrical engi-
neering degree in 1979 and his phd degree in 1983. in 1984 he joined the institute for perception research at
eindhoven university of technology, where he performed research on image quality, psychometrics, image
processing and coding. he published a book entitled ‘image technology design—a perceptual approach’(kluwer academic publishers) on these topics in 2003. he joined the department of industrial design in
2003, where he is a full-time professor in the ﬁeld of visual interaction. his current research interests are
in tangible interaction, augmented reality, interactive visualization, and understanding user experiences.
p. h. a. van der putten received his bachelor in electrical engineering from the ihbo in eindhoven. since
1974 he was a research employee and part-time electrical engineering student at the eindhoven university
of technology (tu /e). he received the professional doctorate in engineering (pdeng) in 1993 from the
stan ackermans institute (tu /e). since 1993 he is an assistant professor at the tu /e (electronic systems
group). in 1997 he received his phd from the eindhoven university of technology. he teaches various
topics for post-msc programs and his current research interests are object-oriented analysis, requirementsanalysis, and model-driven design methods for hardware /software systems.
a. j. m. m. weijters is an associate professor at the technology management department of the eindhoven
university of technology and member of the beta research group. his current research focuses on data and
process mining (i.e. to extract knowledge from event logs recorded by an information system to analyze theunderlying business processes). he is the author of many scientiﬁc publications in the mentioned research
ﬁeld. his papers appeared in journals such as data mining and knowledge discovering ,information systems ,
ieee transactions on knowledge and data engineering ,artiﬁcial intelligence in medicine ,european
journal of operational research ,computers in industry ,knowledge-based systems ,ai-review .
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qrea. koca et al.
a. c. brombacher obtained a bsc, msc (cum laude) in electrical engineering and a phd in engineering,
science at twente university of technology. aarnout brombacher has experience in industrial product
development projects with special focus on ‘user-perceived quality and reliability’ and in the development
of supporting quality and reliability analysis methods and tools. he has authored and coauthored over 50
papers on these subjects and has written a book with the title ‘reliability by design’. aarnout brombacheris currently a professor in the faculty of industrial design of eindhoven university of technology, heading
the group business process design. he is also, as senior specialist, connected to philips cft industrial and
innovation support (iis).
copyright q2008 john wiley & sons, ltd. qual. reliab. engng. int. (2008)
doi: 10.1002/qre