process mining on databases:
unearthing historical data from redo logs
e. gonz ´alez l ´opez de murillas1;2, w.m.p van der aalst1, and h.a. reijers1
1department of mathematics and computer science, eindhoven university of technology
p .o. box 513, 5600 mb eindhoven, the netherlands
2lexmark enterprise software, gooimeer 12, 1411de naarden, the netherlands
{e.gonzalez,w.m.p.v.d.aalst,h.a.reijers}@tue.nl
abstract. process mining techniques rely on the existence of event data. however, in
many cases it is far from trivial to obtain such event data. considerable efforts may need
to be spent on making it systems record historic data at all. but even if such records
are available, it may not be possible to derive an event log for the case notion one is
interested in, i.e., correlating events to form process instances may be challenging. this
paper proposes an approach that exploits a commonly available and versatile source
of data, i.e. database redo logs . such logs record the writing operations performed in a
general-purpose database for a range of objects, which constitute a collection of events .
by using the relations between objects as specified in the associated data model, it
is possible to turn such events into an event log for a wide range of case types. the
resulting logs can be analyzed using existing process mining techniques.
keywords: process mining ,database ,redo log ,historical data ,trace creation ,tran-
sitive relations ,data model .
1 introduction
process mining heavily depends on event data. but to get proper data, it is either necessary (i)
to build a customized storage facility oneself or (ii) to rely on data that is already stored by exist-
ing it systems. the former approach requires extensive knowledge of the application domain
and a potentially hybrid technology landscape to create a facility that records all possible events
that are related to a pre-defined notion of a case. this is potentially costly and not very flexible.
the second approach requires the transformation of available data – which is not specif-
ically stored for process mining purposes – into an event log. approaches exist to accomplish
this for the data stored in and generated by sap systems [6, 13, 14], edi messages [4], and
erp databases in general [16]. some efforts for generalization have been made here, as can
be seen in [15]: xesame is a tool that allows transforming database records into events, traces
and logs, prior definition of the mappings between database elements and log concepts like
timestamp, case, activity and resource. nonetheless, the drawback of these approaches is that
they are restricted to the specific it system or data format that they are developed for.
artifact-centric approaches are more generic and also fit within the second strategy [3, 5,
8–11]. these techniques provide a way to get insights into the contents of a database, showing
the life-cycle of objects without presenting data convergence (one event is related to multiple
cases) and data divergence (several events of the same type are related to a single case) issues,
as happens in classical log extraction. however, both issues are not fully solved in the artifact
field. in fact, one could argue that the key problems are evaded by restricting the contents2 process mining on databases
of a single artifact, in order to avoid data divergence, and hiding data convergence in the
discovered relations between them.
the technique explained in this paper, based on the ideas introduced in [1], also relies on
the use of existing data. however, it exploits so-called redo logs that several data base man-
agement systems (dbmss), like oracle rdbms and mysql, maintain for data integrity
and recovery reasons. this source of data has the potential to create a full historic view on
what has happened during the handling a wide range of data objects. note that by simply
looking at the regular content of a dbms, one cannot see which events led to its current state.
fortunately, redo logs provide an opportunity to learn about the historical evolution of data on
the basis of a generic-purpose data source, exactly tuned to the purpose of the process mining
analysis one wishes to perform.
redo logs already contain a list of events, but the challenge is how to correlate these
events to create the traces one is interested in. in this paper, we explain how to create a trace
on the basis of a configurable concept of a case (i.e. the process instance), exploiting the
relations expressed in the data model of the dbms in question. the result is a log which
represents a specific point of view on the objects in the database, including the stages of their
historical evolution and the causal relations between them.
the analysis technique presented in this paper can be used, whenever redo logs are avail-
able, as an alternative to building a specific recording facility. the approach is also generic,
in the sense that it can be used to extract data from a technology that is used by a wide variety
of organizations. additionally, it is a viable alternative for artifact-centric approaches, since
it allows for a much richer behavior discovery due its incorporation of the data model to infer
causal relations between events. finally, the nature of the extracted logs (events with unique
ids and availability of data schemas) opens the door to developing new discovery techniques
that could exploit the additional information that databases provide, in order to solve data
convergence and divergence issues.
the paper is structured as follows. section 2 presents a walkthrough of the approach on a
simple example to explain the various phases of creating an event log. section 3 provides the
formalization of the important concepts that this work builds upon. section 4 describes the
tool that implements these concepts to generate an event log. section 5 provides an example
case to show how the technique can be flexibly applied to solve a range of business questions.
finally, section 6 presents the conclusion and future work.
2 walkthrough
the aim of this work is to analyze database redo logs, which can be seen as a list of mod-
ifications performed on the database content, so that we use these to generate event logs.
these event logs will be used to perform process mining analyses like process discovery,
conformance checking, and process enhancement. to explain the idea of redo log analysis,
a step by step walk-through using a simple case is performed in this section.
let us consider as an example a database that stores information on a portal for selling
concert tickets. at this point, we will focus on three tables only: customer ,booking , and ticket.
these tables contain information about the customers of the portal, the bookings made by
these customers, and the tickets being booked by them.process mining on databases 3
table 1: fragment of a redo log: each line corresponds to the occurrence of an event
#time + redo undo
op + table
12014-11-27
15:57:08.0 +
insert +
customerinsert into "sampledb".
"customer" ("id", "name",
"address", "birth_date")
values (’17299’,
’name1’, ’address1’,
to_date( ’01-aug-06’,
’dd-mon-rr’));delete from "sampledb".
"customer" where "id"
= ’17299’ and "name" =
’name1’ and "address"
= ’address1’ and
"birth_date" = to_date(
’01-aug-06’, ’dd-mon-rr’)
and rowid = ’1’;
22014-11-27
16:07:02.0 +
upda te +
customerupdate "sampledb".
"customer" set "name" =
’name2’ where "name" =
’name1’ and rowid = ’1’;update "sampledb".
"customer" set "name" =
’name1’ where "name" =
’name2’ and rowid = ’1’;
32014-11-27
16:07:16.0 +
insert +
bookinginsert into "sampledb".
"booking" ("id",
"customer_id") values
(’36846’, ’17299’);delete from "sampledb".
"booking" where "id" =
’36846’ and "customer_id"
= ’17299’ and rowid = ’2’;
42014-11-27
16:07:16.0 +
upda te+
ticketupdate "sampledb".
"ticket" set "booking_id"
= ’36846’ where
"booking_id" is null and
rowid = ’3’;update "sampledb".
"ticket" set "booking_id"
= null where "booking_id"
= ’36846’ and rowid = ’3’;
52014-11-27
16:07:17.0 +
insert +
bookinginsert into "sampledb".
"booking" ("id",
"customer_id") values
(’36876’, ’17299’);delete from "sampledb".
"booking" where "id" =
’36876’ and "customer_id"
= ’17299’ and rowid = ’4’;
62014-11-27
16:07:17.0 +
ticket +
upda teupdate "sampledb".
"ticket" set "id" =
’36876’ where "booking_id"
is null and rowid = ’5’;update "sampledb".
"ticket" set "id" = null
where "booking_id" =
’36876’ and rowid = ’5’;
2.1 event extraction
an example of a fragment of a redo log is shown in table 1. this fragment contains six
changes made to the records of the three tables. each of these events indicates (a) the time
at which it occurred, (b) the operation performed and on which table this was done, (c) an
sql sentence to redo the change, and (d) another sql sentence to undo it. we claim that
these basic fields provide enough information to reconstruct the state of the database at any
intermediate stage. also, they allow us to perform an in-depth analysis to detect patterns on
the behavior of the process or processes that rely on the support by this database.
the first thing we need to do is to transform each of the records in the redo log in table 1
to an event that we can manipulate. to do so, it is necessary to split the contents of redo and
undo sentences into different attributes. table 2 shows the attributes for each event extracted
from the redoandundo columns in table 1. the rows with the symbol =for “v alue after
event” indicate that the value for an “attribute name” did not change after the event. also,
the values between braces fgin the “v alue before event” column were extracted not from
the present event but from previous ones. this is, for instance, the case in the second event: it
is an update on the name of the customer, the record of which was already inserted in the first
event in the table. finally, the values between parentheses () identify the ones that could not be4 process mining on databases
table 2: fragment of a redo log: each line corresponds to the occurrence of an event
# attribute name value after event value before event c
1 customer:id 17299 - 4
customer:name name1 - 4
customer:address address1 - 4
customer:birth date 01-aug-06 - 4
rowid = 1 -
2 customer:id = f17299 g 1
customer:name name2 name1 4
customer:address = faddress1 g 1
customer:birth date = f01-aug-06 g 1
rowid = 1 -
3 booking:id 36846 - 4
booking:customer id 17299 - 4
rowid = 2 -
4 ticket:booking id 36846 null 3
ticket:id = (317132) 1
ticket:belongs to = (172935) 1
ticket:for concert = (1277) 1
rowid = 3 -
5 booking:id 36876 - 4
booking:customer id 17299 - 4
rowid = 4 -
6 ticket:booking id 36876 null 3
ticket:id = (317435) 1
ticket:belongs to = (173238) 1
ticket:for concert = (1277) 1
rowid = 5 -
extracted directly from the redo log, but only from the database content itself. this is because
those columns were not modified in that event, as is the case in events 4 and 6, where only
the field “ticket:booking id” is updated. therefore, the other values remain equal and it is not
necessary to specify them in the redoandundo sentences. it is still necessary to identify on
which row of the database the change must be applied. to do so, the redo log system provides a
rowid identifier to find it. in addition to it, an extra column chas been added, which encodes
a numeric vector for each event representing which columns had its value (1) not modified,
(2) changed from a value to null, (3) from null to a value or (4) inserted/updated.
2.2 exploiting the data model
after extracting the events from the redo log, the next step required is to obtain the data
model from the database. this will be a main ingredient used to correlate events. finding
these correlations will tell us which sets of events can be grouped into traces to finally build
an event log. obtaining the data model involves querying the tables, columns, and keys
defined in the database schema. figure 1 shows the extracted data model. for the selected
tables customer ,booking andticket , we see that two key relations exist between them: (a)
(booking customer fk, customer pk)and (b) (ticket booking fk, booking pk). this means that
we must use the first pair of keys (a) to correlate customer andbooking events, and the second
pair of keys (b) to correlate booking andticket events. both pairs (a) and (b) must be used
to correlate the events of the three tables.process mining on databases 5
fig. 1: database schema for the ticket selling example.
when using pairs of primary and foreign keys, we can consider the attributes referred
by them as equivalent for our purposes, i.e. relating to the same event. we will do so
to actually relate events that belong to different tables, but use different column names
(attributes in the events) to store the same values. therefore, attributes customer:id and
booking:customer idare considered to be equivalent, and the same can be said of the pair
booking:id andticket:booking id. then, using these equivalences and observing the value
after event column in table 2, we see that every event is related to at least one other event
by means of some attribute value. that is the case, for instance, for events 1 and 2, given that
they share the same value for the attribute customer:id . also, event 3 is related to events 1,
2 and 5, sharing the same value for the attributes customer:id andbooking:customer id, and
to event 4, sharing the same value for the attributes booking:id andticket:booking id. event
6 is related to event 5 by means of the attributes booking:id andticket:booking idas well. a
graph in which events are the vertexes and edges show relations between them would look
like the one in figure 2a. this graph helps to understand the structure of a trace we wish to
extract. what needs to be taken care of still is that it contains events of two different ticket
bookings (events 3-4 and events 5-6), which is behavior that we would like to see separately.
2.3 process instance identification
to decide which events go into which traces, it is necessary to define which view is desired
to obtain on a process. in this case, let us assume that it is interesting to see how tickets are
booked by customers. using relations fig 1.a and fig 1.b, we can say that individual traces
should contain behavior for the same user, booking and ticket: our casenotion. applying that
notion, we see that events 1 and 2 point to a single customer. events 3 and 4 point to a single
pair of booking and ticket, and still relate to the customer of events 1 and 2. however, events
5 and 6 represent a different pair of booking-ticket but do relate to the customer in events 1
and 2. therefore, this leads to two separated but not disjoint graphs in which events 1 and
2 are common, as observed in figure 2b. each of these graphs represent the structure of a
trace for a separate case in our event log.
from the initial change log cl =he1;e2;e3;e4;e5;e6iwe obtain two traces t1 =
(e1;e2;e3;e4)andt2 = (e1;e2;e5;e6)following the above reasoning. in this case, applying
a discovery algorithm to these two traces will result in a very simple sequential model. in
section 5, we will present samples of questions and answers to understand the process extend-
ing the same technique to a more extensive dataset and choosing different views on the data.6 process mining on databases
(a) connected graph of related events.
 (b) graphs for the two final traces.
fig. 2: traces as graphs of related events (events e1toe6refer to table 2.
this way we can find patterns and obtain interesting insights regarding the observed behavior.
what follows now is the formal basis that precisely captures the ideas discussed so far.
3 formalizations
the basic idea to use redo logs for the creation of an event log has been introduced in the
previous section. this section provides a formal description of the underlying notions. some
of the notation used in this part originate from [1]. first, we need to define the data model .
definition 1 (data model) assume v to be some universe of values. a data model is a tuple
dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, keyattr , refattr) such that
–c is a set of class names,
–a is a set of attribute names,
–classattr2c!p (a)is a function mapping each class onto a set of attribute names.
acis a shorthand denoting the set of attributes of class c2c, i.e.,ac=classattr (c),
–val2a!p (v)is a function mapping each attribute onto a set of values. va=val(a)
is a shorthand denoting the set of possible values of attribute a2a,
–pk is a set of primary key names,
–fk is a set of foreign key names,
–pk and fk are disjoint sets, that is pk\fk =;. to facilitate further definitions, the
shorthand k is introduced, which represents the set of all keys: k=pk[fk,
–keyclass2k!cis a function mapping each key name to a class. kcis a shorthand
denoting the set of keys of class c2csuch that kc=fk2kjkeyclass (k)=cg,
–keyrel2fk!pkis a function mapping each foreign key onto a primary key,
–keyattr2k!p (a)is a function mapping each key onto a set of attributes, such that
8k2k:keyattr (k)akeyclass (k),
–refattr2fka6!ais a function mapping each pair of a foreign key and an
attribute onto an attribute from the corresponding primary key. that is, 8k2fk :
8a; a02keyattr (k) : (refattr (k; a)2keyattr (keyrel (k))^(refattr (k; a) =
refattr (k;a0) =)a=a0).
definition 2 (notations) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, keyattr ,
refattr) be a data model.
–mdm=fmap2a6!vj8a2dom (map ):map (a)2vagis the set of mappings,
–odm=f(c;map )2cmdmjdom (map )=classattr (c)gis the set of all possible
objects of dm.
a data model defines the structure of objects in a database. such objects can belong to
different classes and varied relations can exist between them. a collection of possible objects
constitutes an object model .process mining on databases 7
definition 3 (object model) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel,
keyattr , refattr) be a data model. an object model of dm is a set omodmof objects.
uom(dm )=p 
odm
is the set of all object models of cm.
the objects in an object model must have a specific structure according to a certain data
model . also, some rules apply to ensure that the object model respects the rules stated by the
data model . this is covered by the notion of a valid object model .
definition 4 (valid object model) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel,
keyattr , refattr) be a data model. v omuom(dm )is the set of valid object models. we
say that om2v om if the following requirements hold:
–8(c;map )2om :(8k2kc\fk :(9(c0;map0)2om :
keyclass (keyrel (k))=c0^(8a2keyattr (k):
map (a)=map0(refattr (k;a))))), i.e., referenced objects must exist,
–8(c;map );(c;map0)2om :(8k2kc\pk :((8a2keyattr (k):
map (a)=map0(a)) =)map =map0)), i.e., pk and uk values must be unique.
different vendors offer database management systems (dbmss) like oracle rdbms, mi-
crosoft’s sql server, mysql, etc. all of them allow to store objects according to a specific
data model . the work of this paper focuses on the analysis of redo logs, being conceptually
independent of the specific implementation. these redo logs can contain information about
changes done either on the data or the structure of the database. we will focus here on the
changes on data , which include insertions of new objects, updates of objects, and deletions.
each of these changes represent an event, which has a type (definition 5) and mappings for
the value of objects before and after the change (definition 6). also, the combination of an
event and a specific time stamp represents an event occurrence (definition 7).
definition 5 (event types) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, key-
attr , refattr) be a data model and vom the set of valid object models. et=etadd[etupd[
etdelis the set of event types composed of the following pairwise disjoint sets:
–etadd=f(;c)jc2cgare the event types for adding objects,
–etupd=f(;c)jc2cgare the event types for updating objects,
–etdel=f(	;c)jc2cgare the event types for deleting objects.
definition 6 (events) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, keyattr ,
refattr) be a data model, vom the set of valid object models and map null2føg!va
function with the empty set as domain. e=eadd[eupd[edelis the set of events composed
of the following pairwise disjoint sets:
–eadd=f((;c);map old;map new))j(c;map new)2odm^map old=map nullg
–eupd=f((;c);map old;map new))j(c;map old)2odm^(c;map new)2odmg
–edel=f((	;c);map old;map new))j(c;map old)2odm^map new=map nullg
definition 7 (event occurrence, change log) let dm = (c, a, classattr , val, pk, fk,
keyclass, keyrel, keyattr , refattr) be a data model, vom the set of valid object models and
e the set of events. assume some universe of timestamps ts. eo=(e;ts)2etsis an event
occurrence. eo(dm;e )=etsis the set of all possible event occurrences. a change log
cl=heo1;eo2;:::;eo niis a sequence of event occurrences such that time is non-decreasing,
i.e.,cl=heo1;eo2;:::;eo ni2(eo(dm;e ))andtsitsjfor any eoi= (ei;tsi)and
eoj=(ej;tsj)with 1i<jn.8 process mining on databases
definition 8 (effect of an event) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel,
keyattr , refattr) be a data model, vom the set of valid object models and e the set of
events. for any two object models om 12v om andom 22v om and event occurrence
eo=(((op;c);map old;map new);ts)2eo(dm;e ), we denote om 1eo!om 2if and only if
om 2=f(d;map )2om 1jmap6=map old_op=g[f (c;map new)jop6=	g.
event e is permissible in object model om, notation ome!, if and only if there exists
an om’ such that ome!om0. if this is not the case, we denote ome
6!, i.e., e is not
permissible in om. if an event is not permissible, it will fail and the object model will remain
unchanged. relatione)denotes the effect of event e. it is the smallest relation such that (a)
ome)om0ifome!om0and (b) ome)om ifome
6!.
when, in definition 8, we say that om 1eo!om 2, it means that om 2must contain (1) all
the objects in om 1except the one that the event occurrence eorefers to, and (2), the object
inserted if eois an insertion or the modified object if it is an update. if eois a deletion, the
object is not included in om 2.
definition 9 (effect of a change log) let dm = (c, a, classattr , val, pk, fk, keyclass,
keyrel, keyattr , refattr) be a data model, vom the set of valid object models, e the set of events
andom 02v om the initial valid object model. let cl=he1;e2;:::;eni2(eo(dm;e ))
be a change log. there exist object models om 1; om 2; :::; om n2v om such that
om 0e1)om 1e2)om 2:::en)omn. hence, change log cl results in object model omn
when starting in om 0. this is denoted by om 0cl)omn.
definitions 1 to 9 establish the basis to understand data models, events and change logs,
among other concepts. however, a mechanism to relate events to each other to build traces is
still missing. for that purpose, and as one of the main contributions of this paper, the concept
oftrace id pattern is introduced. then, subsequent definitions will be presented to show the
trace building technique.
in a classical approach, the notion of case id is given by an attribute common to all the events
in a trace. if traces do not exist yet, they can be created grouping events by the value of
the selected attribute. however, in our setting, we have a collection of events of different
classes with disjoint attribute sets. this means that it will be impossible to find a single
common attribute to be used as case id. a trace id pattern substitutes the idea of a case id
attribute for a set of attributes and keys. by its use, it becomes possible to find a common
set of attributes between events of different classes using foreign-primary key relations. this
relations establish the equivalence between pairs of attributes. the example presented in
section 2.2 illustrates this idea using the pair of keys customer pkandbooking customer fkto
set the equivalence between the attributes customer:id andbooking:customer id. each trace
id pattern configures a view of a process to focus on, determining also which is the central
element of the view, called root. this rootelement will determine the start event for each
trace and will allow, in further steps, to build traces according to such a view.
definition 10 (trace id pattern) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel,
keyattr , refattr) be a data model. a trace id pattern on dm is a tuple tpdm=(tpa;tpk;
root )such that
–tpaais a subset of the attributes in the data model,
–tpkkis a subset of the keys in the data model,process mining on databases 9
–root2tpk is a key of the data model.
to find the equivalence between different attribute names , we define a canonical mapping
(definition 11), i.e., a way to assign a common name to all the attributes linked, directly or
transitively, through foreign-primary key relations. to show a simple example, the canonical
mapping of the attribute booking:customer idwould be customer:id since both are linked
through the foreign-primary pair of keys booking customer fkandcustomer pk.
definition 11 (canonical mapping) let dm = (c, a, classattr , val, pk, fk, keyclass,
keyrel, keyattr , refattr) be a data model and tp = (tpa, tpk, root) a trace id pattern
on dm. a canonical mapping canon2a!ais a function mapping each attribute to its
canonical attribute such that:
canon (a)=8
<
:a ifa62s
k2fkkeyattr (k);
canon (refattr (fk;a )) ifa2s
k2fkkeyattr (k):
withfk2fk2fkja2keyattr (k)g
the combination of a trace id pattern and the canonical mapping results in the canonical
pattern attribute set , i.e., the set of canonical attributes for each of the elements (keys and
attributes) configured in the trace id pattern . this allows us to obtain a minimum set of
attributes to identify traces, avoiding the presence of attributes which, despite being different,
map to the same canonical form.
definition 12 (canonical pattern attribute set) let dm = (c, a, classattr , val, pk, fk,
keyclass, keyrel, keyattr , refattr) be a data model and tp = (tpa, tpk, root) a trace id
pattern on dm. the canonical pattern attribute set of tp is a set cpastp=fcanon (a)j
a2tpa[(s
k2tpkkeyattr (k))g.
definition 13 (notations ii) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, key-
attr , refattr) be a data model, ts some universe of timestamps and eo= (((op;c);map old;
map new);ts)an event occurrence. we define the following shorthands for event occurrences:
–eventclass (eo)=cdenotes the class of an event occurrence,
–time (eo)=tsdenotes the timestamp of an event occurrence,
–mapv al eodenotes the right mapping function to obtain the values of the event in an
event occurrence such that
mapv al eo=
map newifop2f;g;
map oldifop=	
each trace we want to build represents a process instance. however, a process instance, which
is formed by event occurrences , needs to comply with some rules to guarantee that they
actually represent a meaningful and valid trace.the first thing we need to accomplish is to
build a set of traces that do not contain too much behavior according to the selected view
(trace id pattern ). we will call this the set of well-formed traces .
definition 14 (traces, well-formed traces) let dm = (c, a, classattr , val, pk, fk, key-
class, keyrel, keyattr , refattr) be a data model, tpdm=(tpa;tpk;
root )a trace id pattern on dm, cl a change log of event occurrences and t=ft2
p(feoij1ing)gthe set of possible traces on that change log.
wftcltis the set of well-formed traces such that wftcl=ft2tj(8eoi;eoj2t:
8ai2aeventclass (eoi);aj2aeventclass (eoj):(ai2dom (mapv al eoi)^10 process mining on databases
aj2dom (mapv al eoj)^fcanon (ai);canon (aj)gcpastp^canon (ai)=canon (aj)
=)mapv al eoi(ai)=mapv al eoj(aj)
g, i.e., the traces that do not contain event occur-
rences with different values for an attribute of which its canonical form is in the canonical
pattern attribute set.
the same way that a trace id pattern configures a view of the process, each process instance
will be represented by a unique trace id . this concept (definition 15) allows us to distinguish
different traces. these traces aggregate events holding relations that can exist even between
events of different classes .
definition 15 (trace id) let dm = (c, a, classattr , val, pk, fk, keyclass, keyrel, keyattr ,
refattr) be a data model, tpdm = (tpa;tpk;root )a trace id pattern on dm and
t2wft a well-formed trace. tidtp
tcpastpvis a set of pairs attribute-value for
a trace t according to a trace id pattern tp such that tidtp
t=f(a;v)2cpastpvj
eo2t^a=canon (b)^b2dom (mapv al eo)^v=mapv al eo(b)g.
the second goal of the trace building process is to avoid the creation of traces containing
events that do not belong to the same instance, according to the selected view ( trace id pattern ).
definition 17 sets such rules. some of these rules require a way to find connections between
events. such connections or properties are stated in definition 16 as trace id properties .
definition 16 (trace id properties) let dm = (c, a, classattr , val, pk, fk, keyclass,
keyrel, keyattr , refattr) be a data model, tpdm=(tpa;tpk;
root )a trace id pattern on dm, ft; t0gwft are two well-formed traces and
ftidtp
t;tidtp
t0gtheir corresponding trace ids. we define the following properties
–tidtp
ttidtp
t0()9 (a;v)2cpastpv: (a;v)2tidtp
t^(a;v)2tidtp
t0,
i.e.,tidtp
tandtidtp
t0are related if and only if an attribute exists with the same
value in both trace ids,
–tidtp
t=tidtp
t0()8 ((a;v);(a0;v0))2tidtp
ttidtp
t0:(a=a0^v=v0)_(a6=
a0), i.e.,tidtp
tandtidtp
t0are compatible if and only if for each common attribute
the value is the same in both trace ids,
–tidtp
t. /tidtp
t0()tidtp
ttidtp
t0^tidtp
t=tidtp
t0, i.e.,tidtp
tand
tidtp
t0are linkable if and only if they are compatible and related,
–tidtp
ttidtp
t0()tidtp
ttidtp
t0, i.e.,tidtp
tis a subtrace of tidtp
t0if
and only if all the attributes in tidtp
tare contained in tidtp
t0with the same value,
definition 17 (valid traces,event logs) let dm = (c, a, classattr , val, pk, fk, keyclass,
keyrel, keyattr , refattr) be a data model, tpdm=(tpa;tpk;root )a trace id pattern
on dm, cl a change log of event occurrences, wftclthe set of well-formed traces on
that change log and rootcancpastpis the set of canonical attributes of root such
thatrootcan =fb2cpastpj9a2keyattr (root ) :canon (a) =bg. we define
v t(pt;cl )wftclas the set of valid traces for tp such that v t(pt;cl ) =ft2
wftclj8eo2t: ((8c2rootcan :9(c;v)2tidtp
feog)^(@eo02t:time (eo0)<
time (eo)))_(tidtp
feog. /tidtp
feo02tjtime (eo0)<time (eo)g))g:
finally, we define an event log lptas the maximum subset of v t(pt;cl )such that
8t;t02lpt:(tidtp
ttidtp
t0=)t=t0), i.e., the set of valid traces that does not contain
any pair in which one of the traces is a subtrace of the other .process mining on databases 11
fig. 3: architecture of the redo log inspector tool.
in the end, we guarantee that the resulting event log contains the minimum set of traces with
the maximum behavior ( lpt) for a certain view trace id pattern tp). the traces in this event
logstart with an event containing values for the configured rootelement of the tp. also, each
of these traces contain events that are directly or transitively related ( ) and compatible (=).
4 implementation
the techniques presented in this paper allow for the extraction of events from any type of
rdbms with redo logs. our implementation, however, is specific for oracle technology. we
can obtain data models directly from an oracle dbms, which makes it possible to design the
trace id pattern (definition 10) needed to generate an event log in accordance to definition 17
from an event collection. the redo log inspector3tool that we developed to demonstrate
the feasibility of our ideas, fully implements the approach described in the previous sections
and provides a user interface to control all the aspects of the analysis.
the redo log inspector is composed of different components (see figure 3). it uses a
oracle connector component to communicate with the oracle database and with the oracle
log miner functions. this component is used by the redo log extractor to generate an event
collection from the desired tables. the data model extractor also makes use of the oracle
connector to automatically obtain a data model . this last element is used by the trace id
pattern editor to design the desired trace id pattern . then, the three objects ( event collection ,
data model andtrace id pattern ) are used by the log splitter to compute the traces to form
aevent log . finally, the event log can be analyzed with existing process mining tools, such
as prom4[2] and disco5. figure 4 shows a screenshot of the tool while splitting an event
collection into traces using a trace id pattern.
5 demonstration
the database we will use to demonstrate our approach is part of an imaginary portal for
selling concert tickets. as stated, the database stores information about customers, bookings,
and tickets. in addition, the concert venues are represented in the database, along with the
collection of seats they offer. each concert is also stored in it, with the list of bands performing.
figure 1 shows the data schema of the database. it is composed of eight different tables with
several columns each, and a number of relations between them:
–concerts: date and start time of the concert and the venue in which it will take place.
3redo log inspector v1.0: http://www.win.tue.nl/ ˜egonzale/projects/rlpm/
4prom: http://www.promtools.org
5disco: http://fluxicon.com/disco/12 process mining on databases
fig. 4: screenshot of the tool while splitting an event collection into traces.
–band: name and address of bands, which could perform in different concerts.
–band playing: relates bands to concerts, indicating which ones will perform.
–hall: details of the venues in which concerts can take place.
–seat: each of the seats available in a venue.
–ticket: the product being sold to users. they link a concert to a specific seat in a venue.
also, they refer to a booking if they have been acquired.
–booking: objects created by customers when buying tickets.
–customer: address, name and birth date. each entry represents an user account.
tables may be linked to other tables by means of foreign keys. for instance, the table ticket
contains a foreign key named ticket concert fk: it associates the column ticket:for concert
to the primary key of table concert , specifically the column concert:id . this field relates a
ticket to a specific concert in an:1relation, which means that a ticket must refer to a concert ,
but a concert can be related to many tickets .
in this database, like in many other settings, only the last state of the process is stored.
this means we are able to answer questions of the following types:
1. how many concerts have been organized in the past?
2. which venue has hosted most events in the last year?
3. what is the average number of tickets bought per customer in a month?
however, we would also like to find answers to other kinds of questions as well. in particular,
we wish to pose questions that do not focus so much on the data facts, but on the underlying
process that created and modified the data. some of these questions are:
q1. which are the steps followed by a user to book a ticket?
q2. do customers book tickets before all the bands were confirmed?
q3. do bands ever cancel their performances in concerts?
q4. are venues being reserved before or after the bands have confirmed their performance?
it is evident that in order to find answers the inclusion of additional fields in the data
schema would have been helpful. that data could be recorded explicitly in the database,
adding timestamps to rows in every table and recording historical data of operations. it wouldprocess mining on databases 13
be the equivalent of explicitly recording a login the database. however, not every system has
been designed to exploit the benefits of data and process mining. in other words, there are
situations where we cannot rely on explicitly recorded logs of sorted events.
for instance, the fourth question could be answered querying the database only if the
timestamps of execution of every operation are being recorded. however, figure 1 shows
that such timestamps are not present in the data schema of the proposed example. something
similar applies to the third question, which inquires if bands can cancel their performance
at concerts. this requires the database to keep record of all the bands that were to perform in
concerts and also the ones that canceled, for instance, by means of a status flag. unfortunately
the data schema does not store such information. what happens in case of a cancellation
is that the corresponding entry will be removed from the table band playing . this makes it
impossible to know afterward which bands were once scheduled to perform but not anymore.
the focus of this section is to use database redo logs to answer the proposed questions
using the technique presented in this paper. to do so, a dataset6of 8512 events has been
generated based on a simulated environment interacting with the oracle database presented
in figure 1. cpn tools [12] was used to model the creation of concerts and customers, the
selling of tickets, and other operations on the elements of the database. the activities of such
a process connect through a socket to a java application managing the communication with
an oracle database. this way the environment of the system is simulated. this last one also
generated the set of redo log files used to extract the events in our dataset. in the remainder
the four questions are answered step-by-step.
5.1 which are the steps followed by a user to book a ticket?
in order to answer this first question, we need to obtain the process describing the customer
actions from the moment the selling portal is reached until the moment the ticket is sold. to
do so, a log that contains traces showing that behavior has to be generated. this question will
be answered in the next section in conjunction with the second one for the sake of brevity.
5.2 could customers book tickets before all the bands were confirmed?
to answer this second question, two parts of the system must be involved: the ticket booking
by customers and the concert organizing parts. for the first part, we can assume that the tables
customer ,booking andticket must be involved in the process. using the data schema in fig-
ure 1, we see that tables customer andbooking are linked by means of the pair of primary and
foreign keys customer pkandbooking customer fk(figure 1.a). also, the tables booking and
ticket are linked by means of the pair of keys booking pkandticket booking fk(figure 1.b).
now, it is necessary to complete it with the concert organizing part. to do so, we have to
relate each ticket to the concert it belongs to, and the later one to the bands playing. observing
figure 1 we see that there is a relation between tables ticket andconcert by means of the pair
of keys ticket concert fkandconcert pk(figure 1.c). also, tables concert andband playing
share a relation by means of keys concert pkandbpconcert fk(figure 1.d). therefore, we
should add these three keys to the trace id pattern tp=(tpa;tpk;root ), resulting
in the following configuration:
–tpa =ø,
6http://www.win.tue.nl/ ˜egonzale/projects/rlpm/datasets/
ticket-selling-dataset.zip14 process mining on databases
fig. 5: model of the ticket purchase and part of the concert organizing process.
–tpk =fcustomer pk;booking customer fk;booking pk;
ticket booking fk;concert pk;ticket concert fk;bp concert fkg,
–root =customer pk.
given that we want to cover the process from the moment a customer enters the system
until the ticket is bought, it makes sense to select as root element of our trace id pattern the
primary key customer pkin table customer . in other words, the customer is the case we want
to follow through the process.
after this, the splitting process that follows generates a log with 149 cases. in this
case the inductive miner [7] is used, and the log is replayed on it. then, the activity
44+band playing+insert is highlighted, which filters the log to show statistics using
only the traces that contain the selected activity. the result is the annotated model in figure 5.
in it we observe that an insertion in the customer table can be followed either by modifications
on it, or by an insertion in the booking table. an update in the ticket table can only be preceded
by abooking creation. this means that, according to the evidence, the process followed by a
customer to buy a ticket is as follows: (1) create an account, which results in the insertion of a
record in the table customer . (2) create a booking, inserting a record in the table booking . (3)
buy the selected ticket, updating the booking idfield in the desired record in the table ticket .
it can be also observed that modifications on the details of a customer profile can be made
at almost any point in time, but not between the insertion of a booking and the update of a
ticket. this suggests that both steps are performed automatically and in a strict sequence (q1).
to answer the second question it is interesting to see that insertions in the table band playing
can happen at any moment, before or after tickets are booked. this means that new bands
are added to the concert not only after a concert is created, but also after a ticket has been
booked. this does not require a causal relation in the sense that bands are added because a
ticket is booked. however, it shows that both activities can happen in that order, answering
the second proposed question (q2).
5.3 do bands ever cancel their performance in concerts?
to find out the answer to the third question, we should look at the band playing table and see
if any entry has been removed. this would not be possible when just inspecting the current
content of the database. fortunately, thanks to the redo logs, we can reconstruct the life-cycle
of concerts. for the sake of brevity, the answer will be provided using the same experiment
to answer the fourth question.
5.4 are venues being reserved before or after the bands have confirmed their
performance?
to solve the fourth question, we need to see how halls are being assigned to concerts at
the same time that bands are being confirmed to perform on concerts. to do so, we haveprocess mining on databases 15
fig. 6: model showing the process of organizing a concert.
to focus on tables concert ,hallandband playing . observing the data schema in figure 1,
we see that tables concert andhallare linked by means of the pair of primary and foreign
keysconcert hallfkandhallpk(figure 1.e). also, there is a link between the tables concert
andband playing by means of the pair of keys concert pkandbpconcert fk(figure 1.d).
therefore, we will use the four of them in our trace id pattern tp=(tpa;tpk;root ):
–tpa =ø,
–tpk =fhallpk;concert hallfk;concert pk;bp concert fkg,
–root =concert pk.
knowing that concerts are the main object in this view, concert pkwill be selected as the
root element. splitting the dataset using these settings generates a log with 18 traces. using
the inductive miner and replaying the log, the annotated model in figure 6 is obtained. it is
evident that no deletions of records on table band playing have been recorded. therefore, as
far as we can tell, none of the bands ever canceled their performance within a concert (q3).
we can also see that hallcolumn in concerts can be updated before, after, or at the same time
that bands confirm their performance in concerts. therefore, there are no restrictions on the
order of both events (q4).
6 conclusion
this work proposes to systematically use database redo logs as a new source of event data.
the benefits include the existence of a data model and the historical view we obtain from the
database. this represents a considerable innovation compared to the analysis of plain database
content. to make sense of the events and obtain logs, the new concepts of trace id pattern
andtrace id have been introduced, which enable the discovery of transitive relations between
data objects and the causal dependencies of the data modifications. an innovative approach
to group the events in traces has been provided as well. also, the feasibility of the approach
has been shown in the form of a prototype. this prototype has been applied on a synthetic
dataset to demonstrate its potential usefulness to answer a range of business questions that
could not be directly answered by querying the database.
the technique is characterized by some drawbacks. first, the splitting algorithm produces
a log where the same event may appear in different traces. this causes the existing process
discovery algorithms to generate statistics that must be interpreted from the view we selected
on the process. this is due to the fact that they consider events to be unique and only present
in a single trace, when, in our case, they can be repeated and be counted more than once. if not
interpreted correctly, the numbers could lead to the wrong conclusions. also, the algorithm
produces a number of traces that in some cases exceed the number of original events. these
traces need to be analyzed by the discovery algorithms to produce models. this means that, in
the end, we are going through the log many times. it would be useful to reduce the analysis to16 process mining on databases
a single pass through the event collection to compute the structures needed by the discovery
algorithms, e.g. a directly-follows graph . the analysis of real-life event logs is an obvious
next task. performing a case study on non-artificial redo logs will, hopefully, support the
value of the techniques presented in this paper.
references
1.van der aalst, w.m.p .: extracting event data from databases to unleash process mining. in: vom
brocke, j., schmiedel, t. (eds.) bpm - driving innovation in a digital world, pp. 105–128.
management for professionals, springer international publishing (2015)
2.van der aalst, w.m.p ., van dongen, b.f., g ¨unther, c.w., rozinat, a., v erbeek, e., weijters, t.:
prom: the process mining toolkit. in: proceedings of the bpm demonstration track (bpmdemos
2009), ulm, germany, september 8, 2009 (2009)
3.cohn, d., hull, r.: business artifacts: a data-centric approach to modeling business operations
and processes. bulletin of the ieee computer society technical committee on data engineering
32(3), 3–9 (2009)
4.engel, r., van der aalst, w.m.p ., zapletal, m., pichler, c., werthner, h.: mining inter-organizational
business process models from edi messages: a case study from the automotive sector. in: advanced
information systems engineering. pp. 222–237. springer (2012)
5.fahland, d., de leoni, m., v an dongen, b.f., van der aalst, w.m.p .: behavioral conformance
of artifact-centric process models. in: business information systems. pp. 37–49. springer (2011)
6.ingvaldsen, j.e., gulla, j.a.: preprocessing support for large scale process mining of sap
transactions. in: business process management workshops. pp. 30–41. springer (2008)
7.leemans, s.j., fahland, d., van der aalst, w.m.p .: discovering block-structured process models
from event logs-a constructive approach. in: application and theory of petri nets and concurrency,
pp. 311–329. springer (2013)
8.lu, x.: artifact-centric log extraction and process discovery. master’s thesis, technische
universiteit eindhoven, the netherlands (2013), http://repository.tue.nl/761324
9.mueller-wickop, n., schultz, m.: erp event log preprocessing: timestamps vs. accounting logic.
in: design science at the intersection of physical and virtual design, lecture notes in computer
science, vol. 7939, pp. 105–119. springer berlin heidelberg (2013)
10.nigam, a., caswell, n.s.: business artifacts: an approach to operational specification. ibm
systems journal 42(3), 428–445 (2003)
11.nooijen, e.h., van dongen, b.f., fahland, d.: automatic discovery of data-centric and
artifact-centric processes. in: bpm workshops. pp. 316–327. springer (2013)
12.ratzer, a.v ., wells, l., lassen, h.m., laursen, m., qvortrup, j.f., stissing, m.s., westergaard,
m., christensen, s., jensen, k.: cpn tools for editing, simulating, and analysing coloured petri
nets. in: applications and theory of petri nets 2003, pp. 450–462. springer (2003)
13.roest, a.: a practitioner’s guide for process mining on erp systems : the case of sap
order to cash. master’s thesis, technische universiteit eindhoven, the netherlands (2012),
http://repository.tue.nl/748077
14.segers, i.: investigating the application of process mining for auditing pur-
poses. master’s thesis, technische universiteit eindhoven, the netherlands (2007),
http://repository.tue.nl/630348
15.v erbeek, h., buijs, j.c., v an dongen, b.f., van der aalst, w.m.p .: xes, xesame, and prom 6.
in: information systems evolution, pp. 60–75. springer (2011)
16.yano, k., nomura, y ., kanai, t.: a practical approach to automated business process discovery.
in: enterprise distributed object computing conference workshops (edocw), 2013 17th ieee
international. pp. 53–62 (sept 2013)