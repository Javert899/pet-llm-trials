1  
quality metrics for  
business process models 
irene vanderfeesten1, jorge cardoso2, jan 
mendling3, hajo a. reijers1, wil van der aalst1 
1 technische universiteit eindhoven, the netherlands;  2 university of madeira, por-
tugal; 3 vienna university of econ omics and business, austria. 
summary  
in the area of software engineering, quality metrics have shown their importance for 
good programming practices and software de signs. a design developed by the help 
of these metrics (e.g. coupling, cohesion, complexity, modularity and size) as guid-
ing principals is likely to be less error- prone, easy to understand, maintain, and 
manage, and is more efficient. several re searchers already identified similarities 
between software programs and business process designs and recognized the po-
tential of quality metrics in business process management (cardoso, mendling, 
neuman & reijers, 2006; gruhn & laue, 2006;  latva-koivisto, 2001). this chapter 
elaborates on the importance of quality metrics for business process modeling. it 
presents a classification and an overview of current business process metrics and it 
gives an example of the implementation of these metrics using the prom tool. prom is an analysis tool, freely available, that  can be used to study process models im-
plemented in more than eight languages. 
introduction  
key in many instances of innovation is th e transfer of information and understand-
ing developed in one discipline to the other (kostoff, 1999). a prime example is 
workflow management itself, a technology based on the importation of process 
models from manufacturing operations into administrative work. in this chapter we 
embark on further opportunities for knowle dge transfer to the field of process mod-
eling and workflow management, in particular from software engineering. 
in the mid-1960’s software engineers star ted to use metrics to characterize the 
properties of their code. simple count metr ics were designed to be applied to pro-
grams written in languages such as c++, java, fortran, etc. various of these metrics provided a good analysis mechanis m to assess the quality of the software 
program design. since there is a strong analogy between software programs and 
business processes, as previously argued  in (reijers & vanderfeesten, 2004; guce-
glioglu & demiros, 2005), we believe that so ftware metrics, such as coupling, cohe-
sion, and complexity, can be revised and adapted to analyze and study  a business 
process' characteristics. 
a business process model, regardless whet her it is modeled in e.g. bpel, epc, 
bpmn or petri nets, exhibits many similarities with traditional software programs. 
a software program is usually partitioned into modules or functions (i.e. activities), 
which take in a group of inputs and prov ide some output. similar to this composi-
tional structure, a business process model co nsists of activities, each of which con-
tains smaller steps (operations) on elementary data elements (see table 1). more-
over, just like the interactions between modules and functions in a software pro-quality metrics for business process models  
2 gram are precisely specified, the order of activity execution in a process model is 
predefined using logic operators such as sequence, splits and joins.  
in this chapter we elaborate on the transf er and adaptation of quality metrics from 
the software engineering domain to busine ss processes. first, we introduce a gen-
eral outline on software engineering metrics. next, an overview is given of the cur-
rent status in business process metrics, ad opting a widely used classification from 
software engineering. finally, we elaborate on some practical applications of these 
business process metrics and present our conc lusions and a look into the future of 
business process metrics. 
table 1: similarities between software programs and business processes 
software program business process 
module/class activity 
method/function operation 
variable/constant data element 
metrics in the software engineering domain  
in the area of software engineering a wide variety of software quality metrics has been developed. the main purpose of softwa re quality metrics is to obtain program 
designs that are better structured. some of the most important advantages of a 
structured design are, as pointed out in  (conte, dunsmore & shen, 1986), that (i) 
the overall program logic is easier to understand for both the programmers and the 
users and (ii) the identification of the modules is easier, since different functions are 
performed by different modules, which ma kes the maintenance of the software pro-
gram easier. according to (conte, duns more & shen, 1986; shepperd 1993; troy & 
zweben 1981) the quality of a design is  related to five design principles: 
coupling —coupling is measured by the numb er of interconnections among mod-
ules. coupling is a measure for the strength of association established by the 
interconnections from one module of a design to another. the degree of cou-
pling depends on how complicated the co nnections are and on the type of con-
nections. it is hypothesized that prog rams with a high coupling will contain 
more errors than programs with lower coupling. 
cohesion —cohesion is a measure of the relationships of the elements within a 
module. it is also called module strength. it is hypothesized that programs with 
low cohesion will contain more errors than programs with higher cohesion. 
complexity —a design should be as simple as possible. design complexity grows 
as the number of control constructs grow s, and also as the size—in number of 
modules—grows. the hypothesis is that the higher the design complexity the more errors the design will contain. 
modularity —the degree of modularization affe cts the quality of a design. over-
modularization is as undesirable as un der-modularization. the hypothesis is 
that low modularity generally relates to more errors than higher modularity. 
size—a design that exhibits large modules or a deep nesting is considered unde-
sirable. it is hypothesized that programs of large size will contain more errors than smaller programs. 
in literature coupling and cohesion are generally considered to be the most impor-
tant metrics for software quality, although researchers do not agree on their relative 
importance. in (shepperd 1993; troy & zweb en 1981), results of analyses are pre-
sented that indicate that coupling is the most influential of the design principles 
under consideration. however, in (myers , 1978) cohesion and coupling are consid-quality metrics for business process models  
3 ered as equally important. also, complexity and size are seen as a good quality met-
ric for software program de signs (troy & zweben, 1981). 
in addition, various researchers carried out studies to gather empirical evidence 
that quality metrics do indeed improve the quality of a software design. bieman and 
kang, in particular, have shown examples how cohesion metrics can be used to 
restructure a software design (bieman & kang, 1998; kang & bieman, 1996; kang 
& bieman 1999). also, in (selby & basili, 1991) evidence is presented that low cou-
pling and high strength (cohesion) are de sirable. by calculating coupling/strength 
ratios of a number of routines in a softwa re library tool it was found that routines 
with low coupling/strength ration had significantly more errors than routines with 
high coupling/strength ratio. in (card, church & agresti, 1986), a number of for-
tran modules from a national aeronauti cs and space administration project were 
examined. fifty percent of the high strength (high cohesion) modules were fault 
free, whereas only 18 percent of low strength modules were fault free. no relation-
ship was observed between fault rate and co upling in this study. finally, (shen, yu, 
thebaut & paulsen, 1985) have discovered th at, based on their analysis of three 
software program products and their error histories, simple metrics such as the 
amount of data (size) and the structural complexity of programs may be useful in 
identifying those modules most  likely to contain errors. 
quality metrics in the workflow domain  
because of the similarities between software programs and workflow processes, 
explained in the introduction, the applicatio n of similar quality metrics to the work-
flow field is worth investigation. we have  conducted a literature review on business 
process metrics and found that, despite the vast literature on software engineering 
metrics, there is not much research on business process metrics available yet. al-
though some researchers suggest using soft ware metrics to evaluate business proc-
ess designs (baresi et al, 1999), the number of publications on concrete metrics and 
applications in the business process domain is still small and only of a very recent 
date. in this section, the existing "state-of-the-art" in business process metrics is 
summarized using the same classificati on as in software engineering. 
coupling 
coupling measures the number of interconnections among the modules of the 
model. as such, it is highly related to de gree and density metrics in (social) network 
analysis (see e.g. brandes and erlebach , 2005). the application of these measure-
ments is straight forward if the process model is available in a graph-based nota-
tion. the average degree, also called coeffi cient of connectivity in (latva-koivisto, 
2001), refers to the average number of connections that a node has with other nodes of the process graph. in contrast to that, the density metric relates the num-
ber of available connections to the number of maximum connections for the given 
number of nodes. the density metric was used in a survey as a predictor for errors in business process models in (mendling,  2006) with some mixed results. while 
there was actually a connection between density and errors, the explanatory power 
of this metric was found to be limited. an explanation for that might be that density 
is difficult to compare for models of di fferent size since the maximum number of 
connections grows quadratic. it appears that  the average degree of nodes in a busi-
ness process could be better suited to serve as a quality metric. 
moreover, reijers and vanderfeesten (reijers & vanderfeesten, 2004) also developed 
a similar coupling metric counting the overlap of data elements for each pair of ac-tivities using a static description of the product represented by a product data 
model (pdm) (van der aalst, 1999; reijers , 2003; reijers, limam mansar, & van der quality metrics for business process models  
4 aalst, 2003). two activities are 'coupled' if they contain one or more common data 
elements. to calculate the coupling value for a business process the activities are 
selected pairwise and the number of 'coupled ' pairs is counted. finally, the mean is 
determined based on the total number of activities. the outcome always lies be-tween 0 and 1. this data oriented coupling metric is complemented with a cohesion 
metric, which is described  in the next section. 
however, all of these coupling metrics do not yet deal with how complicated the 
connections are as suggested in the defini tion of coupling. a weighted coupling met-
ric, with different weights for the xor, or , and and connectors, is part of our cur-
rent research. 
cohesion 
cohesion measures the coherence within the parts of the model. so far, there is 
only one paper on a cohesion metric for business processes available. reijers and 
vanderfeesten (reijers & vanderfeesten,  2004) developed a cohesion metric for 
workflow processes which looks at the coherence within the activities of the process 
model. similar to their coupling metric this  cohesion metric also focuses on the in-
formation processing in the process and takes a data oriented view. for each activ-ity in the process model the total cohesion is calculated by multiplying the informa-
tion cohesion and the relation cohesion of the activity. finally, a cohesion value for 
the whole process is determined by taking the mean of all activity cohesion values 
(i.e. adding up all cohesion values and dividi ng it by the number of activities).  the 
value for this cohesion metric always lies between 0 and 1. this data oriented cohe-
sion metric is complemented with a coup ling metric, which is described in the pre-
vious section. the combination of these two metrics, as proposed by (selby & basili, 
1991), gives a coupling-cohesion ratio wh ich supports the business process de-
signer to select the best (low coupling, hi gh cohesion) design among several alterna-
tives (reijers & vanderfeesten, 2004). 
complexity 
complexity measures the simpleness and understandability of a design. in this 
area most of the research on business process metrics has been done (cardoso, 
mendling, neumann & reije rs, 2006; gruhn & laue, 2006, latva-koivisto, 2001). 
for instance, both (gruhn & laue, 2006)  and (cardoso, mendling, neumann & rei-
jers, 2006) consider the adaptation of mc cabe's cyclometric number as a complex-
ity metric for business processes. this co mplexity metric directly measures the 
number of linearly independent paths through a program’s source code. in prac-tice, the industry interpretation of mccabe's cyclomatic complexity thresholds are 
the following (frappier, matwin, & mili, 1994) : from 1 to 10, the program is simple; 
from 11 to 20, it is slightly complex; from 21 to 50, it is complex; and above 50 it is 
untestable. 
in (cardoso, 2005a) the control-flow complex ity (cfc) metric is defined, which is 
also derived from software engineering. the cfc metric evaluates the complexity introduced in a process by the presence of xor-split, or-split, and and-split con-
structs. for xor-splits, the control-flow complexity is simply the fan-out of the 
split, i.e. cfc
xor-split (a)= fan-out(a). for or-splits, the control-flow complexity is 2n-1, 
where n is the fan-out of the split. i.e. cfc or-split(a)= 2fan-out(a) -1. for an and-split, the 
complexity is simply 1, i.e. cfc and-split (a)= 1. mathematically, the control-flow com-
plexity metric is additive. thus, it is very easy to calculate the complexity of a proc-
ess, by simply adding the cfc of all split constructs. the greater the value of the 
cfc, the greater the overall architectural complexity of a process. this metric was 
evaluated in terms of weyuker’s properties to guarantee that it qualifies as a good quality metrics for business process models  
5 and comprehensive one (cardoso, 2006). to test the validity of the metric, an ex-
periment has been carried out for empirically validation (cardoso, 2005b). it was 
found that the cfc metric is highly correlated with the control-flow complexity of 
processes. this metric can, therefore, be used by business process analysts and 
process designers to analyze the complexity  of processes and, if possible, develop 
simpler processes. 
other researchers, for instance (latva-koi visto, 2001), also propose graph complex-
ity metrics, such as the coefficient of ne twork complexity (cnc) or the complexity 
index (ci), to evaluate business processes. in general, cardoso et al (cardoso, mendling, neumann & reijers, 2006) have identified three different types of busi-
ness process complexity: (i) computational co mplexity, (ii) psychological complexity, 
and (iii) representational complexity. 
modularity 
modularity measures the degree to which a design is split op into several modules. 
our literature review has not provided an y business process metric that measures 
the modularity of a business process design. this is no surprise regarding the fact 
that activites are most often treated as black boxes in business process modeling. 
size 
size simply measures how big a model is. the size of a business process model can 
be determined using a measure similar to the number of lines of code (loc) from 
software engineering metrics. the loc metric in software engineering has been 
used for years with a significant success rate (jones 1986). cardoso et al., gruhn & 
laue and latva-koivisto (cardoso, mend ling, neumann & reijers, 2006; gruhn & 
laue, 2006; latva-koivisto, 2001) all propose to count the number of activities to establish a measure for size.  
while this size metric is very simple, it is very important to complement other forms 
of process analysis. for example, the cont rol-flow complexity of a process can be 
very low while its activity complexity can be very high. for example, a sequential 
process that has a thousand activities has a control-flow complexity of 0, whereas 
its activity complexity is 100.  
from the "state-of-the-art" in business process metrics, we conclude that this field 
of research is just at its start and that th ere is a lot of potent ial for further develop-
ment of business process metrics. this cl assification, which was adopted from the 
software engineering field, is not yet very precise. for instance, mendling uses a 
coupling metric as means to calculate complexity (mendling, 2006) and latva-
koivisto, gruhn & laue, and cardoso et al. also use size as a measure for complex-
ity (cardoso, mendling, neumann & re ijers, 2006; gruhn & laue, 2006; latva-
koivisto, 2001). perhaps, this classification  of business process metrics should be 
revised in the future when this area is more mature. 
moreover, we observe that the values for each metric do not yet have a clear mean-
ing, e.g. when the value for coupling for a certain business process model is 0.512 
we do not yet know just from the number whether this is high or low, or good or 
bad. according to (cardoso, 2005a) it can take several years and a lot of empirical 
research before such a number really makes sense and quantifies the design in a 
proper way. despite this, business process metric analysis in the current situation 
still gives the designer some insights an d guidance on the quality of the design. 
moreover, we believe in the potential of  these metrics and their importance for 
business process design in the future. quality metrics for business process models  
6 application  
besides the theoretical overview of business process metrics which was provided in 
the previous sections, we would also like to  give some insight in the practical appli-
cation of these metrics so far. because this area emerged only recently, there are 
only a few applications available, while a lot of new research is ongoing at the mo-
ment of writing this chapter. 
the practical applications that we present here mainly have two directions. first of 
all, we look at the capabilities of a set of metrics for predicting errors (i.e. we investi-
gate whether there is a relationship between the value of the metrics and the pres-
ence of errors in the business process model). secondly, we present the early im-plementation of a tool that supports desi gning of business process models guided 
by these metrics. 
prediction of error proba bility based on metrics 
among our hypotheses on the use of business process metrics we state that busi-
ness process models which are designed using the business process metrics con-tain less errors, are easier to understand and maintain. a first step made towards 
the empirical validation of this hypothesis is made in a quantitative analysis about 
the connection between simple metrics an d error probability in the sap reference 
model (mendling et al, 2006a; mendling et al, 2006b). the sap reference model is a 
collection of epc business process models that was meant to be used as a blue-
print for rollout projects of sap’s erp syst em (keller & teufel, 1998). it reflects ver-
sion 4.6 of sap r/3 which was marketed in  2000. the extensive database of this 
reference model contains almost 10,000 sub-models, about 600 of them are epc business process models. 
the survey reported in mendling et al (2006a ) includes two parts: the verification of 
relaxed soundness (which is a minimal correctness criterion for business process models) and the prediction of error prob ability based on statistic methods. the veri-
fication of relaxed soundness  revealed that about 6 % of the epc models (34 of 604) 
contained errors such as e.g. deadlocks.  this result on its own emphasizes the 
need for verification tools in bu siness process modeling projects.  
in the second part, the authors investigate the question whether errors appear by 
chance in a process model, or if there is  some way to use business process metrics 
to predict the error probabilit y. the hypothesis behind this research is that large 
and complex models are more likely to co ntain errors, basically because the human 
modeler is more likely to loose the overview of all interrelations represented in the model. the authors use a set of simple metrics related to size of the models as input 
to a logistic regression model, i.e. a st atistical model to predict occurrence or non-
occurrence of an event. the event in this context is whether the process model has an error or not. the results show that th ese simple metrics are indeed suitable to 
predict the error probability. in particular, it appears that a higher number of join-
connectors is most strongly connected wi th an increase in error probability.  
this survey illustrates one promising applic ation of business process metrics. still, 
there is further research needed to identify more elaborate and sophisticated met-
rics. moreover, there is a need for further empirical investigation in order to estab-
lish an understanding of when a threshold value of a certain metrics indicates bad 
design in terms of maintainability  or likely error proneness.  
the prom tool 
in recent years prom has emerged as a br oad and powerful process analysis tool, 
supporting all kinds of analysis related to  business processes (van dongen et al, quality metrics for business process models  
7 2005). in contrast to many other analysis tools the starting point was the analysis 
of real processes rather than modeled processes, i.e., using process mining  tech-
niques prom attempts to extract non-triv ial and useful information from so-called 
“event logs”. moreover, prom  also allows for the calculation of several quality met-
rics as will be illustrated later in this section. 
traditionally, most analysis tools focu sing on processes are restricted to model-
based analysis , i.e., a model is used as the starting point of analysis. for example, 
a purchasing process can be modeled using epcs and verification techniques can then be used to check the correctness of the protocol while simulation can be used 
to estimate performance aspects. such analysis is only useful if the model reflects 
reality . process mining techniques use event lo gs as input, i.e., information re-
corded by systems ranging from enterpri se information systems to web services. 
hence the starting point is not a model but the observed reality. therefore, we use 
the phrase “real process analysis” to position process mining with respect to classi-cal model-based analysis. the widespread us e of information systems, e.g., systems 
constructed using erp, wfm, crm, scm, and pdm software, resulted in the om-
nipresence of vast amounts of event data. events may be recorded in the form of audit trails, transactions logs, or databases and may refer to patient treatments, 
order processing, claims handling, trading, travel booking, etc. 
figure 1 is used to explain the different types of process analysis supported by 
prom. first of all, it is relevant to note  that when studying business processes one 
can look at models (lower left corner) or  study the observed behavior (lower right 
corner).  
 
figure 1: overview of the functionality of prom: (1) discovery, (2) conformance, and (3) 
model analysis  
using process mining it is possible to au tomatically derive process models using 
process mining techniques (van der aals t, weijters & maruster, 2004). prom offers 
many process discovery techniques. the resu lt may be a petri ne t, epc, or yawl 
model. figure 1 shows some of the modelin g notations supported by prom and also 
mentions some of the products that provide event logs in a format usable by prom. 
also the list of languages suggests a focus on pure process models, discovery does 
not need to be limited to control-flow and may also include temporal, resource, 
data, and organizational aspects.  
if a model is already given, the information stored in logs can be used to check con-
formance, i.e., how well do reality and the model fit together. this can be seen as 
another quality dimension. conformance chec king requires, in addition to an event 
log, some a-priori model. this model may be handcrafted or obtained through proc-quality metrics for business process models  
8 ess discovery. whatever its source, prom  provides various ways of checking 
whether reality conforms to such a mode l (rozinat & van der aalst, 2006). for ex-
ample, there may be a process model indicati ng that purchase orders of more than 
one million euro require two checks. anothe r example is the checking of the so-
called "four-eyes principle''. conformance checking may be used to detect devia-
tions, to locate and explain these deviations, and to measure the severity of these 
deviations.  
last but not least, prom also provides va rious ways of model analysis. prom offers 
various plug-ins to analyze the correctness of a model, e.g., soundness and absence 
of deadlocks. for example, it is possible to load the sap reference model expressed 
in terms of epcs into prom and analyze it  using reduction rules or invariants. prom 
also allows for the verification of a variety of modeling languages (e.g., bpel, 
staffware, etc.) using a mapping onto petri nets. besides model verification, prom 
also allows for the calculation of various other quality metrics, e.g., cohesion and 
coupling, complexity, size, etc. given the topi c of this chapter, we elaborate on these 
metrics. 
complexity and size in prom 
in order to study the complexity of proce ss models we have developed several plug-
ins for the prom framework. as stated pr eviously, prom provides various ways of 
model analysis, such as soundness and ab sence of deadlocks. the newly developed 
plug-ins target the analysis of the quality of process designs. figure 2 shows one of 
the plug-ins analyzing the complexity, couplin g, and size of an epc process model.  
 
figure 2: screen shot of the prom tool showing the analysis sheet for epc's. for the 
epc process model presented, several metrics are calculated. note that this tool is 
still under development and that some concepts on the screen shot (e.g. ba tree, and 
weighted coupling) are not explained in this chapter. 
as can be seen from the figure, the size of a process model has several components, 
such as the number of events, functions, ors, xors, and ands. events and func-tions are specific elements to epcs. the figure also shows the control-flow com-
plexity (cardoso, 2005a) of the process displa yed which is 6, the density (mendling, 
2006) which is 0.048, and the weighted co upling which is 0.515.  while, at this quality metrics for business process models  
9 point in time, these numbers may be rather complicated to interpret for someone 
outside this area of research, we expect  that when organizations have successfully 
implemented quality metrics as part of their process development projects, empiri-
cal results and practical results from real world implementation will set limits and threshold for processes. recall that this scenario happened with the mccabe cyc-
lomatic complexity (frappier, matwin, & mili, 1994). 
data oriented cohesion and coupling in prom 
within the prom framework, also an environment is developed to calculate cohesion 
and coupling metrics based on the theory in (reijers & vanderfeesten, 2004). the 
proposed coupling-cohesion ratio can be used to compare alternative designs de-
fined on the same pdm. in this context, a design is a grouping of data elements and their respective operations into activities, such that every activity contains one or, 
preferably, more operations. the best design is the one with the lowest coupling-
cohesion ratio. 
in figure 2 and figure 3, some screen shots of the cohesion and coupling environ-
ment are shown. both screen shots contai n a different design based on the same 
pdm. in the first design (fig ure 2) the process cohesion value is 0.183, the process 
coupling value is 0.714, and the coupling-cohe sion ratio is 3.902. the part of activ-
ity a is also indicated in the pdm. for the second design (figure 3) these values are: 0.123, 0.867, and 7.049. when comparing the values for the coupling-cohesion 
ratio for both designs we see that the first design has a lower ratio and thus is the 
better alternative of the two process designs.  
 
figure 3: screen shot of the cohesion and coupling environment in prom  for the first 
design. this design contains seven activities that are defined on the pdm. activity a 
is indicated. quality metrics for business process models  
10 
 
figure 4: a screen shot of the cohesion and coupling environment for design b in 
prom. design b contains six activities that are defined on the pdm and it differs from design a because activities a and e are merged. activity ae is indicated in the pdm in 
the screen shot. 
conclusion  
currently, organizations are modeling and designing business processes without 
the aid of metrics to question the quality or properties of their models. as a result, it 
may happen that simple processes are modeled in a complex and unsuitable way. 
this may lead to a lower understandability,  higher maintenance costs, and perhaps 
inefficient execution of the processes in question (e.g. when such models are used 
to enact). considering the efforts that mo dern organizations spend on creating and 
maintaining business processes we can trul y speak of a great opportunity for the 
use of quality metrics here.   
examples of important questions that can be made relative to a process model are: 
“can process p1 be designed in a simpler way?”, “what is the complexity of process 
p2?”. “is process p3 difficult to adapt?” an d “can process p4 be easily restructured 
into sub-processes?” in the future, these kinds of questions can perhaps be satis-
factorily answered with the use of proc ess metrics such as coupling, cohesion, 
complexity, modularity, and size metrics. each of these metrics analyses a business 
process from a particular perspective.  
it is clear that quality metrics for busine ss processes need yet to come to full bloom. 
in particular, much empirical work needs to be done to assess the applicability and 
validity of the various proposed metrics. however, both for practitioners and re-
searchers there is highly attractive prospe ct of sophisticated tools coming available 
that are capable to thoroughly analyze pr ocess models against low cost, at consid-
erable speed, and yielding tangible business benefits. quality metrics for business process models  
11 acknowledgement  
this research is partly supported by th e technology foundation stw, applied sci-
ence division of nwo and the technology programme of the dutch ministry of eco-
nomic affairs. 
references  
aalst, w.m.p. van der (1999). on the automatic generation of workflow processes based on 
product structures. computers in industry, 39, 2, pp. 97-111. 
aalst, w.m.p. van der; weijters, a.j.m.m.; and maruster, l. (2004). workflow mining: discov-
ering process models from event logs. ieee transactions on knowledge and data engi-neering, 16(9), pp.1128–1142. 
baresi, l.; casati, f.; castano, s.; fugini, m.; mirbel, i.; pernici, b.; and pozzi, g. (1999). 
workflow design methodology. in p. grefen, b. pernicii and g. sanchez, editors, database support for workflow management: the wide project, pp. 47-94, kluwer academic pub-
lishers. 
bieman, j.m., and kang, b.-k. (1998). measuring design-level cohesion. ieee transactions 
on software engineering, 24, 2, pp. 111-124. 
brandes, u., and erlebach, t., editors (2005). network analysis: methodological foundations 
[outcome of a dagstuhl seminar, 13-16 april 2004], volume 3418 of lecture notes in computer science. springer-verlag. 
card, d.n.; church, v.e.; and agresti, w.w. (1986). an empirical study of software design 
practices. ieee transactions on software engineering, 12, 2, pp. 264-271. 
cardoso, j. (2005a). how to measure the control-flow complexity of web processes and 
workflows. in: fischer, l., ed., workflow handbook 2005, pp. 199-212, lighthouse point. 
cardoso, j. (2005b). control-flow complexity measurement of processes and weyuker’s prop-
erties. proceedings of the 6
th international enformatika conference (iec 2005), interna-
tional academy of sciences, budapest, hungary. vol. 8pp. 213-218. 
cardoso, j. (2006). process control-flow complexity metric: an empirical validation, ieee in-
ternational conference on services computing (ieee scc 06), chicago, usa, pp. 167-
173, ieee computer society. 
cardoso, j.; mendling, j.; neuman, g. & reijers, h.a. (2006). a discourse on complexity of 
process models. in: eder, j.; dustdar, s. et al, editors, bpm 2006 workshops. lecture 
notes in computer science 4103, springer-verlag, berlin, pp. 115-126. 
conte, s.d.; dunsmore, h.e.; and shen, v.y. (1986). software engineering metrics and mod-
els, benjamin/cummings publishing company, inc.. 
dongen, b.f. van;  alves de medeiros, a.k.; verbeek, h.m.w. ; weijters, a.j.m.m.; and aalst, 
w.m.p. van der (2005). the prom framework: a new era in process mining tool support. in g. ciardo and p. darondeau, editors, application and theory of petri nets 2005, vol-
ume 3536 of lecture notes in computer science, pp. 444–454, springer-verlag, berlin. 
frappier, m., matwin, s. and mili. a. (1994). software metrics for predicting maintainability: 
software metrics study: technical memorandum 2. canadian space agency, january 21. 
gruhn, v., and laue, r. (2006). complexity metrics for business process models. in: witold 
abramowicz and heinrich c. mayer, editors, 9
th international conference on business in-
formation systems (bis 2006), vol. 85 of lecture notes in informatics, pp. 1-12. 
guceglioglu, a.s., and demiros, o.w. (2005). using software quality characteristics to 
measure business process quality. in w.m.p. van der aalst, b. benatallah, f. casati, and f. curbera, editors, business process management (bpm 2005), lecture notes in com-
puter science, volume 3649, pages 374-379, springer-verlag, berlin. 
jones, t. c. (1986). programming productivity. new york, mcgraw-hill. kang, b.-k., and bieman, j.m. (1996). using design cohesion to visualize, quantify, and 
restructure software. 8
th international conference on software engineering and knowl-
edge engineering, knowledge systems institute, skokie il, pp. 222-229. 
kang, b.-k., and bieman, j.m. (1999). a quantitative framework for software restructuring. 
journal of software maintenance, 11, pp. 245-284. quality metrics for business process models  
12 keller, g., and teufel, t. (1998). sap(r) r/3 process oriented implementation: iterative proc-
ess prototyping. addison-wesley.  
kostoff, r.n. (1999). science and technology innovation. technovation, 19, 10, pp. 593-604. 
latva-koivisto, a.m. (2001). finding a complexity measure for business process models. hel-
sinki university of technology, systems analysis laboratory. 
mendling, j. (2006). testing density as a complexity metric for epcs. technical report jm-
2006-11-15. vienna university of economics and business administration. retrieved from http://wi.wu-wien.ac.at/home/mendling/publications/tr06-density.pdf  
mendling, j.; moser, m.; neumann, g.; verbeek, h.m.w.; dongen, b.f. van; and aalst, 
w.m.p. van der (2006a). a quantitative analysis of faulty epcs in the sap reference 
model. bpm center report bpm-06-08, eindhoven university of technology, eindhoven. 
mendling, j.; moser, m.; neumann, g.; verbeek, h.m.w.; dongen, b.f. van; and aalst, 
w.m.p. van der (2006b). faulty epcs in the sap reference model. in: j.l. fiadeiro, s. 
dustdar and a. sheth, editors, proceedings of bpm2006, lecture notes in computer sci-ence, volume 4102, pp. 451-457, springer-verlag, berlin. 
myers, g.j. (1978). composite/structured design. van nostrand reinhold, new york, ny. 
reijers, h.a. (2003). design and control of workflow processes: business process management 
for the service industry. lecture notes in computer science 2617, springer-verlag, berlin. 
reijers, h.a. (2003). a cohesion metric for the definition of activities in a workflow process. 
proceedings of the 8th caise/ifip8.1 international workflop on evaluation of modeling methods in systems analysis and design (emmsad 2003), pp. 116-125. 
reijers, h.a.; limam mansar, s.; and aalst, w.m.p. van der (2003). product-based workflow 
design. journal of management information systems, 20, 1, pp. 229-262. 
reijers, h. a., and vanderfeesten, i.t.p. (2004). cohesion and coupling metrics for workflow 
process design. in j. desel, b. pernici, and m. weske, editors, proceedings of the 2
nd in-
ternational conference on business process management (bpm 2004), lecture notes in computer science volume 3080, pp. 290-305, springer-verlag, berlin. 
rozinat, a., and aalst, w.m.p. van der (2006). conformance testing: measuring the fit and 
appropriateness of event logs and process models. in c. bussler et al, editor, bpm 2005 workshops (workshop on business process intelligence), volume 3812 of lecture notes in 
computer science, pp. 163-176, springer-verlag, berlin. 
selby, r.w., and basili, v.r. (1991). analyzing error-prone system structure. ieee transac-
tions on software engineering, 17, 2, pp. 141-152. 
shen, v.y.; yu, t.-j.; thebaut, s.m.; and paulsen, l.r. (1985). identifying error-prone soft-
ware, ieee transactions on software engineering, 11, 4, pp. 317-324. 
shepperd, m. (1993). software engineering metrics volume i: metrics and validations, 
mcgraw-hill. 
troy, d.a., and zweben, s.h. (1981). measuring the quality of structured designs, journal of 
systems and software, vol. 2, pp. 113-120. 
weyuker, e.j. (1988). evaluating software complexity measures. ieee transactions on soft-
ware engineering, 14, 9, pp. 1357-1365. 