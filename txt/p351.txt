change mining in adaptive process
management systems
christian w. g¨ unther1, stefanie rinderle2,
manfred reichert3, wil van der aalst1
1eindhoven university of technology, the netherlands
{c.w.gunther, w.m.p.v.d.aalst }@tm.tue.nl
2university of ulm, germany
stefanie.rinderle@uni-ulm.de
3university of twente, the netherlands
m.u.reichert@ewi.utwente.nl
abstract. the wide-spread adoption of process-aware information sys-
tems has resulted in a bulk of computerized information about real-world
processes. this data can be utilized for process performance analysis as
well as for process improvement. in this context process mining oﬀers
promising perspectives. so far, existing mining techniques have been ap-
plied to operational processes, i.e., knowledge is extracted from execu-
tion logs (process discovery), or execution logs are compared with some
a-priori process model (conformance checking). however, execution logs
only constitute one kind of data gathered during process enactment.
in particular, adaptive processes provide additional information about
process changes (e.g., ad-hoc changes of single process instances) which
can be used to enable organizational learning. in this paper we present
an approach for mining change logs in adaptive process management sys-
tems. the change process discovered through process mining provides an
aggregated overview of all changes that happened so far. this, in turn,
can serve as basis for all kinds of process improvement actions, e.g., it
may trigger process redesign or better control mechanisms.
1 introduction
the striking divergence between modeled processes and practice is largely due
to the rigid, inﬂexible nature of commonplace process-aware information sys-
tems (paiss) [10]. whenever a small detail is modeled in the wrong manner, or
external changes are imposed on the process (e.g. a new legislation or company
guideline), users are forced to deviate from the prescribed process model. how-
ever, given the fact that process (re-)design is an expensive and time-consuming
task, this results in employees working “behind the system’s back”. in the end,
the pais starts to become a burden rather than the help it was intended to be.
in recent years many eﬀorts have been undertaken to deal with these drawbacks
and to make paiss more ﬂexible. in particular, several approaches for adaptive
process management have emerged (for an overview see [17]). adaptive processesenable users to evolve process deﬁnitions, such that they ﬁt to changed situa-
tions. adaptability can be supported by dynamic changes of diﬀerent process
aspects (e.g., control and data ﬂow) at diﬀerent levels (e.g., instance and type
level). for example, ad-hoc changes conducted at the instance level (e.g., to add
or delete process steps) allow to ﬂexibly adapt single process instances to ex-
ceptional or changing situations [14]. usually, such deviations are recorded in
change logs (see [18]), which results in more meaningful log information when
compared to traditional process management systems (pmss).
adaptive pmss like adept or wasa oﬀer ﬂexibility at both process type
level and process instance level [14, 17, 21]. so far, adaptive pmss have not sys-
tematically addressed the fundamental question what we can learn from this
additional information and how we can derive optimized process models from
it. process mining techniques [2], in turn, oﬀer promising perspectives for learn-
ing from changes, but have focused on the analysis of pure execution logs (i.e.,
taking a behavioral and operational perspective) so far.
this paper presents a framework for integrating adaptive process manage-
ment and process mining: change information gathered within the adaptive
pms is exploited by process mining techniques. the results can be used to learn
from previously applied changes and to optimize running and future processes
accordingly. for this integration, ﬁrst of all, we determine which runtime infor-
mation about ad-hoc deviations is necessary and how it should be represented in
order to achieve optimal mining results. secondly, we develop new mining tech-
niques based on existing ones which utilize change logs in addition to execution
logs. as a result we obtain an abstract change process which reﬂects all changes
applied to the instances of a particular process type so far. more precisely, a
change process comprises change operations (as activities) and the causal rela-
tions between them. we further utilize information about the semantics of change
operations (e.g., commutativity) in order to optimize our mining results. the re-
sulting change process provides valuable knowledge about the process changes
happened so far, which may serve as basis for deriving process optimizations in
the sequel. finally, the approach is implemented within a prototype integrating
process mining framework prom and adept.
sect. 2 introduces our framework for integrating process mining and adaptive
process management. sect. 3 describes how we import change log information
into this framework and how changes are represented. sect. 4 deals with our
approach for discovering change processes from these logs. in sect. 5 we discuss
details of our implementation and show which tool supported is provided. sect. 6
discusses related work and sect. 7 concludes with a summary and an outlook.
2 process optimization by integrating process mining
and adaptive process management
in this section we argue that the value of adaptive pmss can be further leveraged
by integrating them with process mining techniques. after introducing basics
related to process mining, we present our overall integration framework.2.1 process mining
process-aware information systems (paiss), such as wfms, erp, and b2b sys-
tems, need to be conﬁgured based on process models . the latter specify the order
in which process steps are to be executed and therefore enable the information
system to ensure and control the correct execution of operational processes.
usually, relevant events occurring in a pais (e.g., regarding the execution
of tasks or the modiﬁcation of data) are recorded in event logs .process mining
describes a family of a-posteriori analysis techniques exploiting the information
recorded in these logs. typically, respective approaches assume that it is possible
to sequentially record events such that each event refers to an activity (i.e.,
a well-deﬁned step in the process) and is related to a particular case (i.e., a
process instance). furthermore, there are other mining techniques making use
of additional information such as the performer or originator of the event (i.e.,
the person / resource executing or initiating the activity), the timestamp of the
event, or data elements recorded with the event (e.g., the size of an order).
process mining addresses the problem that most “process owners” have very
limited information about what is actually happening in their organization. in
practice there is often a signiﬁcant gap between what is prescribed or supposed
to happen, and what actually happens. only a concise assessment of the orga-
nizational reality, which process mining strives to deliver, can help in verifying
process models, and ultimately be used in a process redesign eﬀort.
(un)desired
propertiesprocess
modelsevent
logs
information
system
operational
process
refers tomodelsconfiguresrecordssupports /
controls
process
discovery
conformance 
testing
log-based verification
fig. 1. process mining and its relation to bpm.
there are three major classes of process mining techniques as indicated in
fig. 1. traditionally, process mining has focused on process discovery , i.e. de-
riving information about the original process model, the organizational context,
and execution properties from enactment logs. an example of a technique ad-
dressing the control ﬂow perspective is the alpha algorithm [2], which can con-
struct a petri net model [6] describing the behavior observed in the event log.
the multi-phase mining approach [7] can be used to construct an event-drivenprocess chain (epc) based on similar information. finally, ﬁrst work regard-
ing the mining of other model perspectives (e.g., organizational aspects [1]) and
data-driven process support systems (e.g., case handling systems) has been done.
another line of process mining research is conformance testing . its aim is to
analyze and measure discrepancies between the model of a process and its actual
execution (as recorded in event logs). this can be used to indicate problems.
finally, log-based veriﬁcation does not analyze enactment logs with respect to
the original model, but rather checks the log for conformance with certain desired
or undesired properties, e.g., expressed in terms of linear temporal logic (ltl)
formulas. this makes it an excellent tool to check a case for conformance to
certain laws or corporate guidelines (e.g. the four-eyes principle).
at this point in time there are mature tools such as the prom framework,
featuring an extensive set of analysis techniques which can be applied to real
process enactments while covering the whole spectrum depicted in fig. 1 [9].
2.2 integration framework
both process mining and adaptive workﬂow address fundamental issues that are
widely prevalent in the current practice of bpm implementations. these prob-
lems stem from the fact that the design ,enactment , and analysis of a business
process are commonly interpreted, and implemented, as detached phases .
although both techniques are valuable on their own, we argue that their full
potential can be only harnessed by tight integration. while process mining can
deliver concrete and reliable information about how process models need to be
changed, adaptive pmss provide the tools to safely and conveniently implement
these changes. thus, we propose the development of process mining techniques,
integrated into adaptive pmss as a feedback cycle . in the sequel, adaptive pmss
need to be equipped with functionality to exploit this feedback information.
the framework depicted in fig. 2 illustrates how such an integration could
look like. adaptive pmss, visualized in the upper part of this model, operate on
pre-deﬁned process models. the evolution of these models over time spawns a set
of process changes, i.e., results in multiple process variants . like in every pais,
enactment logs are created which record the sequence of activities executed for
each case. on top of that, adaptive pmss additionally log the sequence of change
operations imposed on a process model for every executed case, producing a set
ofchange logs . process mining techniques that integrate into such system, in
form of a feedback cycle, fall into one of three major categories:
change analysis: process mining techniques from this category make use of
change log information, besides the original process models and their vari-
ants. their goal is to determine common and popular variants for each
process model, which may be promoted to replace the original model. pos-
sible ways to pursue this goal are through statistical analysis of changes or
their abstraction to higher-level models.
integrated analysis: this analysis uses both change and enactment logs in
a combined fashion. possible applications in this category could perform aadaptive workflow
process miningcontext-aligned 
changes / variantsprocess 
models<refers to>process 
instantiationcase
(data)
context-aware 
adaptationenactment
process 
modelli ngcontinuous 
adaptationdata 
updates
ad-hoc 
adaptation
enactment 
logschange logs
change analysis
integrated analysis
enactment 
analysi sfig. 2. integration of process mining and adaptive process management
context-aware categorization of changes as follows. after clustering change
sequences, as found in the change logs, into groups, the incentive for these
changes can be derived. this is performed by inspecting the state of each
case, i.e. the values of case data objects, at the time of change, as known from
the original process model and the enactment logs. a decision-tree analysis
of these change clusters provides an excellent basis for guiding users in future
process adaptations, based on the peculiarities of their speciﬁc case.
enactment analysis: based solely on the inspection of enactment logs, tech-
niques in this category can pinpoint parts of a process model which need
to be changed. for example, when a speciﬁc alternative of a process model
has never been executed, the original process model may be simpliﬁed by
removing that part. further techniques may also clean the model repository
from rarely used process deﬁnitions.
these examples give only directions in which the development of suitable
process mining techniques may proceed. of course, their concrete realizationdepends on the nature of the system at hand. for example, it may be preferable
to present highlighted process models to a specialist before their deletion or
change, rather than having the system performing these tasks autonomously.
when such feedback cycle is designed and implemented consistently, the re-
sulting system is able to provide user guidance and autonomous administration
to an unprecedented degree. moreover, the tight integration of adaptive pmss
and process mining technologies provides a powerful foundation, on which a new
generation of truly intelligent and increasingly autonomous paiss can be built.
3 change logs
adaptive pmss do not only create process enactment logs, but they also log the
sequence of changes applied to a process model . this section introduces the basics
of these change logs. we ﬁrst discuss the nature of changes and then introduce
mxml as general format for event logs. based on this we show how change logs
can be mapped onto the mxml format. mxml-based log ﬁles constitute the
basic input for the mining approach described in sect. 4.
3.1 general change framework
logically, a process change is accomplished by applying a sequence of change
operations to the respective process model [14]. the question is how to represent
this change information within change logs. in principle, the information to be
logged can be represented in diﬀerent ways. the goal must be to ﬁnd an adequate
representation and appropriate analysis techniques to support the three cases
described in the previous section.
independent from the applied (high–level) change operations (e.g., adding,
deleting or moving activities), for example, we could translate the change into
a set of basic change primitives (i.e., basic graph primitives like addnode or
deleteedge ). this still would enable us to restore process structures, but also
result in a loss of information about change semantics and therefore limit trace-
ability and change analysis. as an alternative we can explicitly store the applied
high–level change operations, which combine basic primitives in a certain way.
high–level change operations are based on formal pre-/post-conditions. this
enables the pms to guarantee model correctness when changes are applied.
further, high-level change operations can be combined to change transactions .
this becomes necessary, for example, if the application of a high-level change
operation leads to an incorrect process model and this can be overcome by
conducting concomitant changes. during runtime several change transactions
may be applied to a particular process instance. all change transactions related
to a process instance are stored in the change log4of this instance (cf. [18]).
4a change log is an ordered series cl:=< ∆ 1, . . . , ∆ n>of change operations ∆i
(i = 1, ..n); i.e., when applying the change operations contained in clto a correct
process schema s, all intermediate process schemas siwith si:=si−1+∆i
(i = 1,. . . , n; s0:=s) are correct process schemas.in the following we represent change log entries by means of high-level change
operations since we want to exploit their semantical content (see fig. 3 for
an example). however, basically, the mining approach introduced later can be
adapted to change primitives as well. table 1 presents examples of high-level
change operations on process models which can be used at the process type as
well as at the process instance level to create or modify models. although the
change operations are exemplarily deﬁned on the adept meta model (see [14]
for details) they are generic in the sense that they can be easily transferred to
other meta models as well (e.g. [15]).
table 1. examples of high-level change operations on process schemas
change operation optype subject paramlist
∆applied to s
insert(s, x, a, b, [ sc]) insert x s, a, b
eﬀects on s: inserts activity x between node sets a and b
(it is a conditional insert if scis speciﬁed)
preconditions: node sets a and b must exist in s, and x must not be contained
in s yet (i.e., no duplicate activities!)
delete(s, x) delete x s
eﬀects on s: deletes activity x from s
preconditions: activity x must be contained exactly once in s
move(s, x, a, b, [ sc]) move x s, a, b
eﬀects on s: moves activity x from its original position between node sets a and b
(it is a conditional insert if scis speciﬁed)
preconditions: activity x and node sets a and b must be contained exactly once in s
3.2 the mxml format for process event logs
mxml is an xml-based format for representing and storing event log data,
which is supported by the largest subset of process mining tools, such as prom.
while focusing on the core information needed for process mining, the format
reserves generic ﬁelds for extra information potentially provided by a pais. due
to its outstanding tool support and extensibility, the mxml format has been
selected for storing change log information in our approach.
the root node of a mxml document is a workﬂowlog . it represents a log
ﬁle, i.e. a logical collection of events having been derived from one system. every
workﬂow log can potentially contain one source element, which is used to de-
scribe that system the log has been imported from. apart from the source de-
scriptor, a workﬂow log can contain an arbitrary number of processes as child
elements, each grouping events that occurred during the execution of a speciﬁc
process deﬁnition. the single executions of a process are represented by child
elements of type processinstance , each representing one case in the system.
finally, process instances group an arbitrary number of audittrailentry ele-
ments as child elements. each of these child elements refers to one speciﬁc event
which has occurred in the system. every audit trail entry must contain at least
two child elements: the workﬂowmodelelement describes the abstract process
deﬁnition element to which the event refers, e.g. the name of the activity thata) process instances  b) change logs    c) change process instances 
 
examine
patientdeliver
reportinform
patientprepare
patientinstance i1 : lab test
enter
ordercli1= (
op1:=insert(s, lab test, examine patient, deliver report),
op2:=move(s, inform patient, prepare patient, examine patient))
cli2(s) = (
op3:=insert(s, xray, inform patient, prepare patient), 
op4:=delete(s, xray),  
op5:=delete(s, inform patient),
op6:=insert(s, inform patient, examine patient, deliver report), 
op2 =move(s, inform patient, prepare patient, examine patient),
op1 =insert(s, lab test, examine patient, deliver report))examine
patientdeliver
reportinform
patientprepare
patientinstance i2 : 
enter
order
examine
patientdeliver
reportinform
patientprepare
patientinstance i3 : lab test
enter
ordercli1= (
op2 =move(s, inform patient, prepare patient, examine patient),
op1 =insert(s, lab test, examine patient, deliver report))
examine
patientdeliver
reportinform
patientprepare
patientinstance i4 : lab test
enter
ordercli4= (
op1 =insert(s, lab test, examine patient, deliver report))
examine
patientdeliver
reportinform
patientprepare
patientinstance i5: 
enter
ordercli5= (
op1 =insert(s, lab test, examine patient, deliver report,
op7:=delete(s, deliver report))lab test
lab testop1 op2 op1 op2
op3 op5
op4 op6
op2op1
op1 op2 op1 op2
op1op1
op1
op7op1
op7
op1 op2
op7op1 op2
op7
op8op8
op8 op2
op9op8 op2
op9
op1
op10examine
patientdeliver
reportinform
patientprepare
patientinstance i6 : lab test
enter
ordercli6= (
op1 =insert(s, lab test, examine patient, deliver report),
op2 =move(s, inform patient, prepare patient, examine
patient),
op7 =delete(s, deliver report))
cli7(s) = (
op8:= insert(s, xray, examine patient, deliver report))
examine
patientdeliver
reportinform
patientprepare
patientinstance i7 : 
enter
order
examine
patientdeliver
reportinform
patientprepare
patientinstance i8 : lab test
enterordercli8= (
op2 =move(s, inform patient, prepare patient, examine patient),
op8 =insert(s, xray, examine patient, deliver report),
op9:=insert(s, lab test, xray, deliver report))
examine
patientdeliver
reportinform
patientprepare
patientinstance i9: lab test
enter
ordercli9= (
op1 =insert(s, lab test, examine patient, deliver report),
op10:=insert(s, xray, examine patient, lab test))xray
xray
xray
 fig. 3. modiﬁed process instances and associated change logs
was executed. the second mandatory element is the eventtype , describing the
nature of the event, e.g. whether a task was scheduled, completed, etc. the op-
tional child elements of an audit trail entry are timestamp andoriginator . the
timestamp holds the date and time of when the event has occurred, while the
originator identiﬁes the resource, e.g. person, which has triggered the event.
to enable the ﬂexible extension of this format with extra information, all
mentioned elements (except the child elements of audittrailentry ) can also have
a generic data child element. the data element groups an arbitrary number of
attributes , which are key-value pairs of strings. the following subsection de-scribes the mapping of change log information to mxml, which is heavily based
on using custom attributes of this sort.
3.3 mapping change log information to mxml
with respect to an adaptive pais, change log information can be structured on
a number of diﬀerent levels. most of all, change events can be grouped by the
process deﬁnition they address. as we are focusing on changes applied to cases ,
i.e. executed instances of a process deﬁnition, the change events referring to one
process can be further subdivided with respect to the speciﬁc case in which they
were applied. finally, groups of change events on a case level are naturally sorted
by the order of their occurrence.
the described structure of change logs ﬁts well into the common organiza-
tion of enactment logs, with instance traces then corresponding to consecutive
changes of a process model, in contrast to its execution. thus, change logs can
be mapped to the mxml format with minor modiﬁcations. listing 1 shows an
mxml audit trail entry describing the insertion of a task “lab test” into a
process deﬁnition, as e.g. seen for instance i1in fig. 3.
<audittrailentry>
<data>
<attribute name="change.postset">deliver_report</attribute>
<attribute name="change.type">insert</attribute>
<attribute name="change.subject">lab_test</attribute>
<attribute name="change.rationale">ensure that blood values
are within specs.</attribute>
<attribute name="change.preset">examine_patient</attribute>
</data>
<workflowmodelelement>insert.lab_test</workflowmodelelement>
<eventtype>complete</eventtype>
<originator>n.e.body</originator>
</audittrailentry>
listing 1: example of a change event in mxml.
change operations are characterized by the type (e.g., “insert”) of change,
thesubject which has been primarily aﬀected (e.g., the inserted task), and the
syntactical context of the change. this syntactical context contains the change
operation’s pre- and post-set , referring to adjacent model elements that are ei-
ther directly preceding or following the change subject in the process deﬁnition.
these speciﬁc change operation properties are not covered by the mxml format,
therefore they are stored as attributes in the “data” ﬁeld. the set of attributes
for a change event is further extended by an optional rationale ﬁeld, storing a
human-readable reason, or incentive, for this particular change operation.
the originator ﬁeld is used for the person having applied the respective
change, while the timestamp ﬁeld obviously describes the concise date and time
of occurrence. change events have the event type “complete” by default, be-
cause they can be interpreted as atomic operations. in order to retain backwardcompatibility of mxml change logs with traditional process mining algorithms,
theworkﬂow model element needs to be speciﬁed for each change event. as the
change process does not follow a prescribed process model, this information is
not available. thus, a concatenation of change type and subject is used for the
workﬂow model element ﬁeld.
on top of having a diﬀerent set of information, change logs also exhibit
speciﬁc properties making them diﬀerent from enactment logs. the next section
investigates these speciﬁc properties and uses them for a ﬁrst mining approach.
4 mining compact change processes
in this section we describe an approach for analyzing change log information, as
found in adaptive pmss. first, we explore the nature of change logs in more de-
tail. this is followed by an introduction to the concept of commutativity between
change operations in sect. 4.2. this commutativity relation provides the foun-
dation for our mining algorithm for change processes, as introduced in sect. 4.3.
4.1 a characterization of change logs
change logs, in contrast to regular enactment logs, do not describe the execution
of a deﬁned process. this is obvious from the fact that, if the set of potential
changes would have been known in advance, then these changes could have al-
ready been incorporated in the process model (making dynamic change obsolete).
thus, change logs must be interpreted as emerging sequences of activities which
are taken from a set of change operations.
in sect. 3.3 it has been deﬁned that each change operation refers to the
original process model through three associations: the subject ,pre-, and post-set
of the change. as all three associations can theoretically be bound to any subset
from the original process model’s set of activities5, the set of possible change
operations grows exponentially with the number of activities in the original
process model. this situation is fairly diﬀerent from mining a regular process
model, where the number of activities is usually rather limited (e.g., up to 50–100
activities). hence, change process mining poses an interesting challenge.
summarizing the above characteristics, we can describe the meta-process of
changing a process schema as a highly unstructured process, potentially involving
alarge number of distinct activities . these properties, when faced by a process
mining algorithm, typically lead to overly precise and confusing “spaghetti-like”
models . for a more compact representation of change processes, it is helpful to
abstract from a certain subset of order relations between change operations.
when performing process mining on enactment logs (i.e., the classical appli-
cation domain of process mining), the actual state of the mined process is treated
like a “black box”. this is a result of the nature of enactment logs, which typ-
ically only indicate transitions in the process, i.e. the execution of activities.
5here we assume that the subset describing the subject ﬁeld is limited to one.however, the information contained in change logs allows to trace the state of
the change process, which is indeed deﬁned by the process schema that is sub-
ject to change. moreover, one can compare the eﬀects of diﬀerent (sequences of)
change operations. from that, it becomes possible to explicitly detect whether
two consecutive change operations might as well have been executed in the re-
verse order, without changing the resulting state.
the next section introduces the concept of commutativity between change
operations, which is used to reduce the number of ordering relations by taking
into account the semantic implications of change events. since the order of com-
mutative change operations does not matter, we can abstract from the actually
observed sequences thus simplifying the resulting model.
4.2 commutative and dependent change operations
change operations modify a process schema, either by altering the set of activ-
ities or by changing their ordering relations. thus, each application of a change
operation to a process schema results in another, diﬀerent schema. a process
schema can be described formally without selecting a particular notation, i.e., we
abstract from the concrete operators of the process modeling language and only
describe the set of activities and possible behavior.
deﬁnition 1 (process schema). a process schema is a tuple ps= (a, ts )
where
–ais a set of activities
–ts= (s, t, s start, send)is a labeled transition system, where sis the set of
reachable states, t⊆s×(a∪ {τ})×sis the transition relation, sstart∈s
is the initial state, and send∈sis the ﬁnal state.
pis the set of all process schemas.
the behavior of a process is described in terms of a transition system ts
with some initial state sstart and some ﬁnal state send. note that any process
modeling language can be mapped onto a labeled transition system. the transi-
tion system does not only deﬁne the set of possible traces (i.e., execution orders);
it also captures the moment of choice. moreover, it allows for “silent steps”. a
silent step, denoted by τ, is an activity within the system which changes the
state of the process, but is not observable in the execution logs. this way we
can distinguish between diﬀerent types of choices (internal/external or control-
lable/uncontrollable) [5]. while all change operations modify the set of states s
and the transition relation t, the “move” operation is the only one not changing
the set of activities a.
in order to compare sequences of change operations, and to derive ordering
relations between these changes, it is helpful to deﬁne an equivalence relation
for process schemas.
deﬁnition 2 (equivalent process schemas). let≡be some equivalence
relation. for ps1, ps 2∈ p :ps1≡ps2if and only if ps1andps2are
considered to be equivalent.there exist many notions of process equivalence. the weakest notion of equiv-
alence is trace equivalence [11, 13, 17], which regards two process models as equiv-
alent if the sets of observable traces they can execute are identical. since the
number of traces a process model can generate may be inﬁnite, such comparison
may be complicated. moreover, since trace equivalence is limited to compar-
ing traces, it fails to correctly capture the moment at which choice occurs in a
process. for example, two process schemas may generate the same set of two
traces {abc, abd }. however, the process may be very diﬀerent with respect
to the moment of choice, i.e. the ﬁrst process may already have a choice after
ato execute either bcorbd, while the second process has a choice between
canddjust after b.branching bisimilarity is one example of an equivalence,
which can correctly capture this moment of choice. for a comparison of branch-
ing bisimilarity and further equivalences the reader is referred to [12]. in the
context of this paper, we abstract from a concrete notion of equivalence, as the
approach described can be combined with diﬀerent process modeling notations
and diﬀerent notions of equivalence.
as stated above, each application of a change operation transforms a process
schema into another process schema. this can be formalized as follows:
deﬁnition 3 (change in process schemas). letps1, ps 2∈ p be two
process schemas and let ∆be a process change.
–ps1[∆/angbracketrightif and only if ∆is applicable to ps1, i.e., ∆is possible in ps1.
–ps1[∆/angbracketrightps2if and only if ∆is applicable to ps1(i.e., ps1[∆/angbracketright) and ps2is
the process schema resulting from the application of ∆tops1.
the applicability of a change operation to a speciﬁc process schema is deﬁned
in table 1, and is largely dictated by common sense. for example, an activity x
can only be inserted into a schema s, between the node sets aandb, if these
node sets are indeed contained in sand the activity xis not already contained
ins. note that we do not allow duplicate tasks, i.e. an activity can be contained
only once in a process schema.
based on the notion of process equivalence we can now deﬁne the concept of
commutativity between change operations.
deﬁnition 4 (commutativity of changes). letps∈ p be a process schema,
and let ∆1and∆2be two process changes. ∆1and∆2are commutative in ps
if and only if:
–there exist ps1, ps 2∈ p such that ps[∆1/angbracketrightps1andps1[∆2/angbracketrightps2,
–there exist ps3, ps 4∈ p such that ps[∆2/angbracketrightps3andps3[∆1/angbracketrightps4,
–ps2≡ps4.
two change operations are commutative, if they have exactly the same eﬀect
on a process schema, regardless of the order in which they are applied. if two
change operations are not commutative, we regard them as dependent , i.e., the
eﬀect of the second change depends on the ﬁrst one. the concept of commutativ-
ity captures the ordering relation between two consecutive change operations. iftwo change operations are commutative according to def. 4 they can be applied
in any given order, therefore there exists no ordering relation between them.
in the next subsection we demonstrate that existing process mining algo-
rithms can be enhanced with the concept of commutativity, thereby abstracting
from ordering relations that are irrelevant from a semantical point of view (i.e.,
their order does not inﬂuence the resulting process schema).
4.3 mining change processes
mining change processes is to a large degree identical to mining regular processes
from enactment logs. therefore, we have chosen not to develop an entirely new
algorithm, but rather to base our approach on an existing process mining tech-
nique. among the available algorithms, the multi-phase algorithm [7, 8] has been
selected, which is very robust in handling fuzzy branching situations (i.e., it
can employ the “or” semantics to split and join nodes, in cases where neither
“and” nor “xor” are suitable). although we illustrate our approach using a
particular algorithm, it is important to note that any process mining algorithm
based on explicitly detecting causalities can be extended in this way (e.g., also
the diﬀerent variants of the α-algorithm).
the multi-phase mining algorithm is able to construct basic workﬂow graphs,
petri nets, and epc models from the causality relations derived from the log.
for an in-depth description of this algorithm, the reader is referred to [7, 8]. the
basic idea of the multi-phase mining algorithm is to discover the process schema
in two steps. first a model is generated for each individual process instance.
since there are no choices in a single instance, the model only needs to capture
causal dependencies. using causality relations derived from observed execution
orders and the commutativity of speciﬁc change operations, it is relatively easy
to construct such instance models. in the second step these instance models are
aggregated to obtain an overall model for the entire set of change logs.
the causal relations for the multi-phase algorithm [7, 8] are derived from the
change log as follows. if a change operation ais followed by another change b
in at least one process instance, and no instance contains bfollowed by a, the
algorithm assumes a possible causal relation from atob(i.e., “ amay cause
b”). in the example log introduced in fig. 3, instance i2features a change
operation deleting “inform patient” followed by another change, inserting the
same activity again. as no other instance contains these changes in reverse order,
a causal relation is established between them.
fig. 4 shows a petri net model [6] of the change process mined from the
example change log instances in fig. 3. the detected causal relation between
deleting and inserting “inform patient” is shown as a directed link between these
activities. note that in order to give the change process explicit start and end
points, artiﬁcial activities have been added. although the model contains only
seven activities, up to three of them can be executed concurrently. note further
that the process is very ﬂexible, i.e. all activities can potentially be skipped. from
the very small data basis given in fig. 3, where change log instances hardly have
common subsequences, this model delivers a high degree of abstraction.start
insert 
labtest
delete 
deliver 
report
endinsert
xray
delete
xraydelete 
inform 
patient
insert 
inform 
patient
move 
inform 
patientfig. 4. mined example process (petri net notation)
if two change operations are found to appear in both orders in the log, it is as-
sumed that they can be executed in any order, i.e. concurrently. (note that there
might be some order between concurrent changes determined by context factors
not directly accessible to the system. we aim at integrating such information in
our future work). an example for this is inserting “xray” and inserting “lab
test”, which appear in this order in instance i8, and in reverse order in instance
i9. as a result, there is no causal relation, and thus no direct link between these
change operations in the model shown in fig. 4.
apart from observed concurrency, as described above, we can introduce the
concept of commutativity-induced concurrency , using the notion of commuta-
tivity introduced in the previous subsection (cf. deﬁnition 4). from the set of
observed causal relations, we can exclude causal relations between change oper-
ations that are commutative. for example, instance i2features deleting activity
“xray” directly followed by deleting “inform patient”. as no other process in-
stance contains these change operations in reverse order, a regular process mining
algorithm would establish a causal relation between them.
however, it is obvious that it makes no diﬀerence in which order two ac-
tivities are removed from a process schema. as the resulting process schemas
are identical, these two changes are commutative . thus, we can safely discard a
causal relation between deleting “xray” and deleting “inform patient”, which
is why there is no link in the resulting change process shown in fig. 4.
commutativity-induced concurrency removes unnecessary causal relations,
i.e. those causal relations that do not reﬂect actual dependencies between change
operations. extending the multi-phase mining algorithm with this concept sig-
niﬁcantly improves the clarity and quality of the mined change process. if it
were not for commutativity-induced concurrency, every two change operationswould need to be observed in both orders to ﬁnd them concurrent. this is espe-
cially signiﬁcant in the context of change logs, since one can expect changes to
a process schema to happen far less frequently than the actual execution of the
schema, resulting in less log data.
5 implementation and tool support
to enable experimentation with change logs and their analysis, an import plug-in
has been implemented for the prom import framework, which allows to extract
both enactment and change logs from instance ﬁles of the adept demonstra-
tor prototype [16]. prom import6is a ﬂexible and open framework for the rapid
prototyping of mxml import facilities from all kinds of paiss. the adept
demonstrator prototype provides the full set of process change facilities found
in the adept distribution, except for implementation features like work dis-
tribution and the like. the combination of both makes it possible to create and
modify a process model in the adept demonstrator prototype, after which a
respective change log can be imported and written to the mxml-based change
log format described in sect. 3.3.
fig. 5. change mining plug-in within prom.
these change logs can then be loaded into the prom framework7. a dedicated
change mining plug-in has been developed, which implements the commutativity-
6prom import is available under an open source license at
http://promimport.sourceforge.net/.
7prom is available under an open source license at http://prom.sourceforge.net/.enhanced multi-phase algorithm described in sect. 4.3. it is also possible to mine
only a selection of the change logs found in the log. the resulting change process
can be visualized in the form of a workﬂow graph, petri net, or epc.
figure 5 shows the change mining plug-in within the prom framework, dis-
playing the example process introduced in fig. 3 in terms of a process graph.
the activities and arcs are annotated with frequencies, indicating how often the
respective node or path has been found in the log.
6 related work
although process mining techniques have been intensively studied in recent
years [2–4, 7, 8], no systematic research on analyzing process change logs has
been conducted so far. existing approaches mainly deal with the discovery of
process models from execution logs, conformance testing, and log-based veriﬁ-
cation (cf. sect. 2.1). however, execution logs in traditional pmss only reﬂect
what has been modeled before, but do not capture information about process
changes. while earlier work on process mining has mainly focused on issues re-
lated to control ﬂow mining, recent work additionally uses event-based data for
mining model perspectives other than control ﬂow (e.g., social networks [1], actor
assignments, and decision mining [19]).
in recent years, several approaches for adaptive process management have
emerged [17], most of them supporting changes of certain process aspects and
changes at diﬀerent levels. examples of adaptive pmss include adept [16],
cbrﬂow [20], and wasa [21]. though these pmss provide more meaningful
process logs when compared to traditional workﬂow systems, so far, only little
work has been done on fundamental questions like what we can learn from this
additional log information, how we can utilize change logs, and how we can derive
optimized process models from them. cbrﬂow has focused on the question how
to facilitate exception handling in adaptive pmss. in this context case-based
reasoning (cbr) techniques have been adopted in order to capture contextual
knowledge about ad-hoc changes in change logs, and to assist actors in reusing
previous changes. [20]. this complementary approach results in semantically
enriched log-ﬁles (e.g., containing information about the frequency of a particular
change, user ratings, etc.) which can be helpful for our future work.
7 summary and outlook
in this paper we presented an approach for integrating adaptive process manage-
ment and process mining in order to exploit knowledge about process changes
from change logs. for this we have developed a mining technique and imple-
mented it as plug-in of the prom framework taking adept change logs as
input. we demonstrated that change log information (as created by adaptive
pmss like adept) can be imported into the prom framework. based on this
we have sketched how to discover a (minimal) change process which capturesall modiﬁcations applied to a particular process instance so far. this discov-
ery is based on the analysis of the (temporal) dependencies existing between
the change operations applied to the respective process instance. how single
change processes can be combined to one aggregated change process (capturing
all instance changes applied) has been presented afterwards. finally we have
described the implementation framework behind our approach. altogether, the
presented approach can be very helpful for process engineers to get an overview
about which instance changes have been applied at the system level and what
we can learn from them. corresponding knowledge is indispensable to make the
right decisions with respect to the introduction of changes at the process type
level (e.g., to reduce the need for ad-hoc changes at the instance level in future).
in our future work we want to further improve user support by augmenting
change processes with additional contextual information (e.g., about the rea-
son why changes have been applied or the originator of the change). from this
we expect better comprehensibility of change decisions and higher reusability
of change knowledge (in similar situations). the detection of this more context-
based information will be accomplished by applying advanced mining techniques
(e.g., decision mining [19]) to change log information.
acknowledgements: this research has been supported by the technology
foundation stw, applied science division of nwo and the technology pro-
gramme of the dutch ministry of economic aﬀairs.
references
1. w.m.p. van der aalst, h.a. reijers, and m. song. discovering social networks
from event logs. computer supported cooperative work , 14(6):549–593, 2005.
2. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128–1142, 2004.
3. r. agrawal, d. gunopulos, and f. leymann. mining process models from work-
ﬂow logs. in sixth international conference on extending database technology ,
pages 469–483, 1998.
4. j.e. cook and a.l. wolf. discovering models of software processes from event-
based data. acm transactions on software engineering and methodology ,
7(3):215–249, 1998.
5. j. dehnert and w.m.p. van der aalst. bridging the gap between business models
and workﬂow speciﬁcations. international journal of cooperative information
systems , 13(3):289–332, 2004.
6. j. desel, w. reisig, and g. rozenberg, editors. lectures on concurrency and petri
nets, volume 3098 of lecture notes in computer science . springer-verlag, berlin,
2004.
7. b.f. van dongen and w.m.p. van der aalst. multi-phase process mining: building
instance graphs. in p. atzeni, w. chu, h. lu, s. zhou, and t.w. ling, editors, in-
ternational conference on conceptual modeling (er 2004) , volume 3288 of lecture
notes in computer science , pages 362–376. springer-verlag, berlin, 2004.8. b.f. van dongen and w.m.p. van der aalst. multi-phase process mining: ag-
gregating instance graphs into epcs and petri nets. in proceedings of the 2nd
international workshop on applications of petri nets to coordination, worklﬂow
and business process management (pncwb) at the icatpn 2005 , 2005.
9. b.f. van dongen, a.k. de medeiros, h.m.w. verbeek, a.j.m.m. weijters, and
w.m.p. van der aalst. the prom framework: a new era in process mining tool
support. in g. ciardo and p. darondeau, editors, proceedings of the 26th interna-
tional conference on applications and theory of petri nets (icatpn 2005) , vol-
ume 3536 of lecture notes in computer science , pages 444–454. springer-verlag,
berlin, 2005.
10. m. dumas, w.m.p. van der aalst, and a.h.m. ter hofstede. process-aware infor-
mation systems: bridging people and software through process technology . wiley
& sons, 2005.
11. r. van glabbeek and u. goltz. reﬁnement of actions and equivalence notions
for concurrent systems. acta informatica , 37(4–5):229–327, 2001.
12. r.j. van glabbeek and w.p. weijland. branching time and abstraction in bisim-
ulation semantics. journal of the acm , 43(3):555–600, 1996.
13. b. kiepuszewski. expressiveness and suitability of languages for control flow
modelling in workﬂows . phd thesis, queensland university of technology, bris-
bane, 2002. (available via http://www.workﬂowpatterns.com/) .
14. m. reichert and p. dadam. adeptﬂex - supporting dynamic changes of
workﬂows without loosing control. journal of intelligent information systems ,
10(2):93–129, 1998.
15. m. reichert, s. rinderle, and p. dadam. on the common support of workﬂow
type and instance changes under correctness constraints. in proc. int’l conf. on
cooperative information systems (coopis’03) , pages 407–425, catania, 2003.
16. m. reichert, s. rinderle, u. kreher, and p. dadam. adaptive process management
with adept2. in proc. 21st int’l conf. on data engineering (icde’05) , pages
1113–1114, tokyo, 2005.
17. s. rinderle, m. reichert, and p. dadam. correctness criteria for dynamic changes
in workﬂow systems – a survey. data and knowledge engineering, special issue
on advances in business process management , 50(1):9–34, 2004.
18. s. rinderle, m. reichert, m. jurisch, and u. kreher. on representing, purging,
and utilizing change logs in process management systems. in proc. int’l conf.
on business process management (bpm’06) , vienna, 2006.
19. a. rozinat and w.m.p. van der aalst. decision mining in prom. in proc. int’l
conf. on business process management (bpm’06) , vienna, 2006.
20. b. weber, s. rinderle, w. wild, and m. reichert. ccbr-driven business process
evolution. in proc. int. conf. on cased based reasoning (iccbr’05) , chicago,
2005.
21. m. weske. formal foundation and conceptual design of dynamic adaptations in
a workﬂow management system. in proc. hawaii international conference on
system sciences (hicss-34) , 2001.