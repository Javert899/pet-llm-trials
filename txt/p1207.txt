concurrency and objects matter!
disentangling the fabric of real operational processes
to create digital twins
wil m.p. van der aalst
1process and data science (informatik 9), rwth aachen university, aachen, germany
2fraunhofer-institut f ¨ur angewandte informationstechnik (fit), sankt augustin, germany
wvdaalst@pads.rwth-aachen.de
abstract. process mining dramatically changed the way we look at process mod-
els and operational processes. even seemingly simple processes like purchase-to-
pay (p2p) and order-to-cash (o2c) are often amazingly complex, and traditional
hand-made process models fail to capture the true fabric of such processes. many
processes are inherently concurrent and involve interaction between different ob-
jects (customers, suppliers, orders, items, shipments, payments, machines, work-
ers, etc.). process mining uses event data to construct process models that can
be used to diagnose performance and compliance problems. if such models re-
ﬂect reality well, they can be used for forward-looking forms of process mining,
including predictive analytics, evidence-based automation, and what-if simula-
tion. the ultimate goal is to create a “digital twin of an organization” that can
be used to explore different improvement actions. this paper provides a high-
level overview of the different process mining tasks followed by a more detailed
discussion on concurrency and object-centricity in process mining.
keywords: process mining · event data · concurrency · digital twins
1 towards a digital twin of an organization
the desire to adequately describe operational processes has been around since the 1890-
ties when the ﬁeld of scientiﬁc management emerged. scientiﬁc management is also
known as taylorism, named after its pioneer frederick winslow taylor (1856-1915)
who tried to systematically improve economic efﬁciency, especially labor productivity.
taylor systematically observed how people work and can be seen as the “ﬁrst process
miner” using pen and paper (see figure 1). in 1950 computers started to inﬂuence busi-
ness processes. however, the systematic use of data about operational processes is much
more recent [1].
the desire to build computer models that mimic organizations and processes is also
not that new. since the 1960-ties so-called discrete event simulation tools have been
available with simula [11] as one of the ﬁrst inﬂuential examples. in discrete event
simulation it is common to estimate parameters and distributions based on observed data
(e.g., service times and arrival rates). however, one still needs to model the process by
hand. the ﬁrst comprehensive approaches to automatically learn complete simulation2 wil m.p. van der aalst
data sheet used in time study 
to analyze performance of 
workers and to compare 
different approaches .detailed analysis of 
shoveling earth data
fig. 1. analyzing event data to improve operational processes is not that new. this is illustrated
by some of the tables in [33]. frederick winslow taylor can be seen as the ﬁrst “process miner”
using manually collected event data.
models from event data became available around 2008 [30, 31]. based on event logs, it
is possible to learn a control-ﬂow model (transition system, petri net, of bpmn model)
that is enriched with information about resources, data, and time using replay or align-
ment techniques [30, 31].
organization with 
multiple operational 
processesevent 
dataprocess 
mining
diagnose , predict , improvehighly concurrent
many 
interacting 
objectsdigital
twins
what if ?
fig. 2. process mining provides a concrete approach to create a digital twin of an organization
and its operational processes. a key element is the creation of a model based on event data that is
able to mimic reality as well as possible. such a model needs to be able to capture concurrency
and interacting objects (customers, workers, products, orders, payments, shipments, etc.).
the notion of a digital twin is part of the industry 4.0 development facilitated
through advanced data analytics (machine learning, process mining, etc.) and the inter-
net of things (iot) connectivity [20, 15]. the notion can be described as an effortless
integration of the “real reality” and a “modeled reality” in both directions. the “mod-
eled reality” is based on the “real reality”, but may also inﬂuence the “real reality”.
this is one of the key concepts in the internet of production (iop) developed at rwth
aachen university [6]. in iop, process mining plays a key role. gartner coined the term
digital twin of an organization to indicate that the desire to create a digital twin is not
limited to speciﬁc industry 4.0 applications [19]. the goal is to create a virtual rep-
resentation of an organization and its operational processes (including assets such asconcurrency and objects matter! 3
architectures, infrastructures, roles, responsibilities, products, etc.) to assess the impact
of change in a controlled environment. note that this is only a vision that is still far
away from reality. however, it illustrates the role that models will need to play in the
future.
reality
digital
modelreality
digital
shadowreality
digital
twinmanual /
offline
automatic /
real-time(a) (b) (c)
fig. 3. the difference between a digital model, a digital shadow, and a digital twin.
figure 3 illustrates the difference between (a) a digital model, (b) a digital shadow,
and (c) a digital twin. building a discrete event simulation model (using e.g. arena,
anylogic, cpn tools, flexsim, vensim, or simul8) in a classical way corresponds to
thedigital model notion in figure 3(a). the dashed lines show that the model is created
by hand. there is no automated connection between reality and the digital model. more-
over, insights generated by the simulation model do not automatically lead to concrete
actions. the digital shadow notion in figure 3(b) uses a model, driven by data automat-
ically extracted from reality. if such a connection is automated, it is often possible and
desirable to update the model continuously. if reality changes, also the model changes.
however, insights and diagnostics still need to be translated into actions manually. the
digital twin notion in figure 3(c) shows that there is an automated and real-time con-
nection between reality and the model(s) in both directions. as a result, the digital twin
directly inﬂuences reality, possibly without human intervention.
it is important to note that many of these ideas have been realized in the context
of process mining, albeit with a focus on individual processes in isolation [1]. most
process mining techniques aim to create a digital shadow, as indicated in figure 3(b).
this ranges from control-ﬂow discovery (from mining directly follows graphs [2] to
scalable inductive mining [23]) to automatically creating simulation models (e.g., [30,
31]). however, under the umbrella term of “operational support” [1], process mining
also aims to impact the process automatically in real-time. an early example is the work
presented in [32], where workﬂow technology is connected to process mining. in [32]
yawl is used as a workﬂow management system, prom as a process mining system,
and cpn tools as the simulation engine. prom is used to learn a faithful simulation
model from the event data of yawl and/or the models in yawl. at any point in time,
the current state of the yawl workﬂow system can be loaded into the simulation model
and simulated using cpn tools. this concept is termed short-term simulation because4 wil m.p. van der aalst
rather than focusing on the steady-state behavior, the focus is on transient behaviors and
answering what-if questions. commercial process mining tools increasingly support
what we call “action-oriented process mining”. this means that diagnostics are turned
into actions. the recent release of the celonis ems (execution management system),
which embeds a low-code workﬂow management system, illustrates this trend.
the above shows that the idea of creating a digital twin was already realized in
the ﬁeld of process mining long before the term became “in vogue”. however, existing
approaches typically focus on well-deﬁned processes that are considered in isolation.
we are still far away from creating a realistic “digital twin of an organization”. in this
paper, we focus on two of the many challenges to create such digital twins:
–concurrency. organizations are like distributed systems or social systems. the dif-
ferent parts operate autonomously but need to synchronize at selected points in
time. although most organizations and systems are highly concurrent, the dominant
paradigm is still the highly sequential turing machine model created in 1936 which
does not allow for concurrency. the von neumann architecture deﬁned in 1945 is
based on the turing machine and also views computation as a sequential process.
moreover, automata, transition systems, markov chains, and many other represen-
tations of behavior do not support concurrency. if concurrency is supported, it is of-
ten added as an afterthought. representations that start from concurrency, like petri
nets, are still the exception. consider for example a petri net without places and
just transitions. even people familiar with petri nets have difﬁculties to accept that
such a petri net allows for any behavior (and that petri nets are much more declara-
tive than commonly assumed). although organizations are highly concurrent, event
logs are viewed as sequential (i.e., events are assumed to be totally ordered). this
complicates the creation of a digital twin from event data.
–object-centricity. most of the modeling notations used (e.g., bpmn, workﬂow
nets, uml activity diagrams, etc.) assume a single case notion. however, events
may involve a variety of objects. consider for example batching where in one event
many objects are affected or an assembly step where a collection of objects is trans-
formed into a new composite object. when drawing for example a bpmn model
one needs to pick one case notion (the process instance). in many applications this
is not so easy. consider for example the hiring process of new employees. is the
vacancy the case or the application? one can also consider the classical example
of ordering books from amazon. one order may include multiple books, a ship-
ment may contain books of different orders, and an order may involved multiple
shipments. possible case notions are order, book, and shipment. it is impossible to
create a digital twin of an organization without being able to represent the different
objects and their interactions.
for example, imagine a car factory producing hundreds of cars per day with each car
assembled from thousands of components. process models that do not allow for con-
currency and object-centricity are clearly unable to describe such a factory as a digital
twin.
the remainder of this paper is organized as follows. section 2 present a short high-
level introduction to process mining. section 3 discusses event logs and the importance
of concurrency and object-centricity. section 4 concludes this short paper.concurrency and objects matter! 5
2 process mining: a top-down view
in recent years, we could witness an uptake in process mining. there used to be a gap
between process science (i.e., tools and techniques to improve operational processes)
anddata science (i.e., tools and techniques to extract value from data). mainstream
machine learning and data mining techniques do not consider operational processes.
business process management (bpm) and operations research (or) tend to start from
models rather than data. process mining bridges this gap [1].
currently, there are over 35 commercial process mining vendors (abbyy time-
line, aris process mining, businessoptix, celonis process mining, disco/fluxicon,
everﬂow, lana, mavim, mpm, minit, pafnow, qpr, etc.) and process mining is ap-
plied in most of the larger organizations in countries such as germany and the nether-
lands. example application domains include: ﬁnance (rabobank, hypovereinsbank,
etc.), telecom (deutsche telekom, v odafone, etc.), logistics (vanderlande, etc.), pro-
duction (bmw, siemens, fiat, bosch, etc.), food (edeka, etc.), fashion (zalando, etc.),
energy (e-on, etc.), transport (uber, db, lufthansa, etc.), healthcare (astrazenica,
medtronic, etc.), consulting (deloitte, ey , kpmg, etc.), and it systems (dell, ibm,
servicenow, etc.).
discover
align
replay
enrich
apply
compare
information 
systems
extract
process 
models
explore
 select
filter
clean
conformance
performance 
diagnostics
predictions
improvements
transform
act
 show
model
adapt
show
interpret
drill down
ml
+
 +
event 
data
fig. 4. overview of the process mining pipeline.
figure 4 shows a high-level overview of process mining. event data need to be
extracted from information systems. such data can be explored, ﬁltered, and cleaned.
process discovery tools transform event data into process models (e.g., bpmn, petri
nets, and uml activity diagrams). there are simple approaches like creating so-called
directly-follows-graphs (dfgs) that do not discover concurrency thus having obvi-
ous problems [2]. the alpha algorithm was the ﬁrst to discover concurrent processes
[7]. this approach provides some guarantees, but most processes do not satisfy the as-
sumptions described in [7]. after the alpha algorithm, dozens of more sophisticated
algorithms were proposed [1, 9, 21–23, 34]. using replay and alignment techniques it6 wil m.p. van der aalst
is possible to relate process models (hand-made or discovered) with event data. this
can be used to discover differences between reality and model [1, 10, 29]. moreover,
the model can be extended with additional perspectives, e.g., organizational aspects,
decisions, and temporal aspects. this way, detailed performance analyses are possi-
ble. root-cause analysis can be performed for both conformance and performance
problems. it is always possible to relate observations to the original event data. such
evidence-based diagnostics aid discussions about root-causes and possible improve-
ments. the right-hand side of figure 4 refers to forward-looking techniques aimed at
improving the processes. process models extended with additional perspectives (orga-
nizational aspects, decisions, and temporal aspects) can be used to predict conformance
and performance problems. as described in [1], predictions can be used to generate
recommendations. figure 4 shows that machine learning (ml) techniques can be used
in this step. these may range from novel deep learning approaches (e.g., artiﬁcial recur-
rent neural networks like lstm) to more traditional approaches like logistic regression
and decision-tree learning.
it should be noted that process mining techniques are different from mainstream
machine learning (ml) techniques. however, as figure 4 shows, process mining can
be used to generate ml problems. the current trend is to make process mining tech-
niques more action-oriented, e.g., automatically trigger a corrective workﬂow when a
problem emerges.
discover
align
replay
enrich
apply
compare
information 
systems
extract
process 
models
explore
 select
filter
clean
conformance
performance 
diagnostics
predictions
improvements
transform
act
 show
model
adapt
show
interpret
drill down
ml
+
 +
event 
data12
3finding , extracting , and transforming event data is still 
taking up to 80% of the time .
most techniques focus on a single case notion (i.e., a single 
process ), whereas problems may be caused by interacting or 
competing processes .
process discovery is not a solved problem despite powerful 
techniques like inductive mining . concurrency is hard to 
discover from event data that provide only a sample .
there is a need to better integrate mining and modeling 
(e.g., user -guided discovery ).
conformance checking is time -consuming and diagnostics 
tend to be non -deterministic .
there is a need for techniques recommending process 
changes (i.e., moving beyond diagnostics ).
machine learning (ml) techniques tend to perform poorly 
because essential aspects are missed (e.g., system load ). 
process mining results need to trigger automated actions 
(e.g., start a corrective workflow ).1
2
3
4
5
6
7
845
67
8
fig. 5. some of the challenges encountered when applying process mining.
the process mining manifesto [17] published a decade ago lists 11 challenges. most
of these challenges still exist and are still relevant. figure 5 maps eight challenges onto
the overview used before (figure 4). these are partly overlapping with the challenges
listed in [17], e.g., basic tasks like data extraction and process discovery remain chal-
lenging. the reader interested in applications of process mining is recommended to
read [26] with experience reports from siemens, bmw, uber, abb, bayer, and several
other organizations.
the top-left corner and bottom-right corner show the interface between the real
systems and organization on the one hand and process mining technology on the otherconcurrency and objects matter! 7
hand. these correspond to the solid arrows in figure 3(c) used to explain the notion of a
digital twin. using state-of-the-art process mining tools it is possible to create a digital
twin with limited scope (e.g., a single process). process mining is probably the most
concrete technology available to create digital twins. most of the proposals are merely
visions or application speciﬁc.
3 process mining: a bottom-up view
after providing a high-level view on process mining, we focus on concurrency and
object-centricity. these are essential to create digital twins that properly reﬂect real
organizations. to illustrate these concepts, we use petri nets. however, it is good to
note that the ideas are generic and not notation-speciﬁc.
3.1 petri nets
figure 6 shows an accepting labeled petri net eight places ( p1;p2;:::;p 8) and seven
transitions (t1;t2;:::;t 7) with initial marking [p1]and ﬁnal marking [p8]. we assume
that the reader is familiar with the semantics of petri nets [5, 13, 25, 27, 28]. however, to
make the paper more self-contained, we informally explain the behavior of an accept-
ing labeled petri net. a transition tisenabled in a marking if each of its input places
contains at least one token. an enabled transition tmay ﬁre, i.e., one token is removed
from each of the input places tand one token is produced for each of the output places
t. this way the petri net can move from one marking to the next. for example, in the
marking shown in figure 6 (with a token in p1) onlyt1is enabled. firing t1means the
consumption of one token and the production of three tokens. the resulting marking
is[p2;p3;p4]. in this marking, four transitions are enabled: there is a choice between
t5ort6because both compete for the token in p4. the ordering of t2,t4, andt5or
t6is not ﬁxed. the transitions in figure 6 are labeled, e.g., executing t6correspond to
taking an x-ray. moreover, next to the initial marking indicated by the black token in
placep1, there is also a ﬁnal target marking with just a token in p8. we are interested in
ﬁring sequences leading from [p1]to[p8]. three examples are: 1=ht1;t2;t4;t5;t7i,
2=ht1;t2;t4;t6;t7i, and3=ht1;t2;t3;t2;t3;t2;t4;t6;t7i. there are inﬁnitely
many ﬁring sequences due to the loop. if we block the loop and do not ﬁre transition
t3, there are 12 possible ﬁring sequences.
figure 7 shows three example runs of the accepting labeled petri net. places in
figure 7 correspond to tokens in figure 6, and transitions in figure 7 correspond to
transition ﬁrings in figure 6. a run of a petri net corresponds to a partial order. for
example,r1in figure 7 does not impose an ordering on the three middle activities. the
transition labels in figure 7 refer to the transitions in figure 6, e.g., t21,t22, andt23in
runr3refer to transition t2(administer medicine). for a formal deﬁnition of the runs
of a petri net, we again refer to standard literature [12, 14, 27]. typically, the number
of runs is much smaller than the number of ﬁring sequences. for example, if we block
the loop and do not ﬁre transition t3, then there are only two runs ( r1andr2) whereas
there where 12possible ﬁring sequences (e.g., 1is one of the six ﬁring sequences
corresponding to run r1). runr3corresponds 76 = 42 ﬁring sequences.8 wil m.p. van der aalst
t1t3t2
t7 t4
initial 
examinationfinal 
examination
t6t5
ct scan
x-raylab testsadminister medicine
re-examinationp2
p3
p4p5
p6
p7p8 p1
fig. 6. an accepting labeled petri net eight places ( p1; p2; : : : ; p 8) and seven transitions
(t1; t2; : : : ; t 7).
t11t21
t71 t41
initial 
examinationfinal 
examination
t51
ct scanlab testsadminister medicine
p21
p31
p41p51
p61
p71p81 p11
t11t21
t71 t41
initial 
examinationfinal 
examination
t61
x-raylab testsadminister medicine
p21
p31
p41p51
p61
p71p81 p11
t11t31 t21
t71 t41
initial 
examinationfinal 
examination
t61
x-raylab testsadminister 
medicinere-examination
p21
p31
p41p53
p61
p71p81 p11r1 r2
p51t32 t22
administer 
medicinep52re-examination
p23t23
administer 
medicinep22r3
fig. 7. three example runs of the accepting petri net: r1,r2, andr3. run r3corresponds to 42
ﬁring sequences.
the fact that run r3corresponds to 42 ﬁring sequences illustrates the challenge of
discovering concurrency. if we assume that t3is executed at most 5 times, then there are
2(1+1+1+1+1+1) = 12 runs and 2(1312+1110+98+76+54+32) = 812
ﬁring sequences. even when our event log has information about thousands of traces,
it is extremely unlikely that one can witness all 812 variants (especially when not allconcurrency and objects matter! 9
variants have an equal probability). this illustrates that one cannot ignore concurrency,
because it will lead to an explosion of possible interleavings of which just a fraction
will be witnessed.
3.2 object-centric partially-ordered event logs
next to the problem of concurrency, we also need to deal with events referring to col-
lections of objects. this is analogous to moving from a classical petri net to a colored
petri net (cpn) [5, 18]. in a cpn, tokens have values and can present different objects.
in a classical petri net, tokens are indistinguishable and transitions cannot consumer or
produce a variable number of tokens.
techniques to discover petri nets from event data assume precisely one case identi-
ﬁer per event [3, 4]. these case identiﬁers are used to correlate events, and the resulting
discovered petri net aims to describe the life-cycle of individual cases. in reality, there
are often multiple intertwined case notions, and it is impossible to pick a single case no-
tion to describe the whole process. for example, events may refer to mixtures of orders,
items, packages, customers, and products. a package may refer to multiple items, multi-
ple products, one order, and one customer. therefore, we need to assume that each event
refers to a collection of objects, each having a type (instead of a single case identiﬁer).
such object-centric event logs are closer to data in real-life information systems (e.g.,
sap, salesforce, oracle, etc.). from an object-centric event log, we want to discover an
object-centric petri net with places that correspond to object types and transitions that
may consume and produce collections of objects of different types. such object-centric
petri nets visualize the complex relationships among objects of different types.
in the remainder, we present object-centric event logs as deﬁned in [3, 4]. note that
this is a simpliﬁed version of the later ocel standard (seeocel-standard.org )
which also adds attributes to objects [16]. ocel also provides json/xml serializa-
tions of object-centric event logs and intends to overcome the limitations of the xes
standard [8]. recall that is the ofﬁcial ieee standard for storing and exchanging event
data assuming a single case notion.
deﬁnition 1 (universes). we deﬁne the following universes (based on [3, 4]):
–ueiis the universe of event identiﬁers,
–uactis the universe of activity names (also used to label transitions in an accepting
petri net),
–utime is the universe of timestamps,
–uotis the universe of object types (also called classes),
–uoiis the universe of object identiﬁers (also called entities),
–type2uoi!uotassigns precisely one type to each object identiﬁer,
–uomap =fomap2uot6!p (uoi)j8ot2dom (omap )8oi2omap (ot)type(oi) =
otgis the universe of all object mappings indicating which object identiﬁers are
included per type,1
1p(uoi)is the powerset of the universe of object identiﬁers, i.e., object types are mapped onto
sets of object identiﬁers. omap2uot6! p (uoi)is a partial function. if ot62dom (omap ),
then we assume that omap (ot) =;.10 wil m.p. van der aalst
–uattis the universe of attribute names,
–uvalis the universe of attribute values,
–uvmap =uatt6!uvalis the universe of value assignments,2and
–uevent =ueiuactutimeuomapuvmap is the universe of events.
an evente= (ei;act;time;omap;vmap )2uevent is characterized by a unique
event identiﬁer ei, the corresponding activity act, the event’s timestamp time , and two
mappings omap andvmap for respectively object references and attribute values.
deﬁnition 2 (event projection). givene= (ei;act;time;omap;vmap )2uevent ,
ei(e) = ei,act(e) = act,time(e) = time ,omap(e) = omap , andvmap(e) =
vmap .
omap(e)2uot6!p(uoi)maps a subset of object types onto sets of object iden-
tiﬁers for an event e. an object-centric event log is a collection of partially ordered
events . event identiﬁers are unique, i.e., two events cannot have the same event identi-
ﬁer.
deﬁnition 3 (object-centric event log). l= (e;e)is an event log with e
uevent andeeesuch that:
–edeﬁnes a partial order (reﬂexive, antisymmetric, and transitive),
–8e1;e22eei(e1) =ei(e2))e1=e2, and
–8e1;e22ee1ee2)time(e1)time(e2).
deﬁnition 3 allows for partially ordered event logs. many process mining tech-
niques require a total order, e.g., events are ordered based on timestamps and when two
events have the same timestamp we assume some order. however, there are process
discovery techniques that take into account causalities [3, 24]. these can exploit such
partial orders. there may be many reasons to use partially ordered event logs: efﬁciency,
imprecise timing information, uncertainty, and explicit partial order information (e.g.,
based on data ﬂow analysis). as argued before, it is unreasonable to assume that all
possible interleavings will indeed be present in the event log. instead of a partial order
one can also use the stricter notion of a weak order. this is particularly suitable when
one has imprecise timestamps (e.g., events on the same day cannot be ordered).
3.3 object-centric petri nets
in this paper, we argued that concurrency and objects matter. to progress the ﬁeld of
process mining, we cannot assume that events are totally ordered and can be correlated
using a single case notion. hence, we need process mining techniques and process
model representations handling concurrency and object-centricity as ﬁrst-class citizens.
in [4], we presented an approach to automatically learn a so-called object-centric petri
net(ocpn) given an object-centric event log (e.g., in ocel format [16]). a detailed
explanation of the approach to discover ocpns is beyond the scope of this short paper.
therefore, we only show the example depicted in figure 8.
2uatt6!uvalis the set of all partial functions mapping a subset of attribute names onto the
corresponding values.concurrency and objects matter! 11
po
in pi
sr
pa
coplace 
order
pick 
itemsend 
invoice
pay 
order send 
reminder
mark as 
completedo2o1
i2
o3
i3
o4i5
o5i1
order
order
order
order
orderi6item
item
item
item
itemststart 
route
enend 
route
r3
routeitem routerouter1
r2 i4po
pi
coplace 
order
pick 
item
mark as 
completedi2
i3
i5i1
i6item
item
item
item
itemststart 
route
enend 
routeitemi4ststart 
route
enend 
route
r3
routerouterouter1
r2po
in
sr
pa
coplace 
order
send 
invoice
pay 
order send 
reminder
mark as 
completedo2o1
o3
o4
o5order
order
order
order
order100
100
100
10050
100 500
500500500500 10
10
10500
100
100
100500 10
500
50
100
10010
105
5
50
50
50
50 5
51
1
1
111
1
1
1
11
1
11
1
1
fig. 8. an object-centric petri net (ocpn) can be learned by ﬁrst learning a classical petri net
per object type and then merging the nets while correcting the multiplicities using variable arcs
(a detailed derivation of the process model was presented in [4]).
letl= (e;e)be an event log. the events in erefer to objects. therefore,
given a speciﬁc object oof typeot, it is possible to create a partial order of all events
that refer to o.(eo;eo), witheo=fe2ejo2omap(e)(ot)gandeo=e
\(eoeo), deﬁnes the corresponding partial order. hence, we can group all partial
orders of events corresponding to objects of a given type otto get the required input for12 wil m.p. van der aalst
a standard process discovery algorithm. note that the same event may appear in multiple
partial orders. next, we can learn a process model per object type. for simplicity, we
assume that we discover a labeled accepting petri net per object type satisfying the
constraint that labels of visible transition are unique. there may be silent transitions
(i.e., transitions that do not refer to an activity). however, there cannot be two transitions
referring to the same activity.
the top part of figure 8 shows three labeled accepting petri nets discovered for 100
orders, 500 items, and 10 routes. these three models happen to be sequential, but could
have been concurrent. the initial and ﬁnal markings are denoted by the places with the
play and stop symbol. next, the three labeled accepting petri nets are merged into an
ocpn. since the visible transitions are unique, merging is trivial. however, the anno-
tations need to be modiﬁed. in an ocpn there is a one-to-one correspondence between
transition ﬁrings and events. a single event (i.e., transition occurrence) may involve a
variable number of objects (e.g., one order may have any number of items). this is
indicated by the double arcs in the lower part of figure 8. for example, on average one
execution of place order corresponds to ﬁve items and one order. on average one exe-
cution of start route corresponds to 50 items and one route. for more details, we refer
to [4].
the discovery object-centric petri nets (ocpns) from object-centric event logs in
ocel format is still in its infancy. however, the topic is important because in most
applications of process mining one faces the problem of one-to-many and many-to-
many relations between different types of objects relevant for an organization. processes
are intertwined and difﬁcult to separate. figure 8 shows that it is possible to create
one, more holistic, process model that is showing the interactions between the different
types of objects. actually, the term “process model” may be misleading in the context
of ocpns that may represent collections of interacting processes.
4 conclusion
to create a “digital twin of an organization” we need to disentangle the fabric of real
operational processes. process mining provides many of the ingredients to make such a
step. in this paper, we provided a high-level overview of process mining and linked it to
historical developments in the ﬁeld of scientiﬁc management and simulation. as shown,
there have been early examples of digital twins (or at least digital shadows) in the ﬁeld
of process mining. we mentioned, for example, the work combining the process mining
framework prom, the workﬂow management system yawl, and cpn tools as the
simulation engine [32]. this enabled new forms of “short-term simulation” that can be
used to see the effects of decisions given the current state and historic information.
however, we are far away from fully capturing the fabric of real operational pro-
cesses in a single model. an important prerequisite is the proper handling of concur-
rency and entangled objects. one event may refer to many objects and organizations
are highly concurrent. it is unrealistic to assume that one can witness all interleavings
of highly concurrent processes. therefore, we elaborated on object-centric petri nets
(ocpns) and ocel as a format for exchanging object-centric event logs [16].concurrency and objects matter! 13
future research needs to address the challenges described in this paper. compared
to the pen-and-paper analyses done by frederick winslow taylor and colleagues more
than a century ago, we have booked tremendous progress. the detailed event data avail-
able today provide unprecedented opportunities to create digital twins (provided we are
able to concurrency and object-centricity properly).
acknowledgments the author thanks the alexander von humboldt (avh) stiftung for
supporting his research.
references
1. w.m.p. van der aalst. process mining: data science in action . springer-verlag, berlin,
2016.
2. w.m.p. van der aalst. a practitioner’s guide to process mining: limitations of the directly-
follows graph. in international conference on enterprise information systems (centeris
2019) , volume 164 of procedia computer science , pages 321–328. elsevier, 2019.
3. w.m.p. van der aalst. object-centric process mining: dealing with divergence and con-
vergence in event data. in p.c. ¨olveczky and g. sala ¨un, editors, software engineering and
formal methods (sefm 2019) , volume 11724 of lecture notes in computer science , pages
3–25. springer-verlag, berlin, 2019.
4. w.m.p. van der aalst and a. berti. discovering object-centric petri nets. fundamenta
informaticae , 175(1-4):1–40, 2020.
5. w.m.p. van der aalst and c. stahl. modeling business processes: a petri net oriented
approach . mit press, cambridge, ma, 2011.
6. w.m.p. van der aalst, t.brockhoff, a. farhang, m. pourbafrani, m.s. uysal, and s. j. van
zelst. removing operational friction using process mining: challenges provided by the
internet of production (iop). in c. quix, editor, selected papers of the international con-
ference on data science, technology and applications (data 2020) , communications in
computer and information science. springer-verlag, berlin, 2021.
7. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining: discovering
process models from event logs. ieee transactions on knowledge and data engineering ,
16(9):1128–1142, 2004.
8. g. acampora, a. vitiello, b. di stefano, w. van der aalst, c. g ¨unther, and e. verbeek. ieee
1849: the xes standard - the second ieee standard sponsored by ieee computational
intelligence society. ieee computational intelligence magazine , 12(2):4–8, 2017.
9. a. augusto, r. conforti, m. marlon, m. la rosa, and a. polyvyanyy. split miner: auto-
mated discovery of accurate and simple business process models from event logs. knowl-
edge information systems , 59(2):251–284, may 2019.
10. j. carmona, b. van dongen, a. solti, and m. weidlich. conformance checking: relating
processes and models . springer-verlag, berlin, 2018.
11. o.j. dahl and k. nygaard. simula: an algol based simulation language. communi-
cations of the acm , 1:671–678, sept 1966.
12. j. desel. validation of process models by construction of process nets. in w.m.p. van
der aalst, j. desel, and a. oberweis, editors, business process management: models, tech-
niques, and empirical studies , volume 1806 of lecture notes in computer science , pages
110–128. springer-verlag, berlin, 2000.
13. j. desel and j. esparza. free choice petri nets , volume 40 of cambridge tracts in theoret-
ical computer science . cambridge university press, cambridge, uk, 1995.14 wil m.p. van der aalst
14. b.f. van dongen, j. desel, and w.m.p. van der aalst. aggregating causal runs into work-
ﬂow nets. in k. jensen, w.m.p. van der aalst, m. ajmone marsan, g. franceschinis,
j. kleijn, and l.m. kristensen, editors, transactions on petri nets and other models of con-
currency (topnoc vi) , volume 7400 of lecture notes in computer science , pages 334–363.
springer-verlag, berlin, 2012.
15. a. fuller, z. fan, c. day, and c. barlow. digital twin: enabling technologies, challenges
and open research. ieee access , 8:108952–108971, 2020.
16. a.f. ghahfarokhi, g. park, a. berti, and w.m.p. van der aalst. ocel standard. www.ocel-
standard.org, 2021.
17. ieee task force on process mining. process mining manifesto. in f. daniel, k. barkaoui,
and s. dustdar, editors, business process management workshops , volume 99 of lecture
notes in business information processing , pages 169–194. springer-verlag, berlin, 2012.
18. k. jensen. coloured petri nets. basic concepts, analysis methods and practical use. vol-
ume 1 . eatcs monographs on theoretical computer science. springer-verlag, berlin,
1997.
19. m. kerremans and j. kopcho. create a digital twin of your organization to optimize your
digital transformation program, research note g00379226. www.gartner.com , 2019.
20. w. kritzinger, m. karner, g. traar, j. henjes, and w. sihn. digital twin in manufacturing: a
categorical literature review and classiﬁcation. ifac-papersonline , 51(11):1016–1022,
2018. 16th ifac symposium on information control problems in manufacturing incom
2018.
21. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. discovering block-structured process
models from event logs: a constructive approach. in j.m. colom and j. desel, editors,
applications and theory of petri nets 2013 , volume 7927 of lecture notes in computer
science , pages 311–329. springer-verlag, berlin, 2013.
22. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. discovering block-structured pro-
cess models from event logs containing infrequent behaviour. in n. lohmann, m. song,
and p. wohed, editors, business process management workshops, international workshop
on business process intelligence (bpi 2013) , volume 171 of lecture notes in business in-
formation processing , pages 66–78. springer-verlag, berlin, 2014.
23. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. scalable process discovery and
conformance checking. software and systems modeling , 17(2):599–631, 2018.
24. x. lu, d. fahland, and w.m.p. van der aalst. conformance checking based on partially
ordered event data. in f. fournier and j. mendling, editors, business process management
workshops, international workshop on business process intelligence (bpi 2014) , volume
202 of lecture notes in business information processing , pages 75–88. springer-verlag,
berlin, 2015.
25. t. murata. petri nets: properties, analysis and applications. proceedings of the ieee ,
77(4):541–580, april 1989.
26. l. reinkemeyer. process mining in action: principles, use cases and outlook . springer-
verlag, berlin, 2020.
27. w. reisig. petri nets: modeling techniques, analysis, methods, case studies . springer-
verlag, berlin, 2013.
28. w. reisig and g. rozenberg, editors. lectures on petri nets i: basic models , volume 1491
oflecture notes in computer science . springer-verlag, berlin, 1998.
29. a. rozinat and w.m.p. van der aalst. conformance checking of processes based on moni-
toring real behavior. information systems , 33(1):64–95, 2008.
30. a. rozinat, r.s. mans, m. song, and w.m.p. van der aalst. discovering colored petri
nets from event logs. international journal on software tools for technology transfer ,
10(1):57–74, 2008.concurrency and objects matter! 15
31. a. rozinat, r.s. mans, m. song, and w.m.p. van der aalst. discovering simulation models.
information systems , 34(3):305–327, 2009.
32. a. rozinat, m. wynn, w.m.p. van der aalst, a.h.m. ter hofstede, and c. fidge. workﬂow
simulation for operational decision support. data and knowledge engineering , 68(9):834–
850, 2009.
33. f.w. taylor. the principles of scientiﬁc management . harper and bothers publishers, new
york, 1919.
34. s.j. van zelst, b.f. van dongen, w.m.p. van der aalst, and h.m.w verbeek. discovering
workﬂow nets using integer linear programming. computing , 100(5):529–556, 2018.