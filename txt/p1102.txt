see discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/335598059
the impact of event log subset selection on the performance of process
discovery algorithms
chapt er · sept ember 2019
doi: 10.1007/978-3-030-30278-8_39
citations
2reads
191
3 author s:
some o f the author s of this public ation ar e also w orking on these r elat ed pr ojects:
data-driv en v alue pr oposition  view pr oject
process quer ying view pr oject
mohammadr eza fani sani
rwth aachen univ ersity
20 publica tions    134 citations    
see profile
seb astiaan v an z elst
fraunhof er instit ute for applied inf ormation t echnolog y fit
52 publica tions    293 citations    
see profile
wil v an der aalst
rwth aachen univ ersity
1,274  publica tions    73,073  citations    
see profile
all c ontent f ollo wing this p age was uplo aded b y mohammadr eza fani sani  on 12 sept ember 2019.
the user has r equest ed enhanc ement of the do wnlo aded file.the impact of event log subset selection on the
performance of process discovery algorithms
mohammadreza fani sani1, sebastiaan j. van zelst1;2, and wil m.p. van der aalst1;2
1process and data science chair, rwth aachen university, aachen, germany
2fraunhofer fit, birlinghoven castle, sankt augustin, germany
{fanisani,s.j.v.zelst,wvdaalst}@pads.rwth-aachen.de
summary. process discovery algorithms automatically discover process models
on the basis of event data, captured during the execution of business processes.
these algorithms tend to use all of the event data to discover a process model.
when dealing with large event logs, it is no longer feasible using standard hard-
ware in limited time. a straightforward approach to overcome this problem is to
down-size the event data by means of sampling. however, little research has been
conducted on selecting the right sample, given the available time and character-
istics of event data. this paper evaluates various subset selection methods and
evaluates their performance on real event data. the proposed methods have been
implemented in both the prom and the rapidprom platforms. our experiments
show that it is possible to speed up discovery considerably using ranking-based
strategies. furthermore, results show that biased selection of the process instances
compared to random selection of them will result in process models with higher
quality.
key words: process mining process discovery subset selection event log
preprocessing performance enhancement
1 introduction
process discovery, one of the main branches of process mining , aims to discover a pro-
cess model that accurately describes the underlying process captured within the event
data [1]. currently, the main research focus in process discovery is on quality issues
of the discovered process models; however, at the same time, the ever-increasing size
of the data handled in process mining leads to performance issues when applying the
existing process discovery algorithms [2]. some process discovery algorithms are im-
practical in big data settings. moreover, some process mining tools impose constraints
on the size of event data, e.g., the number of events. also, in many cases, we do not
require the whole event log, and an approximation of the process can already be dis-
covered by only using a small fraction of the event data.
in real life, process discovery is often of an exploratory nature, that means some-
times we need to apply different process discovery algorithms with several parameters
to generate different process models and select the most suitable process model. when
the discovery algorithms are used repeatedly, such an exploratory approach makes sense
only if performance is reasonable. thus, even a small improvement in performance may2 mohammadreza fani sani et al.
accumulate to a signiﬁcant performance increase when applied several times. further-
more, many process discovery algorithms are designed to also generalize the behavior
that is observed in the event data. in other words, these algorithms are able to reproduce
process behavior extends beyond the example behavior used as input. therefore, it may
still be possible to discover the underlying process using a subset of event data.
this research studies the effectiveness of applying biased sampling on event data
prior to invoking process discovery algorithms, instead of using all the available event
data. in this regard, we present and investigate different biased sampling strategies and
analyze their ability to improve process discovery algorithm scalability. furthermore,
the techniques presented allow us to select a user-speciﬁed fraction of inclusion of the
total available event data. using the prom -based [3] extension of rapidminer [4],
i.e.,rapidprom , we study the usefulness of these sampling approaches, using real
event logs. the experimental results show that applying biased sampling techniques
reduces the required discovery time for all discovery algorithms.
the remainder of this paper is structured as follows. in section 2, we discuss re-
lated work. section 3 deﬁnes preliminary notation. we present different biased sampling
strategies in section 4. the evaluation and corresponding results are given in section 5.
finally, section 6 concludes the paper and presents some directions for future work.
2 related work
many discovery algorithms such as the alpha miner [5], the ilp miner [6, 7], and the
inductive miner [8] ﬁrst create an abstraction of the event data, e.g., the directly follows
graph, and in a second step discover a process model based on it. the performance of all
these algorithms depends on different factors such as the number of process instances
and the unique number of activities.
recently, preprocessing of event data has gained attention. in [9,10], techniques are
proposed to increase the quality of discovered process models by cleansing the event
data. also, in [11] and [12] we have shown that by removing/modifying outlier behav-
ior in event logs, process discovery algorithms are able to discover process models with
higher quality. moreover, [13] uses data attributes to ﬁlter out noisy behavior. filtering
techniques effectively reduce the size of the event data used by process discovery algo-
rithms. however, sometimes the required time for applying these ﬁltering algorithms is
longer than the process discovery time. also, these techniques have no accurate control
on the size of the sampled event log.
filtering techniques focus on removing infrequent behavior from event data; how-
ever, sampling methods aim to reduce the number of process instances and increase the
performance of other algorithms. some sampling approaches have been proposed in the
ﬁeld of process mining. in [14], the authors recommend a random trace-based sampling
method to decrease the discovery time and memory footprint. this method assumes that
process instances have different behavior if they have different sets of directly follows
relations. however, using a unique set of directly follows relations may different types
of process behavior. furthermore, [15] recommends a trace-based sampling method
speciﬁcally for the heuristic miner [16]. in both of these sampling methods, there is no
control on the size of the ﬁnal sampled event data. also, they depend on the deﬁnedvariant subset selection 3
behavioral abstraction that may lead to the selection of almost all the process instances.
in this paper, we analyze random and biased subset selection methods with which we
are able adjust the size of the sampled event data.
3 preliminaries
in this section, we brieﬂy introduce basic process mining terminology and notations
that ease the readability of this paper. given a set x, a multisetmoverxis a function
m:x!n0, i.e., it allows certain elements of xto appear multiple times. m=
fe2xjm(e)>0gis the set of elements present in the multiset. the set of all
possible multisets over a set xis written asm(x).
letxdenote the set of all possible sequences over a set x. a ﬁnite sequence 
of lengthnoverxis a function :f1;2;:::;ng!x, alternatively written as =
hx1;x2;:::;xniwherexi=(i)for1in. the empty sequence is written as .
the concatenation of sequences and0is written as 0. we deﬁne the frequency of
occurrence of 0inbyfreq:xx!n0wherefreq(0;) =jf1ijj 
j0jj0
1=i;:::;0
j0j=i+j0jgj. for example, freq(hbi;ha;b;b;c;d;e;f;hi) = 2
andfreq(hb;di;ha;b;d;c;e;gi) = 1 .
event logs describe sequences of executed business process activities, typically in
the context of some cases (or process instances), e.g., a customer or an order-id. the
execution of an activity in the context of a case is referred to an event . a sequence of
events for a speciﬁc case is referred as a trace . thus, it is possible that multiple traces
describe the same sequence of activities, yet, since events are unique, each trace itself
contains different events. let abe a set of activities. an event log is a multiset of
sequences overa, i.e.,l2m (a). moreover, we let each 2ldescribe a trace-
variant whereasl()denote how many traces of the form are presented within the
event log.slis a subset of event log l, if for any2sl,sl()l(). we callsl
as a sampled event log of l.
different types of behavior abstractions in an event log could be deﬁned. one ab-
straction is the directly follows relation between activities that can be deﬁned as follows.
deﬁnition 1 (directly follows relation). letaandb2a be two activities and =
hx1;::;xnia trace in the event log. a directly follows (df) relation from a to b exists in
, if there isi2f1;::;n 1gsuch thatxi=aandxi+1=band is denoted by a>b.
we can map an event log to a directed graph whose vertices are activities and edges are
directly follows relations and we call it directly follows graph (dfg). so, if there is a
a >bin the event log, there is also a directed edge from atobin the corresponding
dfg of this event log.
in [12], it shows that the occurrence of a low probable sub-pattern, i.e., a sequence
of activities, between pairs of frequent surrounding behavior, which we refer to it as
behavioral contexts has negative effects on the results of process discovery algorithms.
deﬁnition 2 (behavioral context). we deﬁne the set of behavioral contexts present in
event loglaccording to subsequence 0, i.e.,l2p(aa), as follows:
l(0) =
(l;r)2aaj92l;02a 
l0r2	
: (1)4 mohammadreza fani sani et al.
for example, in trace =ha;b;c;d;e;f;hi,ha;biandheiare two subsequences that
surroundhc;di; hence, the pair (ha;bi;hei)is a behavioral context. we inspect the prob-
ability of contextual sub-patterns, i.e., behavior that is surrounded by the frequent be-
havioral contexts and denoted by 0in equation 1. thus, we simply compute the empir-
ical conditional probability of a behavioral sequence, surrounded by a certain context.
deﬁnition 3 ( conditional contextual probability). we deﬁne the conditional contextual
probability ofs, w.r.t.,landrin event logl, i.e., representing the sample based es-
timate of the conditional probability of sbeing surrounded by landrinl. function
l:aaa![0;1], is deﬁned as:
l(s;l;r) =p
2l 
jlsrj
p
2l p
02aj0
l0rj (2)
on the basis of these probabilities, we are able to detect unstructured behavior in traces.
4 subset selection
in this section, we present different subset selection strategies to improve the discovery
procedure’s performance. different behavioral elements of an event log, e.g., events,
directly follow relations, traces, and variants can be used for sampling. however, not
all of them are useful for the purpose of process discovery. by selecting events, it is
possible to consider events from different parts of a process instance that results in
imperfect traces that are harmful for all process discovery algorithms. selecting df
relations is useful for some process discovery algorithms like the alpha miner. but,
they are insufﬁcient for other process discovery algorithms. thus, here we make subset
of event logs only based on traces and variants. consequently, these subset selection
methods take an event log as an input and return a sampled event log. the schematic of
subset selection methods is illustrated in figure 1.
note that in xes standard [3], variants are not stored in event logs separately. how-
ever, there are other structures that we can keep variants and their frequencies as meta-
data [17] that are more efﬁcient for process discovery algorithm. here, we consider
xes standard; however, in sampled event logs, we keep only one trace for each selected
variant. consequently, the frequency of each trace in the sampled event log equals to 1.
event log...〈 …, a, b, b, ...〉   
〈 …, c, b, c, ...〉   〈 …, a, c, b, ...〉   
〈 …, a, b, c, ...〉   
〈 …, a, b, c, ...〉   
〈 …, a, c, b, ...〉   
〈 …, a, b, c, ...〉   detect variants
preprocessing 〈 …, a, b, c, ...〉   ...〈 …, a, b, b, ...〉   m2〈 …, a, b, c, ...〉   m1
〈 …, c, b, c, ...〉   mn〈 …, a, c, b, ...〉   m3multiset event log
subset selection...〈 …, a, b, c, ...〉
〈 …, a, c, b, ...〉   sampled event log
fig. 1: schematic overview of event log
subset selectionin many process discovery algorithms
such as the ilp miner, the family of alpha
miners and the basic inductive miner, the fre-
quencies of traces variants (i.e., l()) have
no important effects on discovered process
models. therefore, here we mainly focus on
selecting variants; but, all these methods can
easily be extended to trace-based subset se-
lection methods. we also used just control-ﬂow related information that is available in
all event logs and this is consistent with the way.variant subset selection 5
one of the most important characteristics of a sampled event log is the number of
its traces, i.e.,jslj. when it is the same as the original event log, there is no reduction
in the size. we can set the size of the sampled event log (i.e., the sampling threshold)
asc=jslj
jljthat0<c11. moreover, the less required subset selection time is more
desirable as it is considered as a preprocessing phase.
we can select traces in an event log randomly or based on some strategies. in the
following, we will explain both of these methods.
4.1 random sampling
in this method, we randomly select cjljtraces in the event log without replacement
and return these traces or just unique the trace-variants among them. this method is fast
because we do not need to traverse the original event log. however, it is possible that
many of sampled traces have similar behavior and we keep just a few unique variants
in the sampled event log. the statistical sampling method [14] works based on this
approach.
as an alternative method, we can ﬁrst ﬁnd all the unique variants in an event log;
afterward, randomly select cjljvariants from them. this approach is a bit slower;
however, it is able to return much behavior compared to the previous approach.
4.2 biased sampling strategies
in general, traversing a big event log is not too time-consuming compared to the time
of process discovery. therefore, as it shown in figure 1, instead of randomly selecting
the variants, we are able to ﬁrst ﬁnd all variants in an event log and use more advanced
strategies (biases) to select them. in this type of approaches, we ﬁrst rank all variants of
an event log based on different strategies. afterward, we return the top cjljvariants
with the highest rank in the sampled event log. we are able to use different ranking
strategies that will be discussed as follows. these ranking strategies have different pre-
processing time and result in different sampled event logs.
as we select variants, the frequency of behavior in the sampled event log will be
unusable. to consider these frequencies, we can beneﬁt from other event log standards
that are able to keep frequencies of variants like [17] or instead of returning cjlj
variants, we should return cjljtraces that correspond to these high ranked variants.
frequency-based selection: the ﬁrst ranking strategy is selecting variants based on
their frequencies in the original event log. this ranking strategy gives higher priority
to a variant that has a higher occurrence frequency in the event log. so, we sort the
variants based on their frequencies or l()and return the top cjljof variants as
a sampled event log. the trace-based version of this strategy is already presented in
many process mining tools that helps users to keep the top most frequent behavior of
the event log. however, in some event logs, the majority of process instances have a
unique trace-variant which makes this subset selection method unusable.
1here, we select only one trace for each variant.6 mohammadreza fani sani et al.
length-based selection: we are able to rank variants based their length (i.e., jj). so,
in this strategy, we sort variants based on their length and choose the longest or the
shortest ones ﬁrst. by using the longer strategy, we keep much behavior in our sampled
event log and at the same time leave out many of incomplete traces, that may improve
the quality of resulted process models. however, if there are self-loops and other loops
in the event log, there is a high probability to choose many infrequent variants with the
same behavior for process discovery algorithms. on the other hand, by applying shorter
strategy, there will be less behavior in the sampled event log; but, it is possible to keep
many incomplete traces that leads to an unsuitable process model.
similarity-based sampling: if we are interested in retaining the main-stream behav-
iors of the event log, we need to rank variants based on the similarity of them to each
other. in this approach, we ﬁrst ﬁnd common behavior of the event log. for this purpose,
we can use different types of behavior; however, the simplest and the most acceptable
type of behavior for process discovery is the df relation. thus, we compute the occur-
rence probability of each directly follows relation (a;b)(thata;b2a) according to the
following equation:
prob (a;b) =j2lja>bj
jlj: (3)
so, we compute the occurrence probability of all of the df relations in a variant. if
prob(a;b)is high enough (i.e., be higher than a deﬁned threshold tp), we expect that
sampled variants should contain it. so, any variant that contains such a high probable
behavior, will give a +1to its rank. otherwise, if a variant does not contain a probable
behavior, we decrease its rank by  1. contrariwise, if a variant contains a low probable
behavior (i.e., prob(a;b)1 tp), we decrease its rank by  1. to normalize the
ranking values, we divide them by the variant length. by using this ranking strategy, we
are looking for variants with much of high probable behavior and less of low probable
ones. note that it is possible that some df relations be neither high probable nor low
probable that we do not consider them in the ranking procedure. finally, we sort the
variants based on their ranks and return the cjljones with the highest rank.
the main advantage of this method is that it helps process discovery algorithms to
depict the main-stream behavior of the original event log in the process model. how-
ever, it needs more time to compute the similarity score of all variants. especially, if we
use more advanced behavioral data structures such as eventually follows instead of df
relations, this computation can be a limitation for this ranking strategy.
structure-based selection: in this subset selection method, we consider the presence
of unstructured behavior (i.e., based on deﬁnition 3) in each variant. in this regard, we
ﬁrst compute the occurrence probability of each sub-patten 0among its surrounding
contextual contexts l(0)(i.e.,l(s;l;r)). if this probability is below the given
threshold, i.e., ts, we consider it as unstructured behavior. we expect that unstruc-
tured subsequences have problematic effects on process discovery algorithms and make
discovered process models inaccurate and complex [12]. thus, for each unstructured
behavior in a variant, we give a penalty to it and decrease its rank by  1. consequently,
a variant with unstructured behavior receives more penalties and it is not appealing to
be placed in the sampled event log.variant subset selection 7
the main reason to use this ranking strategy is that it results in the main skeleton
of the process models. it is designed to reduce improbable parallel behavior and having
simpler process models. however, this subset selection strategy requires longer time to
rank variants in event logs.
5 evaluation
in this section, we aim to ﬁnd out the effects of subset selection methods on the per-
formance of process discovery algorithms. moreover, we will analyze the quality of
process models that are discovered via sampled event logs.
to apply the proposed subset selection methods, we implemented the sample vari-
antplug-in in prom framework2. in this implementation, we used static thresholds for
both similarity and structure-based ranking strategies. using this plug-in, the end user is
able to specify her desired percentage of sampling variants/traces and the ranking strat-
egy. it takes an event log as an input and returns the top fraction of its variants/traces.
in addition, to apply our proposed method on various event logs and use different pro-
cess discovery algorithms with their different parameters, we ported the sample variant
plug-in to rapidprom which extends rapidminer with process analysis capabili-
ties. in our experiment, we also used the statistical sampling method [14]; however, as
we consider only work-ﬂow information, its relaxation parameter is ignored.
table 1: details of real event logs
event log activities# traces# variants# df#
bpic- 2012 26 13087 4336 138
bpic- 2017 28 31509 1593 178
bpic- 2019 44 251734 11973 538
hospital 20 100000 1020 143
road 13 150370 231 70
sepsis 18 1050 846 115information about real event logs that are
used in the evaluation is given in table 13. to
differentiate between the activities that are oc-
curred at the starting and ending parts of traces,
we added artiﬁcial start andend activities to all
of the traces. for process discovery, we used the
inductive miner [18], the ilp miner [7], and the
split miner [19]. on the sampled event logs, we
applied process discovery algorithms just without their built-in ﬁltering mechanisms.
we investigate the probable improvement of subset selection methods on the per-
formance of process discovery algorithms. to measure this improvement, we apply the
following equation:
discoverytimeimprovement =discoverytime (wholelog )
discoverytime (sampledlog ) +samplingtime (wholelog ):
(4)
figure 2 shows improvements in the performance of process discovery algorithms
when we select subset of event logs. here, for ranking-based strategies, we used sam-
pling threshold (i.e., c) equals to 0:1. each experiment was repeated four times (be-
cause the discovery and sampling times are not deterministic) and the average values are
shown. for some event logs, the improvement is more than 100by sampling event logs.
it is shown that the improvement is signiﬁcantly higher when the statistical sampling
2sample variant plug-in in: svn.win.tue.nl/repos/prom/packages/logfiltering
3https://data.4tu.nl/repository/collection:event_logs_real8 mohammadreza fani sani et al.
method is used because it does not need to traverse the input event log. however, for
sepsis that has few traces, ranking based subset selection methods are faster. note that
the structure-based strategy sometimes has no improvement because it requires higher
sampling time. the sampling time of different methods are depicted in figure 4. as
we expected, the statistical sampling method is much faster than other subset selection
methods and the structure-based is the slowest one.
the improvement in performance of process discovery algorithms may be driven
by reducing (1) the number of activities, (2) the number of traces, or (3) the amount
of unique behavior (e.g., df relations or variants). by event log subset selection, it
is possible that some of infrequent activities are not placed in the sampled event log.
moreover, by subset selection we reduce the size of event logs (i.e., jsljjlj) and
also possible behavior in the event log. figure 3 shows the process discovery time of
sampled event logs with different sampling thresholds when we used the ilp miner.
when we used the sampling threshold equals to 0:99, we will have almost all behavior
of the original event log in the sampled event log; however, the number of traces in
the sampled event log is signiﬁcantly lower than the original event log. results show
that for many event logs, the main reason of the improvement in performance of the
process discovery is gained by reducing the number of variants. however, for road and
hospital event logs that there are high frequent variants, reducing the number of traces
has higher impact on the performance of the process discovery algorithms.
as explained, the amount of behavior in the event log has an important role on the
performance of process discovery algorithms. the remained percentage of df relations
in the sampled event logs for different subset selection methods are given in figure 5.
we can see that for most of the event logs the similar and structure based methods keep
fewer df relations. however, according to their ranking policy, they keep the most
common df relations among the original event log.
in the previous experiment, the sampling threshold for ranking-based subset selec-
tion methods equals to 0:1. note that there is no such control on the statistical sampling
method and it aims to keep as much as df relations in sampled event logs. however,
for most of process discovery algorithms variants are more important compared to only
df relations. even the basic inductive miner that uses df relations may result in dif-
ferent process models for event logs that have identical sets of df relations. for ex-
ample,l1= [ha;b;ci;ha;c;bi]andl2= [ha;c;b;ci;ha;bi]have the same sets of df
relations; however, their process models are different. figure 6 indicates the average
percentage of remaining variants in sampled event logs using the statistical sampling
method. it shows that, this method is able to just keep few percentage of variants of
an event log. for ranking-based strategies, the number of remaining variants can be
adjusted by c.
we also compared the average of preprocessing time for trace ﬁltering methods
and the similarity-based subset selection method. for this experiment we ﬁlter event
logs with six different ﬁltering settings and iterate the experiments for four times. also,
we used the similarity-based strategy with the threshold in [0:01;0:05;0:1]. it is clear
that the subset selection method preprocessed the event logs faster. also, with the trace
ﬁltering methods we do not have control over the size of the ﬁltered event logs.variant subset selection 9
fig. 2: discovery time improvement for discovering process using subset selection.
fig. 3: the average of discovery time of the ilp miner for different subset selection
methods and different sampling thresholds.
fig. 4: sampling time of different subset
selection methods.
fig. 5: remained percentage of df rela-
tions in the sampled event logs.
fig. 6: average of remained vari-
ants in sampled event logs using
the statistical sampling [14].
fig. 7: the average of preprocessing time when we
used similarity-based strategy and two state of the
arts trace ﬁltering methods [11, 20].
to analyze the quality of process models that are discovered from sampled event
logs we can use ﬁtness andprecision . fitness measures how much behavior in the event
log is also described by the process model. thus, a ﬁtness value equal to 1, indicates
that all behavior of the event log is described by the process model. precision measures
how much of behavior, that is described by the process model, is also presented in the
event log. a low precision value means that the process model allows for much behavior
compared to the event log. there is a trade-off between these measures [21], sometimes,10 mohammadreza fani sani et al.
fig. 8: comparing the f-measure of discovered process models with different subset se-
lection methods. for ranking-based strategies, we used the sampling threshold in [ 0:01,
0:05,0:1] and the average value of f-measure is presented.
putting aside a small amount of behavior causes a slight decrease in the ﬁtness value,
whereas the precision value increases dramatically. therefore, we use the f-measure
metric that combines both of them according to the following formula:
f-measure =2precisionfitness
precision +fitness: (5)
figure 8 compares the quality of best process models that are discovered with/without
subset selection. we used sampled event logs, just for discovery purpose and the orig-
inal event logs were used for computing f-measure values. for the cases that ranking
based subset selection methods were used, we applied the sampling thresholds [ 0:01,
0:05,0:1], and the average of f-measure values is shown. for the statistical sampling
method, we iterate the experiment four times, and again the average of f-measure val-
ues are considered. according to the results, for the ilp and the inductive miner, we
always have an improvement when we use subset selection. however, for some event
logs, the split miner can discover process models with higher quality via the original
event logs. moreover, the statistical sampling method that randomly selects process
instances results in process models with less quality according to the f-measure com-
pared to ranking based subset selection methods. among ranking strategies, the struc-
ture and similarity-based ones result in better process models for most of the event logs.
for some event logs like bpic-2019 , the similarity-based ranking strategy is the best
choice for all of the process discovery algorithms. for hospital event log, the structure -
based ranking strategy results in process models with the highest f-measure. as there
are frequent variants in road , the best subset selection method for this event log is the
frequency one. results show that length-based methods are not suitable to sample event
logs if the purpose is to have process models with high quality.
in this experiment, we just used the basic versions of the process discovery algo-
rithms. however, in practice users can apply their embedded ﬁltering and select the
best discovered process model. in the next experiment, we applied the inductive miner
and the split miner with 100different ﬁltering thresholds and ﬁnd best process mod-
els according to the f-measure values. figure 9 compares the best f-measure value
that discovered via ranking-based subset selection methods compared to the case that
we used embedded ﬁltering mechanisms of process discovery algorithms on original
event data. as mentioned in subsection 4.2, by applying subset selection methods, we
will lose the frequency of variants. as a result, some embedded ﬁltering mechanismsvariant subset selection 11
fig. 9: the best f-measure of discovered process models when using sampled event
logs compared to applying embedded ﬁltering mechanisms of discovery algorithms.
in process discovery algorithms become unusable. however, the results of this experi-
ment show that we may discover process models with high quality from sampled event
logs, even without these ﬁltering methods. if for any reason we need to use the fre-
quency of variants, it is recommended to apply trace-based subset selection methods
or other standards to store event logs like [17]. also, all of the proposed subset selec-
tion methods needs to load the original event log (like the statistical sampling) that is a
limitation for dealing big event logs when memory is a constraint. moreover, much of
preprocessing time in ranking-based strategies consumed to traverse the event log and
ﬁnd possible variants. this information is already recorded in some event log standards
like mxml [17]. using these standards leads to decrease the preprocessing time of the
proposed approaches.
note that we did not use ﬁltering mechanisms of the process discovery algorithm for
sampled event logs. results show that subset selection methods can increase the quality
of discovered process models; speciﬁcally, if we use the inductive miner. it shows the
weakness of process discovery algorithms in dealing with infrequent behavior [11].
6 conclusion
in this paper, we proposed some subset selection strategies to increase the performance
of process discovery procedure. we recommend to apply process discovery algorithms
on sampled event logs when dealing with large data sets. we implemented different
ranking strategies in prom and ported this functionality into rapidprom and applied it
on some real event logs using different process discovery algorithms.
experimental results show that event log subset selection methods decrease the
required time used by state-of-the-art process discovery algorithms. we found that
ranking-based strategies mostly increase the performance of process discovery by re-
ducing the amount of behavior and the number of traces. therefore, by applying these
methods, we are able to discover an acceptable approximation of the ﬁnal process model
in a shorter time. moreover, results show that for some event logs, subset selection
methods can improve the quality of discovered process models according to the f-
measure metric. results show that to have higher f-measure value, it is better to use
the structure and similarity-based strategies. however, by using random trace sampling
methods, e.g., the statistical method, we can discover process models in a shorter time.12 mohammadreza fani sani et al.
as future work, we aim to ﬁnd out what the best subset selection method is due to the
available time and event data.
references
1. van der aalst, w.m.p.: process mining - data science in action, second edition. springer
berlin heidelberg (2016)
2. van der aalst, w.m.p., et all: process mining manifesto. in: business process management
bpm workshops, clermont-ferrand, france. (2011) 169–194
3. verbeek, h., buijs, j.c., van dongen, b.f., van der aalst, w.m.: xes, xesame, and prom 6.
in: caise, springer (2010) 60–75
4. van der aalst, w.m.p., bolt, a., van zelst, s.: rapidprom: mine your processes and not
just your data. corr abs/1703.03740 (2017)
5. van der aalst, w.m.p., weijters, t., maruster, l.: workﬂow mining: discovering process
models from event logs. ieee trans. knowl. data eng. 16(9) (2004) 1128–1142
6. van der werf, j., van dongen, b., hurkens, c., serebrenik, a.: process discovery using
integer linear programming. fundam. inform. 94(3-4) (2009) 387–412
7. van zelst, s., van dongen, b., van der aalst, w.m.p., verbeek, h.m.w.: discovering work-
ﬂow nets using integer linear programming. computing (nov 2017)
8. leemans, s.j., fahland, d., van der aalst, w.m.p.: discovering block-structured process
models from event logs - a constructive approach. in: application and theory of petri
nets and concurrency. springer berlin heidelberg (2013) 311–329
9. suriadi, s., andrews, r., ter hofstede, a., wynn, m.t.: event log imperfection patterns for
process mining: towards a systematic approach to cleaning event logs. information systems
64(2017) 132–150
10. andrews, r., suriadi, s., ouyang, c., poppe, e.: towards event log querying for data
quality: let’s start with detecting log imperfections. (2018)
11. fani sani, m., van zelst, s.j., van der aalst, w.m.p.: improving process discovery re-
sults by filtering outliers using conditional behavioural probabilities. in: business process
management bpm workshops, barcelona, spain. (2017) 216–229
12. fani sani, m., van zelst, s., van der aalst, w.m.p.: repairing outlier behaviour in event
logs. in: business information systems, springer (2018) 115–131
13. mannhardt, f., de leoni, m., reijers, h.a., van der aalst, w.m.p.: data-driven process
discovery-revealing conditional infrequent behavior from event logs
14. bauer, m., senderovich, a., gal, a., grunske, l., weidlich, m.: how much event data
is enough? a statistical framework for process discovery. in: international conference on
advanced information systems engineering, springer (2018) 239–256
15. berti, a.: statistical sampling in process mining discovery. in: the 9th international con-
ference on information, process, and knowledge management. (2017) 41–43
16. weijters, a.j.m.m., ribeiro, j.t.s.: flexible heuristics miner (fhm). in: cidm. (2011)
17. van dongen, b.f., van der aalst, w.m.p.: a meta model for process mining data.
18. leemans, s.j., fahland, d., van der aalst, w.m.p.: discovering block-structured process
models from event logs containing infrequent behaviour. in: bpi. (2014) 66–78
19. augusto, a., conforti, r., dumas, m., la rosa, m., polyvyanyy, a.: split miner: automated
discovery of accurate and simple business process models from event logs. knowledge
and information systems (2019) 1–34
20. conforti, r., la rosa, m., ter hofstede, a.: filtering out infrequent behavior from business
process event logs. ieee trans. knowl. data eng. 29(2) (2017) 300–314
21. weerdt, j.d., backer, m.d., vanthienen, j., baesens, b.: a robust f-measure for evaluating
discovered process models. in: proceedings of the cidm. (2011) 148–155
view publication statsview publication stats