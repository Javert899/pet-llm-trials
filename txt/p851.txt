process mining in software systems
discovering real-life business transactions and process models from distributed systems
maikel leemans (m.leemans@tue.nl) and wil m. p. van der aalst (w.m.p.v.d.aalst@tue.nl)
eindhoven university of technology, p.o. box 513, 5600 mb, eindhoven, the netherlands.
abstract —this paper presents a novel reverse engineering tech-
nique for obtaining real-life event logs from distributed systems.
this allows us to analyze the operational processes of software
systems under real-life conditions, and use process mining tech-
niques to obtain precise and formal models. hence, the work can
be positioned in-between reverse engineering and process mining.
we present a formal deﬁnition, implementation and an instru-
mentation strategy based the joinpoint-pointcut model. two case
studies are used to evaluate our approach. these concrete exam-
ples demonstrate the feasibility and usefulness of our approach.
index terms —reverse engineering, process mining, event
log, distributed systems, performance analysis, process discov-
ery, joinpoint-pointcut model, aspect-oriented programming
i. i ntroduction
a. behavior analysis of real-life processes
system comprehension, analysis and evolution are largely
based on information regarding the structure, behavior and
operation of the system under study (sus). when no complete
information regarding the behavior is available, one has to
extract this information through dynamic analysis techniques.
typically, dynamic behavior is captured in the form of
process models . these models describe the dynamic, operational
and interactive aspects of the sus. this dynamic behavior is
often an emergent product of the intercommunication between
the different system components . the resulting behavior of these
communicating components is often not well understood, which
makes process models particularly interesting. in addition to
understanding the dynamic behavior, better insight into the
operational aspects, including monitoring real-life performance,
is critical for the success of software systems.
besides monitoring real-life behavior, there is a need to support
dynamic analysis with precise and formal models. uniﬁed model-
ing language (uml) diagrams have become the de facto standard
for describing software. however, uml diagrams have no precise
semantics, are not directly usable for model-based techniques, and
do not support performance analysis. in contrast, event logs show
the actual behavior and, hence, serve as a starting point for pro-
cess mining. the combination of event logs and process mining
techniques provides a powerful way to discover formal process
models and analyze operational processes based on event data.
in this paper, we deﬁne a novel reverse engineering technique
for obtaining real-life event logs from distributed software sys-
tems, spanning across multiple system components . this allows
us to analyze the operational processes of software systems under
real-life conditions, and use process mining techniques to obtain
precise and formal models (see for example figures 4 and 5).
unlike conventional approaches (e.g. proﬁlers), our approach
provides an integrated view, across system components, and
across perspectives (performance, end-to-end control ﬂow, etc.).b. on reverse engineering dynamic models
any dynamic analysis approach based on reverse engineering
techniques must address the following concerns:
information retrieval strategy this concern addresses how
information is obtained from the sus. one has to choose
a retrieval technique (e.g., an instrumentation strategy ),
for which constraints on the target language have to be
considered. in addition to how information is obtained,
one has to address which information is to be obtained,
and at which level of detail (i.e., the granularity ). finally,
one has to take into account the environment that actually
triggers behavior in the sus.
information collecting and processing strategy this
concern addresses how information obtained from the sus
iscollected and processed . first of all, in a distributed
context, one has to specify a data collecting infrastructure
to combine information streams from the different system
components. next, one has to specify a target model .
on the one hand, there is the question of correlating
information , especially in the context of a distributed sus
(e.g., which events belong together). on the other hand,
there are inter-component andinter-thread aspects to be
considered, as well as timing issues in a distributed system.
analysis strategy this concern addresses how, using the
obtained information and target model, the sus is actually
analyzed . one has to consider how to interpret the resulting
artifacts, and how analysis questions can be answered. this
analysis ranges from discovering control-ﬂow patterns to
performance analysis ; and ranges from ﬁnding deviations
with respect to expected behavior to ﬁnding anti-patterns .
c. overview of methodology
our high-level strategy for analyzing distributed systems
consists of the following steps (see also figure 1):
1)we instrument the sus code or binary with tracing code
using instrumentation advices (see section iii-e).
2) we gather event data from real-life user requests, and
convert the data into event logs by discovering business
transactions (see section iii-f).
3) with the resulting event log we can answer various
analysis questions using process mining techniques.
note that our methodology does not require detailed input about
the sus. in fact, many details about the sus are discovered
from data present in the generated event log. we only need
some predicates (pointcuts, see section iii-e ) specifying areas
of interest in the sus. with these pointcuts, we automatically
add tracing code that generates event data upon execution.
d. goal and assumptions
our goal is to analyze the operational processes of software
systems. in particular, we target at analyzing the functionalfig. 1. overview of our high-level strategy and the corresponding input and
output. see section iii for a detailed discussion of the terminology used.
perspective, focusing on user requests. the reverse engineering
technique we use to achieve this goal is designed for any
instrumentable programming language. that is, our approach
and results are language-independent.
our approach supports the analysis of distributed systems,
provided they rely on point-to-point communication. we will
abstract from the details of local clocks in a distributed setting,
as addressed in subsection iv-b.
the current realization is restricted to a single thread per
node. however, the introduced deﬁnitions can be extended to
support multi-threaded distributed software systems, as will be
discussed in section vi.
e. outline
the remainder of this paper is organized as follows. section ii
positions the work in existing literature. a detailed deﬁnition
of the system under study (sus) as well as the conversion
from system events to an event log is given in section iii. the
novel notion of discovering business transactions, as well as
our information retrieval and processing strategies are discussed
in section iv. the approach and implementation are evaluated
in section v using two case studies, performed on existing
open-source software. section vi concludes the paper.
ii. r elated work
reverse engineering models from systems is not new, and
there exists a lot of literature on extracting models from
(software) systems. in the area of dynamic systems analysis,
there exist techniques targeted at understanding the behavior of
systems. in this section we start with a comparison of various
reverse engineering techniques, discussing the current trends
and their advantages and disadvantages in subsection ii-a. we
conclude this section by discussing the techniques available
in the area of process mining in subsection ii-b.
a. dynamic analysis techniques
overview of dynamic analysis techniques: dynamic
system analysis techniques are used for understanding the
behavior of systems. understanding the dynamic part of a
system is often not possible by just analyzing the source code,
especially in the face of polymorphism and dynamic binding. an
even greater challenge is discovering the interactive aspect of the
system, which is an emergent product of the intercommunication
between the different system components . there is an abundance
of literature with proposals and techniques addressing dynamic
analysis techniques. in table i we have compared several
approaches targeted at non-distributed and distributed systems,
evaluated using the following criteria:distributed whether the technique is designed for a distributed
or non-distributed setting.
granularity the level of detail of the analysis technique. in
[1], [3], [4], [5], [6], the behavior is captured down to
the control-ﬂow level (i.e., down to the loop and if-else
statements). at the other end, in [7], [9], [10], [11] the
behavior is captured at the high-level components level
(i.e., external interfaces). in [ 2], only the end-user “pages”
are captured (i.e., the user interface level).
information retrieval strategy the techniques used for
retrieving dynamic information. lion’s share of the existing
techniques uses some form of instrumentation (either
source code transformation and/or binary weaving) to add
tracing code to the sus [1], [2], [3], [6], [8], or adapt
existing tracing code [ 11]. in [ 1], source code analysis is
used for enriching the dynamic information. an altogether
different technique is used in [ 7], where network packages
are monitored, outside of the sus.
environment the environment that triggers behavior in the
system. most of the techniques only considered discovering
the control-ﬂow aspect of the system behavior, and thus
used black-box testing techniques for triggering the system
[1], [2], [3], [4], [5], [6], [10], [11]. a few techniques also
looked at the behavior in a real-life environment [ 7], [8], [9].
target language restrictions on the target language for which
the approach is deﬁned. frequently, the instrumentation
tool aspectj is used (see [12]), thereby targeted at the
java programming language [ 1], [6], [8], [11]. in addition,
for relating events, [ 6] assumes the java rmi distributed
middleware, [ 9] the cobra distributed middleware, and
[7] assumes only low-level tcp/ip communication.
distributed events correlation how, in a distributed setting,
events across different components are correlated. not many
techniques explicitly addressed the concern of correlating
events across components in a distributed setting. in [6],
correlation is handled by introducing extra communication
with correlation indicators. the authors of [7] relied on
deep packet inspection, retrieving sender and receiver
information. the process of inspecting communication
channels used in [10] is similar to our technique.
target model the type of target model produced by the
approach. lion’s share of the techniques produces a
uml sequence diagram (uml sd) [1], [2], [6], [7].
the authors of [ 11] produce communicating finite state
machines (cfsm). in [8], the authors speciﬁed a set of
events, called a monitor log, as target model.
shortcomings of current approaches: the majority of the
techniques considered relies on a testing environment, and pro-
duce a uml sequence diagram. in addition, in many cases the
issue of correlating distributed events is not addressed explicitly.
one complication is that the produced uml models are
imprecise; they have no precise semantics, are not directly usable
for model-based techniques, and do not support performance
analysis. several proposals in literature attempted to address this
by deﬁning a precise subset for uml [ 13], [14], or translating
uml models into precise models like petri nets [15], [16].
however, in the translation steps from events via abstractions
like uml to petri net models, valuable information is lost.
a better approach would be to go directly from events to
precise models with clear semantics, thus enabling the use oftable i
strategy comparison of dynamic analysis techniques
author distributed granularity information
retrieval strategyenvironment target language correlation of
distributed eventstarget model
[1] labiche - control-ﬂow instrumentation + source testing java via aspectj n/a uml sd
[2] alalﬁ - “user pages” instrumentation testing scripting (php) via txl n/a uml sd
[3] briand - control-ﬂow instrumentation testing c++ n/a uml sd
[4] oechsle - control-ﬂow java debug interface testing java n/a uml sd
[5] syst ¨a - control-ﬂow customized debugger testing java n/a uml sd-like
[6] briand + control-ﬂow instrumentation testing java + rmi via aspectj extra communication uml sd
[7] ackermann + components monitor network packets real-life tcp/ip network packet from/to uml sd
[8] van hoorn + varied instrumentation real-life java via aspectj no correlation monitor log
[9] moe + components call interceptors real-life cobra no correlation performance statistics
[10] salah + components jvm proﬁler testing java comm. channel from/to uml sd
[11] beschastnikh + components given log, instrument. testing log-only, java via aspectj predef. comm. channels cfsm
leemans m. + interfaces instrumentation real-life instrumentable + tcp/ip comm. channel from/to process models, with,
i.a., performance info
model-based analysis and techniques.
related to the issues of imprecise uml models and the
use of model-based techniques is the lack of insight into the
performance aspect of system behavior [ 16]. as observed by the
authors of [ 8], real-live monitoring may enable early detection
of quality-of-service problems, and may deliver usage data
for resource management. therefore, we argue that we need
to discover precise models reﬂecting real-life behavior .
b. event logs and process mining
in order to obtain process models, we rely on event logs.
an event log can be viewed as a multiset of traces [17]. each
trace describes the life-cycle of a particular case (i.e., a process
instance ) in terms of the activities executed. in subsection iii-d
a formal deﬁnition for event logs is given, and in [18], [19],
corresponding meta-model, implementations and standardized
exchange format are deﬁned.
process mining techniques use event logs to discover, monitor
and improve real-life processes [17]. the three main process
mining tasks are:
process discovery: learning a process model from example
behavior recorded in an event log.
conformance checking: aligning an event log and a process
model for detecting and diagnosing deviations between
observed (logged) behavior and modelled behavior.
performance analysis: replaying observed behavior on
process models for identifying bottlenecks, delays and
inefﬁciencies in processes.
many process discovery techniques have been presented in
literature. these techniques produce precise models and are read-
ily available through the process mining toolkit prom [ 19]. a
variety of discovery techniques yielding petri nets [ 20], [21], [22],
[23] and process trees [ 24], [25] were proposed. by aligning an
event log and a process model, it is possible to perform advanced
conformance and performance analysis [ 26]. current state of the
art techniques also looks into leveraging additional information
likelocation (which is also present in our event log) to produce
more accurate models [ 27]. in addition, additional insights can
be gained through investigating organizational information (e.g.,
resource collaboration) [ 28] and partial order information [ 29].
iii. d efinitions
before we can discuss the different strategies we developed,
we need a clear understanding of the system under study (sus)
and event logs. we start out with some basic preliminariesin subsection iii-a . next, we present our view on distributed
systems in subsection iii-b. after that, we quickly cover the
basic principles of process mining (subsection iii-c ) and event
logs (subsection iii-d). finally, we will discuss the basic
principle used for instrumenting the sus (subsection iii-e),
and the conversion from system events to an event log
(subsection iii-f). the key concepts and their relations are
captured in the domain model shown in figure 2.
a. preliminaries
sequences: sequences are used to represent traces in an
event log.
given a set x, a sequence over xof lengthnis denoted
as=ha1;a2;:::;a ni2x. we denote the empty sequence
ashi.
intervals: intervals are used to deﬁne the start and end
time of an event.
leti=f(i;j)jijgn2be the set of intervals. we
use?=2ito denote an invalid (empty) interval. given an
intervalx= (i;j)2i, we writexs=iandxe=jfor the
start and end of an interval respectively.
we deﬁne the following relations on i, withx;y2i:
x=ydef= ((xs=ys)^(xe=ye)) equality
xvydef= (ysxsxeye) containment
x<ydef= ((xvy)^(x6=y)) strict containment
x\ydef=
z; ifx6=?^y6=?^z2i;
?;otherwise.intersection
withz= (max(xs;ys);min(xe;ye))
x[ydef=
z; ifx6=?^y6=?;
?;otherwise.union
withz= (min(xs;ys);max(xe;ye))
b. the system under study: anatomy of a distributed system
in this section, we present our view on distributed systems.
to make things more concrete, we will map our view onto an
imaginary distributed software system.
adistributed system consists of a set of interacting system com-
ponents , distributed over a set of logical platforms . each system
component is instantiated on a node , and can offer services via
itsexternal interfaces . each logical platform can deploy multiple
nodes, that is, multiple instantiations of system components. in
our imaginary software system, our system components could
be a business and a data component: a webserver and database,fig. 2. domain model illustrating the key concepts and their relations (using uml notation). see section iii for a detailed description of each concept.
respectively. the logical platforms would be the servers in the
datacenter. the nodes are instances of the webserver and database.
note that the webserver and database may be instantiated multiple
times, for example, in a load-balancing setting. in the latter case,
it would make sense to host the resulting nodes on different
servers (i.e., different logical platforms). the webserver offers
webpage services, while the database offers query services.
aservice request , denoting the use of an external interface, is
processed on a node by a node instance . we assume that a node
instance is executed by one of the execution threads provided by
a node, but future work can remove this restriction. a service can
be requested by a node part of the sus (i.e., intercommunication ),
or by an external user (i.e., a user request ). in our imaginary
software system, the webserver (the node) would have a pool
of execution threads. whenever the client browser requests a
webpage service (a user request), the webserver assigns one
of the available threads to process this request. for this user
request, the node instance is executed by the assigned thread.
whenever two nodes are communicating to execute a service
request, they are sharing a communication channel . a communi-
cation channel is identiﬁed by a pair of communication resources ,
each representing one endpoint of the communication channel.
node instances can acquire communication resources provided by
the node in order to communicate. the moment a communication
resource is used by a node instance, that node instance owns
that resource. note that communication resources can be reused,
and thus can have different owners at different moments in time.
see also the group labeled “system under study” in figure 2.
in our imaginary software system, a communication channel
could be a tcp/ip connection. an endpoint is the combination
of an ip address and a port on a node. hence, a communication
resource is a pair of ‘from’ and ‘to’ endpoints.
for the rest of the paper, we use the formal deﬁnition below
for referencing these concepts.deﬁnition 1 (nodes and communication resources): we
denote the set of nodes with n, the set of node instances at
noden2nwithtn, and lett=s
n2ntn. furthermore,
letrndenote the set of communication resources available at
noden2n, and letr=s
n2nrn. in addition, let rr0
denote that the two communication resources r2rn,r02rn0,
withn;n02n^n6=n0identify the same communication chan-
nel. we impose the following constraint on the above deﬁned sets:
8n;n02n; n6=n0: (rn\rn0) = (tn\tn0) =;
c. process mining
recall, in subsection ii-b we covered how process mining
techniques use event logs to discover, monitor and improve
real-life processes. each trace in the event log describes the
life-cycle of a particular process instance . together, these
process instances describe the behavior of the sus.
as a passive learning technique, the quality of the resulting
model depends on the quality and volume of the behavior that
has been observed. it is therefore important to obtain a large,
high quality event log in order to build an accurate model. see
also the group labeled “process mining” in figure 2.
d. event log and business transactions
event log: the starting point for any process mining
technique is an event log , which is formally deﬁned below.
deﬁnition 2 (event, trace, and event log): letelbe
the set of events occurring in the event log. a trace is a
sequence2e
lof events. an event logle
lis a
collection of traces. each trace corresponds to an execution
of a process, i.e., a case orprocess instance .
business transactions: in the context of analyzing system
behavior, we recognize a special type of traces, called business
transactions . a business transaction consists of a sequence of
related events, which together contribute to serve a user request .events in a single case can span multiple nodes. recall, a
user request is a service requested by an external user. hence,
a business transaction captures the emergent product of the
intercommunication between system components required for
one external interface exposed by the software system as a whole.
see also the group labeled “event log and business
transactions” in figure 2.
e. joinpoint-pointcut model
in order to obtain an event log detailing the dynamic behavior
of the sus, we instrument the sus with tracing code to generate
the necessary events. this tracing instrumentation should mini-
mize the impact on the sus, and provide as little human overhead
as possible. note that the behavior of the instrumented sus may
be different from the unmodiﬁed sus, especially in the context
of deadlines. this is an unavoidable consequence; observing a
system changes the system [ 30]. however, we should nevertheless
strive to minimize the impact of the tracing instrumentation.
to make the instrumentation less intrusive and as systematic
as possible, we use the joinpoint-pointcut model frequently
used in the area of aspect-oriented programming (aop) [ 31].
this way, developers can work on the clean, unmodiﬁed code,
and we can monitor anysus that can be instrumented without
manually modifying the source code .
ajoinpoint is a point in a running program where additional
behavior can be usefully joined or added. a joinpoint needs to
be addressable and understandable by an ordinary programmer to
be useful. a pointcut is an expression (predicate) that determines
whether a given joinpoint matches. an advice is a piece of code
(e.g., event trace code) to be added. an aspect deﬁnes an advice
to be added at all joinpoints matched by a speciﬁed pointcut.
hence, an aspect is a container detailing how to instrument what.
each tracing aspect typically instruments a particular (possible
user-deﬁned) location in a software system, such as a method
in a particular component, library or layer. see also the group
labeled “joinpoint-pointcut model” in figure 2. in section iv-a
we will use the joinpoint-pointcut model to deﬁne which parts
of a software system we wish to include in our event log.
in the remainder of the paper, we use the formal deﬁnition
below for referencing these concepts.
deﬁnition 3 (joinpoint-pointcut model): letjnujdenote
the set of joinpoints available at node n2n. a pointcut is
a predicate matching a subset of joinpoints. in addition, we
denote the set of locations with l. we impose the following
constraint on the above deﬁned set:
8n;n02n; n6=n0: (jn\jn0) =;
f . from system events to event logs
system events: recall, we are interested in discovering
business transactions: a sequence of related events, which
together contribute to serve a user request . therefore, we specify
the structure of system events (described below), capturing
enough information to relate events within and across the
external interfaces of system components. using the data in
these system events, we can discover business transactions, the
basis for instantiating our event log.
deﬁnition 4 (system events): letebe the set of system
events , such that every e2ehas the following structure:
e= (i:i; n :n; t :tn; j:jn; r:rn[f?g; l:l)we writee:nto access attribute nin evente.
the interpretation of the above attributes is as follows:
i:imodels the time interval of the event. that is, it
models the call (start) and return (end) of the involved
joinpoint. typically, this will correspond to the entry and
exit of a method, see also section iv-a.
n:nmodels the node on which the event was generated.
t:tnmodels the node instance, which generated the event.
j:jnmodels the joinpoint that was executed to generate
this event.
r:rn[f?g models the (optional) communication
resource associated with this event.
l:lmodels the location speciﬁed in the aspect that
instrumented this joinpoint.
communication and related events: in order to obtain
business transactions, we need to correctly cluster events. to
correctly cluster events, we will use the notion of communication
intervals andrelated events .
deﬁnition 5 (communication intervals): given a resource
r2r, we can get the set of events erthat are associated with r:
erdef=fe2eje:r=rg
recall, a node instance owns a resource during the time it
uses a resource, and a resource can have different owners at
different moments in time. to deﬁne the time interval where a
node instance owns a resource r2r, we look for ‘evidence’ in
the form of events associated with resource r. given two events
e;e02erassociated with resource r2rn. if the two events
have the same node instance t2tn, and there is no other
evente002er;e00:t6=tin between eande0, then we know
that during the time e:i[e:i0, node instance towns resource
r. formally, the set of intervals where t2tnownsr2rn
is deﬁned by function hr:
hr(t;r)def=fi= (e:i[e0:i)je;e02er; e:t =e0:t=t;
:(9e002er; e00:t6=t: (e00:i\i)6=?)g
given the deﬁnition above, we can deﬁne the set of maximal
intervals where t2tnownsr2rn(i.e., the communication
intervals ) as follows:
fr(t;r)def=fi2hr(t;r)j:(9i02hr(t;r) :i<i0)g
deﬁnition 6 (related events): two events x;y2e;x6=y
are directly related, notation x!y, iff either:
1)xandyare part of the same node instance, and x:iis
contained in y:i.
2)ystarted before x, and there exists related resources
rx;ry2r; r xrythat are at a certain point in time
owned byx:tandy:trespectively (see deﬁnition 5).
formally, the directly related relation is deﬁned as follows:
(x!y)def= ( (x6=y)^( distinct.
(x:t=y:t^x:ivy:i) case 1.
_(x:t6=y:t^y:isx:is^ case 2.
9rx;ry2r; r xry:
9ix2fr(x:t;r x); iy2fr(y:t;r y) :
((x:i\ix)\(y:i\iy))6=?
) ))intuitively, an event xis directly related to y(i.e.,x!y)
ifxis ‘caused’ by y(e.g., in case 1, called by). note that !
is an irreﬂexive and antisymmetric relation.
two eventsx;y2e;x6=yare related, notation xy, iff
there is a path from xtoyin the relation!. formally:
(xy)def= ((x!y) base case.
_(9z2e:xz^z!y)) step case.
note that is an irreﬂexive, antisymmetric and transitive
relation.
letx2p (e)be the set of subsets of related events, and
letyxbe the set of maximal subsets of related events (i.e.,
the basis for business transactions). formally:
xdef=ffeg[fe02eje0egje2eg
ydef=fx02xj:(9x002x:x0x00)g
instantiating event logs: using the set yof maximal
subsets of related events, we can now instantiate an event
logl. we construct our event log le
lfrom the set of
log-eventselby ﬁnding all valid business transactions. the
set of log-events elis obtained from the set of events eby
mapping each event to a start and end event, based on its interval.
deﬁnition 7 (event log instantiation): for an event e2e,
the set of log-events fl(e)corresponds to the start and end
ofe:i. the setelof log-events is the union of all mapped
events. formally:
eldef=[
e2efl(e)withfl(e)def=f(e:is;e);(e:ie;e)g
the event log le
l(see deﬁnition 2) of business
transactions is based on y(see deﬁnition 6), and deﬁned as:
ldef=n
2e
lsorted ()^9x02y:fe2g=[
e2x0fl(e)o
note that we assumed a total ordering on the elements of el,
where events are sorted by time. in the edge case that two related
events are logged with the same timestamp (i.e., a tie), in the
resulting trace , the tie is handled as per a stable sort algorithm.
iv. m ethodology and realization
using the deﬁnition from section iii, we will now discuss
the different strategies we developed. we start out with detailing
our information retrieval strategy in subsection iv-a , specifying
how we instrument the system under study (sus) with tracing
code. after that, we will cover the collecting and information
processing strategy in subsection iv-b, specifying how we
gather the event data and how we convert it into an event log.
as a mental reference, please consult figure 3 as a sample
application of our strategies.
a. dynamic information retrieval strategy
recall from subsection iii-e, that we use the joinpoint-
pointcut model to instrument the sus with tracing code. this
makes the instrumentation less intrusive and as systematic as
possible. in addition, instrumentation minimizes the impact of the
tracing instrumentation on the sus and on the developers. in this
section we will discuss how we use the joinpoint-pointcut model
as the basis for our information retrieval strategy. note that the
question of ‘what information has to be logged’ is answered by
our deﬁnition of system events, as discussed in subsection iii-f .
fig. 3. domain infrastructure model illustrating the implementation of our
information retrieval, collecting and information processing strategy.
instrumentation strategy: our instrumentation strategy
focuses on the resulting behavior from the intercommunication
between different system components via their external
interfaces. therefore, we recognize two types of joinpoints we
will target at: communication resource joinpoints andinterface
joinpoints . the tracing advice code added via pointcuts targeting
these types of joinpoints adds invocations to a logging client
interface.1this logging client interface generates events and
streams these to a logging server; see also figure 3.
the communication resource joinpoint type is about recogniz-
ing when a node instance has acquired which communication
resource. this type of joinpoint can be matched by application-
independent language-dependent pointcuts targeting low-level
network communication methods. the advice code could, for
example, recover the involved communication resource based
on the socket involved. note that this type of aspect needs to
be deﬁned only once for any programming language.
the interface joinpoint type is about recognizing (external)
interfaces, thus providing a context for the system analyst. this
type of joinpoint can be matched by application-dependent
method pointcuts deﬁned by the system analyst. the advice
code associated with interface joinpoint aspects is application-
independent language-dependent. this means that the system
analyst only has to deﬁne the pointcuts and possible location
information, and the rest of the aspect is handled in a generic,
automated fashion. note that this type of advice code needs
to be deﬁned only once for any programming language.
granularity of information retrieval: our instrumentation
aspects are designed to capture enough information to relate
events within and across the external interfaces of system
components. hence, we are primarily retrieving information
on the system component instances. secondarily, through the
option to specify interface pointcuts, we can provide more
detailed context information.
the advice code added to the sus records event data on the
method level. the moments when a method is entered and exited
correspond to the start and end of the event time interval. each
event is enriched with the current node and joinpoint information,
available through the pointcut speciﬁcation. the current node
instance information is a piece of application-independent
language-dependent information. in our evaluation, we used
the notion of thread ids for instance identiﬁcation within a node.
1java tracing advice code online: https://svn.win.tue.nl/repos/prom/xport/note that node instance identiﬁcation needs be deﬁned only
once for any programming language.
this way, all the information speciﬁed for system events in
subsection iii-f is accounted for.
environment: most analysis techniques assume some
controlled environment in which the sus is executed. frequently,
through the use of black-box testing techniques, many scenarios
(i.e., user requests) are triggered. however, our approach focuses
on capturing dynamic information about real-life usage of the
sus. therefore, instead of relying on testing techniques for
generating executions, we assume real-life interaction. one
concern may be that this approach cannot cover enough relevant
distinct scenarios. but since we want to understand the real-life
usage of the system, it stands to reason that the most relevant
scenarios are those scenarios that are (frequently) triggered by
the actual users of the system.
target language considerations: any language for which
method-level aop techniques exist can be supported after
deﬁning the basic communication resource and interface advices.
for the java language one can use techniques like java agents
and javassist [32], [33] or aspectj [12]. the c++ language
can be supported via the use of, for example, aspectc++ [ 34]
or aop++ [35]. most other languages can be supported via
source transformation techniques like the txl processor [ 36].
b. information collecting and processing strategy
in subsection iv-a , we have discussed how we get our sus
to generate events, and stream these to a logging server; see
also figure 3. in this section we will detail how this information
is processed into actual event logs. in addition, we will touch
on two related issues: 1) event timing in a distributed context,
and 2) communication within system components.
collecting strategy: the events generated by the logging
clients are streamed to one central logging server. although
not required by the basic deﬁnitions from section iii, for
practical reasons we assume an ofﬂine event processing strategy.
therefore, our collecting strategy on the server side will be
a simple merge of event streams, and storing the results on disk.
future work could focus on enabling the processing strategy
for real-time event stream processing, but we did not consider
this for our initial proof of concept.
event processing strategy: the event processing strategy
consists of an algorithm that takes the set of generated events,
and produces an event log. that is, we cluster events into traces
based on the notion of related events (see subsection iii-f ). our
basic algorithm consists of two parts: 1) discovering ownership
of communication resources, and 2) discovering actual traces.
the discovery of communication resources ownership is
essentially the function fr(t;r)in deﬁnition 5. we simply
traverse the recorded events in ascending order and build the
resource acquisition intervals.
the discovery of actual traces is now possible by resolving
the ‘related events’ mapping. for each pair of events
x;y2e; x6=ywe can calculate x!yby checking the
two cases of the formal deﬁnition in deﬁnition 6. for ﬁnding
resourcesrx;ryand corresponding acquisition intervals ix;iy,
we can use the function fr(t;r). in order to ﬁnd rx, we can
simply investigate the domain of fr(x:t).
in order to obtain the actual event log, we use the idea
presented in deﬁnitions 6 and 7. we consider each event e2efor which there is no e02esuch thatee0, and create a
trace out of all the events e002ewithe00e.
timing issues: until now we have assumed that time
across platforms in a distributed system can be used for ordering
and comparing event intervals. however, in a distributed system,
local clocks (i.e. the time on different platforms) can be different.
to remedy this issue, we assume the network time protocol
(ntp) is used to synchronize platform clocks via the internet
(see also rfc 5905 [37]). for systems not connected to the
internet, similar algorithms can be employed to synchronize
with a local global clock server.
inter-thread communication: recall from subsection iii-b ,
we assume that a node instance is executed by oneof the execu-
tion threads provided by a node. this restricts the current imple-
mentation to a single thread per node instance. future work can re-
move this restriction, and we will elaborate on this in section vi.
v. e valuation
this section discusses two case studies using existing
open-source software to demonstrate the feasibility and
usefulness of our approach. in addition, the instrumentation
overhead is investigated via a performance benchmark.
a. methodology
we used two existing open-source software applications for our
experiments. the ﬁrst is a pet catalog demo available in the net-
beans ide [ 38]. the second is wordcount mapreduce example
available in the apache hadoop distributed computing framework
(version 2.6.0) [ 39], [40]. for these experiments we used a laptop
with a 2.40 ghz cpu, windows 8.1 and java se 1.7.0 67 (64
bit) with 4 gb of allocated ram. the instrumentation is done
via java agents, using javassist 3.19.0-ga [32], [33].
for the two case studies we indicate the instrumentation
pointcuts used (i.e., the input). after instrumentation, we
simulate a batch of user requests, collect the data and process it
to obtain event logs. finally, we will use process discovery and
performance analysis to answer the following analysis questions:
1)what is the high-level end-to-end process corresponding
to the interfaces of the system under study?
2)what are the main bottlenecks or areas for improvements
in this process?
b. case study - pet catalog
the pet catalog software system consists of a javaee
webserver implementation. at the front external interface, users
can issue webpage requests, handled via javaee servlets. at
the backend, the software interfaces with a mysql database
through the javaee persistence interface.
instrumentation pointcuts: to target communication
resource joinpoints, we deﬁned pointcuts targeting java socket
read and writes, as well as the javaee servlet interface. to
target interface joinpoints, we deﬁned pointcuts targeting the
javaee persistence interface. the exact pointcuts are:
hasinterface: javax.persistence.entitymanager
communication: java.net.socketinputstream,
java.net.socketoutputstream, javax.servlet. *,
javax.faces. *
note that this, together with the actual sus, is only the input
we need.(a) using the complete event log for performance analysis
(b) using a ﬁltered event log (ﬁltered after extraction) for performance analysis, focusing only on cases with database queries
(c) using a ﬁltered event log (ﬁltered after extraction) for performance analysis, exclusively focusing only on the database query events
fig. 4. process model of the pet catalog software system, depicted as a petri net overlaid with performance information as per the replay algorithm [26].
transitions (rectangles) from left to right read: (1) servlet+start , (2)entitymanagerwrapper.createquery()+start , (3)entitymanagerwrapper.create -
query()+complete , (4) socketouputstream.write()+start , (5) socketouputstream.write()+complete , (6) socketinputstream.read()+start ,
(7)socketinputstream.read()+complete , (8)servlet+complete .
(a) using the complete event log for performance analysis
(b) using a ﬁltered event log (ﬁltered after extraction) for performance analysis, ignoring the wrapping main() method
fig. 5. process model of the wordcount mapreduce job on the apache hadoop software system, depicted as a petri net overlaid with performance information
as per the replay algorithm [26].
transitions (rectangles) from left to right read: (1) wordcount.main()+start , (2) tokenizermapper.map()+start , (3) tokenizermapper.map()+start ,
(4)tokenizermapper.map()+complete , (5) tokenizermapper.map()+complete , (6) intsumreduce.reduce()+start , (7) intsumreduce.reduce()
+start , (8) intsumreduce.reduce()+complete , (9) intsumreduce.reduce()+complete , (10) wordcount.main()+complete . (outer map() reduce()
methods are untyped variants, the inner methods are typed variants.)
high-level end-to-end process: the process model
displayed in figure 4 was discovered via the inductive miner
[25]. each transition (rectangle) represents the start or end (i.e.,
the call and return, respectively) of a method. the beginning
(left) and end (right) of this model is the call and return of
the servlet front external interface. in between are the activities
during a servlet call.
at the beginning of executing the servlet, there is a choice
to perform some query requests or skip querying the database.
in the case the decision is to query the database, then a query
statement is ﬁrst created (i.e., prepared). after that, there is
some communication with the database to issue the request
and receive the results (write and read). when done receiving
results, we have the option to loop back and perform additional
queries, or to ﬁnish the servlet process.
main bottlenecks: the color highlighting applied in
figure 4 is the result of replaying (a ﬁltered version of) the event
log on the process model [ 26]. the coloring on the transitions
and places indicate waiting time between calls, between returns
or between a start and end (i.e., a call and return, and hence,
throughput). dark red and light yellow colors indicate a high or
low waiting/throughput time, respectively. the color scale is auto-
matically computed by the replaying algorithm. the arc coloring
indicate how frequently that branch is used (over all traces), withblack and gray denoting a high and low frequency respectively.
as can be seen near the choice in the beginning in figure 4(a),
in most cases querying the database is skipped. the average
case throughput, from start to end, is 2,77 milliseconds.
by applying the appropriate ﬁlter to our event log, we
obtain the in figure 4(b), showing only the cases with database
interaction. note that no re-instrumentation was needed, process
mining techniques provide advanced ﬁltering after extraction [ 17],
[19]. the average case throughput with this log, from start to end,
is 7,96 milliseconds. there are two large delays visible, before
and after the query loop, indicated by the red circle before (2) and
the orange circle before (8). these delays correspond with the
servlet startup and shutdown (in this case, a javaserver facelet).
after ﬁltering out the servlet start and complete events, we
obtain the figure 4(c), focusing only on the database querying.
in this ﬁltered view, thanks to a rescaled performance color
scale, we see a delay between creating queries (the orange circle
between (2) and (3)) and the actual database communication (the
red circle after (3)). in addition, a similar delay is visible during
reading results from the database (the red circle before (7)).
conclusion: after specifying a few simple pointcuts, the
end-to-end process was quickly discovered. for these pointcuts,
no real in-depth knowledge or manual coding was needed,
allowing a quick instrumentation of the system. through the use
of performance analysis, the servlet startup and shutdown, as wellas the transition between query preparation and communication
were identiﬁed as the main bottlenecks. since in most cases
the database is not queried (based on frequency information),
the latter bottleneck could be considered less of an issue.
c. case study - wordcount mapreduce
the wordcount mapreduce job is a simple application that
counts the number of occurrences of each word in a given
input set [ 40]. the job is executed on the hadoop mapreduce
framework. we used the english version of the universal
declaration of human rights as input [ 41]. the front-end of the
application is the mapreduce client, whose main function sets
up the wordcount job. the back-end of the application, i.e., the
actual map() and reduce() tasks, are executed on a different (local)
node. in between these nodes is the hadoop mapreduce system.
instrumentation pointcuts: to target communication
resource joinpoints, we deﬁned pointcuts targeting java channel
socket read and writes. to target interface joinpoints, we deﬁned
pointcuts targeting the client main and backend map and reduce
interfaces. the exact pointcuts are:
interfaces: org.apache.hadoop.examples. *.map( *,
org.apache.hadoop.examples. *.reduce( *,
org.apache.hadoop.examples. *.main( *
communication: java.net.socketinputstream,
java.net.socketoutputstream,
java.nio.channels.socketchannel
note that this, together with the actual sus, is only the input
we need.
high-level end-to-end process: the process model
displayed in figure 5 was discovered via the inductive miner
[25]. the beginning (left) and end (right) of this model is the
call and return of the main() method. the remained of the
model occurs inside, or during this main() method.
the are two main phases in the mapreduce job: the ﬁrst
loop is for the map() methods, and the second loop is for the
reduce() methods. note that both map() and reduce() consists
of two functions: the generic interface implementation, and the
typed variant that is called during executing the generic method.
main bottlenecks: again, we replayed (a ﬁltered version
of) the event log on the process model, resulting in figure 5.
as can be seen in figure 5(a), the methods in the second
phase (i.e., the reduce methods) are executed more frequently:
roughly 4 times the frequency of the map methods. taking
into account that the wordcount job computes the number of
occurrences of each word, we conclude that there are many
small reduce tasks as a result of a few map tasks. the average
case throughput for this application, from start to end, is 17,96
seconds. the biggest delays are at the beginning and end of
the job, before the ﬁrst map call, and after the ﬁnal reduce call.
after ﬁltering out the main() start and complete, we obtain
the figure 5(b), focusing only on the map and reduce methods.
in this ﬁltered view, thanks to a rescaled color scale, we discover
a delay between map-reduce and reduce-reduce transitions.
conclusion: again, a few simple pointcuts and no real in-
depth knowledge or manual coding sufﬁced to quickly discover
the end-to-end process. although the system under study is rather
complex, the initial effort needed to start analyzing its behavior
is very small. through the use of performance analysis, the
mapreduce job startup and shutdown, as well as map-reduce and
reduce-reduce transitions were identiﬁed as the main bottlenecks.d. on instrumentation overhead
we investigate the extend of instrumentation overhead via
a performance benchmark. we measure the time to complete of
both the pet catalog and the wordcount mapreduce software
for both the instrumented and unmodiﬁed version of the
software. this time measurement is performed on the user side,
and thus includes the communication overhead between the
sus and the client user. by measuring on the user side, we
can measure the time of the unmodiﬁed software, and inﬂuence
the sus as little as possible. we repeated these measurements
several times, and calculated the average runtime and associated
95% conﬁdence interval. the results are presented in figure 6.
for the pet catalog case, we requested a batch of 5000
webpages involving database querying. as shown in figure 6(a),
the difference in performance is very small.
for the wordcount mapreduce case, we performed a batch
of 60 jobs in sequence. as shown in figure 6(b), we see a
small difference in runtime.
although in both cases the difference is observable, it is very
small compared to the total time to complete. based on the
above presented observations, we conclude that the impact of
the instrumentation is negligible.
instrumented sus  unmodified sus  
0 1 2 3 4 5 6 7 8
average time to complete (in miliseconds) [95% confidence interval]  instrumentation overhead - pet catalog  
(a) the pet catalog case
instrumented sus  unmodified sus  
0 5 10 15 20 25
average time to complete (in seconds) [95% confidence interval]  instrumentation overhead - wordcount mapreduce  
(b) the wordcount mapreduce case
fig. 6. effect of instrumentation on the average time to complete.
vi. c onclusion
in this paper, we presented a novel reverse engineering tech-
nique for obtaining real-life event logs from distributed systems.
this allows us to analyze the operational processes of software
systems under real-life conditions, and use process mining tech-
niques to obtain precise and formal models. in addition, process
mining techniques can be used to monitor and improve processes
via, for example, performance analysis and conformance check-
ing. we presented a formal deﬁnition, implementation and an
instrumentation strategy based on the joinpoint-pointcut model.
two case studies demonstrated how, after specifying a few
simple pointcuts, we quickly gained insight into the end-to-end
process, and the main performance bottlenecks in the context
of this process. by changing the pointcut speciﬁcations, and
ﬁltering the obtained event logs, system analysts can easily
select the right amount of detail. through the use of frequency
and performance information we can determine the seriousness
of discovered bottlenecks.
the current implementation is limited to single-threaded
distributed software systems, but future work will look intoadapting deﬁnition 6 (related events) to handle multi-threaded
software. note that, in essence, the related events relation in
deﬁnition 6 relates threads based on inter-thread communication.
that is, this basic deﬁnition does not assume that the correlated
threads are on different nodes, and could also be applied in the
context of inter-thread communication within one node. when
adapting this deﬁnition, one should pay special attention to
inter-thread communication via data-structures, such as present
in typical producer-consumer settings.
in this paper, we assumed an ofﬂine process analysis setting,
but developing process mining techniques supporting event
streams could yield valuable real-time insight. finally, with the ad-
vent of software event logs, which are rich in data and semantics,
new process mining techniques could focus on making location
information and subprocesses explicit in the discovered models.
references
[1]y . labiche, b. kolbah, and h. mehrfard, “combining static and dynamic
analyses to reverse-engineer scenario diagrams,” in software maintenance
(icsm), 2013 29th ieee international conference on . ieee, sept 2013,
pp. 130–139.
[2]m. h. alalﬁ, j. r. cordy, and t. r. dean, “automated reverse engineering
of uml sequence diagrams for dynamic web applications,” in software
testing, veriﬁcation and validation workshops, 2009. icstw ’09.
international conference on . ieee, april 2009, pp. 287–294.
[3]l. c. briand, y . labiche, and y . miao, “towards the reverse engineering
of uml sequence diagrams,” 2013 20th working conference on reverse
engineering (wcre) , p. 57, 2003.
[4] “javavis: automatic program visualization with object and sequence
diagrams using the java debug interface (jdi),” in software visualization ,
ser. lecture notes in computer science, s. diehl, ed., 2002, vol. 2269.
[5] t. syst ¨a, k. koskimies, and h. m ¨uller, “shimba – an environment
for reverse engineering java software systems,” software: practice and
experience , vol. 31, no. 4.
[6]l. c. briand, y . labiche, and j. leduc, “toward the reverse engineering
of uml sequence diagrams for distributed java software,” software
engineering, ieee transactions on , vol. 32, no. 9, pp. 642–663, sept 2006.
[7] c. ackermann, m. lindvall, and r. cleaveland, “recovering views of
inter-system interaction behaviors,” in reverse engineering, 2009. wcre
’09. 16th working conference on . ieee, oct 2009, pp. 53–61.
[8]a. van hoorn, m. rohr, w. hasselbring, j. waller, j. ehlers, s. frey, and
d. kieselhorst, “continuous monitoring of software services: design and
application of the kieker framework,” kiel university, research report,
november 2009. [online]. available: http://eprints.uni-kiel.de/14459/
[9]j. moe and d. a. carr, “using execution trace data to improve distributed
systems,” software: practice and experience , vol. 32, no. 9.
[10] m. salah and s. mancoridis, “toward an environment for comprehending
distributed systems,” 2013 20th working conference on reverse
engineering (wcre) , p. 238, 2003.
[11] i. beschastnikh, y . brun, m. d. ernst, and a. krishnamurthy, “inferring
models of concurrent systems from logs of their behavior with csight,” in
proceedings of the 36th international conference on software engineering ,
ser. icse 2014, 2014, pp. 468–479.
[12] j. d. gradecki and n. lesiecki, mastering aspectj. aspect-oriented
programming in java . john wiley & sons, 2003, vol. 456.
[13] f. bouquet, c. grandpierre, b. legeard, f. peureux, n. vacelet,
and m. utting, “a subset of precise uml for model-based testing,”
inproceedings of the 3rd international workshop on advances in
model-based testing , ser. a-most ’07, 2007, pp. 95–104.
[14] “deﬁning precise semantics for uml,” in object-oriented technology ,
ser. lecture notes in computer science, g. goos, j. hartmanis, j. van
leeuwen, j. malenfant, s. moisan, and a. moreira, eds., 2000, vol. 1964.
[15] s. bernardi, s. donatelli, and j. merseguer, “from uml sequence
diagrams and statecharts to analysable petri net models,” in proceedings
of the 3rd international workshop on software and performance , ser.
wosp ’02, 2002, pp. 35–45.
[16] j. p. l ´opez-grao, j. merseguer, and j. campos, “from uml activity
diagrams to stochastic petri nets: application to software performance
engineering,” in proceedings of the 4th international workshop on
software and performance , ser. wosp ’04, 2004, pp. 25–36.
[17] w. m. p. van der aalst, process mining: discovery, conformance and
enhancement of business processes . springer-verlag, berlin, 2011.
[18] c. w. g ¨unther and h. m. w. verbeek, “xes – standard deﬁnition,”
2014. [online]. available: http://repository.tue.nl/777826[19] “xes, xesame, and prom 6,” in information systems evolution , ser.
lecture notes in business information processing, p. soffer and e. proper,
eds., 2011, vol. 72.
[20] w. m. p. van der aalst, a. j. m. m. weijters, and l. maruster, “workﬂow
mining: discovering process models from event logs,” ieee transactions
on knowledge and data engineering , vol. 16, no. 9, pp. 1128–1142, 2004.
[21] w. m. p. van der aalst, a. k. a. de medeiros, and a. j. m. m. weijters,
“genetic process mining,” in applications and theory of petri nets 2005 ,
ser. lecture notes in computer science, g. ciardo and p. darondeau,
eds. springer-verlag, berlin, 2005, vol. 3536, pp. 48–69.
[22] r. bergenthum, j. desel, r. lorenz, and s. mauser, “process mining
based on regions of languages,” in international conference on business
process management (bpm 2007) , ser. lecture notes in computer
science, g. alonso, p. dadam, and m. rosemann, eds., vol. 4714.
springer-verlag, berlin, 2007, pp. 375–383.
[23] w. m. p. van der aalst, v . rubin, h. m. w. verbeek, b. f. van dongen,
e. kindler, and c. w. g ¨unther, “process mining: a two-step approach
to balance between underﬁtting and overﬁtting,” software & systems
modeling , vol. 9, no. 1, pp. 87–111, 2010.
[24] “on the role of ﬁtness, precision, generalization and simplicity in process
discovery,” in on the move to meaningful internet systems: otm 2012 ,
ser. lecture notes in computer science, r. meersman, h. panetto,
t. dillon, s. rinderle-ma, p. dadam, x. zhou, s. pearson, a. ferscha,
s. bergamaschi, and i. cruz, eds., 2012, vol. 7565.
[25] s. j. j. leemans, d. fahland, and w. m. p. van der aalst, “discovering
block-structured process models from incomplete event logs,” in
applications and theory of petri nets 2014 , ser. lecture notes in computer
science, g. ciardo and e. kindler, eds., vol. 8489. springer-verlag,
berlin, 2014, pp. 91–110.
[26] a. adriansyah, “aligning observed and modeled behavior,” ph.d.
dissertation, technische universiteit eindhoven, 2014.
[27] w. m. p. van der aalst, k. a., v . rubin, and h. m. w. verbeek, “process
discovery using localized events,” 2015, to appear in petri nets in 2015.
[28] m. song and w. m. p. van der aalst, “towards comprehensive support
for organizational mining,” vol. 46, no. 1. springer-verlag, berlin, 2008,
pp. 300–317.
[29] m. leemans and w. m. p. van der aalst, “discovery of frequent episodes
in event logs,” in proceedings of the 4th international symposium
on data-driven process discovery and analysis (simpda 2014) .
ceur-ws.org, 2014.
[30] w. schutz, “on the testability of distributed real-time systems,” in reliable
distributed systems, 1991. proceedings., tenth symposium on . ieee,
sep 1991, pp. 52–61.
[31] t. elrad, r. e. filman, and a. bader, “aspect-oriented programming:
introduction,” communications of the acm , vol. 44, no. 10, pp. 29–32,
oct. 2001.
[32] s. chiba, “javassist – a reﬂection-based programming wizard for java,”
inproceedings of oopsla98 workshop on reﬂective programming in
c++ and java , october 1998, p. 5.
[33] ——, “load-time structural reﬂection in java,” in european conference
on object-oriented programming 2000 – object-oriented programming ,
ser. lecture notes in computer science, e. bertino, ed. springer
berlin heidelberg, 2000, vol. 1850, pp. 313–336. [online]. available:
http://dx.doi.org/10.1007/3-540-45102-1 16
[34] o. spinczyk, a. gal, and w. schr ¨oder-preikschat, “aspectc++: an aspect-
oriented extension to the c++ programming language,” in proceedings of
the fortieth international conference on tools paciﬁc: objects for internet,
mobile and embedded applications , ser. crpit ’02. darlinghurst,
australia, australia: australian computer society, inc., 2002, pp. 53–60.
[online]. available: http://dl.acm.org/citation.cfm?id=564092.564100
[35] “aop++: a generic aspect-oriented programming framework in c++,” in
generative programming and component engineering , ser. lecture notes
in computer science, r. glck and m. lowry, eds., 2005, vol. 3676.
[36] j. r. cordy, “the txl source transformation language,” science
of computer programming , vol. 61, no. 3, pp. 190 – 210,
2006, special issue on the fourth workshop on language
descriptions, tools, and applications (ldta 04). [online]. available:
http://www.sciencedirect.com/science/article/pii/s0167642306000669
[37] d. mills, j. martin, j. burbank, and w. kasch, “network time protocol
version 4: protocol and algorithms speciﬁcation,” ietf rfc5905 , june
2010. [online]. available: http://tools.ietf.org/html/rfc5905
[38] netbeans, “pet catalog - java ee 6 sample application,” https://netbeans.
org/kb/samples/pet-catalog.html, [online, accessed 17 april 2015].
[39] the apache software foundation, “apache hadoop,” http:
//hadoop.apache.org/, [online, accessed 17 april 2015].
[40] ——, “mapreduce tutorial,” https://hadoop.apache.org/docs/
current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/
mapreducetutorial.html, [online, accessed 17 april 2015].
[41] united nations, “universal declaration of human rights,”
http://www.un.org/en/documents/udhr/, [online, accessed 17 april 2015].