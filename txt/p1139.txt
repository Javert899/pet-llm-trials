on the pareto principle in process mining, task mining, and
robotic process automation
wil m.p. van der aalst
a
process and data science (pads), rwth aachen university, d-52056 aachen, germany
wvdaalst@pads.rwth-aachen.de
keywords: process mining, task mining, robotic process automation, pareto distribution
abstract: process mining is able to reveal how people and organizations really function. often reality is very different
and less structured than expected. process discovery exposes the variability of real-life processes. confor-
mance checking is able to pinpoint and diagnose compliance problems. task mining exploits user-interaction
data to enrich traditional event data. all these different forms of process mining can and should support
robotic process automation (rpa) initiatives. process mining can be used to decide what to automate and to
monitor the cooperation between software robots, people, and traditional information systems. in the process
of deciding what to automate, the pareto principle plays an important role. often 80% of the behavior in
the event data is described by 20% of the trace variants or activities. an organization can use such insights
to “pick its automation battles”, e.g., analyzing the economic and practical feasibility of rpa opportunities
before implementation. this paper discusses how to leverage the pareto principle in rpa and other process
automation initiatives.
1 introduction
the pareto principle, also called the 80/20 rule, states
that for many phenomena, 80% of the outcomes (e.g.,
effects, outputs, or values) come from 20% of the
causes (e.g., inputs, resources, or activities). the prin-
ciple has been named after vilfredo pareto (1848-
1923), an italian economist, who noted already in
1896 that about 80% of the land in italy belonged
to 20% of the people (pareto, 1896). the same
80/20 distribution was witnessed for other countries.
george kingsley zipf (1902-1950) witnessed a simi-
lar phenomenon in linguistics where the frequency of
a word is inversely proportional to its rank in the fre-
quency table for that language (e.g., 80% of the text in
a book may be composed of only 20% of the words)
(zipf, 1949). bradford’s law, power law, and scaling
law, all refer to similar phenomena.
real-life processes and the event data stored in in-
formation systems often follow the pareto principle,
as illustrated in figure 1. events may have many at-
tributes, but should at least have a timestamp and refer
to both an activity and a case (i.e., process instance).
examples of cases are sales orders, suitcases in an
airport, packages in a warehouse, and patients in a
hospital. activities are executed for such cases, e.g.,
a
https://orcid.org/0000-0002-0955-6940
0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%
0.00500.001000.001500.002000.002500.003000.003500.004000.004500.005000.00
0 5 10 15 20
cumulative percentagefrequency
activities or process variants sorted by frequencyfigure 1: illustration of the pareto principle: 20% of the
most frequent activities or process variants account for 80%
of the observed behavior.
checking-in a suitcase, recording a patient’s blood
pressure, transferring money, or delivering a parcel.
often a few activities may explain most of the events
seen in the event log. the same holds for the process
variants, i.e., unique traces of activities. the so-called
“happy path” in a process refers to the most frequent
process variants involving a limited number of activ-
ities. however, in real-life processes there are often
many different activities that are rare and cases that
are one-of-a-kind (i.e., no other case follows the ex-
act same path).
part of the variability is explained by unde-
sired behaviors of the actors involved (e.g., re-desired undesired
frequent
infrequentfigure 2: classifying behavior into four categories.
work, procrastination, data entry problems, and mis-
communication). however, variability may also be
positive and point to human ﬂexibility and inge-
nuity. human actors are able to handle excep-
tional cases, solve wicked problems, and respond to
changes. figure 2 shows four types of behavior: fre-
quent/desired, frequent/undesired, infrequent/desired,
infrequent/undesired. many it problems are caused
by focusing on frequent/desired behavior only, with-
out understanding and addressing the other three
quadrants. infrequent behavior is not automatically
undesirable, and undesirable behavior may be fre-
quent, and at the same time entirely invisible to im-
portant stakeholder.
process mining can be used to uncover and diag-
nose the different behaviors shown in figure 2 (aalst,
2016). this is important for making decisions on
what can and should be automated. therefore, we
relate process mining to task mining androbotic pro-
cess automation (rpa) (aalst et al., 2018).
the remainder of this paper is organized as fol-
lows. section 2 introduces process mining. task
mining and rpa are brieﬂy introduced in section 3.
these provide the setting to deﬁne variability in sec-
tion 4. we will show that the pareto principle can be
viewed at different abstraction levels. these insights
are related to automation decisions in section 5. sec-
tion 6 concludes the paper.
2 process mining: linking
data and processes
process mining provides a range of techniques to uti-
lize event data for process improvement. the starting
point for process mining is an event log . each event
in such a log, refers to an activity possibly executed
by a resource at a particular time and for a particu-
larcase. an event may have many more attributes,
e.g., transactional information, costs, customer, loca-
tion, and unit. table 1 shows a (simpliﬁed) fragmenttable 1: a small fragment of an event log.
case id activity timestamp costs ...
... ... ... ... ...
qr5753 create po 27-4-2020 230 ...
qr5548 rec. order 27-4-2020 230 ...
qr5754 create po 28-4-2020 230 ...
qr5758 payment 28-4-2020 230 ...
qr5754 send po 28-4-2020 230 ...
qr5753 send po 28-4-2020 230 ...
qr5753 rec. order 29-4-2020 230 ...
qr5753 rec. inv. 29-4-2020 230 ...
qr5753 payment 30-4-2020 230 ...
... ... ... ... ...
of a larger event log. such event data are related to
process models expressed as directly follows graphs
(dfgs), petri nets (various types), transition systems,
markov chains, bpmn (business process modelling
notation) diagrams, uml activity diagrams, process
trees, etc. these diagrams typically describe the life-
cycle of an individual case (although object-centric
process mining techniques try to overcome this limi-
tation (aalst, 2019)).
for a more complete description of the different
types process mining techniques we refer to (aalst,
2016). here we only mention the main types of pro-
cess mining:
•process discovery : automatically learning pro-
cess models to show what is really happening.
•conformance checking : identifying and diagnos-
ing deviations between a model and reality.
•performance analysis : identifying and diagnosing
bottlenecks, rework, blockages, waste, etc.
•root-cause analysis : data-driven explanations
for observed phenomena in the process.
•process prediction : using process models learned
from event data to predict dynamic behavior.
most of the process mining techniques are inter-
active to provide a deeper understanding of the pro-
cess. figure 3 shows how a discovery technique can
generate process models at different abstraction levels
(without any modeling). activities are included based
on their frequency. the yellow dots refer to real or-
ders showing the connection to the underlying event
data.
figure 3 shows only one of the 1500 prom plug-
ins: the so-called inductive visual miner (leemans
et al., 2018). next to open-source software like
prom, there are over 30 commercial tools (e.g., celo-
nis, disco, processgold, myinvenio, pafnow, minit,
qpr, mehrwerk, puzzledata, lanalabs, stereologic,
everﬂow, timelinepi, signavio, and logpickr) illus-
trating the adoption of process mining in industry.figure 3: seamless simpliﬁcation of discovered process models using activity frequencies.
3 task mining and robotic
process automation
process mining can be used to identify work done
by people that could or should be automated (aalst,
2016). note that this is just one of several pro-
cess mining use cases (there are many other ways to
improve performance and compliance in processes).
robotic process automation (rpa) has lowered the
threshold for process automation. repetitive tasks
done by people are handed over to software robots.
for rpa, there is no need to change or replace the
pre-existing information systems. instead, software
robots replace users by interacting with the informa-
tion systems through the graphical user interfaces
(guis) that humans use.
obviously, rpa is related to workﬂow manage-
ment (wfm), which has been around for several
decades (aalst and hee, 2004). in the mid-nineties,
the term straight through processing (stp) was used
to emphasize the desire to replace humans by software
for repetitive tasks (aalst, 2013).
the three leading rpa vendors are uipath
(founded in 2005), automation anywhere (founded
in 2003), and blue prism (founded in 2001) have
been successful in lowering the threshold for automa-
tion. the key idea is that the back-end systems are
not changed; only the activities of people interacting
with these systems are automated. for the informa-
tion system nothing changes. this way, wfm and
stp may become economically feasible where tradi-
tional automation is too expensive. therefore, the au-thor sometimes refer rpa as “the poor man’s work-
ﬂow management solution”. rpa aims to replace
people by automation done in an “outside-in” manner
(i.e., via the user interface rather than the backend).
this differs from the classical “inside-out” approach
to improve information systems (aalst et al., 2018).
although rpa companies often use the terms ma-
chine learning (ml) and artiﬁcial intelligence (ai),
automation projects highly depend on a manual anal-
ysis of the work being done. the focus is on iden-
tifying sequences of manual activities. for example,
starting an application, copying an address, and then
pasting the address into a form on some website. the
usage of ai and ml in the context of rpa is often
limited and only used as a “sales gimmick”, optical
character recognition (ocr) and basic classiﬁcation
problems (e.g., decision trees) are sold as new intelli-
gent solutions. nevertheless, there is a clear relation
between rpa and process mining.
the synergy between rpa and process mining
was ﬁrst discussed in (aalst et al., 2018). this arti-
cle identiﬁes the “long tail of work” and stresses that
humans often provide the “glue” between different it
systems in a hidden manner and that this “glue” can
only be made visible using process mining. process
mining is presented as a way to identify what can
be automated using rpa. however, process mining
should not only be used only in the implementation
phase. by continuously observing human problem re-
solving capabilities (e.g., in case of system errors, un-
expected system behavior, changing forms) rpa tools
can adapt and handle non-standard cases (aalst et al.,2018). moreover, process mining can also be used to
continuously improve the orchestration of work be-
tween systems, robots, and people.
in (geyer-klingeberg et al., 2018) it is shown
how celonis aims to support organizations through-
out the whole lifecycle of rpa initiatives. three steps
are identiﬁed: (1) assessing rpa potential using pro-
cess mining (e.g., identifying processes that are scal-
able, repetitive and standardized), (2) developing rpa
applications (e.g., supporting training and compari-
son between humans and robots), and (3) safeguard-
ing rpa beneﬁts (e.g., identifying concept drift and
compliance checking). the “automation rate” can be
added as a performance indicator to quantify rpa ini-
tiatives.
in (leno et al., 2020) the term robotic process
mining (rpm) is introduced to refer to “a class of
techniques and tools to analyze data collected dur-
ing the execution of user-driven tasks in order to sup-
port the identiﬁcation and assessment of candidate
routines for automation and the discovery of routine
speciﬁcations that can be executed by rpa bots”. the
authors propose a framework and rpm pipeline com-
bining rpa and process mining, and identify chal-
lenges related to recording, ﬁltering, segmentation,
simpliﬁcation, identiﬁcation, discovery, and compila-
tion.
several vendors (e.g., celonis, myinvenio,
nikarpa, uipath) recently adopted the term task
mining (tm) to refer to process mining based on
user-interaction data (complementing business data).
these user-interaction data are collected using task
recorders (similar to spy-ware monitoring speciﬁc
applications) and ocr technology to create textual
data sets. often screenshots are taken to contextualize
actions taken by the user. natural language process-
ing (nlp) techniques and data mining techniques
(e.g., clustering) are used to enrich event data. the
challenge is to match user-interaction data based on
identiﬁers, usernames, keywords, and labels, and
connect different data sources. note that the usage of
task mining is not limited to automation initiatives.
it can also be used to analyze compliance and
performance problems (e.g., decisions taken without
looking at the underlying information). note that
screenshots can be used to interpret and contextualize
deviating behavior. for example, such analysis can
reveal time-consuming workarounds due to system
failures.4 defining v ariability
the pareto principle (pareto, 1896) can be observed
in many domains, e.g., the distribution of wealth, fail-
ure rates, and ﬁles sizes. as shown in figure 1, this
phenomenon can also be observed in process min-
ing. often, a small percentage of activities accounts
for most of the events, and a small percentage of
trace variants accounts for most of the cases. when
present, the pareto distribution can be exploited to dis-
cover process models describing mainstream behav-
ior. however, for larger processes with more activi-
ties and longer traces, the pareto distribution may no
longer be present. for example, it may be that most
traces are unique. in such cases, one needs to abstract
or remove activities in the log to obtain a pareto dis-
tribution, and separate mainstream from exceptional
behavior.
the goal of this section is to discuss the notion of
variability in process mining . to keep things simple,
we focus on control-ﬂow only. formally, events can
have any number of attributes and also refer to prop-
erties of the case, resources, costs, etc. in the context
of rpa, events can also be enriched with screenshots,
text fragments, form actions, etc. these attributes will
make any case unique. however, even when all cases
are unique, we would still like to quantify variability.
therefore, the principles discussed below are generic
and also apply to other attributes.
as motivated above, we only consider activity la-
bels and the ordering of events within cases. consider
again the simpliﬁed event log fragment in table 1. in
our initial setting, we only consider the activity col-
umn. the case id column is only used to correlate
events and the timestamp column is only used to or-
der events. all other columns are ignored. this leads
to the following standard deﬁnition.
deﬁnition 1 (traces) .ais the universe of activities .
atrace t2ais a sequence of activities. t=ais
the universe of traces.
trace t=hcreatepo ;sendpo ;recorder ;recinv ;
paymenti2trefers to 5 events belonging to the same
case (case qr5753 in table 1). an event log is a col-
lection of cases, each represented by a trace.
deﬁnition 2 (event log) .l=b(t)is the universe
of event logs. an event log l2lis a ﬁnite multiset
of observed traces.
an event log is a multiset of traces. event
log l= [hcreatepo ;sendpo ;recorder ;recinv ;
paymenti5;hcreatepo ;canceli3;hsendpo ;recinv ;
recorder ;paymenti3;]refers to 10 cases (i.e.,
jlj=10). in the remainder, we use single letters
for activities to ensure a compact representation.trace variant distribution before activity -based 
filtering : since all 14992 variants are unique we 
cannot filter in a meaningful way .  trace variant distribution after activity -based 
filtering : now we can exploit the pareto -like 
distribution to filter trace variants .  
0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%
0.00500.001000.001500.002000.002500.003000.003500.004000.004500.005000.00
0 5 10 15 20
cumulative percentagefrequency
process variants sorted by frequency
0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%
012345678910
0 5000 10000 15000
cumulative percentagefrequency
process variants sorted by frequencyfigure 4: the left diagram shows an event log where each trace variant is unique, i.e., each of the 14992 cases is unique.
therefore, it is impossible to ﬁlter and it seems that the pareto principle cannot be applied (the blue line is ﬂat, showing
that frequency-based ﬁltering is not possible). the right diagram shows the same data set after activity-based ﬁltering. the
infrequent activities have been removed. now there is a clear pareto-like distribution that can be exploited in analysis and
separate the usual from the unusual behavior.
for example, l= [ha;b;c;di7;ha;c;b;di3].l(t)
is the number of times trace tappears in l, e.g.,
l(ha;b;c;di) =7.
we assume that the usual operators are deﬁned for
multisets. l1]l2is the union of two multisets, jljis
the number of elements, and l1nl2is the difference.
l1\l2is the intersection of two multisets. [t2lj
b(t)]is the multiset of all elements in lthat satisfy
some condition b.
deﬁnition 3 (simple variability measures) .for an
event log l2l, we deﬁne simple variability measures
such as:
•jft2lgj, i.e., the number of trace variants,
•jfa2tjt2lgj, i.e., the number of activities,
• entropy (l), i.e., the entropy of traces,1and
• entropy ([a2tjt2l]), i.e., the activity entropy.
for l1= [ha;b;c;di70;ha;c;b;di30]:
jft2l1gj=2,jfa2tjt2l1gj=4,entropy (l1) =
 (0:7log2(0:7) + 0:3log2(0:3)) = 0:88,
entropy ([a2tjt2l1]) = 2 (since all four ac-
tivities happen 100 times). the above measures can
be normalized, e.g., jft2lgj=jljyields a number
between 0 and 1. the latter value is reached when all
traces are unique, i.e., maximal variability.
l2= [ha;b;c;di65;ha;c;b;di25;he;a;b;c;di2;ha;
f;b;c;di2;ha;b;g;c;di2;ha;b;c;h;di2;ha;b;c;d;ii2]
is another (intentionally similar) event log. now
jft2l2gj=7,jfa2tjt2l2gj=9,entropy (l2) =
1:47, and entropy ([a2tjt2l2]) = 2:17. the
number of unique traces more than tripled and the
number of activities more than doubled. however,
event log l2is similar to l1, only 10 events were
added to the 400 events in l1.
1for a multiset x, the information entropy entropy (x) =
 åx2x(x(x)=jxj)log2(x(x)=jxj).assume now an event log l3based on l1, but were
randomly events are added until each trace is unique.
thenjft2l3gj=100 and entropy (l3) =6:64. these
numbers do not reﬂect that there is still a rather stable
structure. more advanced notions such as the earth
movers’ distance between logs (leemans et al., 2019)
provide a better characterization. however, our goal
is to uncover a pareto-like distribution.
now consider figure 1 again. assume that trace
variants are sorted based on frequency. for l1we
would seeh70;30i(two variants), for l2we would
seeh65;25;2;2;2;2;2i(seven variants), and for l3we
would seeh1;1;;:::; 1i(100 variants). event log l2is
closest to a pareto distribution: 90% of the cases are
described by 33% of the variants.
the distribution in figure 1 is h4999 ;3332 ;2221 ;
1481 ;;987;:::; 3;2i(20 variants), i.e., the four most
frequent variants cover 80% of the cases. let’s refer
to this event log as l4.l4has 14992 cases.
if our event data has a pareto-like distribution,
then ﬁltering can be used to identify the regular main-
stream behavior. there are two types of ﬁltering: re-
moving infrequent variants and removing infrequent
activities. these can be formalized as follows.
deﬁnition 4 (sequence projection) .let aa.a2
a!ais a projection function and is deﬁned recur-
sively: (1)hia=hiand (2) for t2aand a2a:
(hait)a=(
ta if a62a
haitaif a2a
deﬁnition 5 (filtering) .let l2lbe an event log.
• for any aa: ﬁlter (a;l) = [ tajt2l]only
keeps the events corresponding to the activity set
a.slider to filter based 
on path frequenciesslider to filter based on 
activity frequenciesfigure 5: sliders used in the inductive visual miner to search for a pareto-like distribution.
• for any ta: ﬁlter (t;l) = [t2ljt2t]only
keeps the trace variants in t .
• freqact (k;l) =fa2ajåt2lj[x2tjx=a]jkg
are the frequent activities (k 2i n).
• freqtraces (k;l) =ft2ljl(t)kgare the fre-
quent traces (k2i n).
deﬁnition 6 (filtered event logs) .let l2lbe an
event log and k 1;k22i ntwo parameters.
• lk1=ﬁlter(freqact (k1;l);l)is the event log with-
out the infrequent activities.
• lk1;k2=ﬁlter(freqtraces (k2;lk1);lk1)is the event
log without the infrequent variants.
in deﬁnition 6, there are three event logs: lis
the original event log, lk1is the log after removing
infrequent activities, and lk1;k2is the log after also
removing infrequent variants.
l1
2=l2= [ha;b;c;di65;ha;c;b;di25;he;a;b;
c;di2;ha;f;b;c;di2;ha;b;g;c;di2;ha;b;c;h;di2;ha;
b;c;d;ii2](i.e., all activities happened at least once,
so no events are removed). l10
2= [ha;b;c;di75;
ha;c;b;di25](i.e., the ﬁve infrequent activities are
removed). l200
2= [hi100](i.e., none of the activities is
frequent enough to be retained). l1;5
2= [ha;b;c;di65;
ha;c;b;di25](i.e., the ﬁve infrequent variants are
removed). l10;30
2= [ha;b;c;di75]. as mentioned
before, event log l3is based on l1but randomly
events are added until each trace is unique. this
implies that l1;2
2= [ ] (i.e., even for k2=2, none of
the trace variants remains). however, if the randomly
added events all have a frequency lower than 10, then
l10;2
2=l1. this illustrates the interplay between both
types of ﬁltering. if the trace variant distribution does
not exhibit a pareto-like distribution, then it is good
to ﬁlter ﬁrst at the level of activities.figure 4 illustrates the phenomenon just de-
scribed. it may be the case that all cases are unique
and that the variability is too high to see any struc-
ture. however, after abstraction (e.g., removing in-
frequent activities), a pareto-like distribution may
emerge. different forms of abstraction are possible.
we can remove infrequent activities, compose activi-
ties, cluster activities, etc. whenever we are searching
for structure in event data, we should make sure that
the resulting distribution follows a power law.
existing process discovery techniques ranging
from the fuzzy miner (g ¨unther and aalst, 2007) to
the inductive visual miner (leemans et al., 2018) al-
ready try to exploit this. however, they require the
user to set the thresholds. future research should aim
at supporting the quest for “pareto-like phenomena”
in a better way. for example, the activity thresh-
olds should be set in such a way that the resulting
trace variants indeed follow the 80-20 rule. more-
over, ﬁltering should not be done using just frequen-
cies. there may be frequent activities that conceal
regular patterns among less frequent activities.
5 how to pick your
automation battles?
in the previous section, we showed that variability can
be deﬁned and measured. however, regular structures
may be hidden. even when all cases follow a unique
path there may be dominant behaviors that are not vis-
ible at ﬁrst sight. in most applications “pareto-like
phenomena” are present, but one needs to look at the
right abstraction level.
process mining can be used to quickly understand
the best automation opportunities. based on the the-frequency
process variants sorted by frequency(1)(2)(3)figure 6: based on the pareto principle behavior can be classiﬁed in three groups: (1) regular high-frequent subprocesses
automated in the traditional way, (2) frequent standardized subprocesses taken over by robots, and (3) infrequent and/or
exceptional behaviors still handled by people.
oretical concepts presented before, we can sort be-
havior based on frequency. in figure 6, behavior is
split into three groups. the ﬁrst group (green) rep-
resents standardized high-frequent behavior that is so
frequent and standard that it should be automated in
the traditional manner (i.e., not using rpa, but in
the information system itself). the third group (red)
represent non-standard behavior that requires human
judgment (e.g., based on context and ad-hoc commu-
nication). the frequency is too low to learn what hu-
mans do. also, contextual information not stored in
the information system may play an important role in
making the decisions. therefore, it is pointless to try
and automate such behaviors. rpa aims to automate
the second (i.e., intermediate) group of behaviors (or-
ange). these are the subprocesses that are rather fre-
quent and simple, but it is not cost-effective to change
the information system. for example, when people
are repeatedly copying information from one system
to another, it may still be too expensive to change both
systems in such a way that the information is synchro-
nized. however, using rpa, this can be done by soft-
ware robots taking over the repetitive work.
figure 6 oversimpliﬁes reality. there are activi-
ties that cannot be automated because a physical ac-
tion (e.g., checking a product) is needed or because
a human action is required by regulations (e.g., an
approval). moreover, before making any automation
decision, the existing process behaviors need to be
mapped onto the four quadrants in figure 2. rpa
should not be used to automate undesired behaviors.
this shows that any automation project will require
human judgment.6 conclusion
the recent attention for robotic process automation
(rpa) has fueled a new wave of automation initia-
tives. in the 1990-ties, there was similar excitement
about workﬂow management (wfm) systems and
straight through processing (stp). many of the tra-
ditional wfm/stp initiatives failed because of two
reasons: (1) automation turned out to be too expen-
sive and time-consuming (see for example the longi-
tudinal study in (reijers et al., 2016)) and (2) the real
processes turned out to be much more complicated
than what was modeled leading to failures and resis-
tance. also many of the later business process man-
agement (bpm) projects led to similar disappointing
results (expensive and disconnected from reality). as
a result, the term “process management” got a nega-
tive connotation and is often seen as synonymous for
process documentation and modeling.
the combination of process mining and rpa of-
fers a unique opportunity to revitalize process man-
agement and address the traditional pitfalls of pro-
cess modeling and process automation. rpa can
be more cost-effective because the underlying infor-
mation systems can remain unchanged. many of
the transitional bpm/wfm initiatives require com-
plex and expensive system integration activities. rpa
avoids this by simply replacing the “human glue” by
software robots. as stated in (aalst et al., 2018), rpa
uses an “outside-in” rather than the classical classi-
cal “inside-out” approach. although rpa may be
cheaper, it is still important to carefully analyze the
processes before automation. current practices need
to be mapped onto the four quadrants in figure 2.
there is no point in automating non-compliant or in-
effective behavior. hence, process mining must playa vital role in picking the “automation battles” in an
organization. it is possible to objectively analyze the
economic feasibility of automation by analyzing the
current processes. next to business data, also user-
interaction data needs to be used to fully understand
the work done by people. the term task mining refers
to the application of process mining to such user-
interaction data. the application of process mining
is broader than rpa and does not stop after the soft-
ware robots become operational. the orchestration of
processes involving systems, robots, and people re-
quires constant attention. in this paper, we focused on
the pareto principle in event data as a means to iden-
tify opportunities for automation. currently, users can
use variant ﬁltering or activity-based ﬁltering. often a
combination of both is needed to separate mainstream
from exceptional behavior. we advocate more sys-
tematic support for this. if there is no clear pareto-
like distribution and all behaviors are unique, further
abstractions are needed. this also opens the door for
new discovery and conformance checking techniques.
several studies suggest that many jobs will be
taken over by robots in the coming years (frey and
osborne, 2017; hawksworth et al., 2018). this makes
the interplay between process mining and automation
particularly relevant and a priority for organizations.
acknowledgements
we thank the alexander von humboldt (avh)
stiftung for supporting our research.
references
aalst, w. van der (2013). business process management: a
comprehensive survey. isrn software engineering ,
pages 1–37. doi:10.1155/2013/507984.
aalst, w. van der (2016). process mining: data science in
action . springer-verlag, berlin.
aalst, w. van der (2019). object-centric process mining:
dealing with divergence and convergence in event
data. in ¨olveczky, p. and sala ¨un, g., editors, soft-
ware engineering and formal methods (sefm 2019) ,
volume 11724 of lecture notes in computer science ,
pages 3–25. springer-verlag, berlin.
aalst, w. van der, bichler, m., and heinzl, a. (2018).
robotic process automation. business and informa-
tion systems engineering , 60(4):269–272.
aalst, w. van der and hee, k. van (2004). workﬂow man-
agement: models, methods, and systems . mit press,
cambridge, ma.
frey, c. and osborne, m. (2017). the future of em-
ployment: how susceptible are jobs to computerisa-tion? technological forecasting and social change ,
114(c):254–280.
geyer-klingeberg, j., nakladal, j., baldauf, f., and veit, f.
(2018). process mining and robotic process automa-
tion: a perfect match. in proceedings of the industrial
track at the 16th international conference on busi-
ness process management (bpm 2018) , pages 124–
131.
g¨unther, c. and aalst, w. van der (2007). fuzzy min-
ing: adaptive process simpliﬁcation based on multi-
perspective metrics. in international conference on
business process management (bpm 2007) , volume
4714 of lecture notes in computer science , pages
328–343. springer-verlag, berlin.
hawksworth, j., berriman, r., and goel, s. (2018). will
robots really steal our jobs? an international anal-
ysis of the potential long term impact of automa-
tion. technical report, pricewaterhousecoopers.
leemans, s., fahland, d., and aalst, w. van der (2018).
scalable process discovery and conformance check-
ing. software and systems modeling , 17(2):599–631.
leemans, s., syring, a., and aalst, w. van der (2019).
earth movers’ stochastic conformance checking. in
business process management forum (bpm forum
2019) , volume 360 of lecture notes in business infor-
mation processing , pages 127–143. springer-verlag,
berlin.
leno, v ., polyvyanyy, a., dumas, m., rosa, m., and
maggi, f. (2020). robotic process mining: vision
and challenges. business and information systems
engineering (to appear).
pareto, v . (1896). cours d’economie politique . droz,
gen`eve.
reijers, h., vanderfeesten, i., and aalst, w. van der (2016).
the effectiveness of workﬂow management systems:
a longitudinal study. international journal of infor-
mation management , 36(1):126–141.
zipf, g. (1949). human behaviour and the principle of
least effort . addison-wesley, reading, ma.