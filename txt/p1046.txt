software & systems modeling
https://doi.org/10.1007/s10270-018-0664-7
special section paper
connecting databases with process mining: a meta model and toolset
eduardo gonzález lópez de murillas1·hajo a. reijers1,2·wil m. p. van der aalst1,3
received: 27 december 2016 / revised: 8 january 2018 / accepted: 7 february 2018
© the author(s) 2018. this article is an open access publication
abstract
process mining techniques require event logs which, in many cases, are obtained from databases. obtaining these event logs
is not a trivial task and requires substantial domain knowledge. in addition, an extracted event log provides only a single
view on the database. to change our view, e.g., to focus on another business process and generate another event log, it is
necessary to go back to the source of data. this paper proposes a meta model to integrate both process and data perspectives,
relating one to the other. it can be used to generate different views from the database at any moment in a highly ﬂexible way.
this approach decouples the data extraction from the application of analysis techniques, enabling the application of process
mining in different contexts.
keywords process mining ·database ·data schema ·meta model ·event extraction
1 introduction
the ﬁeld of process mining offers a wide variety of tech-
niques to analyze event data. process discovery, conformance
and compliance checking, performance analysis, process
monitoring and prediction, and operational support are some
of the techniques that process mining provides to better
understand and improve business processes. however, most
of these techniques rely on the existence of an event log.
obtaining event logs in real-life scenarios is not a trivial
task. it is not common to ﬁnd logs exactly in the right form.
in many occasions, such logs simply do not exist and need
to be extracted from some sort of storage, like databases. in
communicated by dr. ilia bider and rainer schmidt.
beduardo gonzález lópez de murillas
e.gonzalez@tue.nl
hajo a. reijers
h.a.reijers@tue.nl ; h.a.reijers@vu.nl
wil m. p. van der aalst
w.m.p.v.d.aalst@tue.nl ; wvdaalst@pads.rwth-aachen.de
1department of mathematics and computer science,
eindhoven university of technology, eindhoven,
the netherlands
2department of computer science, vrije universiteit
amsterdam, amsterdam, the netherlands
3department of computer science, rwth aachen university,
aachen, germanythese situations, when a database exists, several approaches
are available to extract events. the most general is the classi-
cal extraction in which events are manually obtained from the
tables in the database. to do so, substantial domain knowl-
edge is required to select the right data scattered across
tables. some work has been done in this ﬁeld to assist in
the extraction and log generation task [ 2]. also, studies have
been performed on how to extract events in speciﬁc envi-
ronments like sap [ 8,10,18]o ro t h e re r ps y s t e m s[ 12].
in [5], we presented a more general solution to extract events
from databases, regardless of the application under study.
the paper describes how to automatically obtain events from
database systems that generate redo logs as a way to recover
from failure. all mentioned approaches aim at, eventually,
generating an event log , i.e., a set of traces, each of them con-
taining a set of events . these events represent operations or
actions performed in the system under study and are grouped
in traces following some criteria. however, there are multiple
ways in which events can be selected and grouped into traces.
depending on the perspective we want to take on the data,
we need to extract event logs differently. also, a database
contains a lot more information than just events. the extrac-
tion of events and its representation as a plain event log can
be seen as a “lossy” process during which valuable infor-
mation can get lost. considering the prevalence of databases
as a source for event logs, it makes sense to gather as much
information as possible, combining the process view with the
actual data.
123e. g. l. de murillas et al.
we see that process mining techniques grow more and
more sophisticated. yet, the most time-consuming activity,event log extraction , is hardly supported. in big industrial
database settings, where event data are scattered through
hundreds of tables, and many processes coexist in the sameenvironment, the queries used to extract event logs canbecome very complicated, difﬁcult to write and hard to mod-
ify. ideally, users should be able to ﬁnd events explicitly
deﬁned and stored in a centralized way. these events shouldbe deﬁned in such a way that event correlation and log build-
ing could be performed effortlessly and be easily adapted to
the business questions to answer in each situation. also, todiscover meaningful data rules, these events should be anno-
tated with enough data attributes.
this paper aims at providing support to tackle the problem
ofobtaining, transforming, organizing and deriving data and
process information from databases , abstracting the data to
high-level concepts. this means that, after the etl procedure
is applied, users will no longer have to deal with low-levelraw data scattered through tables (e.g., timestamps, activity
names and case ids as columns of different tables that need
to be joined together). conversely, users will be able to focuson the analysis, dealing only with familiar process elements
such as events, cases, logs and activities, or with data ele-
ments such as objects, object versions
1, object classes and
attributes. also, the new meta model proposed in this workcan be seen as a data warehouse schema that captures all the
pieces of information necessary to apply process mining to
database environments.
in order to build event logs with events from databases,
languages like sql are the natural choice for many users.
the point of this work is not to replace the use of query lan-guages, but to provide a common meta model as a standard
abstraction that ensures process mining applicability. more-
over, one of the main advantages of the adoption of a standardmeta model has to do with multi-perspective event log build-ing and analysis. many different event logs can be built from
the information stored in a database. looking at data from
different perspectives requires ad hoc queries that extract andcorrelate events in different ways (e.g., (a) order, delivery and
invoice events, versus (b) order, product and supplier events).
our meta model deﬁnes the abstract concepts that need to beextracted to enable this multi-perspective event log building
in a more intuitive way.
additionally, as a result of the adoption of the proposed
meta model, it becomes easier to connect the event recording
1in this context, the term “version” or “object version” refers to an
instance of an object at a point in time (e.g., the database object corre-sponding to a speciﬁc customer had two different values for the attribute“address” at different points in time, representing two versions of thesame customer object). this is different of the usual meaning of “ver-sion” in the software context as a way to distinguish different codereleases (e.g., version 1.1).fig. 1 data gathering from several systems to a meta model
system of enterprises with analysis tools, generating different
views on the data in a ﬂexible way. also, this work presentsa comprehensive integration of process and data information
in a consistent and uniﬁed format. all of this is supported
by our implementation. moreover, the provided solution hasthe beneﬁt of being universal, being applicable regardless of
the speciﬁc system in use. figure 1depicts an environment in
which the process information of a company is scattered overseveral systems from a different nature, like erps, crms,
bpm managers, database systems, redo logs, etc. in such a
heterogeneous environment, the goal is to extract, transformand derive data from all sources to a common representa-tion. by putting all pieces together, analysis techniques like
process mining can be readily applied.
the remainder of this paper is structured as follows: sec-
tion 2presents a running example used throughout the paper.
section 3explains the proposed meta model. implementation
details are presented in sect. 4. the approach is evaluated in
sect. 5on three real-life environments. the results of the
evaluation and the querying of the output of our approach
are analyzed in sect. 6. section 7discusses the related work,
and ﬁnally, sect. 8presents conclusions and future work.
additionally, appendix a provides a formalization of the
meta model, and appendix b extends the formal details of
the real-life environments presented in sect. 5.
2 running example
in this section, we propose a running example to explain and
illustrate our approach. assume we want to analyze a setting
where concerts are organized and concert tickets are sold. todo so, a database is used to store all the information relatedto concerts, concert halls ( hall), seats, tickets, bands, perfor-
mance of bands in concerts ( band_playing ), customers and
bookings. figure 2shows the data schema of the database.
in it we see many different elements of the involved process
represented. let us consider now a complex question that
could have been posed from a business point of view: what
is the behavior through the process of customers between 18
and 25 years old who bought tickets for concerts of band x?
this question represents a challenge starting from the givendatabase for several reasons:
123connecting databases with process mining: a meta model and toolset
(a)(b)
(c)
(d)(e)
fig. 2 data schema of the example database
1. the database does not provide an integrated view of
process and data. therefore, questions related to the exe-
cution of the underlying process with respect to some of
the elements cannot be directly answered with a query.
2. the current database schema ﬁts the purpose of storing
the information in this speciﬁc setting, but it does not have
enough ﬂexibility to extend its functionality allocatingnew kinds of data such as events or objects of a different
nature.
3. the setting lacks execution information in an accessible
way (events, traces and logs are missing so one cannotapply process mining directly), and there is no assistance
on how to extract or derive this information from the
given data.
4. if we plan to use the data as it is, we need to adapt to the
way it is stored for every question we want to answer.
all these reasons make the analysis complex in many set-
tings. at best, such an analysis can only be carried out by
the extraction of a highly specialized event log by creatinga complex ad hoc query. besides, the extraction will need to
be repeated for new questions that require a new viewpoint
on the data.
if we consider that, for the present database, some sort
of event recording system is in place, the extraction of addi-
tional events becomes feasible. listing 1shows an example
of an ad hoc query to answer the sample question postedabove. this query makes use of the event data scattered
through the database ( booking ,band , etc.), as depicted in
fig.2, together with events recorded by a redo log system pro-
vided by the rdbms ( redo_logs_mapping table). the way to
access redo log information has been simpliﬁed in this exam-
ple for the sake of clarity. the ﬁrst part of the query retrievesticket booked events, making use of the booking_date times-tamp stored in the table booking . these events are united to
the ones corresponding to customer events. these last ones
are obtained from redo logs, due to the lack of such event
information in the data schema. then, events belonging tothe same customer are correlated by means of the caseid
attribute, and the cases are restricted to the ones belonging
to customers aged 18–25 at the time the booking was made.additionally, we need to keep only the bookings made on
bands named “x” at the moment of the booking. (a band
could change its name at any point in time.) this is an espe-cially tricky step since we need to look into the redo logs tocheck if the name was different at the time the booking was
made.
listing 1 sample query to obtain a highly specialized event log from
the database
select ∗from (
select
"ticket booked" asactivity,
bk.booking_date as timestamp ,
bk.customer_id ascaseid
from booking asbk
union
select
concat(rl.operation," customer proﬁle") asactivity,
rl.timestamp as timestamp ,
rl_value_of(’id’) ascaseid
from redo_logs_mapping asrl
where
rl.table = "customer"
)ase,
booking asbk,
customer ascu,
ticket ast,
concert asc,
band_playing asbp,
band asb
where
e.caseid = cu.id and
cu.id = bk.customer_id and
bk.id = t.booking_id and
t.for_concert = c.id and
c.id = bp.concert_id and
bp.band_id = b.id and
"0018−00−0000:00:00" <= (bk.booking_date −cu.birth_date) and
123e. g. l. de murillas et al.
"0025−00−0000:00:00" >= (bk.booking_date −cu.birth_date) and
(
(b.name = "x" and
b.id not in
(select rl_value_of(’id’) asid
from redo_logs_mapping asrl
where
rl.table = "band"
)
)
or(b.id in
(select rl_value_of(’id’) asid
from redo_logs_mapping asrl
where
rl.table = "band" and
rl_new_value_of(’name’) = "x" and
rl.timestamp <= bk.id and
order by rl.timestamp desc limit 1
)
)
or
(b.id in
(select rl_value_of(’id’) asid
from redo_logs_mapping asrl
where
rl.table = "band" and
rl_old_value_of(’name’) = "x" and
rl.timestamp >= bk.id and
order by rl.timestamp asc limit 1
)
)
)
it is evident that extracting specialized event logs that
answer very speciﬁc questions, while maintaining the origi-nal data schema, is possible. however, such queries will haveto be adapted or rewritten for every different setting or event
recording system. the fact that users, and especially business
analysts, need to be knowledgeable about the particularitiesof how event data are stored represents an important chal-
lenge to overcome. this is one of the main reasons why so
much time and effort is consumed during the etl phase,that should be devoted to analysis. this work aims at sup-
porting the etl phase and ultimately at decoupling it from
the analysis by providing a set of familiar abstractions readilyavailable for the business analysts.
3 meta model
as has been shown before, a need exists for a way to store
execution information in a structured way, something that
accepts data from different sources and allows building fur-ther analysis techniques independently from the origin of thisdata. efforts in this ﬁeld have already been made as can be
observed in [ 7] with the ieee xes standard. this standard
deﬁnes a structure to manage and manipulate logs, containingevents and traces and the corresponding attributes. there-
fore, xes is a good format to represent behavior. however,
an xes ﬁle is just one view on the data, and despite being anextensible format, it does not provide a predeﬁned structure
to store all the linked information we want to consider.
because of this, it seems necessary to deﬁne a structured
way to store additional information that can be linked to theclassical event log. this new way to generalize and store
information must provide sufﬁcient details about the process,the data types and the relations between all the elements,
making it possible to answer questions at the business level,
while looking at two different perspectives: data and process.
3.1 requirements
to be able to combine the data and process perspectives ina single structure, it is important to deﬁne a set of require-ments that a meta model must fulﬁll. it seems reasonable todeﬁne requirements that consider backwards compatibility
with well-established standards, support of additional infor-
mation, its structure and the correlation between process anddata views:
1. the meta model must be compatible with the current meta
model of xes, i.e., any xes log can be transformed to the
new meta model and back without loss of information,
2. it must be possible to store several logs in the new meta
model, avoiding event duplication,
3. logs stored in the same meta model can share events and
belong to different processes,
4. it must be possible to store some kind of process repre-
sentation in the meta model,
5. the meta model must allow storing additional informa-
tion, like database objects, together with the events, traces
and processes, and the correlation between all these ele-
ments,
6. the structure of additional data must be precisely mod-
eled,
7. all information mentioned must be self-contained in a
single storage format, easy to share and exchange, simi-larly to the way that xes logs are handled.
the following section describes the proposed meta model
which complies with these requirements, providing a descrip-tion of the concepts. a formalization of the meta model canbe found in appendix a.
3.2 description
considering the typical environments subject to study in theprocess mining ﬁeld, we can say that it is common to ﬁnd
systems backed up by some sort of database storage system.
regardless of the speciﬁc technology behind these databases,all of them have in common some kind of structure for data.
we can describe our meta model as a way to integrate process
and data perspectives, providing ﬂexibility on its inspec-tion and assistance to reconstruct the missing parts. figure 3
shows a high-level representation of the meta model. on the
right-hand side, the data perspective is considered, while theleft models the process view. assuming that the starting point
123connecting databases with process mining: a meta model and toolset
fig. 3 diagram of the meta model at a high level
of our approach is data, we see that the less abstract elements
of the meta model, events andversions , are related, providing
the connection between the process and data view. these arethe basic blocks of the whole structure and, usually, the rest
can be derived from them.
the data side considers three elements: data models ,
objects and versions . the data models provide a schema
describing the objects of the database. the objects represent
the unique entities of data that ever existed or will exist in ourdatabase, while the versions represent the speciﬁc values ofthe attributes of an object during a period of time. versions
represent the evolution of objects through time. the process
side considers events ,instances and processes . processes
describe the behavior of the system. instances are traces of
execution for a given process, being sets of events ordered
through time. these events represent the most granular kindof execution data, denoting the occurrence of an activity or
action at a certain point in time.
figure 4depicts the entity-relation diagram of the meta
model. some elements have been omitted in the diagramfor the sake of simplicity. a full version of the er dia-
gram is available online
2, and a formalization is provided
in appendix a. each of the entities in the diagram, repre-sented by a square, corresponds to the basic entities of the
meta model. also, these entities, together with their rela-
tions (diamond shapes), have been grouped in areas that wecallsectors (delimited by dashed lines). these sectors: data
models ,objects ,versions ,events ,cases andprocess models
contain tightly related concepts and provide an abbreviatedrepresentation of the meta model. for the sake of clarity, the“sectorized” representation of the meta model will be used
in further sections. as can be observed, the entity-relation
diagram is divided into six sectors. the purpose of each ofthem is described below:
–data models : this sector is formed by concepts needed
to describe the structure of any database system. many
2https://github.com/edugonza/openslex/blob/master/doc/meta-
model.png .data models can be represented together in this sec-
tor, whose main element is the data model entity. for
each data model, several classes can exist. these classes
are abstractions of the more speciﬁc concept of table,
which is commonly found in rdbmss. looking at thedatabase provided in sect. 2, the tables customer and
booking are examples of classes. these classes con-
tain attributes , which are equivalent to table columns
in modern databases (e.g., id,name ,address , etc.). the
references between classes of the same data model are
represented with the relationship entity. this last entity
holds links between a source and a target class (e.g., book-
ing_customer_fk which relates the source class booking
to the target class customer ).
–objects :t h e object entity, part of the objects sector, rep-
resents each of the unique data elements that belong to aclass. an example of this can be a hypothetical customer
with customer_id =75. additional details of this object
are omitted, given that they belong to the next sector.
–object versions (versions) : for each of the unique object
entities described in the previous sector, one or many
versions can exist. a version is an instantiation of an
object during a certain period of time, e.g., the cus-
tomer object with id 75, existed in the database, during
a certain period of time, for example from “2015-08-01 14:45:00” to “2016-09-03 12:32:00”. during thatperiod of time, the object had speciﬁc values for the
attributes of the customer class that it belongs to. there-
fore, there is a version of customer 75, valid betweenthe mentioned dates, with name “john smith”, address
“45, 5th avenue”, and birth date “1990-01-03”. if at
some point, the value of one of the attributes changed(e.g., a new address), the end timestamp of the previ-
ous version would be set to the time of the change, and
a new version would be created with the updated valuefor that attribute, and a start timestamp equal to the endof the previous version, e.g., version_1 ={object_id
=75, name =“john smith”, address =“45, 5th
avenue”, birth_date =“1990-01-03”, start_timestamp
=“2015-08-01 14:45:00”, end_timestamp =“2016-09-
03 12:32:00”}, and version_2 ={object_id =75, name
= “john smith”, address =“ﬂoor 103, empire state
building”, birth_date =“1990-01-03”, start_timestamp
=“2016-09-03 12:32:00”, end_timestamp =none }.
note that the value of end_timestamp for the newly cre-
ated object version ( version_2 ) is none. that means
that it is the current version for the corresponding object
(
object_id =75). another entity reﬂected in this sector
is the concept of relation . a relation is an instantiation
of a relationship and holds a link between versions of
objects that belong to the source and target classes of the
relationship. for example, a version of a booking objectcan be related to another version of a customer object
123e. g. l. de murillas et al.
fig. 4 er diagram of the meta model. the entities have been grouped in sectors, delimited by the dashed lines
by means of a relation instance, given that a relationship
(booking_customer_fk ) exists from class booking to class
customer .
–events : this sector collects a set of events, obtained from
any available source (database tables, redo logs, changerecords, system logs, etc.). in this sector, events appear
as a collection, not grouped into traces (such grouping
is reﬂected in the next sector). in order to keep processinformation connected to the data side, each event can
be linked to one or many object versions by means of
123connecting databases with process mining: a meta model and toolset
a label ( eventtoovlabel ). this label allows specifying
what kind of interaction exists between the event and thereferred object version, e.g., insert ,update ,delete ,read ,
etc. events hold details such as timestamp ,life cycle and
resource information, apart from an arbitrary number of
additional event attributes.
–cases and instances : the entities present in this sec-
tor are very important from the process mining point of
view. the events by themselves do not provide muchinformation about the control ﬂow of the underlying pro-
cess, unless they are correlated and grouped into traces
(or cases). first, the activity instance entity must be
explained. this entity is used to group events that refer
to the same instance of a certain activity with different
values for its life cycle, e.g., the execution of the activitybook_tickets generates one event for each phase of its life
cycle: book_tickets+start and book_tickets+complete .
both events, referring to the same execution of an activ-
ity, are grouped in the same activity instance. then, asin any other event log format, activity instances can be
grouped in cases , and these cases, together, form a log.
–processes : the last sector contains information about pro-
cesses . several processes can be represented in the same
meta model. each process is related to a set of activities ,
and each of these activities can be associated with severalactivity instances, contained in the corresponding cases
and instances sector.
an instantiation of this meta model fulﬁlls the require-
ments set in sect. 3.1in terms of storage of data and process
view. some characteristics of this meta model that enablefull compatibility with the xes standard have been omittedin this formalization for the sake of brevity. additionally,
an implementation of this meta model has been made. this
was required in order to provide tools that assist in the explo-ration of the information contained within the populated meta
model. more details on this implementation are explained in
the following section.
4 implementation
the library open sql log exchange (openslex3), based
on the meta model proposed in this work has been imple-mented in java. this library provides an interface to insertdata in an instantiation of this meta model and to access
it in a similar way to how xes logs are managed by the
ieee xes standard [ 7]. however, under the hood it relies
on sql technology. speciﬁcally, the populated meta model
is stored in a sqlite
4ﬁle. this provides some advantages
3http://www.win.tue.nl/~egonzale/projects/openslex/ .
4http://www.sqlite.org/ .like an sql query engine, a standardized format as well
as storage in self-contained single data ﬁles that beneﬁts itsexchange and portability. figure 4shows an er diagram of
the internal structure of the meta model. however, it repre-
sents a simpliﬁed version to make it more understandableand easy to visualize. the complete class diagram of themeta model can be accessed in the openslex’s website
3.
in addition to the library mentioned earlier, an inspector has
been included in the process aware data suite (padas)5
tool. this inspector, depicted in fig. 5, allows exploring
the content of openslex ﬁles by means of a gui in an
exploratory fashion, which lets the user dig into the dataand apply some basic ﬁlters on each element of the structure.
the tool presents a series of blocks that contain the activities ,
logs,cases ,activity instances ,events ,event attribute values ,
data model ,objects ,object versions ,object version attribute
values andrelations entities in the meta model. some of the
lists in the inspector ( logs,cases ,activity instances ,events
andobjects ) have tabs that allow one to ﬁlter the content they
show. for instance, if the tab “per activity” in the cases list
is clicked, only cases that contain events of such activity will
be shown. in the same way, if the tab “per case” in the events
list is clicked, only events contained in the selected case will
be displayed. an additional block in the tool displays the
attributes of the selected event.
the goal of providing these tools is to assist in the task
of populating the proposed meta model, in order to query
it in a posterior step. because the meta model structure and
the data inserted into it are stored in a sqlite ﬁle, it is pos-sible to execute sql queries in a straightforward way. in
particular, the whole process of extracting, transforming and
querying data has been implemented in a rapidprom
6work-
ﬂow. rapidprom is an extension that adds process mining
plugins from prom to the well-known data mining workﬂow
tool rapidminer7. for our task, an openslex meta model
population operator has been implemented in a developmentbranch
8of rapidprom. this operator, together with the data
handling operators of rapidminer (including database con-
nectors), lets us extract, transform and query our populatedmeta model automatically in a single execution of the work-
ﬂow. more details on this workﬂow and the extraction and
transformation steps are provided in sect. 5, showing how the
technique can be applied in different real-life environments.
a workﬂow was designed for each of the evaluation environ-
ments with the purpose of extracting and transforming thecontent of the corresponding databases to ﬁt the structuredof the openslex meta model. each of these workﬂows,
also referred to as adapters, is extensible using the collec-
5http://www.win.tue.nl/~egonzale/projects/padas/ .
6http://rapidprom.org/ .
7http://rapidminer.com/ .
8https://github.com/rapidprom/rapidprom-source/tree/egonzalez .
123e. g. l. de murillas et al.
fig. 5 screenshot of the meta model inspector tool
tion of operators available in rapidminer, rapidprom and
other plugins, and can be modiﬁed to ﬁt new environments.moreover, sect. 6demonstrates how querying of the result-
ing openslex populated meta model can be standardized
for all the proposed environments.
the provided implementation of the openslex meta
model is a general and extensible platform for data extrac-
tion and transformation. it provides the means to abstract rawdata to high-level concepts. these concepts (events, cases,
objects, etc.) are easier to deal with for business analysts.
on the other hand, limitations exist to the current implemen-tation. a general method does not exist at the moment toperform this extraction and transformation independently of
the raw data structure. this means that an ad hoc adapter
needs to be designed for different data architectures (sapdata schema, oracle redo logs, in-table versioning, etc.) in
order to properly extract and interpret events and object ver-
sions. however, we believe that the three adapters providedwith the implementation should sufﬁce in most cases and
serve as a template for other environments. it is important
to note that, despite the need to design an extraction adapterfor each type of environment, it supposes an advantage withrespect to writing ad hoc queries for event log extraction. this
is due to the fact that, once the extraction and transformation
to openslex are performed, automated methods (imple-mented as operators in the rapidprom platform) become
available for generating multiple event logs from differentperspectives in a single dataset. this saves time and effort
and is less prone to errors than designing ad hoc queries for
each speciﬁc event log perspective.
5 application in real-life environments
the development of the meta model presented in this paperhas been partly motivated by the need of a general way tocapture the information contained in different systems com-bining the data and process views. such systems, usually
backed up by a database, use very different ways to internally
store their data. therefore, in order to extract these data, it isnecessary to deﬁne a translation mechanism tailored to the
wide variety of such environments. because of this, the eval-
uation aims at demonstrating the feasibility of transforminginformation from different environments to the proposed meta
model . speciﬁcally, three real-life source environments are
analyzed:
1.database redo logs : ﬁles generated by the rdbms in
order to maintain the consistency of the database incase of failure or rollback operations. the data were
123connecting databases with process mining: a meta model and toolset
obtained from real redo logs generated by a running ora-
cle instance. a synthetic process was designed and run inorder to insert and modify data in the database and trig-
ger the generation of redo logs. because of the simplicity
of the data model, this environment inspired the runningexample of sect. 2.
2.in-table version storage : application-speciﬁc schema to
store new versions of objects as a new row in each table.
the data of this analysis was obtained from a real-lifeinstance of a dutch ﬁnancial organization.
3.sap-style change tables : changes in tables are recorded in
a “redo log” style as a separate table, the way it is done insap systems. for this analysis, real sap data, generated
by an external entity, were used. such data are often used
for training purposes by the third-party organization.
the beneﬁt of transforming data to a common repre-
sentation is that it allows for decoupling the application of
techniques for the analysis from the sources of data. in addi-tion, a centralized representation allows linking data fromdifferent sources. however, the source of data may be incom-
plete in itself. in some real-life scenarios, explicitly deﬁned
events might be missing. other scenarios do not have arecord of previous object versions. something common is
the lack of a clear case notion, therefore the absence of pro-
cess instances. in all these cases, when transforming the datato only ﬁt in our meta model is not enough, we need to apply
some automated inference techniques. this allows one to
derive the missing information and create a complete andfully integrated view. figure 6shows these environments,
and which sectors of our meta model can be populated right
away, only extracting what is available in the database. then,following a series of automated steps like version inference
andevent inference ,case derivation , and process discovery ,
all the sectors can be populated with inferred or derived data.
the ﬁrst part of this evaluation (sect. 5.1) presents the
different scenarios that we can ﬁnd when transforming data.each of these scenarios starts from data that correspond todifferent sectors of the meta model. then, we show how to
derive the missing sectors from the given starting point. sec-
tions 5.2,5.3and 5.4analyze the three real-life common
environments mentioned before. we will demonstrate that
data extraction is possible and that the meta model can be
populated from these different sources. section 5.5presents
a method to merge data from two different systems into a
single meta model structure, providing an end-to-end process
view. section 6demonstrates that, from the three resulting
populated meta models, it is possible to standardize the pro-cess mining analysis and perform it automatically, adapting
only a few parameters speciﬁc to each dataset. in addition
to that, an example of the corresponding resulting populatedmeta model for each one of the environments is shown.
5.1 meta model completion scenarios
experience teaches us that it is rare to ﬁnd an environmentthat explicitly provides the information to ﬁll every sector of
our meta model. this means that additional steps need to be
taken to evolve from an incomplete meta model to a complete
one. to do so, fig. 7presents several scenarios in which, start-
ing from a certain input (gray sectors), it is possible to inferthe content of other elements (dashed sectors). depending
on the starting point that we face, we must start inferring the
fig. 6 meta model completion in the three evaluated environments
123e. g. l. de murillas et al.
(a) (b) (c) (d) (e) (f)
fig. 7 input scenarios to complete meta model sectors
missing elements consecutively in the right order, which will
lead us, in the end, to a completely populated meta model:
aschema discovery : one of the most basic elements we
require in our meta model to be able to infer other ele-ments is the events sector . starting from this input and
applying schema, primary key and foreign key discovery
techniques [ 17,25], it is possible to obtain a data model
describing the structure of the original data.
bobject identiﬁcation : if the events and a data model are
known, we can infer the objects that these events repre-
sent. to do so, it is necessary to know the attributes ofeach class that identify the objects (primary keys). find-
ing the unique values for such attributes in the events
corresponding to each class results in the list of uniqueobjects of the populated meta model.
ccase derivation : another possible scenario is the one
in which we derive cases from the combination of eventsand a data model. the event splitting technique describedin [5], which uses the transitive relations between events
deﬁned by the data model, allows generating different
sets of cases, or event logs. this requires, similarly toscenario b, to match the attributes of each event to the
attributes of each class. then, we must use the foreign
key relations to correlate events between them. selectingthe desired combination of relationships between events
will tell us how to group these events in cases to form a
log.
dversion inference : the events of each object can be
processed to infer the object versions as results of the
execution of each event. to do so, events must contain
the values of the attributes of the object they relate to at acertain point in time or, at least, the values of the attributes
that were affected (modiﬁed) by the event. then, order-
ing the events by (descending) timestamp and applyingthem to the last known value of each attribute allow us to
reconstruct the versions of each object.
eevent inference : the inverse of scenario dis the one in
which events are inferred from object versions. lookingat the attributes that differ between consecutive versions
it is possible to create the corresponding event for the
modiﬁcation.
fprocess discovery : finally, a set of cases is required to
discover a process model using any of the multiple miners
available in the process mining ﬁeld.
the following three sections consider real-life environ-
ments that can act as a source for event data. these envi-ronments are used to illustrate the scenarios in fig. 7and
demonstrate that the complete meta model structure can be
derived for each of them. in each scenario, the goal is to cre-
ate an integrated view of data and process, even when eventlogs are not directly available.
5.2 database redo logs
the ﬁrst environment focuses on database redo logs , a mech-
anism present in many dbmss to guarantee consistency, aswell as providing additional features such as rollback and
point-in-time recovery. redo logs have already been consid-
ered in our earlier work [ 5,21] as a source of event data for
process mining. this environment corresponds to the sce-
nario depicted in fig. 7d, where data model, objects and
events are available, but object versions need to be inferred.table 1shows an example fragment of a redo log obtained
from an oracle dbms. after its processing, explained in [ 5],
these records are transformed into events. table 2shows the
derived values for attributes of these events according to thechanges observed in the redo log. for instance, we see that
the ﬁrst row of table 2corresponds to the processed redo log
record observed in the ﬁrst row in table 1. it corresponds to a
customer insertion; therefore, all the values for each attribute
were inexistent before the event was executed (4th column).
the second column holds the values for each attribute rightafter the event occurred. the rest of the events are interpreted
in the same way.
figure 6shows a general overview of how the meta model
sectors are completed according to the starting input data and
123connecting databases with process mining: a meta model and toolset
table 1 fragment of a redo log:
each line corresponds to the
occurrence of an event# time + op + table redo undo
1 2016-11-27 15:57:08.0 +
insert + customerinsert into
“sampledb”.“customer” (“id”,“name”,“address”,“birth_date”)values (‘17299’,‘name1’,‘address1’,to_date(
‘01-aug-06’,
‘dd-mon-rr’));delete from
“sampledb”.“customer” where“id” = ‘17299’ and“name” = ‘name1’and “address” =‘address1’ and“birth_date” =to_date(‘01-aug-06’,
‘dd-mon-rr’) and
rowid = ‘1’;
2 2016-11-27 16:07:02.0 +
update + customerupdate “sampledb”.
“customer” set“name” = ‘name2’where “name” =‘name1’ and rowid= ‘1’;update “sampledb”.
“customer” set“name” = ‘name1’where “name” =‘name2’ and rowid= ‘1’;
3 2016-11-27 16:07:16.0 +
insert + bookinginsert into
“sampledb”.“booking” (“id”,“customer_id”)values (‘36846’,‘17299’);delete from
“sampledb”.“booking” where“id” = ‘36846’ and“customer_id” =‘17299’ and rowid= ‘2’;
4 2016-11-27 16:07:16.0 +
update + ticketupdate “sampledb”.
“ticket” set“booking_id” =‘36846’ where“booking_id” isnull and rowid =‘3’;update “sampledb”.
“ticket” set“booking_id” =null where“booking_id” =‘36846’ and rowid= ‘3’;
5 2016-11-27 16:07:17.0 +
insert + bookinginsert into
“sampledb”.“booking” (“id”,“customer_id”)values (‘36876’,‘17299’);delete from
“sampledb”.“booking” where“id” = ‘36876’ and“customer_id” =‘17299’ and rowid= ‘4’;
6 2016-11-27 16:07:17.0 +
ticket + updateupdate “sampledb”.
“ticket” set “id”= ‘36876’ where“booking_id” isnull and rowid =‘5’;update “sampledb”.
“ticket” set “id”= null where“booking_id” =‘36876’ and rowid= ‘5’;
the steps taken to derive the missing sectors. in this case, the
analysis of database redo logs allows one to obtain a set of
events, together with the objects they belong to and the datamodel of the database. these elements alone are not sufﬁcientto do process mining without the existence of an event log
(cases). in addition, the versions of the objects of the database
need to be inferred from the events as well. therefore, thestarting point is a meta model in which the only populated
sectors are events ,objects anddata model . we need to infer
the remaining ones: versions ,cases andprocess models .fortunately, a technique to build logs using different
perspectives (trace id patterns) is presented in [ 5]. the
existence or deﬁnition of a data model is required for thistechnique to work. figure 6shows a diagram of the data
transformation performed by the technique, and how it ﬁts in
the proposed meta model structure. a more formal descrip-
tion of the mapping between redo logs and our meta modelcan be consulted in “appendix b.2”. the data model is auto-
matically extracted from the database schema and is the one
included in the meta model. this data model, together withthe extracted events, allows us to generate both cases (c)
123e. g. l. de murillas et al.
table 2 fragment of a redo log:
each row corresponds to the
occurrence of an event# attribute name value after event value before event
1 customer:id 17299 –
customer:name name1 –customer:address address1 –customer:birth_date 01-aug-06 –rowid = 1
2 customer:id = {17299}
customer:name name2 name1customer:address = {address1}
customer:birth_date = {01-aug-06}
rowid = 1
3 booking:id 36846 –
booking:customer_id 17299 –rowid = 2
4 ticket:booking_id 36846 null
ticket:id = (317132)
ticket:belongs_to = (172935)
ticket:for_concert = (1277)
rowid = 3
5 booking:id 36876 –
booking:customer_id 17299 –rowid = 4
6 ticket:booking_id 36876 null
ticket:id = (317435)
ticket:belongs_to = (173238)
ticket:for_concert = (1277)
rowid = 5
and object versions (d). then, process discovery completes
the meta model with a process (f). once the meta modelstructure is populated with data, we can make queries on it
taking advantage of the established connections between all
the entities and apply process mining to do the analysis.
5.2.1 database redo logs: transformation
in the previous section, we have described the nature of redo
logs and how they can be extracted and transformed intoevent collections ﬁrst and used to populate the meta modelafterward. the goal of this section is to sketch the content of
this resulting populated meta model and, in further sections,
the kind of analysis that can be done on it.
table 3represents quantitatively the input and output data
for our meta model population process for each of the enti-
ties introduced in sect. 3. these entities match the ones
depicted in fig. 4. moreover, this table provides qualita-
tive properties on the nature of the transformation for each
entity. in this case, we observe that all the data are obtainedfrom the original database through sql (except for the redologs, that require some speciﬁc-to-the-job tools from oracle).
the input entities ( data model ,class ,attribute ,relation-
ship andevent ) are extracted automatically (5th column),
directly transformed given that they are explicitly deﬁned in
the source data (6th column), and maintaining their quantity(3th vs. 4th columns). however, the remaining entities of our
meta model need to be inferred, i.e., derived from the original
entities. in this case, this derivation can be done automati-cally by our tool (padas). the data model that describes thisdataset is depicted in fig. 8and clearly inspired the running
example provided in sect. 2.
5.2.2 database redo logs: adapter implementation
in order to assist in the task of extracting and processing redo
logs, the tool padas
9has been developed. this tool is able
to connect to an existing oracle rdbms, load a collection of
redo logs, extract their content and ﬁll the missing data froma database snapshot, obtain the data model automatically andexport the resulting collection of events to an intermediate
9https://www.win.tue.nl/~egonzale/projects/padas/ .
123connecting databases with process mining: a meta model and toolset
table 3 redo log dataset
transformation to populate the
openslex meta modelentity format # input els. # output els. aut/manual derivation
data model sql 1 1 aut explicit
class sql 8 8 aut explicitattribute sql 27 27 aut explicitrelationship sql 8 8 aut explicitobject – 0 6740 aut inferredversion – 0 8424 aut inferredrelation – 0 13384 aut inferredevent redo log 8512 8512 aut explicitactivity instance – 0 8512 aut inferredcase – 0 21 aut inferredlog – 0 1 aut inferredactivity – 0 14 aut inferredprocess – 0 1 aut inferred
fig. 8 data model of the redo
log dataset as obtained from thepopulated meta model
format. then, the rest of the processing can be done ofﬂine
from the oracle database.
in addition to this, the tool assists in selecting case notions
from the data model and building logs for a speciﬁc view. as
can be observed in fig. 9, once the three main sectors have
been extracted, i.e., data model, events and cases, the metamodel population can begin. this is an automatic process
that does not require any further user interaction.
5.3 in-table versioning
it is not always possible to get redo logs from databases.
sometimes they are disabled or not supported by the dbms.also, we simply may not be able to obtain credentials to
access them. whatever the reason, it is common to face the
situation in which events are not explicitly stored. this seri-ously limits the analysis that can be performed on the data.the challenge in this environment is to obtain, somehow, an
event log to complete our data.
it can be the case that in a certain environment, despite
lacking events, versioning of objects is kept in the database,
i.e., it is possible to retrieve the old value for any attribute
of an object at a certain point in time. this is achieved bymeans of duplication of the modiﬁed versions of rows. this
environment corresponds to the scenario depicted in fig. 7e,
where data model, objects and object versions are available,but events need to be inferred. the table at the bottom left
corner of fig. 10shows an example of in-table versioning of
objects. we see that the primary key of customer is formed
by the ﬁelds idandload_timestamp . each row represents a
version of an object, and every new reference to the same
idat a later load_timestamp represents an update. there-
fore, if we order rows (ascending) by idandload_timestamp ,
we get sets of versions for each object. the ﬁrst one (with
older load_timestamp ) represents an insertion, and the rest
123e. g. l. de murillas et al.
fig. 9 padas tool settings to convert a redo log dataset into a populated meta model
are updates on the values. a more formal description of the
mapping from in-table versioning environments to our metamodel can be found in appendix b.3.
5.3.1 in-table versioning: transformation
looking at fig. 10, it is clear that, ordering by timestamp
the versions in the original set (bottom left), we can recon-struct the different states of the database (right). each new
row in the original table represents a change in the state of
the database. performing this process for all the tables allowsinferring the events in a setting where they were not explic-
itly stored. figure 6shows that, thanks to the meta model
proposed, it is possible to derive events starting from a datamodel, a set of objects, and their versions as input (fig. 7e).
the next step is to obtain cases from the events and data
model applying the technique from [ 5] to split event col-
lections into cases selecting an appropriate trace id pattern(scenario c). finally, process discovery will allow us to obtain
a process model to complete the meta model structure (sce-nario f). a more formal description of the mapping between
in-table versioning sources and our meta model can be found
in appendix b.3.
as a result of the whole procedure, we have a meta model
completely ﬁlled with data (original and derived) that enables
any kind of analysis available nowadays in the process ofmining ﬁeld. moreover, it allows for extended analysis com-bining the data and process perspectives. table 4shows the
input/output of the transformation process, together with
details of its automation and derivation. we see that, inthis environment, the input data correspond to the follow-
ing entities of the meta model: data model ,class ,attribute
andversion . these elements are automatically transformed
from the input data, where they are explicitly deﬁned. how-
ever, the rest of elements need to be either manually obtained
from domain knowledge (relationships between classes), or
123connecting databases with process mining: a meta model and toolset
fig. 10 example of in-table versioning and its transformation into objects and versions
table 4 in-table versioning
dataset transformation topopulate the openslex metamodelentity format # input els. # output els. aut/manual derivation
data model csv 1 1 aut explicit
class csv 8 8 aut explicitattribute csv 65 65 aut explicit
relationship – 0 15 manual dom know
object – 0 162287 aut inferredversion csv 277107 277094 aut explicitrelation – 0 274359 aut inferredevent – 0 277094 aut inferredactivity instance – 0 267236 aut inferredcase – 0 9858 aut inferredlog – 0 1 aut inferredactivity – 0 62 aut inferredprocess – 0 1 aut inferred
automatically inferred from the rest of elements. this is so
because, given the conﬁguration of the database, there was
no way to query the relationships between tables, as they
were not explicitly deﬁned in the dbms but managed at theapplication level. therefore, these relationships needed to bespeciﬁed by hand after interviewing domain experts on the
process at hand.
the resulting data model is depicted in fig. 11. it is based
on the database of a ﬁnancial organization, focusing on the
claim and service processing for a speciﬁc ﬁnancial product.
as can be observed, this dataset links information corre-sponding to customers ( customer (3) ), their products
(product (8) andservice_provition (6) ), incoming and outgo-
ing phone calls ( call_in (2) andcall_out (7) ), letters being
sent to customers ( letter_out (4) ), monthly statements fromcustomers ( monthly_statement (1) ) and customer
details change forms ( user_details_change_form_in (5) ).
in further sections, we will show how to obtain a view on this
dataset, exploiting the structure of the meta model.
5.3.2 in-table versioning: adapter implementation
the task of transforming the data from an environment that
presents an in-table versioning structure into our meta model
can be automated by means of an adapter. in this case, we
make use of the rapidminer
10platform together with the
rapidprom11extension to implement a workﬂow that auto-
10http://www.rapidminer.com .
11http://www.rapidprom.org/ .
123e. g. l. de murillas et al.
fig. 11 data model of the in-table versioning dataset as obtained from the populated meta model
mates this transformation. speciﬁcally, the operator needed
for this task can be found in one of the development branches
of the rapidprom repository12. this adapter, as depicted in
fig. 12, takes as input the set of csv ﬁles that contain the
unprocessed dump of the tables to be analyzed directly from
the source database. also, if we get direct access to such
database, the data can be queried directly, skipping the trans-formation to csv ﬁles.
5.4 sap-style change table
the last environment we will consider for our feasibility
study is related to widespread erp systems such as sap.
these systems provide a huge amount of functionalities tocompanies by means of conﬁgurable modules. they can run
on various platforms and rely on databases to store all their
information. however, in order to make them as ﬂexibleas possible, the implementation tries to be independent ofthe speciﬁc storage technology running underneath. we can
observe sap systems running on ms sql, oracle or other
technologies, but they generally do not make intensive useof the features that the database vendor provides. therefore,
data relations are often not deﬁned in the database schema,
but managed at the application level. this makes the life of theanalyst who would be interested in obtaining event logs rather
12https://github.com/rapidprom/rapidprom-source/tree/egonzalez .complicated. fortunately, sap implements its own redo-log-
like mechanism to store changes in data, and it represents
a valid source of data for our purposes. in this setting, we
lack event logs, object versions, a complete data model andprocesses. without some of these elements, performing any
kind of process mining analysis becomes very complicated.
for instance, the lack of an event log does not allow for thediscovery of a process, and without it, performance or confor-mance analyses are not possible. to overcome this problem,
we need to infer the lacking elements from the available infor-
mation in the sap database.
first, it must be noted that, despite the absence of an
explicitly deﬁned data model, sap uses a consistent nam-
ing system for their tables and columns, and there are lotsof documentation available that describe the data model of
the whole sap table landscape. therefore, this environment
corresponds to the scenario depicted in fig. 7d, where data
model, objects and events are available, but object versionsneed to be inferred. to extract the events, we need to pro-
cess the change log. this sap-style change log, as can be
observed in fig. 13, is based on two change tables: cdhdr
andcdpos . the ﬁrst table ( cdhdr ) stores one entry per
change performed on the data with a unique change id
(changenr ,objectclas ,objectid ) and other addi-
tional details. the second table ( cdpos ) stores one entry per
ﬁeld changed. several ﬁelds in a data object can be changed
at the same time and will share the same change id. for
123connecting databases with process mining: a meta model and toolset
fig. 12 rapidminer workﬂow to convert an in-table versioning dataset in order to populate the openslex meta model
cdhdr 
objectclas string 
changenr integer 
objectid integer 
username string 
udate date 
utime time 
tcode integer customer 
(pk) id integer 
name string 
address string 
birth_date date objectclas changenr objectid username udate utime tcode 
cust 0001 0000001 user1 2014-11-27 15:57:08.0 0001 
cust 0002 0000001 user2 2014-11-27 16:07:02.0 0002 
cust 0003 0000002 user2 2014-11-27 17:48:09.0 0003 
cust 0004 0000002 user1 2014-11-27  19:06:12.0 0004 
... ... ... ... ... ... ... 
objectclas changenr tabname tabkey fname value_new value_old 
cust 0001 customer 17299 name name1 
cust 0001 customer 17299 address address1 
cust 0001 customer 17299 birth_date 01-aug-06 cust 0002 customer 17299 name name2 name1 cust 0003 customer 17300 name name3 
cust 0003 customer 17300 address address2 
cust 0003 customer 17300 birth_date 14-jun-04 
cust 0004 customer 17300 address address3 address2 
... ... ... ... ... ... ... cdpos 
objectclas string 
changenr integer 
tabname string 
tabkey string 
fname string 
value_new string 
value_old string 
fig. 13 example of sap change tables cdhdr and cdpos
each ﬁeld changed, the table name is recorded ( tabname )
together with the ﬁeld name ( fname ), the key of the row
affected by the change ( tabkey ) and the old and new values
of the ﬁeld ( value_old ,value_new ). a more formal
description of the mapping between sap change tables and
our meta model can be found in “appendix b.4”. this struc-
ture is very similar to the one used for redo logs. however,one of the differences is that changes on different ﬁelds of the
same record are stored in different rows of the cdpos table,
while in the redo logs they are grouped in a single operation.5.4.1 sap-style change table: transformation
as can be seen in fig. 6, after processing the change log and
providing a sap data model, we are in a situation in whichthe events, objects and data model are known. therefore, we
can infer the versions of each object (d) split the events in
cases (c) and ﬁnally discover a process model (f). with allthese ingredients, it becomes possible to perform any process
mining analysis and answer complex questions combining
process and data perspectives.
123e. g. l. de murillas et al.
table 5 sap dataset
transformation to populate the
openslex meta modelentity format # input els. # output els. aut/manual derivation
data model sql 1 1 aut explicit
class sql 87 87 aut explicitattribute sql 4305 4305 aut explicitrelationship – 0 296 aut dom knowobject sql 7340011 7339985 aut explicitversion – 0 7340650 aut inferredrelation – 0 7086 aut inferredevent sql 26106 26106 aut explicitactivity instance – 0 5577 aut inferredcase – 0 22 aut inferredlog – 0 1 aut inferredactivity – 0 172 aut inferredprocess – 0 1 aut inferred
table 5shows that, in order to populate our meta model,
what we obtain from sap are the following entities: data
model ,class ,attribute ,object andevent . all these elements
are explicitly deﬁned and can be automatically transformed.
however, the rest need further processing to be inferred from
the input data. only the relationships cannot be inferred, butcan be obtained automatically from domain knowledge. thedifference here between in-table versioning and sap is that,
despite the need for domain knowledge to obtain the relation-
ships in both scenarios, in the case of sap we can query theonline documentation that speciﬁes the connections between
different tables of their data model. to do so, a script
13has
been developed which, from a set of table names, ﬁnds andobtains the relationships and stores then in a csv ﬁle.
to get an idea of the complexity of the data model of this
dataset, fig. 14shows a full picture of it. as can be noticed, it
is a very complex structure. however, the tool allows one tozoom in and explore the interesting areas. figure 15shows a
zoomed area of the data model, where the ekpo table points
to the ekko table.
5.4.2 sap-style change table: adapter implementation
as has been mentioned before, sap systems often use rela-
tional databases to store the documents and information they
manage. despite the wide variety of database systems used,the extraction of the relevant information is always possible,
as long as a jdbc driver exists. this allows one to connect
to the source database directly from rapidminer, as shown inthe workﬂow in fig. 16. in this speciﬁc case, the database was
sap adaptive server enterprise (ase), originally known as
sybase sql server. the process is as simple as providingthe list of table names we are interested in. the workﬂow
13https://www.win.tue.nl/~egonzale/createkeysﬁle-sh/ .will loop through them (fig. 17) and extract their content
into csv ﬁles that will be processed in a further step by a
different workﬂow.
once we have the csv ﬁles of the sap tables, we need to
process them. to do so, the rapidprom workﬂow in fig. 18
has been developed, which loops through these ﬁles (includ-ing the sap change tables) to build our meta model.
5.5 distributed system merge
the three previous environments have been considered in iso-lation, showing the etl process on individual systems. on
the other hand, as depicted in fig. 1, it is not rare that com-
panies’ it landscape is constructed by several systems suchas erps, crms, bpm workﬂow managers, and so on, whichoperate separately. these systems store data related to differ-
ent aspects of the functioning of the company. it is possible
that, in order to perform our analysis, we need to make useof data from several of these sources combined, e.g., to link
user requests made through a website with internal request
handling managed by an internal sap system. in that case,we need to merge the data coming from these independent
systems into a single structure. in order to achieve this goal,
we require, at least, one common connecting concept to beshared by these systems.
figure 19shows an example of two systems being merged
in a single data model. on the left side of the ﬁgure, the data
model described in sect. 5.3is shown as an abstract group
of tables from which one of them, the customer table, has
been selected. on the right side, the ticket selling platform
described in sect. 5.2is represented, from which the cus-
tomer table has been selected as well. both customer tables
(customer_a andcustomer_b ) have a customer iden-
tiﬁcation ﬁeld. both could share the same ids to represent thesame concept. however, that is not needed to be correlated.
123connecting databases with process mining: a meta model and toolset
adr2t005
tcurcadrcadrct adrt cskt dd07t
bkpft001
t003t880
bsegekkoekpo
lfa1mara
t001kt001w
t042z
tbslt024et161ebaneinat001l t023 marm
csks
usr02lfbk lfc1
t024dt024
t161s
eine
ekabekbet156
ekes
eket
ekknlfb1makt mbew
lfm1mlan
marct003t
mkpft005t
mseg
rbco
rbkp
reguh
reguprsegt006a t007s t008t t023t t023ut t052u t077y t134t t156t t158w t161tt161u t163c t163f t163i t163m t163y t16ft t460t t681b t685t tbdlsttbslt tcurf tcurr tcurx tinct tka02 tstct tvzbt
fig. 14 general view of the data model of the sap dataset as obtained from the populated meta model. due to the size of the data model, the
attributes or the tables have been omitted from this graph
the merging process requires the creation of an additional
linking table ( customer_link ) which holds the relation
between customer ids for both separate data models.
to make the combination of these data models ﬁt the struc-
ture of our meta model, it is necessary to make the connection
between objects at the object version level. this can be doneautomatically as long as we know (a) which customer id from
table a is related to which customer id from table b, and (b)during which period of time did this relation exist. it can
be that the correlation is permanent, for example, when wemap customer ids of two systems which always represent thesame entity (e.g., social security numbers on one system with
national identiﬁcation numbers on the other). in such a case,
the connecting object will always relate to versions of thesame two connected objects. this situation is represented in
fig. 20, where an object of class a (top line) is connected
123e. g. l. de murillas et al.
adr2t005
tcurcadrcadrct adrt cskt dd07t
bkpft001
t003t880
bsegekkoekpo
lfa1mara
t001kt001w
t042z
tbslt024et161ebaneinat001l t023 marm
csks
usr02lfbk lfc1
t024dt024
t161s
eine
ekabekbet156
ekes
eket
ekknlfb1makt mbew
lfm1mlan
marct003t
mkpft005t
mseg
rbco
rbkp
reguh
reguprsegt006a t007s t008t t023t t023ut t052u t077y t134t t156t t158w t161tt161u t163c t163f t163i t163m t163y t16ft t460t t681b t685t tbdlsttbslt tcurf tcurr tcurx tinct tka02 tstct tvzbt
ekkoekpoeban
eine
eket
rseg
fig. 15 detail of the data model of the sap dataset as obtained from the populated meta model
process
read csv
fil outloop examples
exa exa
out
outinpres
res
fig. 16 rapidminer workﬂow to connect to a sap database and extract
the content of all the tables
loop examples
extract macro
exa exaread database
outannotate
inp outexa
fig. 17 rapidminer subprocess to extract each table when connecting
to a sap database
to an object of class b (bottom line) by means of a linking
object (middle line). for each pair of coexisting object ver-sions of obja andobjb , a new version of objl must exist
to relate them. this method has the beneﬁt of providing a
great ﬂexibility, being also possible to hold relations betweenobjects that change through time. an example of this is the
case in which the connection between two tables represents
employer-to-employee relations. an employer can be relatedto many employees, and employees can change employers
at any time. therefore, the mapping between employer and
employee objects will not be permanent through time, butwill evolve and change and only exist during a speciﬁc period.such a case is supported by the proposed mapping method,
as presented in fig. 21, by means of new object versions for
the linking object.
as demonstrated with this example, the proposed meta
model is able to merge information from different systemsinto a single structure, enabling the analysis of process and
data in a holistic way, beyond the boundaries of it sys-
tems infrastructure. throughout all this section, we haveseen how data extraction and transformation into the pro-
posed meta model (fig. 22) can be performed when dealing
with environments of very different nature. it is importantto note that, despite its apparent complexity, all the stepspreviously mentioned are carried out automatically by the
provided implementations of the adapters. these adapters
can be modiﬁed and extended to add support for new envi-ronments.
6 analysis of the resulting populated meta
model
the main advantage of transforming all our source informa-
tion into the proposed meta model structure is that, regardlessof the origin of data, we can pose questions in a standard way.
let us consider the three environments described in previous
sections: redo logs, in-table versions and sap systems. inthe examples we chose to illustrate the transformation, it is
evident that they belong to very different processes and busi-
nesses. the redo log example corresponds to a concert ticketselling portal. the second example, based on in-table ver-
123connecting databases with process mining: a meta model and toolset
fig. 18 rapidminer workﬂow to populate the meta model based on sap dataset
sioning, corresponds to the claim and service processing for
a ﬁnancial product within an organization. the third example,
based on sap system, represents the procurement process of
a ﬁctitious company. due to the diversity of data formats anddesigns of the systems, in a normal situation these three envi-
ronments would require very different approaches in order to
be queried and analyzed. however, we claim that our metamodel provides the standardization layer required to tackle
these systems in a similar manner.
6.1 standardized querying
our goal is to query the data from different sources in a stan-
dardized fashion. to do so, as demonstrated in sect. 5,w e
extracted the raw input data from the databases of each sys-
tem. then, our automated workﬂows were used to transform
these data and obtaining, as a result, a structure compliantwith the proposed meta model. in this way, we hope to make
plausible that, given a speciﬁc business question, it is possi-
ble to make a sql query that answers this question on anypopulated meta model, regardless of the process it representsor the original format of the data. we demonstrate our claim
with the following example. let’s assume we want to ﬁnd
the group of cases in our logs that comply with a speciﬁcrule, or represent a part of the behavior we want to analyze.
as an example, for the redo log dataset we could deﬁne the
following business question (bq1):
bq1: in which cases, the address of a customer was
updated?fig. 19 link between different data models through tables representing
a common concept ( customer_a andcustomer_b ) by means of
al i n kt a b l e( customer_link )
to do this in the original data would require to go through
the redo log, looking for events on the table customer , link
them to the speciﬁc customer_id , correlate the other events
of the same customer, group them in traces and build the log.
this is not an easy task, especially if such a complicated query
needs to be built speciﬁcally for each new business questionthat comes to mind. let us consider another example, now
for the in-table versioning dataset (bq2):
bq2: in which cases, a service request was resolved?
to answer this, we need to go through the table prod-
uctand check in which row the value of the ﬁeld decision
changed respect to the previous one, having the same cus-
tomer_id in common. then, correlate events from any other
table through the same customer_id and build the log again.
this requires a sequence of non-trivial and time-consuming
123e. g. l. de murillas et al.
fig. 20 merging method to map versions of objects belonging to classes of different data models (class a and class b) through a linking class
(class link)
fig. 21 merging method to map versions of multiple objects belonging to classes of different data models (class a and class b) through a linking
class (class link). in this case, the related objects change through time
customer 
(pk) id integer
name string 
address string 
birth_date date id start_ 
/g415mestamp end_ 
/g415mestamp name address birth_date object_id 
17299 2014-11-27 
15:57:08.0 2014-11-27 
16:07:02.0 name1 address1 01-aug-06 1 
17299 2014-11-27 
16:07:02.0 name2 address1 01-aug-06 1 
17300 2014-11-27 
17:48:09.0 2014-11-27 
19:06:12.0 name3 address2 14-jun-04 2 
17300 2014-11-27 
19:06:12.0 name3 address3 14-jun-04 2 id class_id 
1 1 
2 1 
id /g415mestamp lifecycle resource ac/g415vity_instance_id
1 2014-11-27 
15:57:08.0 name1 address1 1 
2 2014-11-27 
16:07:02.0 name2 address1 2 
3 2014-11-27 
17:48:09.0 name3 address2 3 
4 2014-11-27 
19:06:12.0 name3 address3 4 id ac/g415vi/g415y_id 
1 1 
2 2 
3 1 
4 3 id case_name 
1 casea 
2 caseb 
id name pid
1 customer-crea/g415on 1 
2 customer-update-name 1 
3 customer-update-address 1 id process_name 
1 process 1 versions 
events cases 
ac/g415vity instances 
ac/g415vi/g415es objects 
data model 
processes 
fig. 22 fragment of resulting populated meta model
123connecting databases with process mining: a meta model and toolset
steps. as a ﬁnal example, we could think of the following
question to ask for the sap dataset (bq3):
bq3: in which cases, the quantity of a purchase requi-
sition order was modiﬁed?
this time we need to go to the ekpo14table, which
contains information about purchasing document items.
however, unlike the in-table versioning case, the previousvalues of the items are not in the original table anymore. to
ﬁnd historical values for each ﬁeld, we need to go through
the cdhdr and cdpos tables to ﬁnd a row that corre-sponds to the modiﬁcation of the ﬁeld menge (purchase
order quantity). to do so, we must look at the value of the
ﬁeld fname , that should be equal to the string “menge” ,
and the value of the ﬁeld tabname , that should be equal
to the string “ekpo” . then, match it with the correct pur-
chase order (sharing the same tabkey ) and correlate it to
any other event (from the cdhdr andcdpos tables) that
affected the same purchase orders and group them in cases
to form a log. doing so we can see the changes in context.
we see that, as the complexity of data increases, the processto query it increases as well.
therefore, to summarize, we want to ask three different
questions in three different environments, and each questionneeds to be answered in a speciﬁc way. however, we claimthat we can derive a standard way to answer questions of
the same nature on different datasets, as long as the data
have been previously normalized into our meta model. if welook at the three proposed business questions (bq1, bq2 and
bq3), all of them have something in common: they rely on
identifying cases in which a value was modiﬁed for a speciﬁcﬁeld. to complicate things a bit more, we can even ﬁlter the
cases to obtain only the ones that cover a speciﬁc period of
time. this means that we can generalize these questions intoone common general question (gq):
gq: in which cases (a) there was an event that hap-
pened between time t1 and t2, (b) that performed a
modiﬁcation in a version of class c (c) in which thevalue of ﬁeld f changed from x to y?
this is a question we can answer on the basis of our meta
model, regardless of the dataset it represents. all we need to
do is to specify the values of each parameter (t1, t2, c, xand y) according to the data model at hand. we translatedthe general question gq into the sql query in listing 2.
listing 2 standard query executed on the three populated meta models
select c.id as"t:concept:name",
e.timestamp as "e:time:timestamp",
ac.name as"e:concept:name",
e.resource as"e:org:resource",
e.lifecycle as"e:lifecycle:transition",
14http://www.sapdatasheet.org/abap/tabl/ekpo.html .e.ordering
from
event ase,
"case" asc,
activity_instance asai,
activity_instance_to_case asaitc,
activity asac
where
c.id = aitc.case_id and
aitc.activity_instance_id = ai.id and
e.activity_instance_id = ai.id and
ai.activity_id = ac.id and
c.id in
(
select c.id
from
"case" asc,
class ascl,
object aso,
object_version asov,
object_version asovp,
event ase,
activity_instance asai,
activity_instance_to_case asaitc,
event_to_object_version asetov,
attribute_name as at ,
attribute_value asav ,
attribute_value asav p
where
e.activity_instance_id = ai.id and
aitc.activity_instance_id = ai.id and
aitc.case_id = c.id and
etov.event_id = e.id and
etov.object_version_id = ov.id and
ov.object_id = o.id and
o.class_id = cl.id and
cl.name = "%{class_name}" and
e.timestamp > "%{ts_lower_bound}" and
e.timestamp < "%{ts_upper_bound}" and
at.name = "%{attribute}" and
a v.attribute_name_id = at.idand
a v.object_version_id = ov.id and
av .value like "%{new_v alue}" and
a vp.attribute_name_id = at.idand
a vp.object_version_id = ovp.id and
av p . value like "%{old_v alue}" and
ovp.id in
(
select ovp.id
from object_version asovp
where
ovp.start_timestamp < ov.start_timestamp and
ovp.object_id = ov.object_id
order by ovp.start_timestamp desc limit 1
)
)
order by c.id, e.ordering;
this query is standard and independent of the dataset
thanks to the use of macros (rapidminer macros) denoted
by %{macro _name }. we just need to instantiate their val-
ues according to the dataset to process. table 6shows the
values for each macro to be replaced in each case. notice
that the timestamps ( ts_lower_bound andts_upper_
bound ) are expressed in milliseconds, and the string % in
new_value andold_value will match any value.
this query can be easily executed in the rapidminer
environment, using the query database operator. figure 23
shows the workﬂow executed to set the macro-values and
make the query for each dataset. some details of the logs
obtained for each of the three datasets can be observed intable 7.
123e. g. l. de murillas et al.
table 6 parameters to query the
three different populated meta
models with the same queryvariable value-rl value-itv value-sap
class_name customer product ekpo
ts_lower_bound 527292000000 527292000000 527292000000ts_upper_bound 1480531444303 1480531444303 1480531444303attribute address decision mengenew_v alue % toekenning %old_v alue % nog geen beslissing %
fig. 23 analysis workﬂow to
process the three resultingpopulated meta models
process
sap query params
thr thrsap mm
outpm analysis 3
in 
in out
out
out
out
outrl query params
thr thrredo log mm
outpm analysis 1
in 
in out
out
outout
out
int query params
thr thrin−table mm
outpm analysis 2
in 
in out
out
out
out
outinpres
res
res
res
res
res
res
res
res
res
res
res
res
table 7 query results for the three different populated meta models
properties redo log in-table ver. sap
# cases 17 3177 1
# events 301 69410 12# event classes 5 50 11
it is important to notice that this technique does not
exclude the classical way to analyze logs. on the contrary, it
enables the use of all the existing process mining techniques.a proof of it is the workﬂow in fig. 23, which executes the
subprocess in fig. 24for each of the resulting logs. as can be
observed, this process mining task transforms the log tableinto a xes log. then, a process tree is discovered using the
inductive miner, which transforms it into a petri net. then,
a performance analysis is done on the discovered model andthe log, together with a conformance checking. in additionto that, a more interactive view of the process is obtained
with the inductive visual miner. a social network (when theresource ﬁeld is available) can be discovered using the social
network miner. all these analyses are performed automati-
cally and with exactly the same parameters for each resultinglog. it is completely independent of the source, which meansthat, just by modifying the values of the macros of our query,
we can standardize the process of analyzing sublogs for value
modiﬁcation cases.
6.2 process mining results
to continue with the demonstration of viability of this tech-
nique, in this section we show the results of the automated
analysis performed on the three different datasets by meansof the same query and same workﬂow.
123connecting databases with process mining: a meta model and toolset
pm analysis 1
numerical to dat...
exa exa
oridata table to ev...
exa evemultiply (5)
inp out
outout
out
outout
inductive visual ...
eve modsocial network ...
eve modinductive miner (...
eve modprocess tree to ...
mod modmultiply (6)
inp out
out
out
conformance ch...
eve
modali
exa
exaexa
exaanalyze perform...
eve
modmodin 
in out
out
out
out
out
fig. 24 process mining analysis subprocess included in the analysis workﬂow of the three populated meta models. it illustrates that queries on the
populated meta models can be converted into event logs that can be analyzed by a range of process mining algorithms
fig. 25 discovered model and deviations for the redo log dataset
6.2.1 the redo log environment: ticket selling process
in the case of the ticket selling process, we are interested
in the traces that modiﬁed the address of a customer. first,the discovered model is shown in fig. 25. we see ﬁve main
activities in this model: customer insertions, booking inser-
tions, and three different kinds of customer updates. one
can notice that the activity customer +update +1411 ,
which corresponds to updates in the address ﬁeld, is the most
frequent of the three modiﬁcations, only behind the activ-
itybooking +insert +44activity. one can also see that,
as expected, a customer record can only be modiﬁed when
it has been inserted before. the dashed lines in the model
represent deviations from the discovered model, usually rep-resenting infrequent cases that were ﬁltered out during themining phase.
another technique that can be used during the execution
of the automated workﬂow is the performance analysis. todo so, the log is replayed on the discovered model, and time
differences are computed between consecutive events. then,
this information is displayed on top of the original model tovisualize bottlenecks and performance issues. in this case,
fig.26shows that the most time-consuming activity is one
of the customer updates. however, this information can be
misleading since these modiﬁcations were atomic, while dataabout start and complete events are missing. to have a bet-ter view on performance metrics and accurate task duration,
the life-cycle attribute should be properly assigned to pair
events in activity instances. this is done automatically bythe transformation adapter. however, in this case, there are
no activities to be paired.
to ﬁnalize the analysis of this process, fig. 27shows the
result of checking the conformance of the log respect to the
process. here we see that, framed in red, activities cus-
tomer+update+1411 and booking+insert+44
show move on log deviations. however, as the green bar atthe bottom and the number in parentheses show, these devia-
tions occur in a very small share of the total cases. the rest of
the activities are always executed synchronously accordingto the model.
123e. g. l. de murillas et al.
fig. 26 performance analysis of the model for the redo log dataset
fig. 27 conformance analysis of the model for the redo log dataset
6.2.2 the in-table versioning environment: claim
management process
the second environment corresponds to the claim manage-
ment process of a ﬁnancial organization. the result of our
query represents the cases in which the value of the deci-
sion ﬁeld of product service claim changed from undecided
(nog geen beslissing )t ogranted (toekenning ). figure 28
shows the model as obtained from inductive visual miner.
it can be observed that the process starts with a monthly
statement being initialized. then, for most of the following
steps three possibilities exist: the activity is executed, the
activity is re-executed (in a loop), or the activity is skipped.
the frequencies in the arcs demonstrate that, in most of thecases, each subsequent activity happens at least once. these
activities consist of the reception andchecking ofmonthlystatements ,incoming calls ,outgoing letters ,changes in
user details andoutgoing calls . finally, the claim is resolved
and three outcomes are possible: not decided ,accepted or
rejected .
when looking at the performance view in fig. 29,w e
notice that most of the places of the net are colored in red.
this means that the waiting time in this places is higher thanthe rest. it makes sense since, for some automatic activities,the waiting time between tasks will be very low, of the order
of milliseconds or seconds, and those places will set the lower
bound for the performance scale. on the other hand, for mostof the human-driven activities, the times will be much longer,
of the order of days. with respect to activities, we see some
variability, having some very costly activities, especially atthe beginning. these activities are mainly the ones related to
123connecting databases with process mining: a meta model and toolset
fig. 28 discovered model and deviations for the in-table versioning dataset
fig. 29 performance analysis of the model for the in-table versioning dataset
123e. g. l. de murillas et al.
fig. 30 conformance analysis of the model for the in-table versioning dataset
processing of monthly statements ,incoming calls andout-
going letters .
figure 30shows some conformance results. here we can
observe that most of the activities happen without signiﬁcant
deviations. however, some model moves are observed, butin a very low rate, mainly because of the infrequent skips
explained previously in fig. 28.
6.2.3 the sap environment: procurement process
finally, we focus on the procurement process of an sap
environment. we are particularly interested in the cases in
which the quantity of a purchase order was modiﬁed. afterquerying the populated meta model and obtaining the relevant
cases, we used inductive visual miner to get a model of the
process. figure 31shows a very structured process, which is
not strange given the few cases that fulﬁlled our criteria.it seems clear that some activities are executed repeat-
edly because of the loops. the process always starts withan update in the value of the effective price in purchasing
info record (infosatz_u_effpr ). this is followed by
updates in the purchasing document (einkbeleg object
class), speciﬁcally in the purchase order not yet complete
ﬁeld ( memory ). only then, the purchase order quantity
(menge ) ﬁeld is updated.
according to fig. 32, the performance seems evenly dis-
tributed, except for the update in purchase order quantities ,
which seems to take a shorter time than the rest. from theconformance point of view (fig. 33), we see that the pro-
cess does not show deviations. this was expected, given the
size of the log and the existence of only one case with this
particular behavior.
something interesting to note in this dataset with respect
to the other environments is the existence of resource infor-
mation. this means that we know who performed a change
123connecting databases with process mining: a meta model and toolset
fig. 31 discovered model and deviations for the sap dataset
fig. 32 performance analysis of the model for the sap dataset
fig. 33 conformance analysis of the model for the sap dataset
fig. 34 social network for the
sap dataset
in the documents in sap. this can be exploited to analyze the
data discovering the social network behind the interactions.
in this case, fig. 34represents the discovered social network,
showing the handout of work between resources ujbssen
andehani .
7 related work
this paper is based on our previous work [ 4] in which we
already proposed the same meta model. however, in this
paper, a special emphasis is put on the applicability of the
approach and the standardization of the analysis. for this,we provide a formal description of the mapping between
real-life system and our meta model. moreover, we provide
an implementation of all our techniques, making possibleto transform the data to populate our meta model, captur-ing the data and process perspectives of the system under
analysis. this makes possible to query it in a standard way,
obtaining the relevant cases for the business question at hand,in order to proceed with the analysis. finally, a demonstra-
tion of the analysis is made, covering all the phases of the
process, starting from the data extraction, and continuingwith data transformation, querying, process discovery, con-
formance and performance analysis.
despite the efforts made by the community, we can iden-
tify a few areas in which the problem of modeling, collectingand analyzing process execution data remains a challenge.
business process management and workﬂow manage-
ment are areas in which the community has focused on
providing models to describe their functioning and makepossible the analysis of their behavior. papers like [ 16,27]
provide meta models to give structure to audit trails on
workﬂows. however, they focus mainly on the workﬂow orprocess perspective.
process mining has different needs and the desire to store
event data in a uniﬁed way is obvious. in [ 19], the authors pro-
vide a meta model to deﬁne event logs, which would evolvelater in the ieee xes standard format [ 7]. this format rep-
resents a great achievement from the standardization point
of view and allows exchanging logs and developing miningtechniques assuming a common representation of the data.
however, the current deﬁnition of xes is not able to effec-
tively and efﬁciently store the data perspective of our metamodel. from our point of view, the xes format is, in fact,
a target format and not a source of information. we aim at,
from a richer source, generating different views on data inxes format to enable process mining.
artifact-centric approaches provide a different point of
view. these approaches give more relevance to the data
entities in business management systems. data artifacts areidentiﬁed within business process execution data, in order to
discover how changes and updates affect their life cycle [ 15].
techniques like the one proposed in [ 9] are able to combine
123e. g. l. de murillas et al.
the individual life cycles of different artifacts to show the
interrelation of the different steps. however, a limitation ofthese techniques is the difﬁculty to interpret correctly the
resulting models, in which clear execution semantics are not
always available. the application of other techniques likeconformance and performance analysis to the resulting mod-els has not been solved yet.
data warehousing is an area in which there is a great
interest to solve the problem of gathering business processinformation from different sources. work has been done on
this aspect [ 3,14,26,28,29], proposing ways to centralize the
storage of heterogeneous data, with the purpose of enablingprocess monitoring and workﬂow analysis. however, some
of these approaches, besides being process-oriented, do not
allow applying process mining analysis. this is due to thelack of event data, when measures of process performanceare stored instead. on the other hand, when they allow storing
more detailed information [ 13], force an ad hoc structure,
only relevant for the process at hand. the main distinctionbetween our approach and existing work on data warehousing
for process mining is the generality and standardization of
our meta model, being independent of the speciﬁc process,while combining both data and process perspectives.
process cubes [20] are techniques, closely related to
data warehousing, that aim at combining aspects of multi-dimensional databases (olap cubes) with event logs, inorder to enable the application of process mining techniques.
further work, with functional implementations, has been pre-
sented in [ 1,22–24]. these approaches allow one to slice,
dice, roll up and drill-down event data using predeﬁned cat-
egories and dimensions. it represents a great improvement
for the analysis of multi-dimensional data. however, thesetechniques still rely on event data as their only source of
information, and they require any additional data to be part
of each individual event. therefore, given that it is impossi-ble to efﬁciently represent in an event log all the aspects wecover in our meta model, process cubes as they are right now
do not represent an alternative for our purposes.
sap process observer is a component of the wide spec-
trum of sap utilities. it makes possible to monitor the
behavior of some sap business suite processes (e.g., order
to cash). this component monitors business object (bo)events and creates logs correlating them. the tool provides
insights such as deviation detection, real-time exception han-
dling, service level agreement tracking, etc. these event logscan be used as well to perform other kinds of analytics. one ofthe main advantages of this solution is the fact that it enables
real-time monitoring, without the need to deal with raw data.
however, it needs to be conﬁgured specifying the activitiesthat must be monitored. the kind of logs that it generates is
no different of the event logs provided by other tools or tech-
niques, in the sense that they use the same structure, havingevents grouped in sets of traces, and data only represented asplain attributes of events, traces and logs. also, it lacks gener-
icity, being only applicable in sap environments to monitorbusiness suite processes.
nowadays, several commercial process mining tools are
available on the market. these tools import either xes eventlogs (e.g., disco
15), or event tables (e.g., celonis16), support-
ing attributes at the event, trace and log levels. most of them
provide advanced ﬁltering features based on these attributes,
as is the case of disco. additionally, celonis has two interest-ing features. first, it implements olap capabilities, making
possible to perform some operations like slicing and dic-
ing event data. second, celonis provides its own language toexpress formulas. these formulas are used to compute statis-
tics and key performance indicators (kpis) and to express
complex conditions for olap operations. all these featuresare remarkable. however, they are restricted by the ﬂat struc-ture of their input event log format. it is not possible to keep
complex data relations in such a restricted representation.
these complex relations are needed to perform advanced dataquerying, like query gq in sect. 6.1, where relevant event
data must be selected based on the evolution of database
objects of a speciﬁc class.
the main ﬂaw of most of these approaches resides in the
way they force the representation of complex systems by
means of a ﬂat event log. the data perspective is missing,only allowing one to add attributes at the event, trace or loglevel. more recent works try to improve the situation, ana-
lyzing data dependencies [ 11] in business models with the
purpose of improving them, or even observing changes onobject states to improve their analysis [ 6]. however, none of
the existing approaches provides a generic and standard way
of gathering, classifying, storing and connecting process anddata perspectives on information systems, especially when
dealing with databases where the concept of structured pro-
cess can be fuzzy or nonexistent.
8 conclusion
in this paper, a meta model has been proposed that providesa way to capture a more descriptive image of the reality ofbusiness information systems. this meta model aligns the
data and process perspectives and enables the application
of existing process mining techniques. at the same time, itunleashes a new way to query data and historical informa-
tion. this is possible thanks to the combination of data and
process perspectives on the analysis of information systems.the applicability of the technique has been demonstrated
by means of the analysis of several real-life environments.
also, an implementation of the proposed solution has been
15https://ﬂuxicon.com/disco/ .
16https://www.celonis.com .
123connecting databases with process mining: a meta model and toolset
developed and tested, including adapters for each of these
well-known environments. however, from our point of view,the main contribution of this work is the universality of
the proposed solution. its applicability to so many different
environments provides a common ground to separate dataextraction and analysis as different problems, in this waygenerating an interface that is much richer and powerful than
the current existing standards. to summarize, it provides a
standardization layer to simplify and generalize the analysisphase.
nevertheless, several challenges remain open. for instance,
a system that enhances the query building experience, allow-ing for a more natural and user-friendly way, is desirable.
also, mechanisms to exploit the beneﬁts of the process side
(control ﬂow, performance information, conformance, etc.)of the meta model when combined with the data (e.g., datarules discovery) will make the solution really beneﬁcial in
comparison with the regular queries that can be performed
on the source database systems.
open access this article is distributed under the terms of the creative
commons attribution 4.0 international license ( http://creativecomm
ons.org/licenses/by/4.0/ ), which permits unrestricted use, distribution,
and reproduction in any medium, provided you give appropriate creditto the original author(s) and the source, provide a link to the creativecommons license, and indicate if changes were made.
appendix a: meta model formalization
this section proposes a formalization of the elements in this
meta model, starting from the data and continuing with the
process side. as has been mentioned before, we can assumea way to classify elements in types or classes exists. looking
at our running example, we can distinguish between a ticket
class and a customer class. this leads to the deﬁnition of data
model as a way to describe the schema of our data.
deﬁnition 1 (data model) a data model is a tuple dm=
(cl,at,classofattribute ,rs,sourceclass ,targetclass )
such that
– cl is a set of class names,
– at is a set of attribute names,–classofattribute ∈at→cl is a function that maps
each attribute to a class,
– rs is a set of relationship names,–sourceclass ∈rs→clis a function that maps each
relationship to its source class,
–targetclass ∈rs→cl is a function that maps each
relationship to its target class
each of these elements belonging to a class represents
a unique entity, something that can be differentiated fromthe other elements of the same class, e.g., customer a and
customer b . we will call them objects , being unique entities
according to our meta model.
deﬁnition 2 (object collection) let obj be the set of all
possible objects. an object collection oc is a set of objects
such that oc⊆obj .
something we know as well is that, during the execution
of a process, the nature of these elements can change over
time. modiﬁcations can be made on the attributes of these
objects . each of these represents mutations of an object, mod-
ifying the values of some of its attributes, e.g., modifying the
address of a customer. as a result, despite being the same
object, we will be looking at a different version of it. the
notion of object version is therefore introduced to show the
different stages in the life cycle of an object .
during the execution of a process, operations will be
performed, and many times, links between elements areestablished. these links allow relating tickets toconcerts ,
or
customers tobookings , for example. these relationships
are of a structured nature and usually exist at the data modellevel, being deﬁned between classes . therefore, we know
upfront that elements of the class ticket can be related some-
how to elements of the class concert .relationships is the
name we use to call the deﬁnition of these links at the data
model level. however, the actual instances of these relation-
ships appear at the object version level, connecting speciﬁc
versions of objects during a speciﬁc period of time. thesespeciﬁc connections are called relations .
deﬁnition 3 (version collection) let v be some universe
of values, ts a universe of timestamps and dm=(cl,
at,classofattribute ,rs,sourceclass ,targetclass )a data
model. a version collection is a tuple ovc=(ov,attvalue ,
starttimestamp ,endtimestamp ,rel)such that
– ov is a set of object versions,
–attvalue ∈(at×ov)/notarrowrightvis a function that maps a
pair of object version and attribute to a value,
–starttimestamp ∈ov→tsis a function that maps
each object version to a start timestamp,
–endtimestamp ∈ov→tsis a function that maps
each object version to an end timestamp such that ∀ov∈
ov:endtimestamp (ov)≥starttimestamp
(ov),
–rel⊆(rs×ov×ov)is a set of triples relating pairs
of object versions through a speciﬁc relationship.
at this point, it is time to consider the process side of
the meta model. the most basic piece of information we can
ﬁnd in a process event log is an event. these are deﬁned by
some attributes, among which we ﬁnd a few typical ones liketimestamp ,resource orlife cycle .
123e. g. l. de murillas et al.
deﬁnition 4 (event collection) assume v to be some uni-
verse of values and ts a universe of timestamps. an eventcollection is a tuple ec=(ev,evat,eventattributevalue ,
eventtimestamp ,eventlifecycle ,eventresource )such that
–evis a set of events,
–evat is a set of event attribute names,
–eventattributevalue ∈(ev×ev at)/notarrowrightvis a function
that maps a pair of an event and event attribute name to
a value,
–eventtimestamp ∈ev→tsis a function that maps
each event to a timestamp,
–eventlifecycle ∈ev→{start,complete ,...}is a func-
tion that maps each event to a value for its life-cycle
attribute,
–eventresource ∈ev→vis a function that maps each
event to a value for its resource attribute.
when we consider events of the same activity but relat-
ing to a different life cycle, we gather them under the sameactivity instance . for example, two events that belong to the
activity make booking could have different life-cycle values,
being start the one denoting the beginning of the operation
(ﬁrst event) and complete the one denoting the ﬁnalization of
the operation (second event). therefore, both events belong
to the same activity instance . each of these activity instances
can belong to different cases ortraces . at the same time,
cases can belong to different logs, that represent a whole set
oftraces on the behavior of a process.
deﬁnition 5 (instance collection)
an instance collection is
a tuple ic=(ai,cs,lg,aisofcase ,casesoflog )such that
–aiis a set of activity instances,
–csis a set of cases,
–lgis a set of logs,
–aisofcase ∈cs→ p(ai)is a function that maps each
case to a set of activity instances17,
–casesoflog ∈lg→ p(cs)is a function that maps
each log to a set of cases.
the last piece of our meta model is the process model col-
lection . this part stores process models on an abstract level,
i.e., as sets of activities , ignoring details about the control
ﬂow or how these activities relate between them. an activity
can belong to different processes at the same time.
deﬁnition 6 (process model collection) a process model
collection is a tuple pmc=(pm,ac,actofproc )such that
–pm is a set of processes,
17p(x)is the powerset of x, i.e., y∈p(x)ify⊆x.–acis a set of activities,
–actofproc ∈pm→ p(ac)is a function that maps
each process to a set of activities.
now we have all the pieces of our meta model, but it
is still necessary to wire them together. a connected meta
model deﬁnes the connections between these blocks. there-
fore, we see that versions belong to objects (objectofversion )
andobjects belong to a class ( classofobject ). in the same
way, events belong to activity instances (eventai ),activity
instances belong to activities (activityofai ) and can belong
to different cases (aisofcase ),cases to different logs (cas-
esoflog ) and logs to process ( processoflog ). connecting
both data and process views, we ﬁnd events andversions .
they are related ( eventtoovlabel ) in a way that can be inter-
preted as a causal relation between events andversions , i.e.,
when events happen they trigger the creation of versions as
a result of modiﬁcations on data (the update of an attribute
for instance). another possibility is that the event represents
a read access or query of the values of a version .
deﬁnition 7 (connected meta model) a connected meta
model is deﬁned as a tuple cmm =(dm,oc,classofobject ,
ovc,objectofversion ,ec,eventtoovlabel ,ic,eventai ,
pmc,activityofai ,processoflog )such that
–dm=(cl,at,classofattribute ,rs,sourceclass ,
targetclass )is a data model,
–oc is an object collection,
–classofobject ∈oc→cl is a function that maps
each object to a class,
–ovc=(ov,attvalue ,starttimestamp ,endtimestamp ,
rel)is a version collection,
–objectofversion ∈ov→oc is a function that maps
each object version to an object,
–ec=(ev,evat,eventattributevalue ,eventtimestamp ,
eventlifecycle ,eventresource )is an event collection,
–eventtoovlabel ∈(ev×ov)/notarrowrightvis a function that
maps pairs of an event and an object version to a label.if(ev,ov)∈
dom(eventtoovlabel ), this means that
both event and object version are linked. the label itself
deﬁnes the nature of such link, e.g., “insert” ,“update” ,
“read” ,“delete” , etc.,
–ic=(ai,cs,lg,aisofcase ,casesoflog )is an
instance collection,
–eventai ∈ev→aiis a function that maps each event
to an activity instance,
–pmc =(pm,ac,actofproc )is a process model col-
lection,
–activityofai ∈ai→acis a function that maps each
activity instance to an activity,
–processoflog ∈lg→pm is a function that maps
each log to a process.
123connecting databases with process mining: a meta model and toolset
to deﬁne validity, we introduce some shorthands:
deﬁnition 8 (notations i) let cmm =(dm,oc,
classofobject ,ovc,objectofversion ,ec,eventtoovlabel ,
ic,eventai ,pmc,activityofai ,processoflog )be a con-
nected meta model. we deﬁne the following shorthands:
–eai={e∈ev|eventai (e)=ai},
–aics={ai∈ai|ai∈aisofcase (cs)},
–ovobj={ov∈ov|objectofversion (ov)=obj},
–oc cl={obj∈oc|classofobject (obj)=cl},
–rel rs=/braceleftbig
(rs/prime,ov,ov/prime)∈rel|rs/prime=rs/bracerightbig
,
–aiac ac={ai∈ai|activityofai (ai)=ac}.
however, in order to consider a meta model as valid, the
data and connections in it must fulﬁll certain criteria set inthe following deﬁnition:
deﬁnition 9 (valid connected meta model) letcmm =
(dm,oc,classofobject ,ovc,objectofversion ,ec,
eventtoovlabel ,ic,eventai ,pmc,activityofai ,
processoflog )be a connected meta model. a valid con-
nected meta model vcmm is a connected meta model suchthat:
–attvalue is only deﬁned for attributes of the same class of
the object version, that is, (at,ov)∈domain (attvalue )
⇐⇒
classofobject (objectofversion (ov)) = classof
attribute (at),
– none of the object versions of a same object overlap in
time, that is, ∀obj∈oc:∀ov
a∈ovobj:∀ovb∈
ovobj:endtimestamp (ova)≤starttimestamp (ovb)∨
endtimestamp (ovb)≤starttimestamp (ova),
–∀log∈lg:∀cs∈casesoflog (lg):∀ai∈aics:
activityofai (ai)∈actofproc (processoflog (lg)), that
is, all the cases of a log contain only activity instancesthat refer to activities of the same process of the log.
appendix b: extended formalizations
the meta model proposed in this paper has been describedin sect. 3and formalized in “appendix a”. then, sect. 5
presents three environments in which data were extracted
from their corresponding databases and transformed to adapt
to the structure of our meta model. this section presentsa formal description of these three environments, and their
mapping to the meta model.
b.1 common definitions to the three environments
the three environments we are dealing with use a relational
database to store all the relevant information. therefore, thethree of them share some characteristics. in this subsection,
formalizations common to the three environments are pre-sented. we start with the source data model in deﬁnition 10.
deﬁnition 10 (source data model) a s s u m evt ob es o m e
universe of values. a source data model is a tuple sdm=(c,
a,classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,
refattr )such that
– c is a set of class names,
– a is a set of attribute names,
–classattr ∈c→ p(a)is a function mapping each
class onto a set of attribute names. a
cis a shorthand
denoting the set of attributes of class c∈c, i.e., ac=
classattr (c),
–val∈a→ p(v)is a function mapping each attribute
onto a set of values. va=val(a)is a shorthand denoting
the set of possible values of attribute a∈a,
– pk is a set of primary key names,– fk is a set of foreign key names,– pk and fk are disjoint sets, that is pk∩fk=∅ .
to facilitate further deﬁnitions, the shorthand k is intro-
duced, which represents the set of all keys: k=pk∪
fk,
–keyclass ∈k→cis a function mapping each key name
to a class. k
cis a shorthand denoting the set of keys of
class c∈csuch that kc={k∈k|keyclass (k)=c},
–keyrel ∈fk→pkis a function mapping each foreign
key onto a primary key,
–keyattr ∈k→ p(a)is a function mapping each key
onto a set of attributes, such that ∀k∈k:keyattr (k)⊆
akeyclass (k),
–refattr ∈(fk×a)/notarrowrightais a function mapping
each pair of a foreign key and an attribute onto an
attribute from the corresponding primary key, i.e., ∀k∈
fk:∀a,a/prime∈keyattr (k):(ref attr (k,a)∈
keyattr (keyrel (k))∧(ref attr (k,a)=ref attr (k,a/prime)
/equal1⇒ a=a/prime).
the following notations in deﬁnition 11will help us to
express mappings and objects in a more natural way.
deﬁnition 11 (notations) letsdm =(c,a,classattr ,
val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )be a
source data model.
–msdm={map∈a/notarrowrightv|∀a∈dom(map):
map(a)∈va}is the set of mappings,
–osdm={(c,map)∈c×msdm|dom(map)=
class attr (c)}is the set of all possible objects of sdm.
another common aspect of all the environments is the
concept of source object model . deﬁnition 12describes this
123e. g. l. de murillas et al.
concept, which corresponds to a snapshot of the database at
a certain moment in time.
deﬁnition 12 (source object model) letsdm =(c,a,
classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )
be a source data model. a source object model of sdm is a set
som ⊆osdmof objects. usom(sdm)=p/parenleftbig
osdm/parenrightbig
is
the set of all object models of sdm.
to ensure the validity of the source object model, i.e., the
fulﬁllment of all the constraints such as primary and foreign
keys, we introduce the concept of valid source object model
in deﬁnition 13.
deﬁnition 13 (valid source object model) assume sdm =
(c,a,classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,
refattr )to be a source data model. vsom ⊆usom(sdm)
is the set of valid source object models. we say that som ∈
vsom if the following requirements hold:
–∀(c,map)∈som :/parenleftbig
∀k∈kc∩fk:/parenleftbig
∃(c/prime,map/prime)∈
som :
keyclass (keyrel (k))=c/prime∧(∀a∈keyattr (k):
map(a)=map/prime(refattr (k,a))/parenrightbig/parenrightbig/parenrightbig
, i.e., referenced
objects must exist,
–∀(c,map),(c,map/prime)∈som :/parenleftbig
∀k∈kc∩pk:
((∀a∈keyattr (k):
map(a)=map/prime(a)/parenrightbig
/equal1⇒ map=map/prime/parenrightbig/parenrightbig
, i.e., pk and
uk values must be unique.
in one or another form, we ﬁnd events when extracting
our data. something in common between the events we ﬁnd
in the three mentioned environments is that three types canbe distinguished: additions, updates and deletions. we call
these types source event types , as explained in deﬁnition 14.
deﬁnition 14 (source event types) letsdm =(c,a,
classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )
be a source data model and vsom the set of valid source
object models. set=et
add∪etupd∪etdelis the set
of source event types composed of the following pairwise
disjoint sets:
–etadd={(⊕,c)|c∈c}are the event types for adding
objects,
–etupd={(/circledivide,c)|c∈c}are the event types for updat-
ing objects,
–etdel={(/circleminus,c)|c∈c}are the event types for deleting
objects.
each of the three environments we are trying to formalize
presents different characteristics when recording execution
events. deﬁnition 15provides a common, base concept of
source events . however, in further sections we will see whatare the particularities of this concept in each of the environ-
ments.
deﬁnition 15 (source events) let sdm =(c,a,
classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )
be a source data model, vsom the set of valid source objectmodels, set the set of source event types, and map
null∈
{∅}→ va function with the empty set as domain. seis the
set of source events such that ∀e∈se:∃et∈set:e=
(et,map old,map new).
finally, something needed for the mapping between these
systems and our meta model is the existence of a concept ofsource event occurrence andsource change log . deﬁnition 16
provides a description of these concepts. however, we will
see how each environment interprets this differently, and how
a change log can be inferred from each of them.
deﬁnition 16 (source event occurrence and source
change log) letsdm =(c,a,classattr ,val,pk,fk,
keyclass ,keyrel ,keyattr ,refattr )be a source data model,
vsom the set of valid source object models and se the set
of source events. assume some universe of timestamps ts.
eo=(e,ts,resource ,li f ecycle )∈se×ts×v×vis a
source event occurrence. seo(sdm,se)=se×ts×
v×vis the set of all possible source event occurrences. a
source change log scl=/angbracketlefteo
1,eo2,..., eon/angbracketrightis a sequence
of source event occurrences such that time is non-decreasing,i.e., scl =/angbracketlefteo
1,eo2,..., eon/angbracketright∈(seo(sdm,se))∗
andtsi≤tsjfor any eoi=(ei,tsi)andeoj=(ej,tsj)
with 1 ≤i<j≤n.
finally, deﬁnition 17establishes some useful notations
for further deﬁnitions, with respect to source event occur-
rences and source object ids.
deﬁnition 17 (notations ii) assume a universe of times-
tamps, a source event occurrence eo=(((evt,c),map old,
map new),ts,resource ,li f ecycle ), and a source data model
sdm. we deﬁne the following shorthand: object id (c,map)
={(a,v)∈(a,v)|a∈keyattr (pk c)∧map(a)=v}, i.e.,
it returns a set of pairs (attribute ,value)for a mapping map
according to the attributes of the primary key of such class c
in the source data model.
now, all the common elements of the three environments
have been formalized. these represent the common groundneeded in order to make the mapping to our meta model. the
three following sections (sects. b.2,b.3, and b.4) formalize
some of the concepts that differ between the three environ-ments. finally, sect. b.5 proposes a mapping to our meta
model.
b.2 database redo logs: formalization
the redo log environment presents some particularities with
respect to how the events are represented. deﬁnition 18for-
123connecting databases with process mining: a meta model and toolset
malizes this concept while maintaining compatibility with
the common description of source events in deﬁnition 15.
deﬁnition 18 (redo log events) letsdm =(c,a,
classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )
be a source data model, vsom the set of valid source object
models and map null∈{∅}→ va function with the empty
set as domain. se=eadd∪eupd∪edelis the set of source
events composed of the following pairwise disjoint sets:
–eadd={((⊕,c),map old,map new))|(c,map new)∈
osdm∧map old=map null}
–eupd={((/circledivide,c),map old,map new))|(c,map old)∈
osdm∧(c,map new)∈odm}
–edel={((/circleminus,c),map old,map new))|(c,map old)∈
osdm∧map new=map null}
this is, mainly, the only difference between the redo log
environment and the common description provided in the
previous section. however, for the other two environments,
some additional details need to be taken into account.
b.3 in-table versioning: formalization
in the case of in-table versioning environments, change logsare not explicitly recorded. however, we ﬁnd object versions
implicitly recorded within the tables of the database. deﬁni-
tion 19formalizes this structure.
deﬁnition 19 (implicit versions record) letsdm =(c,
a,classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,
refattr )be a source data model. an implicit versions record
is a tuple ivr=(tabname ,objectid ,timestamp ,
verid ,fnames ,value)such that:
–tabname is a set of table names,
–objectid ⊆nis a set of object identiﬁers,
–timestamp is a set of timestamps of versions,
–verid ⊆tabname ×objectid ×timestamp is
a set of unique version identiﬁers formed by the combi-nation of a table name, an object id and a time stamp,
–fnames ⊆ais the set of attributes of the object version,
–value∈(verid ×a)/notarrowrightvis a mapping between a pair
(versionid ,attributename )and its new value after the
change.
now that we know how object versions are being deﬁned
in this speciﬁc environment, we can show how it affects
our previous deﬁnition of source object model . deﬁnition 20
shows the compatibility in this particular case.
deﬁnition 20
(itv source object model) given a source
data model sdm =(c,a,classattr ,val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )and an implicit ver-
sions record ivr, an itv source object model som isa set of objects such that som ⊆o
sdmsuch that:
∀o=(c,map)∈som :c∈ta bna me ∧∀a∈
domain (map):a∈fnames ∧∃v=(tab,ob,ts)∈
verid :map(a)=value(v,a)∧∄v/prime=(tab,ob,ts/prime)∈
verid :ts/prime>ts, i.e., the itv source object model som
is formed by all the most recent object versions for each
object id.
deﬁnition 21 (notations iii) letsdm=(c,a,classattr ,
val,pk,fk,keyclass ,keyrel ,keyattr ,refattr )be a
source data model, and ivr=(tabname ,objectid ,
timestamp ,verid ,fnames ,value)an implicit versions
record. given a version id vid=(tab,ob,ts)∈verid ,
we deﬁne the following shorthands:
– itvevtype(id)
–itvevtype ∈verid →{ ⊕ ,/circledivide}is a function map-
ping object version ids to types of change, such thatitvevtype (vid)=⊕ ⇐ ⇒ ∄vid
/prime=(tab/prime,ob/prime,ts/prime)∈
verid :tab/prime=tab∧ob/prime=ob∧ts/prime<ts, i.e.,
if a previous version of the same object does not exist,it is considered to be an addition change. otherwise,
itvevtype (vid)=/circledivide , i.e., it is considered to be an update,
–itvts ∈verid →times ta mp is a function
mapping object version ids to timestamps such that,
itvts(vid)=ts,
–itvtabname ∈verid →ta bna me is a func-
tion mapping object version ids to table names such that,itvtabname (vid)=tab,
–itvobjid ∈verid → objectid is a func-
tion mapping object version ids to object ids such that,itvobjid (vid)=ob,
–itvprevverid ∈verid →(verid ∪{∅})is a
function mapping object version ids to their predeces-sor object version id (or null id if a predecessor does
not exist) such that, itvprevverid (vid)=vid
/prime∧(vid/prime=
(tab/prime,ob/prime,ts/prime)∧tab=tab/prime∧ob=ob/prime∧ts>ts/prime∧
∄(tab/prime,ob/prime,ts/prime/prime)∈verid :ts>ts/prime/prime>ts/prime)∨
(itvevtyp e(vid)=⊕∧ vid/prime=∅),
finally, the only building block left to deﬁne in this case
is the correlation between the previous deﬁnition of source
event occurrences (deﬁnition 16) with this environment.
deﬁnition 22shows that equivalence and provides the key
to translate implicit object versions into source event occur-
rences.
deﬁnition 22 (itv source event occurrences) let ts
be a universe of timestamps, v a universe of values and
sdm a source data model sdm =(c,a,classattr ,
val,pk,fk,keyclass ,keyrel ,keyattr ,refattr ).w e
say that seo(sdm,ivr)=se×ts×v×vis a
123e. g. l. de murillas et al.
set of itv source event occurrences such that ∀eo=
(((evt,c),map old,map new),ts,resource ,li f ecycle )∈
seo(sdm,ivr):∃id∈verid :
–evt=itvevtyp e(id)∧
–c=itvtabname (id)∧
–({itvprevve rid (id)}×domain (map old))⊆domain
(value)∧
–({id}×domain (map new))⊆domain (value)∧
–∀a∈domain (map old):map old(a)=value(itvprev
ve rid (id),a)∧
–∀a∈domain (map new):map new(a)=value(id,a)∧
–ts=itvts(id)∧
–itvobjid (id)=object id (c,map new)∧
–resource =∅∧
–li f ecycle =itvevtyp e(id),
that is, for each source event occurrence in seo(sdm,ivr)
exists an implicit version record in ivr that shares values for
all its properties.
b.4 sap-style change table: formalization
sap systems are a different kind of environment. they are
closely related to the redo log environments, with the differ-ence that the change record contains the relevant information
in a slightly different way. deﬁnition 23provides details on
thissap change record .
deﬁnition 23 (sap change record) assume a universe
of values v , a universe of date values date and a uni-
verse of time values time . given a source data model
sdm =(c,a,classattr ,val,pk,fk,keyclass ,
keyrel ,keyattr ,refattr ), a sap change record is a tuple
scr =(chnr ,objectclas ,objectid ,chnid ,
uname ,udate ,utime ,change _ind,tabname ,tabkey ,
fname ,value _new,value _old)such that:
–chnr ⊆nis a set of sap change numbers,
–objectclas is a set of sap object classes,
–objectid ⊆nis a set of sap object ids,
–chnid ⊆chnr ×objectclas ×objectid is
a set of unique change identiﬁers formed by the com-bination of a change number ( chnr
), an object class
(objectclas ) and an object id ( objectid ),
–uname ∈chnid →vis a mapping between change
ids and user name strings,
–udate ∈chnid →date is a mapping between change
ids and date values,
–utime ∈chnid →time is a mapping between change
ids and time values,
–change _ind∈chnid →{ ⊕ ,/circledivide,/circleminus}is a mapping
between change ids and a change type,–tabname ∈chnid →cis a mapping between change
ids and a class name,
–tabkey ∈chnid →vis a mapping between change
ids and the primary key of the modiﬁed object,
–fname ∈chnid → p(a)is a mapping between
change ids and a set of changed attributes,
–value _new∈(chnid ×a)/notarrowrightvis a mapping between
a pair (change id,attribute name) and its new value after
the change,
–value _old∈(chnid ×a)/notarrowrightvis a mapping between
a pair (change id,attribute name) and its old value before
the change.
the previous deﬁnition gives us the ground to build the
mapping to source event occurrences. deﬁnition 24describes
how to obtain the source event occurrences previously intro-
duced from the sap change record. this allows inferring the
change log necessary to build our meta model.
deﬁnition 24 (sap source event occurrences) assume a
universe of timestamps ts, a universe of values v , and a
function convertdatetime ∈(date ×time)→tsthat
maps pairs of date and time values into a timestamp. given asource data model sdm=(c,a,classattr ,val,pk,fk,
keyclass ,keyrel ,keyattr ,refattr )and a sap change record
scr =(chnr ,objectclas ,objectid ,chnid ,
uname ,udate ,utime ,change _ind,tabname ,tabkey ,
fname ,value _new,value _old), we deﬁne seo(sdm,scr)
=se×ts×v×vas a set of source event occurrences
such that ∀eo=
(((evt,c),map old,map new),ts,resource ,
lifecycle )∈seo(sdm,scr):∃id∈chnid :
–evt=change _ind(id)∧
–c=tabname (id)∧
–map old(fname (id))=value _old(fname (id))∧
–map new(fname (id))=value _new(fname (id))∧
–ts=convert datet ime (udate(id),utime(id))∧
–tabkey (id)=objectid (c,map new)∧
–resource =uname (id)∧
–lifecycle =change _ind(id),
that is, for each event occurrence in seo(sdm,scr)exists
an event record in scr that shares values for all its properties.
b.5 common meta model mapping for the three
environments
in the previous sections of this appendix, we have deﬁned
the common aspects of the three environments under study,together with the particularities of each of them. now, we
have deﬁned the necessary notions to map each concept from
the original sources to our meta model. in this section, wewill deﬁne this mapping for each of the main elements of
123connecting databases with process mining: a meta model and toolset
the meta model, except for the cases andprocess models
sectors, which are independent from the source data and canbe inferred from the extracted information once it has been
already mapped to our meta model. we will start with the
mapping of the source data model in deﬁnition 25.
deﬁnition 25 (mapped data model) given a source data
model sdm=(c,a,classattr ,val,pk,fk,keyclass ,
keyrel ,keyattr ,refattr ), a mapped data model is a tuple
mdm =(cl,at,classofattribute ,rs,sourceclass ,
targetclass )such that:
–cl=cis a set of class names,
–at=ais a set of attribute names,
–classofattribute ∈at→cl is a function that maps
each attribute to a class, such that ∀at∈at:at∈
classattr (classofattribute (at)),
–rs=fk is a set of relationship names,
–sourceclass ∈rs→clis a function that maps each
relationship to its source class, such that ∀
rs∈rs:
sourceclass (rs)=keyclass (rs),
–targetclass ∈rs→cl is a function that maps each
relationship to its target class, such that ∀rs∈rs:
targetclass (rs)=keyclass (keyrel (rs)).
the same can be done with the source object model .i t s
mapping is formalized in deﬁnition 26. also, in deﬁni-
tion 27we redeﬁne the concept of timestamps to include
the situations in which the beginning or end of a period is
unknown. then, some useful notations for further deﬁnitions
are expressed in deﬁnition 28.
deﬁnition 26 (mapped object collection) assume som ∈
vsom to be a valid source object model, mdm a mapped
data model, and obj the set of all possible objects for mdm .
a mapped object collection moc is a set of objects such that
moc ⊆obj , and mappedobj ∈moc ↔som is a bijec-
tive function that maps every mapped object to a source object
and vice versa.
deﬁnition 27 (universe of global timestamps) assume
tsto be a universe of timestamps. a universe of global times-
tamps is a set gts such that gts=ts∪{ − œ}∪{ + œ},
where −φrepresents an undeﬁned timestamp in the past, and
+φan undeﬁned timestamp in the future. these timestamps
fulﬁll the following condition: ∀ts∈ts:−œ<ts<+œ.
deﬁnition 28 (notations iv) assume a universe of times-
tamps ts. given a source change log scl , and a mapped
data model mdm =(cl,at,classofattribute ,rs,
sourceclass ,targetclass ), we deﬁne the following short-
hands:
–sclb
a(c,oid)={ eoi∈scl|eoi=((evt,c),
map old,map new,t)∧b>i>a∧object id (c,map new)=oid}, is the set of event occurrences in the source change
log such that all of them occurred after the a-th and beforethe b-th element of the sequence, and they correspond to
objects of class c and with the object id oid,
–tseo∈p(scl)→ p(ts)is a function that returns a
set of timestamps corresponding to the provided set ofevent occurrences.
one of the key elements of this mapping is the version
collection. in deﬁnition 29, we use the source change log
and object models to infer the content of the versions part ofour meta model.
deﬁnition 29 (mapped version collection) assume a uni-
verse of global timestamps gts , a mapped object collection
moc , a source data model sdm =(c,a,classattr ,
val,pk,fk,keyclass ,keyrel ,keyattr ,refattr ),a
mapped data model mdm =(cl,at,classofattribute ,
rs,sourceclass ,targetclass ), and a source change log
scl=<eo
1,eo2,..., eon>. a mapped object version col-
lection is a tuple movc =(ov,attvalue ,starttimestamp ,
endtimestamp ,rel)such that:
–ov={(c,map,ts,te)}is a set of object versions for
which the following holds:
–∀o=(c,map)∈moc :∃ov∈ov:ov=
(c,map,ts,te)∧ts=max(tseo(scln
1(c,id)),−φ)∧
te=+φ, i.e., for every object in the mapped object
collection exists a version for which the start times-
tamp is either unknown or the timestamp of the last
source event occurrence that affected that object, andthe end timestamp is unknown,
–∀i∈1..n:eo
i=((evt,c),map old,map new,tev)∈
scl :∃ov∈ov:ov=(c,map,ts,te)∧
(((evt=⊕ ∨ evt=/circledivide)∧map new=map∧
ts=tev∧te=min(ts(scln
i+1(c,id)),+φ))∨
(evt=/circleminus ∧ map old=map∧te=tev∧ts=
max(ts(scli−1
1(c,id)),−φ))) , i.e., for every source
event occurrence that represents an addition or a modi-
ﬁcation, there is an object version with the values after
the event occurrence, start timestamp equal to the eventoccurrence timestamp, and end timestamp equal to the
one of the next event occurrence that affected the same
object. in case it is the ﬁrst version of the object, thestart timestamp will be unknown. if it is the last ver-sion of the object, the end timestamp will be unknown
instead.
–attvalue ∈(at×ov)/notarrowrightvis a function that maps val-
ues to pairs of attributes and object versions such that,
given an attribute at∈atand an object version ov=
(c,map,t
s,te),attvalue (at,ov)=map(at),
123e. g. l. de murillas et al.
–starttimestamp ∈ov→gts is a function that returns
the start timestamp of an object version such that, given anobj. version ov=(c,map,t
s,te),starttimestamp (ov)
=ts,
–endtimestamp ∈ov→gts is a function that returns
the end timestamp of an object version such that, given anobject version ov=(c,map,t
s,te),endt imestamp (ov)
=te,
–rel⊆(rs×ov×ov)is a set of triples relating
pairs of object versions through speciﬁc relationships
such that, given a relationship rs∈rs, and two
object versions ova=(ca,map a,tsa,tea)∈ov and
ovb=(cb,map b,tsb,teb)∈ov, it is always true
that(rs,ova,ovb)∈rel ⇐⇒ rs∈fk∧
sourceclass (rs)=ca∧targetclass (rs)=cb∧(tsa≤
tsb≤tea∨tsb≤tsa≤teb)∧∀at∈keyatt (rs):
map a(at)=map b(ref attr (rs,at)), i.e., two object
versions are related through a relationship if they belong
to the source and target classes of the relationship, respec-tively, and if there is a mapped foreign key from the source
data model such that both versions share the same val-
ues for the key attributes. in addition, both versions musthave coexisted in time.
finally, in deﬁnition 30we describe how the events of the
meta model are mapped to the source change log.
deﬁnition 30 (mapped event collection) assume v to be
some universe of values, ts a universe of timestamps and
scl a mapped change log scl=<eo
1,eo2,..., eon>.
a mapped event collection is a tuple mec=(ev,evat,
eventattributevalue ,eventtimestamp ,eventlifecycle ,
eventresource )such that:
–evis a set of events,
–evat is a set of event attribute names,
–eventattributevalue ∈(ev×evat)/notarrowrightvis a function
that maps a pair of an event and event attribute name toa value,
–eventtimestamp ∈ev→tsis a function that maps
each event to a timestamp,
–eventlifecycle ∈ev→{start,complete ,...}is a func-
tion that maps each event to a value for its life-cycle
attribute,
–eventresource ∈ev→vis a function that maps each
event to a value for its resource attribute.
and∀ev∈ev:∃eo∈scl:eo=(((evt,c),map
old,
map new),ts,resource ,lifecycle )∧ts=eventtimestamp (ev)
∧resource =eventresource (ev)∧lifecycle =eventlifecycle
(ev)∧∀(ev,at)∈domain (eventattributevalue ):map new(at)
=eventattributevalue (ev,at), i.e., for each event in the
mapped event collection, there is an event occurrence in themapped change log that shares timestamp, resource, life cycle
and attribute values.
the presented mapping speciﬁes how data are represented
in each of the considered environments, and how these datacan be transformed in order to populate with content our meta
model, with the purpose of performing further analysis in a
more standardized and automated way.
references
1. bolt, a., van der aalst, w.m.p.: multidimensional process mining
using process cubes. in: enterprise, business-process and informa-tion systems modeling—16th international conference, bpmds2015, 20th international conference, emmsad 2015, held atcaise 2015, stockholm, sweden, june 8–9, 2015, proceedings,pp. 102–116 (2015)
2. buijs, j.: mapping data sources to xes in a generic way. master’s
thesis, technische universiteit eindhoven, the netherlands (2010)
3. eder, j., olivotto, g.e., gruber, w.: a data warehouse for workﬂow
logs. in: engineering and deployment of cooperative information
systems, pp. 1–15. springer (2002)
4. gonzález lópez de murillas, e., reijers, h.a., van der aalst,
w.m.p.: connecting databases with process mining: a meta modeland toolset. in: international workshop on business process mod-eling, development and support, pp. 231–249. springer (2016)
5. gonzález-lópez de murillas, e., van der aalst, w.m.p., reijers,
h.a.: process mining on databases: unearthing historical data fromredo logs. in: business process management. springer (2015)
6. herzberg, n., meyer, a., weske, m.: improving business process
intelligence by observing object state transitions. data knowl. eng.98, 144–164 (2015)
7. ieee standard for extensible event stream (xes) for achiev-
ing interoperability in event logs and event streams. ieee std1849-2016, pp. 1–50 (2016). https://doi.org/10.1109/ieeestd.
2016.7740858
8. ingvaldsen, j.e., gulla, j.a.: preprocessing support for large scale
process mining of sap transactions. in: business process manage-ment workshops, pp. 30–41. springer (2008)
9. lu, x., nagelkerke, m., van de wiel, d., fahland, d.: discovering
interacting artifacts from erp systems. ieee trans. serv. comput.8(6), 861–873 (2015)
10. mahendrawathi, e., astuti, h.m., wardhani, i.r.k.: material
movement analysis for warehouse business process improvementwith process mining: a case study. in: asia paciﬁc business processmanagement, pp. 115–127. springer (2015)
11. meyer, a., pufahl, l., fahland, d., weske, m.: modeling and
enacting complex data dependencies in business processes. in: pro-ceedings of the 11th international conference on business processmanagement, pp. 171–186. springer (2013)
12. mueller-wickop, n., schultz, m.: erp event log preprocessing:
timestamps vs. accounting logic. in: design science at the inter-section of physical and virtual design, lecture notes in computerscience, vol. 7939, pp. 105–119. springer, berlin (2013). https://
doi.org/10.1007/978-3-642-38827-9_8
13. neumuth, t., mansmann, s., scholl, m.h., burgert, o.: data ware-
housing technology for surgical workﬂow analysis. in: 21st ieeeinternational symposium on computer-based medical systems,2008, cbms’08, pp. 230–235. ieee (2008)
14. niedrite, l., solodovnikova, d., treimanis, m., niedritis, a.: goal-
driven design of a data warehouse-based business process analysissystem. in: proceedings of the 6th conference on 6
thwseas
123connecting databases with process mining: a meta model and toolset
interanational conference on artiﬁcial intelligence, knowledge
engineering and data bases, pp. 243–249 (2007)
15. popova, v ., fahland, d., dumas, m.: artifact lifecycle discovery.
int. j. cooper. inf. syst. 24(01), 1550,001 (2015)
16. rosemann, m., zur muehlen, m.: evaluation of workﬂow man-
agement systems-a meta model approach. aust. j. inf. syst. 6(1),
103–116 (1998)
17. sismanis, y ., brown, p., haas, p.j., reinwald, b.: gordian: efﬁcient
and scalable discovery of composite keys. in: proceedings of the32nd international conference on very large data bases, pp. 691–702. vldb endowment (2006)
18. štolfa, j., kopka, m., štolfa, s., kobˇ ersk`y, o., snášel, v .: an appli-
cation of process mining to invoice veriﬁcation process in sap. in:innovations in bio-inspired computing and applications, pp. 61–74. springer (2014)
19. van dongen, b.f., van der aalst, w.m.p.: a meta model for process
mining data. emoi-interop 160, 30 (2005)
20. van der aalst, w.m.p.: process cubes: slicing, dicing, rolling up
and drilling down event data for process mining. in: asia paciﬁcbusiness process management—first asia paciﬁc conference,ap-bpm 2013, beijing, china, 29–30 august , 2013. selectedpapers, pp. 1–22 (2013)
21. van der aalst, w.m.p.: extracting event data from databases to
unleash process mining. bpm–driving innovation in a digitalworld, management for professionals, pp. 105–128. springer,berlin (2015). https://doi.org/10.1007/978-3-319-14430-6_8
22. v ogelgesang, t., appelrath, h.: a relational data warehouse for
multidimensional process mining. in: proceedings of the 5thinternational symposium on data-driven process discovery andanalysis (simpda 2015), vienna, austria, 9–11 december, 2015,pp. 64–78 (2015)
23. v ogelgesang, t., appelrath, h.: pmcube: a data-warehouse-based
approach for multidimensional process mining. in: business pro-cess management workshops—bpm 2015, 13th internationalworkshops, innsbruck, austria, august 31–september 3, 2015,revised papers, pp. 167–178 (2015)
24. v ogelgesang, t., kaes, g., rinderle-ma, s., appelrath, h.: mul-
tidimensional process mining: questions, requirements, and lim-itations. in: proceedings of the caise’16 forum, at the 28thinternational conference on advanced information systems engi-neering (caise 2016), ljubljana, slovenia, june 13–17, 2016., pp.169–176 (2016)
25. zhang, m., hadjieleftheriou, m., ooi, b.c., procopiuc, c.m., sri-
vastava, d.: on multi-column foreign key discovery. proceedingsof the vldb endowment 3(1–2), 805–814 (2010)
26. zur muehlen, m.: workﬂow-based process controlling-or: what
you can measure you can control. workﬂow handbook, pp. 61–77
(2001)
27. zur muhlen, m.: evaluation of workﬂow management systems
using meta models. in: proceedings of the 32nd annual hawaiiinternational conference on systems sciences, hicss-32 (1999)
28. zur muehlen, m., rosemann, m.: workﬂow-based process mon-
itoring and controlling-technical and organizational issues. in:proceedings of the 33rd annual hawaii international conferenceon system sciences, 2000, pp. 10–pp. ieee (2000)
29. zur muehlen, m.: process-driven management information sys-
tems combining data warehouses and workﬂow technology. in:proceedings of the international conference on electronic com-merce research (icecr-4), pp. 550–566 (2001)
eduardo gonzález lópez de muril-lashas been a ph.d. student at the
eindhoven university of technol-ogy, the netherlands, since 2014.his research interests include pro-cess mining, data extraction andtransformation, data querying, auto-mated event log building, and busi-ness process management.
hajo a. reijers is a full pro-
fessor of business informatics atvrije universiteit amsterdam, thenetherlands. he also holds a posi-tion as part-time, full professor ateindhoven university of technol-
ogy. previously, he worked as a
management consultant and r&dmanager. his research interestsrelate to business process man-agement, data analytics and con-ceptual modeling. on these andother topics, he published over200 scientiﬁc papers, book chap-ters and professional publications.
prof.dr.ir. wil m. p. van deraalst is a full professor at rwth
aachen university. he is also avisiting researcher at fondazionebruno kessler (fbk) in trentoand a member of the board ofgovernors of tilburg university.
until 2018 he was also the scien-
tiﬁc director of the data sciencecenter eindhoven (dsc/e). hispersonal research interests includeprocess mining, petri nets, busi-ness process management, work-ﬂow management, process model-ing and process analysis. wil van
der aalst has published over 200 journal papers, 20 books (as author oreditor), 450 refereed conference/workshop publications and 65 bookchapters. next to serving on the editorial boards of over 10 scien-tiﬁc journals he is also playing an advisory role for several com-panies, including fluxicon, celonis and processgold. van der aalstreceived honorary degrees from the moscow higher school of eco-nomics (prof. h.c.), tsinghua university, and hasselt university (dr.h.c.). he is also an elected member of the royal netherlands academyof arts and sciences, the royal holland society of sciences andhumanities, and the academy of europe.
123