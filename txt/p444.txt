the need for a process mining evaluation
framework in research and practice
position paper
a. rozinat, a.k. alves de medeiros, c.w. g¨ unther, a.j.m.m. weijters, and
w.m.p. van der aalst
eindhoven university of technology
p.o. box 513, nl-5600 mb, eindhoven, the netherlands
{a.rozinat,a.k.medeiros,c.w.gunther,a.j.m.m.weijters,
w.m.p.v.d.aalst }@tue.nl
abstract. although there has been much progress in developing process
mining algorithms in recent years, no eﬀort has been put in developing
a common means of assessing the quality of the models discovered by
these algorithms. in this paper, we motivate the need for such an evalu-
ation mechanism, and outline elements of an evaluation framework that
is intended to enable (a) process mining researchers to compare the per-
formance of their algorithms, and (b) end users to evaluate the validity
of their process mining results.
1 introduction
process mining has proven to be a valuable approach that provides new and
objective insights into the way business processes are actually conducted within
organizations. taking a set of real executions (the so-called “event log”) as the
starting point, these techniques attempt to extract non-trivial and useful process
information from various perspectives, such as control ﬂow, data ﬂow, organi-
zational structures, and performance characteristics. a common mining xml
(mxml) log format was deﬁned in [3] to enable researchers and practitioners
to share their logs in a standardized way. however, while process mining has
reached a certain level of maturity and has been used in a variety of real-life
case studies (see [1] for an example), a common framework to evaluate process
mining results is still lacking . we believe that there is the need for a concrete
framework that enables (a) process mining researchers to compare the perfor-
mance of their algorithms, and (b) end users to evaluate the validity of their
process mining results. this paper is a ﬁrst step into this direction.
the driving element in the process mining domain is some operational pro-
cess, for example a business process such as an insurance claim handling proce-
dure in an insurance company, or the booking process of a travel agency. nowa-
days, many business processes are supported by information systems that help
coordinating the steps that need to be performed in the course of the process.
workﬂow systems, for example, assign work items to employees according to
their roles and the status of the process. typically, these systems record eventsrelated to the activities that are performed, e.g., in audit trails or transaction
logs [3].1these event logs form the input for process mining algorithms.
in this paper we focus on providing a means of comparison for algorithms
that discover the control-ﬂow perspective of a process (which we simply refer to as
process discovery algorithms from now on). in particular, we focus on validation
techniques for these process discovery algorithms. we argue that this evaluation
can take place in diﬀerent dimensions, and identify ingredients that are needed
for an evaluation framework. note that in an extended version of this paper [11]
we describe two diﬀerent validation approaches: one based on existing validation
metrics, and another based on the so-called k-fold cross validation technique
known from the machine learning domain. we applied both approaches to the
running example. furthermore, in [11] we also present an extensible control
flow benchmark plug-in to directly support the evaluation and comparison of
diﬀerent mining results in the context of the prom framework2.
the remainder of this paper is organized as follows. section 2 motivates the
need for an evaluation framework. then, section 3 outlines ﬁrst steps towards
such a common framework. finally, section 4 concludes the paper.
2 process discovery: which model is the “best”?
the goal of a process discovery algorithm is to construct a process model which
reﬂects the behavior that has been observed in the event log. diﬀerent process
modeling languages3can be used to capture the causal relationships of the steps,
or activities, in the process. the idea of applying process mining in the context
of workﬂow management was ﬁrst introduced in [5]. over the last decade many
process mining approaches have been proposed [6, 9]. while all these approaches
aim at the discovery of a “good” process model, often targeting particular chal-
lenges (e.g., the mining of loops, or duplicate tasks), they have their limitations
and many diﬀerent event logs and quality measurements are used. hence, no
standard measure is available.
to illustrate the dilemma, we consider the simple example log in figure 2(a),
which contains only ﬁve diﬀerent traces. we applied six diﬀerent process mining
algorithms that are available in prom and obtained six diﬀerent process models
(for every plug-in, we used the default settings in prom 4.1). figure 1 depicts the
mining results for the alpha miner [4], the heuristic miner [12], the alpha++
miner [13], the duplicates genetic miner and the genetics miner [8], and
1it is important to note that information systems that do not enforce users to follow
a particular process often still provide detailed event logs, e.g., hospital information
systems, erp systems etc.
2prom oﬀers a wide range of tools related to process mining and process analysis.
both documentation and software (including the source code) can be downloaded
from http://www.processmining.org .
3in the remainder of this paper we will use petri nets, motivated by their formal
semantics. note that in our tool prom there exist translations from process modeling
languages such as epc, yawl, and bpel to petri nets and vice-versa.
2ih
c
bag
f
edhab
dd
e
i
cg
h fab de
i
c g
hf
(b) heuristic  miner
(d) duplicates 
     genetic  miner
(f) petrify  minerab d e
i
c g h f
ab de
i
c g hf
ab
de
ic
g
hf(a) alpha  miner
(c) alpha++  miner
(e) genetic  minerfig. 1. process models that were discovered by diﬀerent process discovery algorithms
based on the same log
thepetrify miner [2]. the models seem similar, but are all diﬀerent4. are they
equivalent? if not, which one is the “best”?
these questions are interesting both for researchers and end users: (a) re-
searchers typically attempt to let their process discovery algorithms construct
process models that completely and precisely reﬂect the observed behavior in a
structurally suitable way. it would be useful to have common data sets contain-
ing logs with diﬀerent characteristics, which can be used within the scientiﬁc
community to systematically compare the performance of various algorithms in
diﬀerent, controlled environments. (b) users of process discovery techniques, on
the other hand, need to know how well the discovered model describes reality,
how many cases are actually covered by the generated process description etc.
for example, if in an organization process mining is to be used as a knowledge
discovery tool in the context of a business process intelligence (bpi) frame-
work, it must be possible to estimate the “accuracy” of a discovered model, i.e.,
the “conﬁdence” with which it reﬂects the underlying process. furthermore, end
users need to be able to compare the results obtained from diﬀerent process
discovery algorithms.
3 towards a common evaluation framework
in an experimental setting, we usually know the original model that was used to
generate an event log. for example, the log in figure 2(a) was created from the
simulation of the process model depicted in figure 2(b). knowing this, one could
leverage process equivalence notions to evaluate the discovered model with re-
spect to the original model. but in many practical situations no original model is
4note that throughout this paper the invisible (i.e., unlabeled) tasks need to be
interpreted using the so-called “lazy semantics”, i.e., they are only ﬁred if they
enable a succeeding, visible task [8].
3fig. 2. the evaluation of a process model can take place in diﬀerent dimensions
available. however, if we assume that the behavior observed in the log is what re-
ally happened (and somehow representative for the operational process at hand),
it is possible to compare the discovered model to the event log that was used
as input for the discovery algorithm. this essentially results in a conformance
analysis problem [10, 7]. in either case quality criteria need to be determined.
evaluation dimensions figure 2 depicts an event log (a) and four diﬀerent
process models (b-e). while figure 2(b) depicts a “good” model for the event log
in figure 2(a), the remaining three models show undesirable, extreme models
that might also be returned by a process mining algorithm. they illustrate that
the evaluation of an event log and a process model can take place in diﬀerent,
orthogonal dimensions.
fitness. the ﬁrst dimension is ﬁtness, which indicates how much of the
observed behavior is captured by (i.e., “ﬁts”) the process model. for example,
the model in figure 2(c) is only able to reproduce the sequence abdei , but not
the other sequences in the log. therefore, its ﬁtness is poor.
precision. the second dimension addresses overly general models. for ex-
ample, the model in figure 2(d) allows for the execution of activities a–iin
any order (i.e., also the sequences in the log). therefore, the ﬁtness is good,
but the precision is poor. note that the model in figure 2(b) is also considered
to be a precise model, although it additionally allows for the trace acghdfi
(which is not in the log). because the number of possible sequences generated
by a process model may grow exponentially, it is not likely that all the possi-
ble behavior has been observed in a log. therefore, process mining techniques
4strive for weakening the notion of completeness (i.e., the amount of information
a log needs to contain to be able to rediscover the underlying process [4]). for
example, they want to detect parallel tasks without the need to observe every
possible interleaving between them.
generalization. the third dimension addresses overly precise models. for
example, the model in figure 2(e) only allows for exactly the ﬁve sequences from
the log. in contrast to the model in figure 2(b) no generalization was performed.
determining the right level of generalization remains a challenge, especially when
dealing with logs that contain noise (i.e., distorted data). similarly, in the con-
text of more unstructured and/or ﬂexible processes, it is essential to further
abstract from less important behavior (i.e., restriction rather than generaliza-
tion). in general, abstraction can lead to the omission of connections between
activities, which could mean lower precision or lower ﬁtness (e.g., only captur-
ing the most frequent paths). furthermore, steps in the process could be left
out completely. therefore, abstraction must be seen as a diﬀerent evaluation
dimension, which needs to be balanced against precision and ﬁtness.
structure. the last dimension is the structure of a process model, which is
determined by the vocabulary of the modeling language (e.g., routing nodes with
and and xor semantics). often there are several syntactic ways to express the
same behavior, and there may be “preferred” and “less suitable” representations.
for example, the ﬁtness and precision of the model in figure 2(e) are good, but
it contains many duplicate tasks, which makes it diﬃcult to read. clearly, this
evaluation dimension highly depends on the process modeling formalism, and is
diﬃcult to assess in an objective way as it relates to human modeling capabilities.
evaluation framework to systematically compare process mining algorithms,
it would be useful to have common data sets, which can be used and extended by
diﬀerent researchers to “benchmark” their algorithms on a per-dataset basis. for
instance, in the machine learning community there are well know data sets (e.g.,
the uci machine learning repository, cmu nn-bench collection, proben1,
statlog, elena-data, etc.) that can be used for testing and comparing diﬀer-
ent techniques. such a process mining repository could be seen as an element in
a possible evaluation framework, and should also provide information about the
process or log characteristics as these may pose special challenges. furthermore,
the results of an evaluation could be stored for later reference.
at the same time it is necessary to be able to inﬂuence both the process
and log characteristics. for example, one might want to generate an event log
containing noise (i.e., distorting the logged information), or a certain timing
behavior (some activities taking more time than others), from a given model. for
log generation, simulation tools such as cpn tools can be used. another example
for log generation is the generation of “forbidden” scenarios as a complement to
the actual execution log.
clearly, many diﬀerent approaches for evaluation and comparison of the dis-
covered process models are possible. as a ﬁrst step, in [11] we have looked at
existing evaluation techniques both in the process mining and data mining do-
main.
54 conclusion
adequate validation techniques in the process mining domain are needed to eval-
uate and compare discovered process models both in research and practice. many
obstacles such as bridging the gap between diﬀerent modeling languages, deﬁn-
ing good validation criteria and metrics for the quality of a process model etc.
remain, and should be subject to further research. moreover, a comprehensive
set of benchmark examples is needed.
references
1.w.m.p. van der aalst, h.a. reijers, a.j.m.m. weijters, b.f. van dongen, a.k.
alves de medeiros, m. song, and h.m.w. verbeek. business process mining: an
industrial application. information systems , 32(5):713–732, 2007.
2.w.m.p. van der aalst, v. rubin, b.f. van dongen, e. kindler, and c.w. g¨ unther.
process mining: a two-step approach using transition systems and regions.
bpm center report bpm-06-30, bpmcenter.org, 2006.
3.w.m.p. van der aalst, b.f. van dongen, j. herbst, l. maruster, g. schimm, and
a.j.m.m. weijters. workﬂow mining: a survey of issues and approaches. data
and knowledge engineering , 47(2):237–267, 2003.
4.w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128–1142, 2004.
5.r. agrawal, d. gunopulos, and f. leymann. mining process models from work-
ﬂow logs. in sixth international conference on extending database technology ,
pages 469–483, 1998.
6.j.e. cook and a.l. wolf. discovering models of software processes from event-
based data. acm transactions on software engineering and methodology ,
7(3):215–249, 1998.
7.j.e. cook and a.l. wolf. software process validation: quantitatively measuring
the correspondence of a process to a model. acm transactions on software
engineering and methodology , 8(2):147–176, 1999.
8.a.k. alves de medeiros. genetic process mining . phd thesis, eindhoven univer-
sity of technology, eindhoven, 2006.
9.j. herbst. a machine learning approach to workﬂow management. in proceedings
11th european conference on machine learning , volume 1810 of lecture notes in
computer science , pages 183–194. springer-verlag, berlin, 2000.
10.a. rozinat and w.m.p. van der aalst. conformance checking of processes based
on monitoring real behavior. accepted for publication in information systems:
doi 10.1016/j.is.2007.07.001 .
11.a. rozinat, a.k. alves de medeiros, c.w. g¨ unther, a.j.m.m. weijters, and
w.m.p. van der aalst. towards an evaluation framework for process mining
algorithms. bpm center report bpm-07-06, bpmcenter.org, 2007.
12.a.j.m.m. weijters and w.m.p. van der aalst. rediscovering workﬂow models
from event-based data using little thumb. integrated computer-aided engi-
neering , 10(2):151–162, 2003.
13.l. wen, j. wang, and j.g. sun. detecting implicit dependencies between tasks
from event logs. in asia-paciﬁc web conference on frontiers of www research
and development (apweb 2006) , lecture notes in computer science, pages 591–
603. springer, 2006.
6