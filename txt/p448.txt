software tools for technology transfer manuscript no.
(will be inserted by the editor)
discovering colored petri nets from event logs
a. rozinat⋆and r.s. mans and m. song⋆⋆and w.m.p. van der aalst
eindhoven university of technology, p.o. box 513, nl-5600 mb, eindhoven, the netherlands
e-mail: {a.rozinat,r.s.mans,m.s.song,w.m.p.v.d.aalst }@tue.nl
received: date / revised version: date
abstract. process-aware information systems typically
log events (e.g., in transaction logs or audit trails) re-
lated to the actual execution of business processes. anal-
ysis of these execution logs may reveal important knowl-
edge that can help organizations to improve the quality
of their services. starting from a process model, which
can be discovered by conventional process mining al-
gorithms, we analyze how data attributes inﬂuence the
choices made in the process based on past process execu-
tions using decision mining, also referred to as decision
point analysis. in this paper we describe how the re-
sulting model (including the discovered data dependen-
cies) can be represented as a colored petri net (cpn),
and how further perspectives, such as the performance
and organizational perspective, can be incorporated. we
also present a cpn tools export plug-in implemented
within the prom framework. using this plug-in, simula-
tion models in prom obtained via a combination of var-
ious process mining techniques can be exported to cpn
tools. we believe that the combination of automatic dis-
covery of process models using prom and the simulation
capabilities of cpn tools oﬀers an innovative way to im-
prove business processes . the discovered process model
describes reality better than most hand-crafted simula-
tion models. moreover, the simulation models are con-
structed in such a way that it is easy to explore various
redesigns.
1 introduction
process mining has proven to be a valuable approach
that provides new and objective insights into the way
⋆supported by the iop program of the dutch ministry of eco-
nomic aﬀairs.
⋆⋆supported by the technology foundation stw.business processes are really handled within organiza-
tions. taking a set of real process executions (the so-
called “event logs”) as the starting point, these tech-
niques can be used for process discovery and confor-
mance checking . process discovery [5,7] can be used to
automatically construct a process model reﬂecting the
behavior that has been observed and recorded in the
event log. conformance checking [1,22] can be used to
compare the recorded behavior with some already ex-
isting process model to detect possible deviations. both
may serve as input for designing and improving busi-
ness processes, e.g., conformance checking can be used
to ﬁnd problems in existing processes, and process dis-
covery can be used as a starting point for process analysis
and system conﬁguration. while there are several pro-
cess mining algorithms that deal with the control ﬂow
perspective of a business process [5] less attention has
been paid to how data attributes aﬀect the routing of a
case. classical process mining approaches consider all
choices to be non-deterministic. the approach presented
in this paper investigates how values of data attributes
inﬂuence particular choices in the model.
most information systems (cf. wfm, erp, crm,
scm, and b2b systems) provide some kind of event log
(also referred to as transaction log or audit trail) [5]
where an event refers to a case (i.e., process instance)
and an activity, and, in most systems, also a time stamp,
a performer, and some additional data. nevertheless,
many process mining techniques only make use of the
ﬁrst two attributes to construct a process model which
reﬂects the causal relations that were observed among
the activities. in this paper we start from a discovered
process model (i.e., a model discovered by conventional
process mining algorithms), and we try to enhance the
model by integrating patterns that can be observed from
data modiﬁcations, i.e., a decision point analysis [24] will
be carried out to ﬁnd out which properties (i.e., valua-
tions of data attributes) of a case might lead to takingcertain paths in the process. colored petri nets (cpns)
[16,17] are used as a representation for the enhanced
model because of their expressiveness and the good tool
support provided through cpn tools [27] (for example,
we will show how cpn tools enables simulation-based
performance analysis of the discovered model). further-
more, the hierarchy concept allows for the composition
of a cpn model in a modular way. the time concept and
the availability of many probability distributions in cpn
tools allow for the modeling of performance aspects.
moreover, by introducing resource tokens, also organi-
zational and work distribution aspects can be modeled.
figure 1 illustrates the overall approach. first of all,
some process mining algorithm is used to discover a pro-
cess model in terms of a petri net (e.g., the α-algorithm
[7]). note that conventional process mining techniques
(e.g., based on the α-algorithm) only use the ﬁrst two
columns of the event log depicted in figure 1. however,
the event log may also contain information about the
people performing activities (cf. originator column), the
timing of these activities (cf. time stamp column), and
the data involved (cf. data column). in the next step we
make use of the additional information, the data column
to be precise. the decision miner uses this information
to discover rules for taking alternative paths based on
values of the data attributes present in the process. the
enhanced model may be extended with additional in-
formation about time and resources. this information
may be manually included or is extracted from the log
based on the time stamp column and originator column.
note that in this paper we focus on the cpn representa-
tion of the enhanced model rather than on the discovery
techniques for the diﬀerent perspectives. therefore, de-
cision mining is only one example of how a model can
be enhanced by extracting additional information from
the log. many other techniques (e.g., role discovery [3],
or the extraction of timing information as described in
[15]) are possible. finally, the process model including
the data (and time and resource) perspective is exported
as a cpn model.
we argue that the discovered process model describes
reality better than most hand-crafted simulation models
because it is based on objective information rather than
on perceptions of people (i.e., there is no modeler bias).
of course, such a generated model does not make domain
knowledge and modeling expertise obsolete. however, in
this paper we propose a method that—in contrast to
the manual creation of a model— can be easily repeated
in an iterative manner as soon as the process changes .
no modeling eﬀorts are needed to generate an initial
model, which can be further evaluated, and potentially
modiﬁed.
to directly support the generation of a cpn model
for business processes we have implemented a cpn tools
2.0 export plug-in (in the remainder of this paper re-
ferred to as cpn export plug-in) in the context of theprom framework1. the prom framework oﬀers a wide
range of tools related to process mining and process
analysis, and the cpn export plug-in in prom fully au-
tomatically generates the cpn models discovered using
process mining. note that we have applied our process
mining techniques to many real-life logs from, e.g., hospi-
tals, banks, municipalities etc. (see [4,25] for two exam-
ples). for structured processes this works well. however,
for more chaotic processes it is diﬃcult to produce mod-
els that are easy to interpret and analyze. for some of
the case studies we constructed cpn models. however,
a discussion of these case studies is outside the scope of
this paper.
the paper is organized as follows. first, related work
is discussed in section 2. then, section 3 introduces
a simple example process that is used throughout the
paper. afterwards, the decision mining approach is ex-
plained brieﬂy in section 4. in section 5, we describe
how a business process (including multiple perspectives)
can be represented as a cpn. then, section 6 presents
thecpn export plug-in of the prom framework, and
section 7 shows how the generated cpn models can be
simulated and analyzed in cpn tools. finally, the paper
concludes by discussing directions for future research.
2 related work
the work reported in this paper is related to earlier
work on process mining, i.e., discovering a process model
based on some event log. the idea of applying process
mining in the context of workﬂow management was ﬁrst
introduced in [8]. cook and wolf have investigated simi-
lar issues in the context of software engineering processes
using diﬀerent approaches [9]. herbst and karagiannis
also address the issue of process mining in the context
of workﬂow management using an inductive approach
[14]. they use stochastic task graphs as an intermediate
representation and generate a workﬂow model described
in the adonis modeling language. alternatively, there
are several variants of the αalgorithm [7,28]. in [7] it
is shown that this algorithm can be proven to be cor-
rect for a large class of processes. in [28] a heuristic
approach using rather simple metrics is used to con-
struct so-called “dependency/frequency tables” and “de-
pendency/frequency graphs”. this is used as input for
theαalgorithm. as a result it is possible to tackle the
problem of noise. for more information on process min-
ing we refer to a special issue of computers in industry
on process mining [6] and a survey paper [5]. however, as
far as we know, this is the ﬁrst attempt to mine process
models including other dimensions, such as data. (note
that [3] only considers the social network in isolation and
does not use it to provide an integrated view.)
1both documentation and software (including the source code)
can be downloaded from www.processmining.org .fig. 1. the approach described in this paper
our work on decision mining [24,23] (which we build
upon in this paper) is closely related to [12], in which the
authors describe the architecture of the business process
intelligence (bpi) tool suite situated on top of the hp
process manager (hppm). whereas they outline the use
of data mining techniques for process behavior analysis
in a broader scope, we show how a decision point analysis
can be carried out in conjunction with process mining,
i.e., we do not assume some a priori model.
in [18] cpns have been used to develop a prototype
computer tool for task scheduling. a cpn model of the
particular planning domain was created, and a simula-
tion image of this model was extracted and directly used
in the tool to calculate schedules (based on state space
exploration algorithms), thereby automatically bridging
the gap between the formal speciﬁcation and its imple-
mentation. while a domain-speciﬁc graphical user inter-
face (gui) enables planners to modify parameters (i.e.,
the initial state of the cpn model), the structure of the
cpn remains unchanged (and hidden to the planner).
with the cpn export plug-in presented in this paper,
we support the generation of cpn models for arbitrary
business processes in a variety of conﬁgurations, which
results in very diﬀerently structured models.
in [11] a translation of protos simulation models to
cpn tools is presented. in addition, three types of data
collector monitors (measuring the total ﬂow time per
case, the waiting time per task, and the resource avail-
ability/utilization per resource type), and conﬁguration
features enabling the dynamic elimination of unneces-
sary parts of the process model are generated. besides
the work in [11], we are not aware of further attempts to
export business process models to cpn tools. the workreported in this paper has a diﬀerent starting point as
it is not limited by the simulation information present
in a protos model, but aims at discovering the process
characteristics to be simulated from the event logs of real
process executions.
this paper is based on a paper for the cpn workshop
[26]. (the best papers of this workshop were selected for
this special section.)
3 running example
as pointed out in figure 1, the ﬁrst step in the decision
mining process is to obtain a process model without data
through some classical process miner , e.g., a petri net
discovered using the α-algorithm. figure 2(a) shows an
event log in a schematic way, i.e., as a set of event traces.
note that this information can be extracted from the
ﬁrst two columns of the event log shown in figure 1.
based on this information the α-algorithm automatically
constructs the process model shown in figure 2(b).
the example process used throughout the paper out-
lines the processing of a liability claim within an insur-
ance company: ﬁrst, some data related to the claim is
registered (cf. activity ain figure 2), and then either a
full check or a policy-only check is performed ( borc).
afterwards, the claim will be evaluated ( d), and then it
is either rejected ( f) or approved ( eandg). finally,
the case is archived and closed ( h).
now, we have discovered the control ﬂow perspective
of the process. but the process execution log contains
much more valuable information. to generate a simula-
tion model that reﬂects as close as possible the processfig. 2. process mining phase: based on a set of log traces a process model is constructed
fig. 3. fragment of the whole example log in mxml format viewed using xml spy
that has been observed, data attributes, time stamps,
and originator information can be analyzed to reveal
characteristics related to the data,performance , and or-
ganizational perspectives. figure 3 depicts a screenshot
of the event log in mxml2format. a process log in
mxml contains several process instances (i.e., cases),
whereas each process instance contains a number of au-
dit trail entries (i.e., events). the depicted screenshot
shows the details about one of the process instances (cf.
dotted oval in figure 3), which contains six audit trail
entries. in the following we will have a closer look at
which information can be found in the log, considering
the perspectives mentioned.
(a)data perspective . here a data attribute within
an audit trail entry (i.e., an event) is interpreted as a
case data attribute that has been created, or modiﬁed.
in the example log one can observe that only activi-
tiesregister claim andevaluate claim have associated
data attributes (cf. the two bold ovals in figure 3). dur-
ing the execution of activity register claim information
2both the corresponding schema deﬁnition and the prom import
framework [13], which converts logs from a wide variety of sys-
tems to the xml format used by prom, can be downloaded from
www.processmining.org .about the amount of money involved ( amount ), the cor-
responding customer ( customerid ), and the type of pol-
icy (policytype ) are provided, while after handling the
activity evaluate claim the outcome of the evaluation is
recorded ( status ). semantically, the amount attribute is
a numerical attribute, customerid is an attribute which
is unique for each customer, and both policytype and
status are enumeration types (being either “normal” or
“premium”, or either “approved” or “rejected”, respec-
tively).
(b)performance perspective . in the example, for sim-
plicity, activities are considered as being atomic and
carry no time information. however, information sys-
tems dealing with processes typically log events on a
more ﬁne-grained level, e.g., they may record schedule ,
start, and complete events (including time stamps) for
each activity. thus, time information can be used to in-
fer, e.g., activity durations, or the arrival rate of new
cases. furthermore, the frequency of alternative paths
represents quantitative information that is implicitly con-
tained in the event log. for example, the event log shown
in figure 3 contains in total 10 process instances, of
which 7 executed activity check policy only and only
3 performed the full check procedure check all .(c)organizational perspective . in figure 3 one can
observe an event carrying information about the resource
that executed the activity. often, resources are people.
however, in principle a resource can be anything (for
example, an application performing automated tasks in
the process). in the whole insurance claim handling ex-
ample process (i.e., considering all the 10 cases), 7 diﬀer-
ent persons have worked together: howard, fred, mona,
vincent, robert, linda, and john.
as illustrated in figure 1, the discovered process
model and the detailed log are the starting point for
thedecision miner , which analyzes the data perspective
of the process to discover data dependencies that inﬂu-
ence the routing of a case. the idea of decision mining is
brieﬂy explained in the next section (see [23] for further
details), and implemented as a plug-in in prom. the
decision miner constructs an enhanced model incorpo-
rating the data perspective (highlighted by the depicted
decision rule in figure 1) and passes this on to the
cpn export . however, in addition to the control-ﬂow
and data perspective the enhanced model may also con-
tain information about probabilities, time, and resources
(i.e., the performance and organizational perspectives).
the representation of all these perspectives in terms of
a generic cpn model, and the capabilities of the cpn
export plug-in in prom, are described in section 5 and
section 6.
4 decision mining
to analyze the choices in a business process we ﬁrst
need to identify those parts of the model where the
process splits into alternative branches, also called de-
cision points . based on data attributes associated with
the cases in the event log we subsequently want to ﬁnd
rules for following one route or the other [24].
in terms of a petri net, a decision point corresponds
to a place with multiple outgoing arcs. since a token can
only be consumed by one of the transitions connected to
these arcs, alternative paths may be taken during the
execution of a process instance. the process model in
figure 2(b) exhibits three such decision points: p0(if
there is a token, either borccan be performed), p2
(seen from this place, either eorfcan be executed) and
p3(seen from this place, either forgmay be carried
out). the idea is to convert every decision point into a
classiﬁcation problem [20,21,29], where the classes are
the diﬀerent decisions that can be made. as training ex-
amples we use the process instances in the log (for which
it is already known which alternative path they followed
with respect to the decision point). the attributes to be
analyzed are the case data attributes contained in the
log, and we assume that all attributes that have been
fig. 4. enhanced process model
written before the choice construct under consideration
are relevant for the routing of a case at that point3.
however, because there is no explicit information in
the log about which decision was made at a decision
point for some process instance, we ﬁrst have to infer
this information from the log. starting from the identi-
ﬁcation of a choice in the process model (i.e., a decision
point) a decision can be detected if the execution of an
activity in the respective alternative branch of the model
has been observed, which requires a mapping from that
activity to its “occurrence footprint” in the event log. so,
if a process instance contains the given “footprint”, this
means that there was a decision for the associated al-
ternative path in the process. for simplicity we examine
the occurrence of the ﬁrstactivity per alternative branch
to classify the possible decisions. however, to make de-
cision mining operational for real-life business processes
several challenges posed by, for example, invisible activ-
ities,duplicate activities , and loops need to be met. we
refer the interested reader to our technical report [23],
where these issues are addressed in detail.
after identifying a decision point in a business pro-
cess and classifying the decisions of all the process in-
stances in the log, the next step is to determine whether
decisions might be inﬂuenced by case data, i.e., whether
cases with certain properties typically follow a speciﬁc
route. to solve the formulated classiﬁcation problem,
various algorithms are available [20,29]. we decided to
use an algorithm based on decision trees (the c4.5 al-
gorithm [21] to be precise). decision trees are a popular
tool for inductive inference and the corresponding algo-
rithms have been extended in various ways to improve
practical applicability. for example, they are able to deal
with continuous-valued attributes, missing attribute val-
ues, and they include eﬀective methods to avoid over-
ﬁtting the data (i.e., that the tree is too much tailored
towards the particular training examples).
3we also allow the user to set other scoping rules, e.g., only the
data set in a directly preceding activity, or all case data including
the data that is set later.using decision point analysis we can extract knowl-
edge about decision rules as shown in figure 4. each of
the three discovered decision points corresponds to one
of the choices in the running example. with respect to
decision point p0the extensive check (activity b) is only
performed if the amount is greater than 500 and the pol-
icytype is “normal”, whereas a simpler coverage check
(activity c) is suﬃcient if the amount is smaller than
or equal to 500, or the policytype is “premium” (which
may be due to certain guarantees given by “premium”
member corporations). the two choices at decision point
p2andp3are both guided by the status attribute, which
is the outcome of the evaluation activity (activity d).
now that we have automatically discovered a model
integrating both the control-ﬂow and data perspective of
the example process, we describe how this information
(and information about the performance and organiza-
tional perspective) can be represented in a cpn model
(section 5), and show how such a cpn model can be
generated in prom (section 6). recall that we have also
developed mining techniques for discovering these addi-
tional perspectives [3,15]. however, a detailed descrip-
tion is beyond the scope of this paper.
5 cpn model of a business process
the goal is to deﬁne a cpn representation that can be
used to capture process characteristics as, e.g., obtained
via process mining techniques. to ensure practical ap-
plicability, the chosen cpn model must be generic and
suitable for automatic generation. at the same time it
must be readable to a human analyst to enable further
evaluation and manipulation of the model. here, the hi-
erarchy concept helps to create understandable compo-
nents, and separate diﬀerent layers of abstraction.
in the following we illustrate diﬀerent perspectives in
isolation with the help of the insurance claim handling
example. however, these perspectives can be combined
and complement each other (for example, the model may
contain both data and time information). in our cpn
export plug-in in prom, the user can select options
and/or provide information that will aﬀect the gener-
ation of the cpn. section 6 describes these options in
more detail.
5.1 general structure
since we want to make use of the simulation facilities
of cpn tools, we provide the actual process model to-
gether with a simulation environment (amongst others,
to generate arriving cases and to measure performance
indicators). the top-level page in the hierarchical cpn
model is shown in figure 5(a). for each process model
this page will look identical; the environment generatescases and puts them into the start place. finally, it re-
moves those that have reached the end place. we as-
sume that the discovered process—represented by the
sub-page process —is sound, i.e., any case that enters the
sub-page via place start leaves the sub-page via place
end.
figure 5(b) depicts the simulation environment in
more detail. one can observe that the case id color
set is used to refer to particular process instances (i.e.,
cases). to give each case a unique id a counter is sim-
ply incremented for each generated process instance. for
the data perspective, a separate token containing the
case id and a record of case data attributes (deﬁned via
the data color set) is created and initialized. the ini-
tial values represent default values for data attributes
until they are explicitly speciﬁed. for example, it is de-
sirable to set the status attribute initially to “rejected”
so that—if for some reason the evaluation activity was
skipped—a claim could not be automatically approved.
note that the initial values cannot be automatically dis-
covered from our example log, but they can be speciﬁed
in the cpn export in prom (cf. section 6)4. the place
case data is modeled as a fusion place as activities may
need to inspect or modify data attribute values on dif-
ferent pages in the hierarchical model. furthermore, the
resources fusion place contains the available resources
for the process, and therefore determines the environ-
ment from an organizational perspective. finally, each
time a token is put back in the next case id place a time
delay5is added to it, which is used to control the gener-
ation of new cases. in figure 5(b) a constant time delay
of 3 is used to realize an arrival process where every 3
time units a new case arrives. note that the inter-arrival
times may also be sampled from some probability distri-
bution discovered by prom (e.g., a negative exponential
delay to realize a poisson arrival process).
figure 5(c) shows the sub-page containing the actual
process model, which looks exactly like the original, low-
level petri net. note that the tokens routed from the
start to the end place are of type case id, so that
tokens belonging to diﬀerent instances are not mixed
up.
every activity on the process page has its own sub-
page containing the actual simulation information. de-
pending on the covered perspectives (and their conﬁgu-
ration) these activity sub-pages may look very diﬀerent.
in the remainder of this section we will present how cer-
tain process characteristics can be represented in terms
of a cpn sub-page.
4however, if the initial values would be somehow visible in the
event log, we could convert this into an initialization event and
thus discover the initial probability distribution.
5note that in our simulation model the time delay is always
attached to an arc (depending on the token that should be delayed)
rather than using the time delay of a transition to avoid side eﬀects
on other tokens that should actually not be delayed (such as the
case data token).fig. 5. hierarchical structure of cpn model
5.2 data
taking the enhanced model from figure 4 as the starting
point, we now want to incorporate the discovered data
dependencies in the simulation model. the discovered
decision rules are based on attributes provided by activ-
ityregister claim andevaluate claim respectively (see
the result described in section 4). since the attribute
customerid is not involved in the discovered rules, we
discard it from the process model and deﬁne process-
speciﬁc data types for each of the remaining attributes
(i.e., amount, policytype, and status).
figure 6 shows how the provision of case data can be
simulated using random values. while a random value
for a nominal attribute can be generated by applying the
ran() function directly to the color set6, a dedicated ran-
dom function is needed to simulate numeric attributes.
in the action part of transition register claim complete
the function policytype.ran() is used to randomly
select the policy type (“normal” or “premium”). fur-
thermore, a function randomamount() was declared to
generate a random amount . in this case, the amount is
sampled from a discrete distribution generating a value
between the lowest and the highest attribute value ob-
served in the event log. however, many other settings
6note that—for performance reasons—the ran() function can
only be used for enumerated color sets with less than 100 elements.are possible. at the end the modiﬁed data values are
stored in the corresponding case data token (manipu-
lating the corresponding entries using the setfunctions
on the data record color set).
figure 7 shows how the discovered data dependencies
can then be modeled with the help of transition guards.
if the transition is enabled from a control-ﬂow perspec-
tive, it additionally needs to satisfy the given guard con-
dition to be ﬁred. note that the sub-page of activity
“archive claim” is not depicted here as it neither pro-
vides nor depends on a data attribute.
5.3 time
although there is no time information in the example
event log, we want to include the time dimension in our
simulation model because it is relatively easy to extract
from most real-life logs. moreover, this perspective is of
utmost importance from a practical point of view. to
explain this perspective we assume that—in contrast to
the policy-only check, which takes between 3 and 8 time
units—the full check procedure needs between 9 and 17
time units to complete. furthermore, the time between
the point where the activity could have been started (i.e.,
all required previous activities were completed) and the
point where someone actually starts working on it may
vary from 3 to 5 time units. whereas the sub-page shown
in figure 7(a) models the activity check all in an atomicfig. 6. writing data attributes using random values
fig. 7. modeling data dependencies using transition guardsfig. 8. diﬀerent variants of modeling time on sub-page check all
depending on the event types (i.e., schedule, start, and complete)
present in the log
way, one can distinguish between schedule ,start, and
complete transitions to incorporate the waiting time and
execution time of this activity. figure 8 shows three ways
to model this for activity check all .
in figure 8(a) only the execution time of the activity
is modeled. when transition check all start is ﬁred, a
token is produced with the indicated time delay. similar
to the case generation scheme in figure 5(b), the token
will remain between 9 and 17 time units in place e(i.e.,
the activity is in the state executing ) before transition
check all complete will ﬁre.
in figure 8(b) both the execution time and the wait-
ing time are explicitly modeled. analogously to the exe-
cution time, the waiting time is realized by a time delay
that forces the token to reside in place w(i.e., the activ-
ity is in the state waiting ) between 3 and 5 time units
before transition check all start will ﬁre.
in figure 8(c) the sum of the waiting time and the
execution time is modeled. this may be useful if no infor-
mation is available about the actual start of an activity,
i.e., only the time when it becomes enabled and when
it is ﬁnished is known. (this is for example the case for
the event log of the staﬀware system.)
5.4 resources
to gain insight into the organizational perspective we
can, for example, analyze the event log with the social
network miner of prom [3]. one possible analysis is to
ﬁnd resources that perform similar work [3], i.e., two
people are linked in the social network if they execute
similar activities. the more similar their execution pro-
ﬁles are, the stronger their relationship. for example,
one can observe that vincent and howard execute a set
of activities which is disjoint from those executed by all
other employees. more precisely, they only execute the
activity issue payment and, therefore, might work, e.g.,
in the finance department of the insurance company.
furthermore, the work of fred and linda seems rather
manager managerc (c,manager)(c,manager)@+discrete(20,80)
cc
evaluate_claim
startevaluate_claim
complete
resources
resources["howard","fred","mona",
"vincent","robert","linda","john"]
resourcee
case_idxmanager p3
outp2
out
p1
inin
resourcescase_idcase_idout
outcase_idfig. 9. sub-page evaluate claim including resource modeling
similar and quite diﬀerent from the other three people;
they are the only people performing the evaluate claim
activity, although they also execute other activities (such
assend rejection letter andarchive claim ). one expla-
nation could be that the activity evaluate claim requires
some manager role, whereas all the remaining activities
can be performed by people having a clerk role.
a simple way to incorporate this information in our
simulation model is to create three groups of resources,
namely finance = {howard, vincent }, manager = {fred,
linda }, and clerk = {fred, linda, john, robert, mona },
and to specify for each activity which kind of resource
is required (if no particular group has been speciﬁed for
an activity, it can be performed by any resource). as
indicated, this resource classiﬁcation can be discovered
semi-automatically7. however, it could also be derived
from some explicit organizational model. figure 9 de-
picts how the fact that activity evaluate claim requires
the role manager is modeled in the corresponding cpn
model. the role is modeled as a separate color set man-
ager, which contains only “linda” and “fred”. be-
cause the variable manager is of type manager, only
the resource token “linda” or “fred” can be consumed
by transition evaluate claim start . as soon as transition
evaluate claim start is ﬁred, the corresponding resource
token resides in the place e, i.e., it is not available for
concurrent executions of further activities, until transi-
tion evaluate claim complete ﬁres and puts the token
back.
5.5 probabilities and frequencies
closely related to the modeling of time aspects is the
likelihood of taking a speciﬁc path. both may be of a
stochastic nature, i.e., a time duration may be sampled
from some probability distribution, and similarly, the se-
lection of an alternative branch may be selected ran-
domly (if there are no data attributes clearly inﬂuencing
the choice). hence, the probabilistic selection of a path
also needs to be incorporated in the cpn model. fig-
ure 10 shows how often each arc in the model has been
used, determined through the log replay analysis carried
7note that the name of a group cannot be automatically dis-
covered from the log, but it can be speciﬁed in the cpn export in
prom (cf. section 6).fig. 10. frequencies of alternative paths in the example model
out by the conformance checker in prom8. looking at
the ﬁrst choice it becomes clear that activity check policy
only has been executed 7 (out of 10) times and activity
check all was performed only 3 times. similarly, activity
send rejection letter happened for 4 (out of 10) cases,
while in 6 cases both activity send approval letter and
activity issue payment were executed.
to reﬂect frequencies of alternative paths in the simu-
lation model we use two diﬀerent approaches, depending
on the nature of the choice.
simple choice the ﬁrst choice construct in the exam-
ple model is considered to be a so-called simple choice
as it is only represented by a single place and multi-
ple output transitions. we can model such a simple
choice using a probability token that is shared among
all the activities involved in this choice via a fusion
place.
figure 11 shows this solution for the choice at place
p0. both sub-pages check all andcheck policy only con-
tain a fusion place p0probability that initially contains
a token with a random value between 0 and 99. after
each ﬁring of either transition check all start or tran-
sition check policy only start a new random value be-
tween 0 and 99 is generated. because of the guard con-
dition, the decision at the place p0is then determined
for each case according to the current value of the to-
ken in place p0probability . for example, the transition
check all start needs to bind the variable prob to a value
greater than or equal to 70 to be enabled, which will only
happen in 30% of the cases.
dependent choices the second choice construct in the
example model actually consists of two dependent
choices . this means that the choices represented by
places p2andp3) cannot be considered in isolation;
they need to be coordinated to consistently either
approve or reject a claim. therefore, it is clear that
two dependent choices cannot be controlled properly
by two independently generated probability tokens,
because the cpn model will deadlock as soon as the
values of the probability tokens indicate contrasting
8note that the place names and the markup of the choices have
been added to the diagnostic picture obtained from prom for ex-
planation purposes.
fig. 11. using a probability token for simple choices
decisions (e.g., the probability token in p2indicates
a reject while the other probability token in p3sug-
gests to approve the claim).
figure 12 shows a solution for modeling the depen-
dent choices at place p2andp3. the idea is to increase
the likelihood of choosing a certain activity through ac-
tivity duplication (using the fact that during simulation
in cpn tools all enabled transitions will be ﬁred with an
equal probability). the activity duplication is realized on
an intermediate sub-page (between process and activity
page), which points to multiple instances of the actual
activity sub-page (i.e., the activity sub-page is only mod-
eled once). this way, the observed relative frequency9of
the transitions involved in the dependent choices can
be incorporated in the simulation model (i.e., the more
likely an activity is, the more instances of that activity
are contained on its intermediate sub-page). figure 12(a)
shows an intermediate sub-page for activity issue pay-
ment , where three substitution transitions issue payment
point to diﬀerent instances of the same sub-page issue
payment . figures 12(b) and (c) show similar intermedi-
ate sub-pages for the activities send approval letter (also
9to obtain the relative frequency, the absolute frequency is di-
vided by the greatest common divisor (i.e., 6 /2 = 3 and 4 /2 = 2).fig. 12. modeling dependent choices via activity duplication
duplicated three times) and send rejection letter (dupli-
cated twice).
more advanced (and better scalable) solutions may
seek to detect dependencies between choices, and coor-
dinate them, e.g., via a shared probability token.
5.6 logging and monitoring simulation runs
the cpn models described in this section deal with
time, resources, and data. when running a simulation
in cpn tools we are interested in statistics (e.g. aver-
age, variance, minimum, and maximum) related to (a)
the utilization of resources and (b) the throughput times
of cases during the simulation run. this information can
be automatically obtained (see also section 7) via data
collector monitors as described in the following.
(a)resource utilization. if resources have been spec-
iﬁed for the process, all the available resources are con-
tained in a resources fusion place, which is located on
theenvironment page and on every activity sub-page.
for obtaining statistics about the resource utilization
during the simulation we can deﬁne a marking size mon-
itor[10] for this resources fusion place, which records
the number of available resources plus the current time
(and step) as soon as a resource becomes (un-)available.
(b)throughput time. if the time perspective is cov-
ered, tokens are created with a time stamp. we record
the time stamp of each case’s creation together with
the case id token that is routed through the process.
this way, we can determine the throughput time of a
case by deﬁning a data collector monitor [10] for the
clean up transition on the environment page (cf. fig-
ure 5(b)), which simply calculates the diﬀerence between
the current model time and the start time of a case10,
10because the type of the current model time is inﬁnite integer
and to not lose precision when calculating the diﬀerence between
the current model time and the start time of a case, the model timeand records the throughput time, the end time and end
step for each case.
note that these are only two examples of possible
measures that can be interesting. for example, the cur-
rent run time of a case could be easily determined at any
stage in the process via adding some custom monitor. in
section 7, we brieﬂy demonstrate, how such monitor-
ing components can be used for simulation-based per-
formance analysis of the given model.
moreover, we want to generate process execution logs
for the business process in the cpn model. this can be
very useful for, e.g., the creation of artiﬁcial logs that are
needed to evaluate the performance of process mining
algorithms.
for each ﬁring of a transition on an activity sub-page
an event is logged, which includes case id, the type of
transition (i.e., schedule ,start, orcomplete ), current time
stamp, originator, and additional data (if available). for
generating these process execution logs we use the log-
ging functions that have been described in [19]. how-
ever, in contrast to [19]—where the code segments of
transitions have to be modiﬁed to invoke these logging
functions—we decided to use user deﬁned monitors [10]
to clearly separate the logging from the rest of the sim-
ulation model.
6 exporting cpn models from prom
we are able to generate cpn models as presented in the
previous section (i.e., including simulation environment
and the described monitors) using the cpn tools 2.0
export plug-in in the prom framework11. it accepts a
is mapped onto a string value, i.e., color set start time is
of type string and is used to encode inﬁnite integers.
11note that the layout of the generated models was slightly ad-
justed to improve the readability.fig. 13. cpn tools export settingssimulation model that has been discovered by another
plug-in in prom (for example, the enhanced model in-
cluding data dependencies obtained from the decision
miner plug-in). alternatively, a simple low-level petri
net can be provided (in this case all information must
be provided manually). before the actual export takes
place, the cpn export plug-in allows for the manipula-
tion and conﬁguration of the simulation information in
the model. the following options are available:
figure 13(a) shows the conﬁguration settings , where
the user can choose which dimensions should be included
in the generated cpn model. in fact, although the rele-
vant simulation information may be provided, it will be
ignored if the corresponding conﬁguration option was
not chosen. this way, it is easy to play with diﬀerent
conﬁgurations of the same simulation model. note that
because the organizational part of the model is rather
simple (for example, in reality people may work on mul-
tiple processes at the same time, work only 4 days per
week, etc.), resources tend to be too “eager” to approx-
imate realistic waiting times for a process. therefore, it
is possible to combine the race for resources among con-
current activities with some explicit waiting time (it can
be conﬁgured by the user which percentage of the activ-
ity waiting time should be used for this). furthermore,
one can choose whether resources should be assigned in
a “push” or “pull” mode. in the “push” mode, an ac-
tivity is already assigned to one speciﬁc resource in the
scheduling phase, whereas in the “pull” mode any avail-
able resource that has the required role can start execut-
ing the activity. the monitors described in section 5.6
are automatically generated if the activity logging (i.e.,
mxml logging), throughput time monitor , orresource
availability monitor option is selected, respectively.
in figure 13(b) one can see the process settings ,
where the user can adjust global parameters such as
the case arrival rate, or the interpretation of one time
unit in the cpn model (which is relevant, e.g., for the
mxml logging). the case arrival rate can be automat-
ically discovered by the performance analysis with
petri net plug-in in prom.
figure 13(c) depicts the attribute settings of the pro-
cess. new data attributes can be provided by specifying
their name, type (nominal or numeric), possible values
(a list of string values for a nominal attribute, and some
probability distribution12for a numeric one), and initial
value. note that for our example process the available
data attributes were already discovered by the decision
miner plug-in. we can now choose an appropriate initial
value for each of them. for this particular example it
makes sense to delete the customerid attribute, since
it is not involved in any of the discovered data depen-
dencies.
12the cpn export plug-in supports all probability distributions
currently available in cpn tools that are meaningful for the spec-
iﬁcation of execution times etc., namely constant, binomial, dis-
crete, erlang, exponential, normal, and uniform distribution.in figure 13(d) the resource settings are depicted.
here, one can add groups and resources, and assign re-
sources to groups. this way, the cpn export plug-in
also supports the speciﬁcation of information about re-
sources. this information is then used when creating the
sub-pages shown earlier. note that the organizational
miner in prom can be used to discover groups of re-
sources, which are then called “group1”, “group2” etc. in
theresource settings , a meaningful name for these dis-
covered groups can be provided before the actual cpn
model is generated.
in figure 13(e), a screenshot of the activity settings
for activity evaluate claim is displayed. in this view,
the provided data attributes, the execution time, wait-
ing time, sojourn time, and the required resource group
may be speciﬁed for each of the activities in the pro-
cess. the attached data attributes were already pro-
vided by the decision miner plug-in (recall that if fur-
ther information would have been discovered from the
log, it would be “ﬁlled in” as well), and we can decide
whether the old value of the attribute should be kept
(i.e., reuse) or whether a random value will be gener-
ated (i.e., re-sample). furthermore, we can assign some
execution time (note the discrete distribution between
20 and 80 time units), and choose the suitable group of
resources from the list of groups available in the process
(note the manager role).
figure 13(f) shows the choice conﬁguration view,
where the user can determine for each decision point in
the process whether it should be based on either prob-
abilities or frequencies (cf. section 5.5), or on data at-
tributes, or whether it should not be guided by the sim-
ulation model (one of the alternative paths is then ran-
domly chosen by cpn tools). in figure 13(f) the data
dependency settings are displayed for the choice point
p0. we can see the data-based decision rules which were
discovered and ﬁlled in by the decision miner. in the
current version of the export plug-in such a dependency
value is simply a string containing the condition to be
placed in the transition guard of the corresponding cpn
transition. alternatively, a probability may be provided
between 0.0 and 1.0 for every alternative branch. as dis-
cussed in section 5.5, for dependent choices one should
specify a relative frequency value instead.
to demonstrate what can be done with a generated
cpn model, the following section highlights the simula-
tion capabilities oﬀered by cpn tools.
7 simulation in cpn tools
although it is possible to do a state space analysis of the
model using cpn tools, this is not tractable in most
practical settings. therefore, we exploit the fact that
cpn tools also allows for performance analysis based
on simulation [17]. monitors can be used to collect data
in log ﬁles, create simulation reports, and even generatefig. 14. running automatic simulation replications in cpn tools using the ml function cpn’replications.nreplications n
gnuplot scripts that visualize the collected data from
diﬀerent simulation runs.
we take a cpn model that was automatically gener-
ated by the cpn export plug-in in prom as the starting
point. the model already contains a simulation environ-
ment that generates new cases according to a certain dis-
tribution. let us assume that the case generation scheme
of our insurance claim handling example process follows
an exponential distribution with the intensity value 0 .01.
this corresponds to a mean inter-arrival time of 100 time
units (which we here interpret as minutes). furthermore,
for simplicity, we assume that each activity in the pro-
cess has a constant waiting and execution time of 30 min-
utes; only activity check all has a (constant) execution
time of 60 minutes. the generated cpn model already
contains two monitors for measuring resource utilization
and throughput time as described in section 5.6. how-
ever, it is easy to add further monitoring components.
for example, we additionally want to measure the num-
ber of cases in the process (i.e., the current work load).
for this, we simply select the pre-deﬁned marking size
monitor tool and apply it to the case data place, which
holds one token for each case that is currently handled
by the process.
the model can be readily simulated in cpn tools
for an arbitrary number of steps. in the meantime, nu-
merical data about the resources utilization, throughput
times, and work load is automatically extracted from
the model and recorded in log ﬁles. from this, a per-formance report indicating statistical measures such as
minimum, maximum, and average values is generated,
and conﬁdence intervals can be used to indicate how
precise these estimates of a performance measure are.
however, for statistical validity, independent and identi-
cally distributed (iid) estimates of the performance mea-
sures must be collected. it is clear that, for example the
throughput time of a case is inﬂuenced by other cases in
the process as they ﬁght for the same resources. there-
fore, measures collected from a single simulation run are
notindependent. cpn tools provides support for perfor-
mance analysis using simulation replications of indepen-
dent, terminating simulations, and figure 14 shows how
such simulation replications can be automatically run
using an auxiliary text ﬁeld containing the ml function
cpn’replications.nreplications n, whereas ndeter-
mines the number of replications (here n= 2). note
that to let each simulation run terminate, we limited
the number of cases to be generated (cf. dotted oval in
figure 14).
if we run two such replicated simulations, with each
20 insurance claims being handled by the process, cpn
tools automatically generates gnuplot scripts that can
be used to visualize the data collected by the three mon-
itors. for example, the two top-most graphs in figure 15
visualize the throughput times of ﬁnishing cases and the
number of cases in the process over time for the two
simulation runs. one can see that in the beginning of
the second simulation run there were up to 10 cases pro- 300 320 340 360 380 400 420
 0  500  1000  1500  2000  2500throughput_time_monitor 1
throughput_time_monitor 2
 0 2 4 6 8 10
 0  500  1000  1500  2000  2500marking_size_environment’case_data_1 1
marking_size_environment’case_data_1 2
 1 2 3 4 5 6 7
 0  500  1000  1500  2000  2500marking_size_environment’resources_1 2
 250 300 350 400 450 500
 0  20  40  60  80  100throughput time
simulation no.throughput time statistics for 100 simulation runs
average
minimum
maximum
 309.6 309.8 310 310.2 310.4 310.6 310.8 311
 85  90  95  100  105confidence interval
confidence levelconfidence intervals for average throughput time of 100 replications
confidence intervals
 306 308 310 312 314 316
 0  20  40  60  80  100confidence interval
number of simulations95% confidence intervals for average throughput time
95% confidence intervalsfig. 15. gnuplot graphs that are plotted based on log ﬁles written by data collector monitors in cpn tools
cessed at the same time. furthermore, the throughput
times of ﬁnishing cases were considerably higher in the
ﬁrst phase of the second simulation, which hints that
cases were delayed because of the unavailability of re-
sources. this can also be observed in the middle-left
graph in figure 15, which displays the number of re-
sources that were available over time for the second sim-
ulation run only.while this gives an idea of the developments within
a single simulation run, we are also interested in gen-
eral statistics, e.g., about the throughput times of cases.
for this, we need a larger data basis and, therefore, we
run 100 simulations with each 200 cases being handled
by the process. figure 16(a) shows a screenshot of the
performance output options in cpn tools, where one
can select the measures to be calculated. after the sim-fig. 16. performance reports are automatically generated by cpn tools
ulation has ﬁnished, a replication performance report
as depicted in figure 16(b) is automatically generated
by cpn tools. for example, from the data collected
by the throughput time monitor, the average minimum
(min iid), maximum ( max iid), accumulated ( sum iid),
and average ( avrg iid) values are calculated for the 100
simulation runs. the number of samples taken ( count iid)
is equal for all the replicated simulations as 200 cases are
the stop criteria to ﬁnish a simulation run.
in addition to the performance report, which pro-
vides an overview of the performance measures from
the replicated simulations, cpn tools also records the
statistics for each of the simulation runs in log ﬁles. al-
though for this there is no gnuplot script generated by
cpn tools, we can easily plot the developments of, e.g.,
average, minimum and maximum values based on these
log ﬁles with the help of a custom gnuplot script as
shown in the middle-right graph in figure 15 (in our cus-
tom gnuplot script we now also added meaningful labels
for the axes). one can see that compared to the maxi-
mum throughput time, the average throughput time is
relatively stable across the 100 simulations. the mini-
mum throughput time is 300 minutes for every single
simulation run, which can be explained by the chosen
constant waiting and execution time for the activities
in the model. if at the end of the process the insurance
claim is rejected, ﬁve activities are executed for this case.if there was no delay in processing the case (due to the
unavailability of resources), and only a policy check was
performed, then the throughput time is exactly 300 min-
utes13.
the replication performance report reﬂects the pre-
cision of a performance measure over the diﬀerent simu-
lation runs using conﬁdence intervals. for example, fig-
ure 16 highlights the 90%, 95%, and 99% conﬁdence in-
tervals for the average throughput time, e.g., [309.96,
310.39] is the 90% conﬁdence interval. similar to the
observations from the middle-right graph in figure 15,
one can see that—compared to the average throughput
time conﬁdence intervals—the conﬁdence intervals for
the maximum throughput time are much bigger, and for
the minimum throughput time they have length 0.
in addition to the performance report itself, the con-
ﬁdence intervals for the various performance measures
are also recorded in conﬁdence interval ﬁles. again, these
data can be used to create custom gnuplot graphs vi-
sualizing, e.g., the 90%, 95%, and 99% conﬁdence in-
tervals for the average throughput time as depicted in
the bottom-left of figure 15. the bottom-right graph in
figure 15 demonstrates the eﬀect of the number of repli-
13note that the constant execution and waiting time was chosen
for demonstration purposes. however, to approximate a real-life
situation one would rather discover, e.g., a service time distribu-
tion from the time information in the event log that was used to
discover the simulation model.table 1. eﬀect of the individual modiﬁcations on the average
throughput times (95% conﬁdence interval)
cpn model average throughput times
initial model [309.92, 310.43]
(1) always full check [333.36, 333.94]
(2) twice as many resources [307.72, 308.13]
(3) increased case load [3400.25, 3446.39]
(4) sequentialized tasks [339.67, 340.53]
cations (here 3, 5, 10, 50, and 100 replications were run
during a simulation) on both the estimate of the per-
formance measure as well as the conﬁdence intervals. in
the graph of figure 15 95% conﬁdence intervals for the
average throughput time are plotted, and one can see
that the estimated performance measures become more
accurate with an increasing number of replications.
simulation-based performance analysis as shown in
this section can be used to explore “what if” scenarios.
for example, one could predict the ﬂow times for alter-
native conﬁgurations (e.g., less specialists for a certain
task) or re-designs of the process under consideration.
in the following, we demonstrate the eﬀect of a few sim-
ple modiﬁcations in the context of the running example:
(1) we removed the option to do a partial check (i.e.,
check policy only ), which takes 30 minutes, and always
perform the full check (i.e., check all ), which takes 60
minutes, (2) we assumed that we have exactly twice as
many resources of each type (i.e., there are two “johns”,
two “lindas” etc.), (3) we increased the case load from
a mean inter-arrival time of 100 minutes to 10 minutes
(i.e., now on average every 10 minutes a new insurance
claim arrives at the process), and (4) we sequential-
ized the tasks send approval letter andissue payment to
make sure the letter is sent before the payment is made,
as a courtesy to the claimant. table 1 shows the eﬀect of
these individual modiﬁcations with respect to the initial
model (i.e., as previously described in this section). for
every cpn model, we run 100 simulation replications for
200 cases, and give the 95% conﬁdence interval of the
average throughput times of the (modiﬁed) process. as
can be expected, from the four alternative scenarios only
the increase of resources leads to decreased throughput
times compared to the initial model.
note that in this section we have treated our insur-
ance claim handling process as a terminating process
[17], i.e., we have created a number of ﬁnite simulation
runs to evaluate how the process behaves when handling
a certain number of cases. similarly, one could also eval-
uate the performance during a certain period of time,
for example, a work day at the insurance company (ter-
minating the process after 8 hours of model time). via
simulating terminating systems we seek to study the
transient behavior of the system. for this, cpn tools
provides support using simulation replications of inde-
pendent, terminating simulations. in a non-terminating
system, the system’s behavior is not limited to sometime window. one could think of the insurance claim
handling process also as a non-terminating process han-
dling continuously incoming cases. such simulations are
typically used to investigate the long-term behavior of a
system ( steady-state behavior analysis). see [17] for fur-
ther information on this topic. generally, it is important
to align the goals of a simulation study with the ex-
perimental design. systematic decisions are required to
ensure validity and to produce useful results. however,
this is beyond the scope of this paper.
8 conclusion
in this paper, we have shown that it is possible to dis-
cover process models with data from event logs. fur-
thermore, we have presented a cpn representation that
captures a business process from multiple perspectives
(i.e., data, performance, and organizational perspective),
and which can be automatically generated using the cpn
export plug-in in prom. finally, we demonstrated how
such generated simulation models can be analyzed in
cpn tools.
future work includes the reﬁnement of the gener-
ated cpn models. for example, a more realistic resource
modeling scheme may allow for the speciﬁcation of a
working scheme per resource (e.g., whether the person
works half-time or full-time) and include diﬀerent al-
location mechanisms. moreover, we plan to apply our
approach to real-life data. for this, the discovery of fur-
ther perspectives of a business process will be integrated
in the mined process models. note that existing plug-
ins in prom deliver also time-related characteristics of
a process (such as the case arrival scheme, and execu-
tion and waiting times) and frequencies of alternative
paths, or organizational characteristics (such as the roles
of the employees involved in the process). all these dif-
ferent pieces of aggregate information (discovered from
the event log) can then be combined in one comprehen-
sive simulation model, which may be exported to cpn
tools, or, e.g., translated to an executable yawl model
[2]. note that a yawl model can be used to enact a busi-
ness process using the yawl workﬂow engine. for en-
actment all perspectives play a role and need to be taken
into account. hence, successfully exporting to yawl is
another interesting test case for the mining of process
models with data and resource information.
acknowledgements
this research is supported by the technology founda-
tion stw, eit, super, nwo, and the iop program
of the dutch ministry of economic aﬀairs. furthermore,
the authors would like to thank all prom developers for
their on-going work on process mining techniques. we
would also like to thank lisa wells and kurt jensen for
their support in using cpn tools.references
1. w.m.p. van der aalst. business alignment: using pro-
cess mining as a tool for delta analysis. in j. grund-
spenkis and m. kirikova, editors, proceedings of the 5th
workshop on business process modeling, development
and support (bpmds’04) , volume 2 of caise’04 work-
shops , pages 138–145. riga technical university, 2004.
2. w.m.p. van der aalst and a.h.m. ter hofstede. yawl:
yet another workﬂow language. information systems ,
30(4):245–275, 2005.
3. w.m.p. van der aalst, h.a. reijers, and m. song. dis-
covering social networks from event logs. computer
supported cooperative work , 14(6):549–593, 2005.
4. w.m.p. van der aalst, h.a. reijers, a.j.m.m. weijters,
b.f. van dongen, a.k. alves de medeiros, m. song, and
h.m.w. verbeek. business process mining: an industrial
application. information systems , 32(5):713–732, 2007.
5. w.m.p. van der aalst, b.f. van dongen, j. herbst,
l. maruster, g. schimm, and a.j.m.m. weijters. work-
ﬂow mining: a survey of issues and approaches. data
and knowledge engineering , 47(2):237–267, 2003.
6. w.m.p. van der aalst and a.j.m.m. weijters, editors.
process mining , special issue of computers in industry,
volume 53, number 3. elsevier science publishers, am-
sterdam, 2004.
7. w.m.p. van der aalst, a.j.m.m. weijters, and
l. maruster. workﬂow mining: discovering process
models from event logs. ieee transactions on knowl-
edge and data engineering , 16(9):1128–1142, 2004.
8. r. agrawal, d. gunopulos, and f. leymann. mining
process models from workﬂow logs. in sixth inter-
national conference on extending database technology ,
pages 469–483, 1998.
9. j.e. cook and a.l. wolf. discovering models of software
processes from event-based data. acm transactions
on software engineering and methodology , 7(3):215–249,
1998.
10. cpn group, aarhus, denmark. cpn tools help pages.
http://wiki.daimi.au.dk/cpntools-help/.
11. f. gottschalk, w.m.p. van der aalst, m.h. jansen-
vullers, and h.m.w. verbeek. protos2cpn: using col-
ored petri nets for conﬁguring and testing business
processes. in k. jensen, editor, proceedings of the
seventh workshop on the practical use of coloured
petri nets and cpn tools , pages 137–156. university
of aarhus, denmark, 2006.
12. d. grigori, f. casati, m. castellanos, u. dayal, m. sayal,
and m.-c. shan. business process intelligence. comput-
ers in industry , 53(3):321–343, 2004.
13. c.w. g¨ unther and w.m.p. van der aalst. a generic
import framework for process event logs. in j. eder
and s. dustdar, editors, business process management
workshops, workshop on business process intelligence
(bpi 2006) , volume 4103 of lecture notes in computer
science , pages 81–92. springer-verlag, berlin, 2006.
14. j. herbst. a machine learning approach to work-
ﬂow management. in proceedings 11th european confer-
ence on machine learning , volume 1810 of lecture notes
in computer science , pages 183–194. springer-verlag,
berlin, 2000.15. p.t.g. hornix. performance analysis of business pro-
cesses through process mining. master’s thesis, eind-
hoven university of technology, department of com-
puter science, eindhoven, the netherlands, 2007.
16. k. jensen. coloured petri nets. basic concepts, analy-
sis methods and practical use . springer-verlag, 1997.
17. k. jensen, l.m. kristensen, and l. wells. coloured
petri nets and cpn tools for modelling and validation
of concurrent systems. software tools for technology
transfer (sttt) , 9(3–4):213–254, 2007.
18. l.m. kristensen, p. mechlenborg, l. zhang, b. mitchell,
and g.e. gallasch. modelbased development of a course
of action scheduling tool. in k. jensen, editor, pro-
ceedings of the seventh workshop on the practical use
of coloured petri nets and cpn tools , pages 1–16. uni-
versity of aarhus, denmark, 2006.
19. a.k. alves de medeiros and c.w. g¨ unther. process min-
ing: using cpn tools to create test logs for mining al-
gorithms. in k. jensen, editor, proceedings of the sixth
workshop and tutorial on practical use of coloured
petri nets and the cpn tools , pages 177–190, 2005.
20. t.m. mitchell. machine learning . mcgraw-hill, 1997.
21. j.r. quinlan. c4.5: programs for machine learning .
morgan kaufmann, 1993.
22. a. rozinat and w.m.p. van der aalst. conformance
testing: measuring the fit and appropriateness of event
logs and process models. in c. bussler et al., editor,
business process management 2005 workshops , volume
3812 of lecture notes in computer science , pages 163–
176. springer-verlag, berlin, 2006.
23. a. rozinat and w.m.p. van der aalst. decision mining
in business processes. bpm center report bpm-06-10,
bpmcenter.org, 2006.
24. a. rozinat and w.m.p. van der aalst. decision mining
in prom. in s. dustdar, j.l. fiadeiro, and a. sheth, ed-
itors, bpm 2006 , volume 4102 of lecture notes in com-
puter science , pages 420–425. springer-verlag, berlin,
2006.
25. a. rozinat, i.s.m. de jong, c.w. g¨ unther, and w.m.p.
van der aalst. process mining of test processes: a case
study. beta working paper series, wp 220, eindhoven
university of technology, eindhoven, 2007.
26. a. rozinat, r.s. mans, and w.m.p. van der aalst. min-
ing cpn models: discovering process models with data
from event logs. in k. jensen, editor, proceedings of the
seventh workshop on the practical use of coloured petri
nets and cpn tools , pages 57–76. university of aarhus,
denmark, 2006.
27. a. vinter ratzer, l. wells, h.m. lassen, m. laursen,
j.f. qvortrup, m.s. stissing, m. westergaard, s. chris-
tensen, and k. jensen. cpn tools for editing, simu-
lating, and analysing coloured petri nets. in w.m.p.
van der aalst and e. best, editors, applications and the-
ory of petri nets 2003: 24th international conference,
icatpn 2003 , volume 2679 of lecture notes in com-
puter science , pages 450–462. springer verlag, 2003.
28. a.j.m.m. weijters and w.m.p. van der aalst. redis-
covering workﬂow models from event-based data using
little thumb. integrated computer-aided engineering ,
10(2):151–162, 2003.
29. i.h. witten and e. frank. data mining: practical ma-
chine learning tools and techniques, 2nd edition . mor-
gan kaufmann, 2005.