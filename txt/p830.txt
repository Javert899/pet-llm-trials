handling big(ger) logs: connecting prom 6 to apache
hadoop
sergio hern `andez1and s.j. van zelst2
1department of computer science and systems engineering
university of zaragoza, spain
shernandez@unizar.es
2department of mathematics and computer science
eindhoven university of technology, the netherlands
s.j.v.zelst@tue.nl
abstract. within process mining the main goal is to support the analysis, im-
provement and apprehension of business processes. a vast amount of techniques
has been developed within the ﬁeld. the majority of these techniques however
comprise of a rather classical computational fashion and do not incorporate the
latest developments aimed at tackling the information explosion phenomenon,
e.g. techniques related to scalability and distributed computing. in this paper we
present a newly developed integrative framework connecting the process mining
framework prom with the distributed computing environment apache hadoop.
the integration allows for the execution of mapreduce jobs on any apache
hadoop cluster enabling practitioners and researchers to explore and develop new
work related to scalable and distributed computation, from a process mining per-
spective.
keywords: process mining, big data, scalability, distributed computing, prom,
apache hadoop
1 introduction
we assume the reader to be knowledgeable w.r.t. the basics of process mining and refer
to [1] for an in-depth overview.
nowadays we are able to store huge quantities of data related to business process ex-
ecution. classical process mining techniques however are simply not able to cope with
these quantities of data, i.e. within the prom framework [2, 3]3it is currently impossi-
ble to analyse event data which size exceeds the computer’s physical memory. within
process mining only a limited amount of research has been done concerning the inte-
gration of techniques that are designed to cope with enormous amounts of data. w.r.t.
event log based process discovery, techniques have been developed to apply divide and
conquer based approaches in order to reduce computational complexity [4]. some work
has been done w.r.t. the incorporation stream mining techniques within the context of
process mining [5–7], i.e. the input data is regarded as a stream of events rather than a
3http://www.promtools.org/2
static event log. finally, some work has been conducted w.r.t. the application of mapre-
duce techniques to process discovery [8].
apache hadoop4provides open-source multi-purpose software which main aim is
to provide reliable, scalable and distributed computing. typically, hadoop runs on a
large-scale computational cluster of servers. it comprises of a set of different modules
that handle different perspectives of the aforementioned aim. the hadoop distributed
file system (hdfs) component provides a distributed ﬁle system whereas the mapre-
duce component implements a programming model aimed at processing vast amounts
of data. in particular mapreduce ﬁnds its fundamental concepts within the area of func-
tional programming and is particularly aimed at handling big amounts of semi- and/or
unstructured data.
although [8] shows some interesting results that hint on a thorough investigation
of techniques related to distributed computation models w.r.t. process mining, a uni-
fying implementation that allows development of prototypes of the like does not yet
exit. in this paper we present a newly developed framework integrating the process
mining framework prom and apache hadoop. the integration allows any user of the
prom framework to executed mapreduce jobs on any given apache hadoop cluster.
moreover the framework is intended to provide an easy entry point for the development
mapreduce-based techniques from a process mining perspective.
the remainder of this paper is organized as follows. in section 2 we present the core
concepts of the newly created integrative framework. section 3 presents an example
of execution of a process discovery based mapreduce job within hadoop using the
framework. section 4 concludes the paper.
2 core concepts
the main purpose of the integration between apache hadoop and prom is to enable re-
searchers, practitioners etc. to use, develop and/or publish hadoop-based process min-
ing techniques. hence the newly developed framework acts as a core platform that
establishes access to a variety of apache hadoop based functionality. conceptually,
the basic goal of the integration is to enable any user and/or developer to connect their
apache hadoop cluster5to prom and design/execute mapreduce-based process min-
ing tasks.
at the core of the integration between apache hadoop and prom acts the
hadoopclusterparameters (hcp) interface, specifying the connection a hadoop
cluster within prom. the hcp object is needed by all hadoop plugins. currently the
hcp object provides means to verify whether a connection can be established to the
hadoop cluster, by verifying whether the hdfs can be mounted and the user has ac-
cess to the cluster. a second core element of the integration is the hdfsxlog interface
which extends the well known xlog interface6. as apache hadoop provides a dis-
tributed ﬁle system, i.e., hdfs, we can store (xes) event logs on the cluster. within
the hadoop integration we provide means to import such event logs from the cluster.
4http://hadoop.apache.org/
5given that the hadoop cluster ﬁts a speciﬁc range of hadoop release lines, e.g. 2.x.y.
6http://www.xes-standard.org/openxes/start3
importing an event log will create an hdfsxlog object within prom. by default the
log is not actually loaded in the local memory as it is rather a pointer to the external
location of the event log. however, it can also be used in the standard prom plugins. in
this situation, the log is directly read from the hdfs and loaded in the local computer’s
memory. note that currently, when actually importing the event log, it’s size may not
exceed the computer’s physical memory.
the main architectural challenge within establishing the connection between prom
and hadoop is the actual execution of a hadoop mapreduce job. as the event data
under study, possibly too large to store on a regular computer, is stored in the hdfs
component of the hadoop cluster, all computation should preferably be performed on
the cluster rather than on the local machine. this implies that as a prerequisite for
executing a speciﬁc mapreduce job, the corresponding implementing code (which is
programmed using the ja v a language) needs to be packed into a java archive (jar)
and send to the hadoop cluster. from a developer’s perspective this means that after
developing a mapreduce job, the corresponding jar ﬁle is to be included within the
prom sources. then, the framework provides methods to transfer the executable jar,
execute the mapreduce job and transfer back the results of the computation which can
consequently be visualized by prom.
3 case study - executing the alpha miner on apache hadoop
as a case study we have implemented calculation of a directly follows graph as a set of
mapreduce tasks. te directly follows graph can be used as an input for the inductive
miner [9]. we applied it on a event log which total size is 218 gb. any plugin that is
able to execute mapreduce jobs on a hadoop cluster needs an hcp object as well as an
hdfsxlog object. importing an hdfsxlog can be done both with or without the use
of an hcp object, i.e. we provide a plug-in that generates both objects.
to import a hdfsxlog we run the “import a xlog from hadoop distributed file
system” plugin in prom. if we start the plugin without using an hcp, the user is ﬁrst
asked to connect to a hadoop cluster as depicted in figure 1a. the plugin will generate
two artifacts, one hcp object and one hdfsxlog object. after specifying a connec-
tion, the user is prompted to provide a path to the speciﬁc log of choice as depicted in
figure 1b.
if the user has speciﬁed the path to the event log and clicked the ﬁnish button the
hcp object and the hdfsxlog will be created. using the latter two objects as an input
we execute the “mine a process tree in hadoop using inductive miner (from dfg)”
plugin. executing this plugin triggers a copy of the associated jar ﬁles, i.e. specifying
the mapreduce jobs, to the hadoop cluster. if all ﬁles are transferred successfully the
hadoop job will be started. the progress of the job can be inspected within the hadoop
cluster metrics overview (which is part of apache hadoop software) as depicted in
figure 2a.7the result of applying the inductive miner on the directly follows graph
computed using apache mapreduce is depicted in figure 2b.
7note that we removed some of the user and cluster speciﬁc information from the screenshot.4
(a) screenshot of the dialog request-
ing the host, user and password for the
hadoopclusterparameters (hcp).
(b) screenshot of the dialog requesting the
path of the xes log in the hdfs to gener-
ate the hdfsxlog artifact.
fig. 1: screenshots of the dialogs shown by the “import a xlog from hadoop dis-
tributed file system” plugin in prom.
(a) apache hadoop cluster metrics.
 (b) inductive miner result.
fig. 2: the hadoop cluster metrics web interface showing the progress of the mapre-
duce jobs and the result of applying the inductive miner on a directly follows graph
learned on an event log of 218gb using apache mapreduce.
4 conclusion
the newly created integration between prom and apache hadoop allows developers,
bpm professionals, etc. to use and/or develop big data related techniques within a pro-
cess mining context. the integration allows users and developers to connect, in a trivial
manner, to an arbitrary hadoop cluster. thus, without any in-depth technical knowledge
of hadoop users can start exploring new types of analysis in data intensive environ-
ments.
future work we identify several interesting directions for future work. currently, the
user needs to specify the exact path to a ﬁle. in the future we want to integrate support
for browsing the hdfs using some graphical interface. additionally we want to inte-
grate user authentication more thoroughly throughout all plugins using the hcp object.
also, we want to develop a new importer for hdfs logs that reads the log as a stream.
in this way we can import logs which size exceeds the computer’s physical memory.5
another interesting addition is “on-the-ﬂy” jar generation. currently, when devel-
oping a plug-in that (partially) consists of mapreduce tasks, the developer needs to
manually generate a jar ﬁle and include it in the project source. we think of extending
the framework in such way that the jars needed for the execution of mapreduce on
hadoop will be automatically generated within prom. this allows developers to pri-
marily focus on the implementation of mapreduce related code and abstract from the
administrative details of handling the execution on a cluster.
references
1. aalst, w.v.d.: process mining: discovery, conformance and enhancement of business pro-
cesses. 1st edn. springer publishing company, incorporated (2011)
2. dongen, b.v., alves de medeiros, a., verbeek, h., weijters, a., aalst, w.v.d.: the prom
framework: a new era in process mining tool support. in ciardo, g., darondeau, p., eds.:
applications and theory of petri nets 2005. v olume 3536 of lecture notes in computer
science. springer berlin heidelberg (2005) 444–454
3. aalst, w.v.d., dongen, b.v., g ¨unther, c., mans, r., alves de medeiros, a., rozinat, a., rubin,
v ., song, m., verbeek, h., weijters, a.: prom 4.0: comprehensive support for real process
analysis. in kleijn, j., yakovlev, a., eds.: petri nets and other models of concurrency –
icatpn 2007. v olume 4546 of lecture notes in computer science. springer berlin heidel-
berg (2007) 484–494
4. aalst, w.m.p.v.d.: decomposing petri nets for process mining: a generic approach. dis-
tributed and parallel databases 31(4) (2013) 471–507
5. maggi, f.m., burattin, a., cimitile, m., sperduti, a.: online process discovery to detect
concept drifts in ltl-based declarative process models. in: on the move to meaningful in-
ternet systems: otm 2013 conferences - confederated international conferences: coopis,
doa-trusted cloud, and odbase 2013, graz, austria, september 9-13, 2013. proceedings.
(2013) 94–111
6. burattin, a., sperduti, a., aalst wil, m.p.v.d.: control-ﬂow discovery from event streams. in:
proceedings of the ieee congress on evolutionary computation, cec 2014, beijing, china,
july 6-11, 2014. (2014) 2420–2427
7. zelst, s.j.v., burattin, a., dongen, b.f.v., verbeek, h.m.w.: data streams in prom 6: a
single-node architecture. in: proceedings of the bpm demo sessions 2014 co-located with
the 12th international conference on business process management (bpm 2014), eindhoven,
the netherlands, september 10, 2014. (2014) 81
8. evermann, j.: scalable process discovery using map-reduce. services computing, ieee
transactions on pp(99) (2014) 1–1
9. leemans, s.j.j., fahland, d., van der aalst, w.m.p.: discovering block-structured process
models from event logs - a constructive approach. in: application and theory of petri nets
and concurrency - 34th international conference, petri nets 2013, milan, italy, june 24-
28, 2013. proceedings. (2013) 311–329