evaluating the quality of discovered
process models
a. rozinat1;2, m. veloso2, and w.m.p. van der aalst1
1information systems group, eindhoven university of technology, nl-5600 mb,
eindhoven, the netherlands. fa.rozinat,w.m.p.v.d.aalst g@tue.nl
2computer science department, carnegie mellon university, pittsburgh pa
15213-3890, usa. veloso@cmu.edu
abstract. in the domain of process mining the evaluation of models
(i.e., \how can we measure the quality of a mined process model?") is
still subject to ongoing research. because the types of models used in
process mining are typically on a higher level of abstraction (they, for
example, allow to capture concurrency), the problem of model evaluation
is challenging. in this paper, we elaborate on the problem of process
model evaluation, and we evaluate both new and existing tness metrics
for dierent levels of noise. the new metrics and the noise generation
are based on hidden markov models (hmms).
1 introduction
process mining deals with the discovery of process models (i.e., structures that
model behavior) from event-based data. the goal is to construct a process model
which reects the behavior that has been observed in some kind of event log .
an event log is a set of nite event sequences, whereas each event sequence
corresponds to one particular materialization of the process. process modeling
languages, such as petri nets [5], can then be used to capture the causal rela-
tionships of the steps, or activities, in the process. while many dierent process
mining approaches have been proposed over the last decade, no standard measure
is available to evaluate the quality of such a learned model [10]. quality measures
are needed because a learned model cannot always explain all the data, and there
are multiple models for the same data (\which one is the best?"). these prob-
lems are due to the fact that a log typically does not contain negative examples
and that there may be syntactically dierent models having the same (or very
similar) behavior. this paper deals with the topic of evaluating process models
and we use petri nets as a typical representation of the class of graph-based
process modeling languages (epcs, bpmn, uml activity diagrams etc.).
the remainder of the paper is organized as follows. first, we explain the
problem domain of process model evaluation in more detail (section 2). then,
we briey sketch our approach (section 3) and present experimental results (sec-
tion 4). finally, we conclude the paper (section 5).2 process model evaluation
process mining techniques focus on discovering behavioral aspects from log data.
since the mid-nineties several groups have been concentrating on the discovery
ofprocess models from event-based data. process models are structures|usually
directed graphs|that model behavior. the idea of applying process mining in
the context of workow management was rst introduced by agrawal et al. in [2].
over the last decade many process mining approaches have been proposed. in
[1] van der aalst et al. provide an overview about the early work in this domain.
while all these approaches aim at the discovery of a \good" process model, often
targeting particular challenges (e.g., the mining of loops, duplicate tasks, or in
the presence of noise), they have their limitations and many dierent event logs
and quality measurements are used. hence, no commonly agreed upon measure
is available.
in [10] we motivate the need for a concrete framework that evaluates the
quality of a process model, and thus enables (a) process mining researchers to
compare the performance of their algorithms, and (b) end users to estimate the
validity of their process mining results (\how much does this model reect real-
ity?"), and to choose between alternative models (\which model is the best"?).
it is important to understand that there is never only one single model for a
given event log, but multiple models are possible due to two main reasons.
1.there are syntactically dierent models having the same (or very similar)
behavior . furthermore, there are numerous dierent modeling formalisms
that can be employed. we do not focus on this aspect in our work.
2.for a given input, an innite number of models can be constructed . resulting
models might not always accommodate all the traces in the event log, and
they might allow for behavior not represented by any trace in the log.
to illustrate the second aspect, consider figure 1 which depicts four dierent
process models that could be constructed based on the event log in the center.
the event log contains ve dierent traces with frequencies ranging from 1207
to 23 instances per trace. for example, the sequence abdei (i.e., afollowed by
b, etc.) occurred 1207 times. while the model in figure 1(c) does not accom-
modate all these ve traces but only the rst one (lack of tness ), the model in
figure 1(d) allows for much more behavior than just the traces in the event log
(lack of precision ).
cook and wolf [3] approach the discovery of finite state machine (fsm)
models for software processes as a grammar inference problem, and, reecting on
the \goodness" of a model, they cite gold [6] who showed that both positive and
negative samples are required to construct `accurate' (the fsm accepting all legal
sentences and rejecting all illegal sentences of the language) and `minimal' (the
fsm containing the minimum number of states necessary) models. furthermore,
the samples must be complete (i.e., cover all possible inputs). however, the event
logs used for process discovery cannot be assumed to be complete, and they
normally do not contain negative examples. note that the ve traces in the
event log in figure 1(a) are positive examples, but no negative, or forbidden,
2fig. 1. process model evaluation can place in dierent dimensions [10].
traces are given. furthermore, in some situations it can be the case that the
positive examples are in fact distorted (noise), or contain exceptions that should
not be included in the model. therefore, process discovery algorithms have to
face the following problems.
dealing with incompleteness if the log would be complete, it would be easy
to assume that every sequence not present in the log is a negative example,
and thus should not be possible according to the discovered model. unfor-
tunately, total completeness is an unrealistic assumption as the number of
possible interleavings of concurrent activities increases exponentially3. thus,
generalization beyond the observed sequences to accommodate concurrent
or combined behavior is often desirable.
further abstraction besides generalizing to deal with incompleteness, fur-
ther abstraction may be necessary to obtain meaningful process models. for
example, in the presence of overly complex processes it often does not make
sense to show a very detailed (\spaghetti-like") model. furthermore, one
might want to deal with noise, or show the main ow of a process and thus
ignore possible exceptions. in theses case, abstraction can lead to models
with a decreased precision and a decreased tness .
so, we can see that|while on the rst glance it seems logical to aim at
models with perfect tness and precision|this is not always desirable. instead,
algorithms strive to nd \the right degree of abstraction", depending on the
assumed circumstances and the purpose of the discovered model. as a conse-
quence, process model evaluation needs to take these goals into account, and
3already 5 concurrent activities can generate 5! = 120 possible traces, and 10 con-
current activities can result in 10! = 3628800 dierently ordered sequences.
3may have an unwanted bias if applied in the wrong context. for example, the
model depicted in figure 1(c) might be considered a good abstraction of the
80% most frequent behavior, but would be a relatively poor model if the goal is
to obtain a complete picture of the overall process.
3 approach
although it is vital to develop practical methods that are able to take the desired
abstractions made by process discovery algorithms into account when evaluating
the resulting models, in reality there are often also simpler processes (or parts of
processes) that only exhibit sequential routing, alternative behavior, and loops
(but no parallelism). examples of such processes can be found in administrative
procedures employed in municipalities, insurance companies etc.
in our approach we focus on such simple processes with the assumption
that the models should be as accurate, i.e., tting and precise , as possible for a
given event log. we dene new evaluation metrics and compare them to existing
evaluation methods used in the process mining eld. it is important to note
that here we do not seek for the all-encompassing standard measure (for this,
posing restrictions on concurrency would not be a good idea), but rather aim at
providing ecient base line metrics against which other evaluation metrics can
be compared to see how they perform in these simple situations. ultimately, the
goal should be to give better support for what is the \right" quality metric for
the situation at hand.
in our approach we use hidden markov models (hmms) [8] to represent these
simpler models. hmms, as opposed to plain markov chains, allow for a separation
of states and observation elements (the states are hidden ). then we use these
hmms to calculate metrics and generate logs with varying, yet determined levels
of noise:
metrics we dene a mapping from labeled petri nets without parallelism onto
hmms, whereas we create one state per labeled task in the process model
(unlabeled tasks are not represented by a separate state in the hmm as they
are not observable), and where we rst link the corresponding observation
element with 100% probability to that state (but multiple states may be
linked to the same observation). based on this hmm and the event log, we
dene quality metrics for the tness and precision dimension.
noise generation then, we introduce successive degrees of noise into the
hmm model (equally distributed over all states). we then use this hmm to
simulate logs for dierent levels of noise, and evaluate the development of a
number of tness metrics over these logs with varying degrees of noise.
for more details, we refer the interested reader to our technical report [11], which
in addition provides denitions of the used formalisms and our quality metrics,
explains our mapping from sequential petri nets to hmms based on a simple
example, describes the precise experimental setup, and provides more detailed
results. furthermore, [11] also shows the dierences in representational power
by comparing hmms and petri nets as a modeling technique.
44 experimental results
we performed experiments for varying degrees of noise based on process models
from dierent domains. first, we used four process models that were mined
based on log data collected by the cmdragons team during the international
robot soccer competition 'robocup' 2007, and thus constitute models of the
behavior in a multi-agent robotic system. second, we evaluated three dierent
models of administrative processes within a municipality in the netherlands.
finally, we selected three suitable models from the sap reference models, which
is a publically available model that contains more than 600 enterprise models,
and analyzed them with our approach.
the original models were given in terms of heuristics nets and epcs, and
they were translated into petri nets using conversion facilities in the prom frame-
work, respectively. we performed experiments on these models with varying pa-
rameters. the results of these experiments are very similar, and in the following
we use a single, but representative, example to point out the main conclusions
that we can draw from them. the detailed results are provided in our technical
report [11].
as for the process model evaluation metrics, we used the following two metrics
from the process mining domain.
{ the token fitness [9] is based on replaying the log in a petri net process
model and relating the number of \missing" and \remaining" tokens dur-
ing log replay to the total number of produced and consumed tokens (here
referred to as token based ). a similar metric is the continuous parsing mea-
sure [12] which measures the number of missing and remaining activations
while replaying a heuristics net.
{ the improved continuous semantics tness [4] is used by the genetic miner
to select the best process models in each generation of the genetic algorithm,
and incorporates a tness evaluation similar to the continuous parsing mea-
sure, a precision evaluation, and gives some extra weight based on the number
of traces that have problems.
furthermore, we used the following three new hmm-based metrics, which are
dened in our technical report [11].
{ a trace based metric simply calculates the percentage of traces that have
no errors given the model. similar metrics are used in the process mining
domain (e.g., [12]).
{ another metric measures tness on the model level , i.e., relating to how
many \forbidden" transitions in the model have been \broken" by the log.
{ the event level metric evaluates the occurrence of these \forbidden transi-
tions" with respect to the whole log.
now consider figure 2, which depicts the 2 + 3 = 5 tness values (y axis) for
50 dierent noise levels (x axis), with 100 traces per log, and a maximum of 100
events per trace, for one of the models mined from the robot soccer data. since
5 0 0.2 0.4 0.6 0.8 1
 0 20 40 60 80 100fitness value
noise levelfitness measurements for different noise levelstoken basedimproved continoustrace basedmodel levelevent levelfig. 2. fitness values for 50 dierent noise levels on larger model.
this gure is representative for the larger set of experiments in [11], we can use
it to illustrate our main conclusions from the experiments.
1.existing metrics have a bias when applied to simple models . we can see
that the token based tness, which is representative for other similar tness
approaches in the process mining domain, does not drop to much less than a
tness of 0.4 throughout the whole experiment. this can be explained by the
fact that the log replay \leaves tokens behind" for potential later use, which
is appropriate for models containing parallelism (as one needs to look more
than one step forward to satisfy all the dependencies) but not for simple
petri nets as evaluated in our experiments. thus, in these situations, and
more severely with an increasing level of noise, this metric provides overly
optimistic results for sequential models.
2.metrics should measure only one thing . we can see that the improved con-
tinuous tness suddenly drops dramatically when the noise level is increased
above 40%. this can be explained by the fact that this metric was designed
to steer a genetic algorithm based on a mixture of dierent measurements.
for that purpose, it is a good metric. however, if one wants to use the metric
to gain insight into the quality of a process model, it becomes dicult as
the interpretation of results becomes dicult (\which aspect contributed to
this value?", \is it the property i want to measure?").
3.trace-based metrics do not make much sense for large models and long pro-
cess instances . based on the trace based tness, we can see that this metric
6drops very quickly towards 0, already for low levels of noise. this is due to
the fact that in case of longer process instances (as here up to 100 events per
trace) already one small error in the trace renders the whole trace to have
a negative impact on the overall tness. with an increasing level of noise,
there will be soon no more traces that have no errors, thus rendering the
measure to provide pessimistic results if compared to our notion of noise.
the results indicate that the event level metric seems to be the best metrics
in our setup. this is not surprising since it best matches the notion of noise used
in the experiments. if we would, for example, generate noise by distorting an
increasing number of log traces, the trace based metric would match this notion
best. furthermore, one could introduce distortions gradually over dierent parts
of the process, which would render the model level metric less pessimistic.
note further that, depending on the goal of the evaluation, the evaluation
setup can be dierent. for example, we here assumed that it is desirable for
a tness metric to scale as linearly as possible with respect to the degree of
distortion in the log. since lower portions of noise (e.g., 5% or 10%) are much
more common in real processes than a very high portion (e.g., 80%), one might
prefer the metric to scale linearly from, e.g., 0% (metric yields 1) to 20% noise
(metric yields 0). however, for a rst general evaluation the presented setup
seems reasonable.
5 discussion
we generally use metrics to obtain information about the quality of a process
model. but to be sure that the conclusions that we draw from the measurements
are valid, we must rst ensure the quality of the metric itself . the following re-
quirements are are generally considered relevant [7] to ensure the usefulness of a
metric: validity (the measure and the property to measure must be suciently
correlated), reproducibility (be independent of subjective inuence), stability (be
as little as possible aected by properties that are notmeasured) and analyz-
ability (relates to the properties of the measured values).
given the fact that the validity and reproducibility are typically not an is-
sue, we have proposed a structured approach to evaluate the analyzability and
stability of certain process mining metrics. as a rst step, we focused on simple
process models and the tness and precision quality dimensions. we have seen
that|in this simple setting|existing process mining metrics can yield overly
optimistic results, which aects their analyzability . furthermore, some metrics
measure more than one aspect, which makes interpretation of the results di-
cult and negatively aects their stability . further research is required to develop
evaluation approaches for further dimensions and more complex scenarios. the
ultimate vision for process model evaluation would then be to have a method-
ology that assists in selecting the \right" metric for the \right" situation, and
based on the goal of the evaluation.
7acknowledgements
this research is supported by the iop program of the dutch ministry of eco-
nomic aairs. the authors also thank the cmdragons team for sharing their
robocup log data, and particularly stefan zickler and douglas vail who helped
to create the ascii logs used for conversion to the mxml format. furthermore,
we thank the town hall in the netherlands for letting us use their process mod-
els, and florian gottschalk and boudewijn van dongen for the epc reference
models. finally, we thank all prom developers for their on-going work on process
mining techniques. special thanks to christian w. g unther for nikefs2, which
makes it possible to work with large real-world logs, and for his ui redesign,
which continues to impress people and makes using prom so much more fun.
references
1. w.m.p. van der aalst, b.f. van dongen, j. herbst, l. maruster, g. schimm, and
a.j.m.m. weijters. workow mining: a survey of issues and approaches. data
and knowledge engineering , 47(2):237{267, 2003.
2. r. agrawal, d. gunopulos, and f. leymann. mining process models from work-
ow logs. in sixth international conference on extending database technology ,
pages 469{483, 1998.
3. j.e. cook and a.l. wolf. discovering models of software processes from event-
based data. acm transactions on software engineering and methodology ,
7(3):215{249, 1998.
4. a.k. alves de medeiros. genetic process mining . phd thesis, eindhoven univer-
sity of technology, eindhoven, 2006.
5. j. desel, w. reisig, and g. rozenberg, editors. lectures on concurrency and petri
nets, volume 3098 of lecture notes in computer science . springer-verlag, berlin,
2004.
6. e.m. gold. complexity of automaton identication from given data. information
and control , 37(3):302{320, 1978.
7. p. liggesmeyer. software-qualit at { testen, analysieren und verizieren von soft-
ware. spektrum akademischer verlag, heidelberg, berlin, 2002.
8. l.r. rabiner. a tutorial on hidden markov models and selected applications in
speech recognition. proceedings of the ieee , 77(2):257{286, 1989.
9. a. rozinat and w.m.p. van der aalst. conformance checking of processes based
on monitoring real behavior. information systems , 33(1):64{95, 2008.
10. a. rozinat, a.k. alves de medeiros, c.w. g unther, a.j.m.m. weijters, and
w.m.p. van der aalst. the need for a process mining evaluation framework
in research and practice. in arthur h. m. ter hofstede, boualem benatallah, and
hye-young paik, editors, business process management workshops , volume 4928
oflecture notes in computer science . springer, 2008.
11. a. rozinat, m. veloso, and w.m.p. van der aalst. using hidden markov models
to evaluate the quality of discovered process models. extended version. bpm
center report bpm-08-10, bpmcenter.org, 2008.
12. a.j.m.m. weijters and w.m.p. van der aalst. rediscovering workow models
from event-based data using little thumb. integrated computer-aided engi-
neering , 10(2):151{162, 2003.
8