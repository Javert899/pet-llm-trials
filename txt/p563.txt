process mining online assessment data 
 
mykola pechenizkiy, nikola tr čka, ekaterina vasilyeva, wil van der aalst, paul de bra  
{m.pechenizkiy, e.vasilyeva, n.trcka, w. m.p.v.d.aalst}@tue.nl, debra@win.tue.nl 
department of computer science, eindhoven un iversity of technology, the netherlands 
abstract.  traditional data mining techniques have been extensively applied to 
find interesting patterns, build descriptive and predictive models from large 
volumes of data accumulated through the use of different information systems. 
the results of data mining can be used for getting a better understanding of the 
underlying educational processes, for generating recommendations and advice 
to students, for improving management of learning objects, etc. however, most 
of the traditional data mining techniques focus on data dependencies or simple 
patterns and do not provide a visual representation of the complete educational 
(assessment) process ready to  be analyzed. to allow for these types of analysis 
(in which the process plays the central ro le), a new line of data-mining research, 
called process mining , has been initiated. process mining focuses on the 
development of a set of intelligent tools and techniques aimed at extracting process-related knowledge from event logs recorded by an information system. 
in this paper we demonstrate the applicability of process mining, and the prom 
framework in particular, to educational data mining context. we analyze 
assessment data from recently organi zed online multiple choice tests and 
demonstrate the use of process di scovery, conformance checking and 
performance analysis techniques. 
1 introduction 
online assessment becomes an important compon ent of modern education. it is used not 
only in e-learning, but also within blended le arning, as part of the learning process. 
online assessment is utilized both for self-e valuation and for “real” exams as it tends to 
complement or in some cases even replace traditional methods for evaluating the 
performance of students. 
intelligent analysis of assessment data a ssists in achieving a better understanding of 
student performance, the quality of the test and individual questions, etc. besides, there 
are still a number of open i ssues related to authoring a nd organization of different 
assessment procedures. in multiple-choice questions (mcq) testing it might be 
important to consider how students are supposed to navigate from one question to 
another, i.e. should the students be able to  go back and forward and also change their 
answers (if they like) before they commit the whole test, or should the order be fixed so 
that students have to answer the questions one after another? is it not necessarily a trivial question since either of two options may allo w or disallow the use of certain pedagogical 
strategies. especially in the context of personalized adaptive assessment it is not 
immediately clear whether an implied strict  order of navigation results in certain 
advantages or inconveniences fo r the students. in general, th e navigation of students in e-
learning systems has been actively studied in  recent years. here, researchers try to 
discover individual navigational st yles of the students in order to reduce cognitive load of 
the students, to improve usability and learning efficiency of e-learning systems and support personalization of navigation [2]. some  recent empirical studies demonstrated the 
educational data mining 2009
279feasibility and benefits of f eedback personalization during onlin e assessment, i.e. the type 
of immediately presented feedback and th e way of its presentation may significantly 
influence the general performa nce of the students [9][10]. however, some students may 
prefer to have less personaliz ation and more flexibility of navigation if there is such a 
trade-off. overall, there seem to be no “best”  approach applicable for every situation and 
educators need to decide whether current practices are effective. 
traditional data mining techniques includi ng classification, associ ation analysis and 
clustering have been successfully applied to di fferent types of educational data [4], also 
including assessment data, e.g. from intellig ent tutoring systems or learning management 
systems (lms) [3]. data mining can help to identify group of (c or)related questions, 
subgroups (e.g. subsets of students performing similarly of a subset of questions), 
emerging patterns (e.g. discovering a set of patt erns describing how the performance in a 
test of one group of students, i.e. followi ng a particular study program, differs from the 
performance of another group), estimate th e predictive or discriminative power of 
questions in the test, etc. however, most of the traditional data mining techniques do not 
focus on the process perspective and theref ore do not tell much about the assessment 
process as a whole. process mining on the contrary focuses on the development of a set of 
intelligent tools and techniques aimed at ext racting process-related knowledge from 
event logs recorded by an information system.  
in this paper we briefly introduce process mi ning [7] and our prom tool [8] for the edm 
community and demonstrate the use of a few prom plug-ins for the analysis of 
assessment data coming from two recent studies. in one of the studies the students had to 
answer to the tests’ questions in a strict order and had a po ssibility to request immediate 
feedback (knowledge of correct  response and elaborated feed back) after each question. 
during the second tests student had a possibility  to answer the questions in a flexible 
order, to revisit and earlier answ ers and revise them as well.  
the remainder of the paper is organized as follows. in section 2 we explain the basic process mining concepts and present the prom framework. in section 3 we consider the use of prom plug-ins on real assessment data , establishing some useful results. finaly, 
section 4 is for discussions. 
2 process mining framework 
process mining has emerged from the field of  business process management (bpm). it 
focuses on extracting process-re lated knowledge from event logs1 recorded by an 
information system. it aims particularly at discovering or an alyzing the complete 
(business, or in our case educational) process and is supported by powerful tools that allow getting a clear visual representation of the whole process. the three major types of process mining applications are (figure 1):  
1) conformance checking - reflecting on the observed real ity, i.e. checking whether the 
                                                
 
1 typical examples of event logs may include resource usage and activity logs in an e-learning environment, an 
intelligent tutoring system, an educational adaptive hypermedia system.  
educational data mining 2009
280modeled behavior matches the observed behavior; 
2) process model discovery - constructing complete and compact process models able to 
reproduce the observed behavior, and 
3) process model extension  - projection of information ex tracted from the logs onto the 
model, to make the tacit know ledge explicit and facilitate better understanding of the 
process model.   
process mining is supported by the powe rful open-source framework prom. this 
framework includes a vast number of different techniques for process discovery, 
conformance analysis and model extension, as  well as many other tools like convertors, 
visualizers, etc. the prom t ool is frequently used in pr ocess mining projects in industry. 
moreover, some of the ideas and algorithms have been incorporated  in commercial bpm 
tools like bpm|one (pallas at hena), futura reflect (futur a process intelligence), aris 
ppm (ids scheer), etc. 
 
figure 1 . the process mining spectrum supported by prom  
3 case studies 
we studied different issues related to authoring and pers onalization of online assessment 
procedures within the series  of the mcq tests organized during the mid-term exams at 
eindhoven university of technology using moodle2 (quize module tools) and sakai3 
(mneme testing component) open source lmss. 
to demonstrate the applicability of process mining we use data collected during two 
exams: one for the data modeling and databases (db) course and one for the human-computer interaction (hci) course . in the first (db) test students (30 in total) answered 
to the mcqs (15 in total) in a strict or der, in which questions appeared one by one. 
students after answering each question were ab le proceed directly to the next question 
                                                
 
2 http://www.moodle.org 
3 http://www.sakai.org 
educational data mining 2009
281(clicking “go to the next question”), or firs t get knowledge of corre ct response (clicking 
the “check the answer”) and after that eith er go the next question (“go to the next 
question”) or, before that, request a detailed explanation about their response (“get 
explanations”). in the second (hci) test stud ents (65 in total) had the possibility to 
answer the mcqs (10 in total) in a flexible order, to revis it (and revise if necessary) the 
earlier questions and answers. flexible na vigation was facilitated by a menu page for 
quick jumps from one question to any other que stion, as well as by “next” and “previous” 
buttons. 
in the mcq tests we asked students to also include the confidence level of each answer. 
our studies demonstrated that knowledge of the response certitude (specifying the student’s certainty or confidence of the correct ness of the answer) together with response 
correctness helps in understanding the learning behavior and allows for determining what 
kind of feedback is more preferable and more  effective for the students thus facilitating 
personalization in assessment [3]. 
for every student and for each question in the test we collected all the possible 
information, including correctness, certit ude, grade (determined by correctness and 
certitude), time spent for answering the questi on, and for the db test whether an answer 
was checked for correctness or not, whether de tailed explanation was requested on not, 
and how much time was spent reading it, a nd for the hci test whether a question was 
skipped, revisited, whether answer wa s revised or the certitude changed.
4  
in the remainder of this section we dem onstrate how various pr om plug-ins supporting 
dotted chart analysis, process discovery (heuristic mine r and fuzzy miner), conformance 
checking, and performance analysis [1][6] allo w to get a significant better understanding 
of the assessment processes. 
3.1 dotted chart analysis 
the dotted chart is a chart similar to a gantt chart. it shows the spread of events over 
time by plotting a dot for each event in the log thus allowing to gain some insight in the 
complete set of data. the chart has three (o rthogonal) dimensions: one showing the time 
of the event, and the other two showing (pos sibly different) components (such as instance 
id, originator or task id) of the event. ti me is measured along the horizontal axis. the 
first component considered is shown along the vertical axis, in boxes. the second 
component of the event is gi ven by the color of the dot. 
figure 2 illustrates the output  of the dot chart analysis of the flexible-order online 
assessment. all the instances (one per student) are sorted by the duration of the online 
assessment (reading and answering the question and navigation to the list of questions). 
in the figure on the left, point s in the ochre and green/red color denote the start and the 
                                                 
4 further details regarding the organization of the test (including an illustrative example of the questions and the ef) 
and the data collection, preprocessing and transformation from lms databases to prom mxml format are beyond 
the scope of this paper, but interested readers can  find this information in an online appendix at 
http://www.win.tue.nl/~ mpechen/research/edu.html . 
educational data mining 2009
282end (passed/failed) of the test. triangles denote the moment when the student submits an 
answer or just navigates to another ques tion. green triangles de note correct responses 
with low (lccr – light green) and high (hccr – dark green) certainty, red triangles 
correspondingly – wrong responses (light red – lcwr, dark red – hcwr), white 
triangles – the cases when the student navi gated to the next que stion without providing 
any response. the blue squares show the moments when the students navigated from the list of the questions (menu) to a question of the quiz (or just submitted the whole test). 
 
figure 2. two dotted charts extracted from the test with flexible order navigation; (1) the overall 
navigation and answering of questions (left chart), and (2) the effects of changes (right chart)  
we can clearly see from the figure that most  of the students answered the questions one 
by one, and provided more correct answers for the first questions of th e test than for the 
last questions. they used the possibility to flex ibly navigate mainly at  the end of the test: 
students navigating to the list of the questions and then to the different questions from the 
list. it can be also clearly seen that only  few students read and skipped some questions, 
not providing their answers first, and then retu rning to those questions back to provide an 
answer.  
in the figure on the right, we can see the when students revisited the questions.  points in 
yellow correspond to the situations when correctness of the answers did not change, and 
points in red and green correspond accordingly to changes to wrong and correct answers. 
we can see that in a very few cases the correctness was changed, most changes do not 
affect correctness (e.g., a wrong answer wa s changed to another wrong answer). 
moreover, changes from right to wrong or from wrong to write had similar frequencies, 
thus not significantly changing the end results. 
3.2 process discovery  
in some cases, given a usage log we ma y have limited knowledge about the exact 
procedure of the assessment but want to disc over it based on the data from the log. there 
exist several algorithms that can automatically construct a depiction of a process. this 
educational data mining 2009
283process representation typical ly comes in form of a (formal) mathematical model 
supporting concurrency, sequential and alternative behavior (lik e, e.g., the model of petri 
nets, heuristic or fuzzy miner). 
figure 3 illustrates for the db test a part (for the first 3 questions) of the discovered 
process (left) as a heuristic net, and animation of the same  part after conversion to the 
fuzzy model (middle), and for the hci test th e complete heuristic ne t (right), abstracted 
from the type of the answer, but from which it is clear which jumps between the 
questions were popular. from the visualization of the db test process we can see what 
possibilities students had, and what the main “ flows ” were globally or at a particular time.  
 
figure 3. heuristic nets of strict ord er (left) and flexible order tests (right) 
3.3 process analysis   
in some cases, the goal is not to discover th e real learning process but to analyze some 
normative or descriptive model that is given a-priori. for example, the petri net shown in 
figure 4 (formally) describes the generic patter n of answering questions in the db test 
allowing for answer-checks and feedbacks. now it is interesting to see whether this model conforms to reality (and vice versa) and augment it with additional information learned from the event logs. the advantage of having the answering pattern represented 
as a petri net is that this allows for many different analysis tec hniques. prom offers 
various plug-ins to analyze pe tri nets (verificat ion, performance analysis, conformance, 
etc.). models like the one in figure 4 can be  discovered or made by hand. it is also 
possible to first discover a model and then re fine it using the tool yasper (incorporated 
educational data mining 2009
284into prom). figure 4 was constructed using ya sper and this was a one-time task for this 
test-type and in principle an authoring tool can be develope d to facilitate an automatic 
translation of the multiple-choice tests with varying properties to petri nets.  
as every question can be answ ered correctly or wrongly, an d with either high or low 
confidence, there are four possi bilities for the first step in  the net from figure 4. the 
transition hccr, for example, denotes that th e answer is given with high confidence and 
that it was correct; the other three starting tr ansitions are similar. after answering the 
question the student can check his answer or just go the next questi on. the latter decision 
is modeled by an internal transition (painted in black) that goes to the final place of the 
net. in case the student has decided to ch eck the answer, he can also ask for some 
feedback afterwards. 
 
figure 4 . a petri net representing the question pattern  
to illustrate the many analysis possibilities of prom, we show some results obtained 
using the conformance checker and the performance analysis with petri net plugin. 
the purpose of conformance analysis is to fi nd out whether the information in the log is 
as specified. this analysis may be used to detect deviations, to locate and explain these 
deviations, and to measure the severity of th ese deviations. we are mostly interested in 
the notion of fitness  which is concerned with the investigation whether a process model is 
able to reproduce all execution sequences th at are in the log, or , viewed from another 
angle, whether the log traces comply with th e description in the m odel (the fitness is 
100% if every trace in the l og corresponds to a possible exec ution of the model). this 
notion is particularly useful for finding out whether (or how often) the students respected 
the specified order for answering questions (to discover frauds, for example).   
figure 5 shows the result of conformance checking when applied on our log and the petri 
net from figure 4. in this, so-called log perspective  of the result, each trace from the log 
educational data mining 2009
285has all its mismatched events colored in ora nge. in our case, howeve r, there are no orange 
events, therefore there are no mismatches be tween the specified an swering pattern and 
the actual exam data. 
 
figure 5.  result of conformance checking showing a 100% fitness  
our next analysis is of a diffe rent kind. instead of checking fo r the correctnes of the exam 
behavior, we provide a means to assess the performance of the answering process. the 
performance analysis with petri net  plugin can extract the ke y performance indicators 
from the log, summarizing them in an intui tive way, and graphically present them on a 
petri net describing the process under consid eration. for our purpose we apply the plugin 
with the exam data log and the answering pattern from figure 6 (only for the first 
question of the test). 
 
figure 6 . results of applying the performance analysis with petri net  plug-in  
educational data mining 2009
286the result of the analysis is shown in fi gure 6. in the right pa nel different throughput-
type metrics are displayed; from there we, e.g., see that the average duration of the test 
was 64.41 minute. the central panel shows the answering pattern, colored and annotated 
with performance information. the numbers on the arcs represen t probabilities. as 
shown, 35% percent of the students answer ed the first questi on right and had high 
confidence.  we could also see that almost all students checked their answers and asked 
for feedback afterwards. places are colored with  respect to their souj ourn time, i.e., with 
respect to the time the process spends in this place. from the picture we can thus see that 
the answering time was short (the first que stion was easy), and th at the students who 
answered with high confidence spent more  time on the feedback (regardless on the 
correctness of the answer). 
4 conclusions and future work 
data mining techniques have been successfully  applied to different types of educational 
data and have helped to address many issues  by using traditional classification, clustering 
and association analysis techniques. alt hough the process perspect ive in educational 
domains has received some attention, most of the traditional intelligent data analysis approaches applied in the cont ext of educational data mini ng do not consider the process 
as a whole (i.e., the focus is no data or si mple sequential structures rather than full-
fledged process models). 
in this paper, we illustrated some of the potential of process mining techniques applied to 
online assessment data where students in one of  the tests were able to receive tailored 
immediate ef after answering each of the que stions in the test one by one in a strict 
order, and in the other test – to receive no feedback but to answer question in a flexible 
order. this data was of a sequential nature, i.e. it did not include concurrency. however, 
other educational processes have  lots of concurrency and this  can be discovered by prom. 
applying process mining techniques for other types of assessment data, e.g. grades for 
traditional examinations is therefore an interesting possibility. 
prom 5.0 provides a plugable environment for process mining offering a wide variety of 
plug-ins for process discovery, confor mance checking, model extension, model 
transformation. our further work includes th e development of edm tailored prom plug-
ins. on the one hand, this would help bringi ng process mining tools closer to the domain 
experts (i.e. educational speci alists and researchers), who not necessarily have all the 
technical background. on the other hand, this w ill help to better address some of the 
edm specific challenges relate d to data preprocessing a nd mining. besides this, the 
development of the authoring tools for asse ssment modules with specialized prom plug-
ins would allow to significantly simplify some of the processes for conformance analysis 
as e.g. a petri net representing certain assessm ent procedure can be generated completely 
automatically. 
acknowledgements 
this work is supported by nwo (the dutch science foundation). we would like to thank 
the many people involved in the development of prom. 
educational data mining 2009
287references 
[1] günther, c.w., van der aalst, w. m.p. fuzzy mining: adaptive process 
simplification based on multi-perspective metrics. in: g. al onso et al. (eds), proc. of 
int. conf. on business process management , lncs 4714, p. 328-343. springer-verlag, 
2007. 
[2] makany, t., engelbrecht, p.c., meadmo re, k., dudley, r., redhead, e.s., & dror, 
i.e.: giving the learners control of navigation:  cognitive gains and losses. in l. gomez et 
al. (eds.), proceedings of inted’07, 2007. [3] romero, c., ventura, s., garcía, e. data mining in c ourse management systems: 
moodle case study and tutorial. computers and education , 51. pp. 368-384, 2007. 
[4] romero, c., ventura, s. educationa l data mining: a survey from 1995 to 
2005. expert systems with applications , 33(1), p. 135-146, 2007. 
[5] rozinat, a., van der aalst, w.m.p. conformance checking of processes based on 
monitoring real behavior. information systems  33(1), p. 64-95. 
[6] song, m., van der aalst, w.m.p. supportin g process mining by showing events at a 
glance. in k. chari, a. kumar (eds), 7th annual workshop on information technologies 
and systems  (wits’07), p. 139–145, 2007. 
[7] van der aalst, w.m.p., weijters, a.j.m.m., maruster, l. workflow mining: 
discovering process models from event logs. ieee transactions on knowledge and 
data. engineering 16(9), p. 1128–1142, 2004. 
[8] van dongen, b.f., de medeiros, a.k.a., verbeek, h.m.w., weijters, a.j.m.m., van 
der aalst, w.m.p. the prom framework: a new era in process mining tool support. in: 
ciardo, g., darondeau, p. (eds .) application and theory of petri nets
, lncs 3536, p. 
444–454. springer, heidelberg, 2005. 
[9] vasilyeva, e., de bra, p., pechenizkiy, m., puuronen, s. tailoring feedback in online 
assessment: influence of lear ning styles on the feedback preferences and elaborated 
feedback effectiveness. in: proc. of 8th int. conf. on advance learning technologies  
(icalt 2008), ieee cs press, p. 834-838, 2008. 
[10] vasilyeva, e., pechenizkiy, m., and de bra, p.: adaptation of elaborated feedback 
in e-learning, in: w. nejdl et al. (eds.), proc. of int. conf. on adaptive hypermedia  
(ah’08), lncs 5149, springer-verlag, berlin, heidelberg, p. 235-244 (2008) 
educational data mining 2009
288