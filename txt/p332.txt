a generic import framework
for process event logs
industrial paper
christian w. g¨ unther and wil m.p. van der aalst
department of technology management, eindhoven university of technology
p.o. box 513, nl-5600 mb, eindhoven, the netherlands
{c.w.gunther, w.m.p.v.d.aalst }@tm.tue.nl
abstract. the application of process mining techniques to real-life cor-
porate environments has been of an ad-hoc nature so far, focused on
proving the concept. one major reason for this rather slow adoption has
been the complicated task of transforming real-life event log data to the
mxml format used by advanced process mining tools, such as prom. in
this paper, the prom import framework is presented, which has been
designed to bridge this gap and to build a stable foundation for the
extraction of event log data from any given pais implementation. its
ﬂexible and extensible architecture, adherence to open standards, and
open source availability make it a versatile contribution to the general
bpi community.
1 introduction
process-aware information systems (paiss) are a commonplace part of the
modern enterprise it infrastructure, as dedicated process management systems
or as workﬂow management components embedded in larger frameworks, such
as enterprise resource planning (erp) systems.
at this point in time, most business process monitoring solutions focus on
the performance aspects of process executions, providing statistical data and
identifying problematic cases. the area of process mining [3], in contrast, is
based on the a-posteriori analysis of process execution event logs. from this
information, process mining techniques can derive abstract information about
the diﬀerent perspectives of a process, e.g. control ﬂow, social network, etc.
there exists a great variety of pais implementations in ﬁeld use, of which
each one follows a custom manner of specifying, controlling and interpreting
business processes. as an example, consider the utter diﬀerence in paradigm be-
tween a traditional, rigid workﬂow management system (wfms) like staﬀware
on the one side, and a ﬂexible case handling [5] system like flow er[7] on the
other. this scale brings with it a corresponding plethora of event log formats,
and concepts for their storage and accessibility.
in order to render the design of process mining techniques and tools inde-
pendent of the target pais implementation, the mxml event log format has2
fig. 1. positioning the prom import framework in the bpi landscape
been devised. while this format has been designed to meet the requirements of
process mining tools in the best possible way, the conversion from many pais’s
custom formats to mxml is a non-trivial task at best.
this combination of recurring and time-consuming tasks calls for a generic
software framework, which allows the implementation of import routines to con-
centrate on the core tasks which diﬀerentiate it from others. providing a common
base for a large number of import routines further enables to leverage the com-
plete product with marginal additional implementation cost, e.g. by providing a
common graphical user interface (gui) within the host application.
the prom import framework addresses these requirements, featuring a ﬂex-
ible and extensible plug-in architecture. hosted import plug-ins are provided
with a set of convenience functionality at no additional implementation cost,
thus making the development of these plug-ins eﬃcient and fast.
this paper is organized as follows. section 2 introduces process mining and
the prom framework, followed by an introduction to the underlying mxml
format in section 3. section 4 describes requirements, design, architecture, and
implementation of the prom import framework. subsequently, section 5 gives
an overview about target systems for which import plug-ins have already been
developed, after which section 6 draws conclusions.
2 process mining and prom
process-aware information systems, such as wfms, erp, crm and b2b sys-
tems, need to be conﬁgured based on process models specifying the order in
which process steps are to be executed [1]. creating such models is a complex
and time-consuming task for which diﬀerent approaches exist. the most tradi-
tional approach is to analyze and design the processes explicitly, making use of
a business process modeling tool. however, this approach has often resulted in
discrepancies between the actual business processes and the ones as perceived
by designers [3]; therefore, very often, the initial design of a process model is
incomplete, subjective, and at a too high level. instead of starting with an ex-
plicit process design, process mining aims at extracting process knowledge from
“process execution logs”.
process mining techniques such as the alpha algorithm [4] typically assume
that it is possible to sequentially record events such that each event refers to3
an activity (i.e., a well-deﬁned step in the process) and to a case (i.e., a process
instance). moreover, there are other techniques explicitly using additional in-
formation, such as the performer and timestamp of the event, or data elements
recorded with the event (e.g., the size of an order).
this information can be used to automatically construct process models, for
which various approaches have been devised [6, 8, 11, 12]. for example, the alpha
algorithm [4] can construct a petri net model describing the behavior observed
in the log. the multi-phase mining approach [9] can be used to construct an
event-driven process chain (epc) [14] based on similar information. at this
point in time there are mature tools such as the prom framework to construct
diﬀerent types of models based on real process executions [10].
so far, research on process mining has mainly focused on issues related to con-
trol ﬂow mining. diﬀerent algorithms and advanced mining techniques have been
developed and implemented in this context (e.g., making use of inductive learn-
ing techniques or genetic algorithms). tackled problems include concurrency
and loop backs in process executions, but also issues related to the handling of
noise (e.g., exceptions). furthermore, some initial work regarding the mining of
other model perspectives (e.g., organizational aspects) and data-driven process
support systems (e.g., case handling systems) has been conducted [2].
3 the mxml format
themxml format (as in mining xml ) is a generic xml-based format suitable
for representing and storing event log data. while focusing on the core informa-
tion necessary to perform process mining, the format reserves generic ﬁelds for
extra information that is potentially provided by a pais.
fig. 2. schema of the mxml format (uml diagram)4
the structure of an mxml document is depicted in figure 2, in the format
of a uml 2.0 class diagram. the root node of each mxml document is a work-
ﬂowlog , representing a log ﬁle. every workﬂow log can potentially contain one
source element, which is used to describe the system the log has been imported
from.
a workﬂow log can contain an arbitrary number of processes as child ele-
ments. each element of type “process” groups events having occurred during the
execution of a speciﬁc process deﬁnition. the single executions of that process
deﬁnition are represented by child elements of type processinstance . thus, each
process instance represents one speciﬁc case in the system.
finally, process instances each group an arbitrary number of audittrailentry
child nodes, each describing one speciﬁc event in the log. every audit trail entry
must contain at least two child elements: the workﬂowmodelelement describes
the process deﬁnition element to which the event refers, e.g. the name of the task
that was executed. the second mandatory element is the eventtype , describing
the nature of the event, e.g. whether a task was scheduled, completed, etc. two
further child elements of an audit trail entry are optional, namely the timestamp
and the originator . the timestamp holds the exact date and time of when the
event has occurred, while the originator identiﬁes the resource, e.g. person, which
has triggered the event in the system.
the elements described above provide the basic set of information used by
current process mining techniques. to enable the ﬂexible extension of this for-
mat with additional information extracted from a pais, all mentioned elements
(except the child elements of audittrailentry ) can also have a generic data child
element. the data element groups an arbitrary number of attributes , which are
key-value pairs of strings.
4 the prom import framework
while the prom tool suite, which is based on interpreting event log data in the
mxml format, has matured over the last couple of years, there is still a gap in
actually getting these logs in the mxml format. creating event logs in mxml
has, in the past, been mostly achieved by artiﬁcial means, i.e. simulation, or by
ad-hoc solutions which are not applicable to production use.
theprom import framework steps in to bridge this gap. its incentive is, on
the one hand, to provide an adequate and convenient means for process mining
researchers to actually acquire event logs from real production systems. on the
other hand, it gives the owners of processes, i.e. management in organizations
relying on pais operations, a means for productively applying process mining
analysis techniques to their installations.
the following subsection introduces the incentives and high-level goals which
have triggered the development of the prom import framework. section 4.2
derives from these goals a set of prominent design decisions, which form the
basis of the system architecture, introduced in section 4.3.5
4.1 goals and requirements
in order to further progress the ﬁeld of process mining it is essential to adapt and
tailor both present and future techniques towards real-life usage scenarios, such
that process mining can evolve into production use. this evolution fundamentally
depends on the availability of real-life event log data, as only these can provide
the necessary feedback for the development of process mining techniques.
conversely, the process of actually applying process mining techniques in
real world scenarios has to be eased and streamlined signiﬁcantly. while several
successful projects have proved the concept, it is a necessity to improve tool
support for the entire process mining procedure from beginning to end.
a practical process mining endeavor is characterized by three, mainly inde-
pendent, phases: at ﬁrst, the event log data has to be imported from the source
system. secondly, the log data needs to be analyzed using an appropriate set
of process mining techniques. third and last, the results gained from process
mining need thorough and domain-dependent interpretation, to ﬁgure out what
the results mean in the given context, and what conclusions can be drawn.
the process mining specialist is required in the second and third phase, while
the user, or process owner, is involved mainly in the third phase. what makes
the ﬁrst phase stick out is that it is at the moment the one task which can
be performed with the least domain and process mining knowledge involved.
therefore, it is the logical next step for the progression of process mining to
provide adequate and convenient tool support for the event log extraction phase.
a tool for supporting the event log extraction phase should thus address the
following, high-level goals:
– the tool must be relatively easy to operate , such that also less qualiﬁed
personnel can perform the task of event log extraction. this requirement
implies, that:
– by separating a conﬁguration and adjustment phase from the extraction
phase, which can potentially run unattended, the whole process can be lever-
aged and rendered more eﬃcient.
– while ease of use is among the top goals, it must not supersede ﬂexibility
and conﬁgurability of the application. it must be applicable in as great an
array of pais installations as possible.
– the tool must provide an extensible and stable platform for future develop-
ment.
– it is advisable to provide the tool on a free basis , in order to encourage its
widespread use and lower potential barriers for user acceptance. further,
providing the code under an open source license is expected to attract also
external developers to participate. this enables the application to beneﬁt
from community feedback and contribution, thereby greatly leveraging the
tool and, ultimately, process mining as a whole.
the subsequent subsection introduces the design decisions which were derived
from these high-level goals.6
4.2 design decisions
the prom import framework has been developed from scratch, with the fun-
damental goal to provide a most friendly environment for developing import
ﬁlters1. consequently, a strong focus has been on extensibility and stability of
the design, while including as much functionality as possible in the framework
itself.
this emphasis has led to six crucial design choices, which have served as the
cornerstone for developing the architecture of the system:
1. extensibility: the design must incorporate a strict separation between gen-
eral framework code and extension components. an additional requirement
is to shift as much application logic as possible into the core framework, to
prevent code duplication and to ease the development of extensions.
2. anonymization of log information: the framework shall enable users to
anonymize sensitive information contained in event logs in a transparent and
convenient manner, thereby providing a means to protect the log owner’s
intellectual property.
3. flexible log-writing pipeline: a logical log-writing pipeline shall be im-
plemented, allowing to transparently chain a random number of log data-
altering algorithms between event extraction and ﬁnal storage.
4. decoupled conﬁguration management: it shall be suﬃcient for an im-
port routine to specify its conﬁguration options, and their corresponding
types. based on this information, the framework should transparently han-
dle presenting these options to the user and allowing him to change them in
a convenient manner.
5. decoupled dependencies management: one further requirement towards
the framework is to transparently satisfy all import routines’ external re-
quirements, e.g. database connectivity libraries.
6. convenient and decoupled user interface: the application shall be rel-
atively easy to use, i.e. it shall not require the user to have knowledge about
process mining internals or the process of importing the event log data.
these design principles, together with the high-level goals presented in sec-
tion 4.1, have been used as an imperative in shaping the application’s concrete
architecture, which is presented in the following subsection.
4.3 architecture
the architecture reﬂects the design principles stated in section 4.2, and thus
directly supports the high-level goals of section 4.1. figure 3 describes the ab-
stract architecture of the framework, identifying the major classes involved and
their mutual relationships in form of a uml 2.0 class diagram.
a ﬂexible plug-in architecture satisﬁes the requirement for extensibility. for
every target pais implementation, one dedicated import ﬁlter is supposed to
1the distribution is available at http://promimport.sourceforge.net.7
fig. 3. architecture of the prom import framework, core components (uml diagram)
be implemented as a plug-in. each import ﬁlter plug-in is contained within one
dedicated class, derived from the abstract superclass importfilter . from this
base class, every import ﬁlter plug-in inherits a set of methods which it can call
in its constructor, to notify the system of its conﬁguration options and external
dependencies. for the actual import routine, the plug-in is passed an object
implementing the interface filterenvironment , connecting the import ﬁlter to
fundamental framework capabilities during the import procedure.
all elements of the log-writing pipeline implement the logfilter interface,
which allows for their ﬂexible arrangement within the pipeline at will. this in-
terface is used in a sequential manner, i.e. it incorporates methods to start and
ﬁnish log ﬁles, processes and process instances, and a method for passing au-
dit trail entries. the ﬁnal endpoint of the log-writing pipeline is marked by an
object derived from the abstract class logwriter providing basic mxml format-
ting, while actual writing to permanent storage is implemented in logwriter’s
subclasses.
intermediate elements of the log-writing pipeline, such as the loganonymizer ,
are derived from the abstract class logfilterproxy , implementing their transpar-
ent integration into the pipeline. at this point in time the anonymizer component
is the only intermediate pipeline transformer available.
thefiltermanager groups the set of import ﬁlters, provides named access to
them, and provides their conﬁguration within the framework for abstract access
and modiﬁcation. the importcontroller , which incorporates the ﬁlter manager,
manages the persistency of conﬁguration data for the whole application and
transparently manages and satisﬁes import ﬁlters’ external requirements.
the class importfilterframe implements the main graphical user interface
of the application, including basic user interaction logic.
4.4 disk-buﬀered event sorting
the log writing pipeline in the framework expects process instances to be trans-
mitted one after another, while audit trail entries are supposed to be transmitted8
in their natural order (i.e., order of occurrence). as not all import routines can
expect their events in an ordered fashion, the framework provides the plug-in
developer with a simple interface for transmitting unsorted event data, while
ensuring that the sorting takes place in a transparent, resource-eﬃcient manner.
fig. 4. disk-buﬀered sorting in the framework
as this concept implies that all audit trail entries of an import session have to
be buﬀered, before the ﬁrst of them can be written, the process instance buﬀers
are implemented to swap their content partially to disk storage.
this disk-buﬀered sorting mechanism is described in figure 4.
1. every buﬀer is equipped with a ﬁxed-size buﬀer residing in heap space. this
heap buﬀer is ﬁlled, as new audit trail entries are added to the process
instance buﬀer.
2. when the heap buﬀer is completely ﬁlled with audit trail entries, it needs to
beﬂushed . first, the events contained within the heap buﬀer are sorted using
a quicksort [13] algorithm. then, all events in the heap buﬀer are appended
to a swap ﬁle. thus, the swap ﬁle contains subsequent segments, of which
each contains a ﬁxed number of sorted audit trail entries corresponding to
one ﬂush operation.
3. after all events have been received, the buﬀer needs to be emptied into the
log writing pipeline in a sorted manner. an array called the merge table ,
with one cell per ﬂush segment in the swap ﬁle, is initially ﬁlled with the
ﬁrst audit trail entry from each segment. then, a modiﬁed merge sort [15]
algorithm picks the ﬁrst (in terms of logical order) event from the merge
table, writes it to the log writing pipeline, and replaces it with the next
entry from the respective ﬂush segment in the swap ﬁle. this procedure is
repeated, until all audit trail entries from the swap ﬁle have been loaded and
the merge table is empty.
the presented disk-buﬀered sorting mechanism manages to eﬀectively limit
memory usage of the application. at the same time, a performance lag due to
disk i/o is minimized by pre-buﬀering and sorting events in the heap buﬀer.
note that the algorithm scales well with the degree, in which incoming audit
trail entries are already ordered. the less audit trail entries are in wrong order,
the faster the initial sorting can be performed.9
4.5 user interface
fig. 5. user interface of the prom import framework
the graphical user interface, which is depicted in figure 5, is kept rather
simple. on the left, an overview list allows the user to pick the import ﬁlter
plug-in to be used. the upper right part shows general import ﬁlter properties,
such as name, description, and author. further, this part includes controls for
the import procedure and the log anonymizer component.
the lower right part of the interface can either display a console view, or a
conﬁguration pane allowing to modify conﬁguration settings for import ﬁlters.
when the import procedure is started, the view switches to show the console,
which is used to display feedback and error messages to the user.
5 target systems
the number of target systems, for which import plug-ins have been developed,
has been steadily growing and diversifying since the development of the prom
import framework began2. on the one hand, this development is driven by
advances in industry and practice, making ever more real-life pais implemen-
tations available for process mining research. on the other hand, this research
triggers new applications from within, thus extending the ﬁeld of “interesting”
target systems.
in both directions, the ﬂexible and extensible architecture of the prom im-
port framework has allowed developers to quickly implement solid and versatile
solutions, taking advantage of the broad set of support functionality and clean
2the current distribution of the framework, including all plug-ins, can be downloaded
from http://promimport.sourceforge.net.10
user interface which the framework provides. at the time of this writing, there
exist import plug-ins for the following target systems:
flow er:this product is an implementation of the case handling paradigm,
which represents a very ﬂexible, data-driven approach within the greater
family of workﬂow management systems.
websphere process choreographer: as a part of ibm’s websphere suite,
the process choreographer is used to implement high-level business pro-
cesses, based on the bpel language.
staﬀware: a workﬂow management system in the traditional sense, which has
an impressive market coverage.
peoplesoft financials: part of the peoplesoft suite for enterprise resource
planning (erp), this module is concerned with ﬁnancial administration
within an organization.
cpn tools: cpn tools provides excellent tool support for modelling colored
petri nets (cpn), a family of high-level petri nets, including a simulation
engine for executing models. an extension to cpn tools has been developed,
allowing to create synthetic event logs during a model simulation.
cvs: the process of distributed software development, as reﬂected in the com-
mits to a source code repository like cvs, can also be analyzed with tech-
niques from the process mining family.
subversion: the subversion system addresses fundamental ﬂaws present in
cvs, providing change logs that can also be interpreted by means of process
mining.
apache 2: as the access logs of web servers, like apache 2, reveal the identity of
users from their ip, the exact time and items requested, it is straightforward
to distill process event logs from them.
as diverse as this list may read, it shows the impressive capabilities of the
framework in enabling rapid development of import capabilities. the complexity
of demanding import ﬁlters is signiﬁcantly reduced by standard functionality
oﬀered by the framework. on top of that, the existence of a powerful framework
allows for rapid prototyping of event log import capabilities.
thereby it stimulates and supports experiments with less obvious systems,
which may otherwise have been deemed not worth the eﬀort. these can serve
as eﬀective and eﬃcient means to evaluate the feasibility and usefulness of an
import eﬀort. an excerpt of ad-hoc solutions to import custom data sets, which
were rapidly and successfully implemented using the prom import framework,
includes:
– import of event logs describing the process of patient treatments from raw
database tables provided by a dutch hospital.
– production unit test logs from an international manufacturer of ic chip
production equipment.
– conversion of spreadsheets containing patient treatment processes, from an
ambulant care unit in israel and a large dutch hospital.
– versatile and highly conﬁgurable import from the wfms adept [16], which
is known for its rich set of features addressing ﬂexibility.11
6 conclusions
the mxml format is the most widely adopted standard for the storage of pro-
cess event logs in process mining research. this is most notably due to the fact
that the prom framework, providing a wide selection of process mining analysis
techniques, relies on mxml for reading event logs.
however, due to a lack of convenient conversion tools, the availability of real-
life event logs in mxml format has not been satisfactory so far. on the one
hand, this lack of actual logs had a serious impact on the credibility of process
mining techniques with respect to real-life applications. on the other hand, these
techniques could not be used to analyze and improve industrial processes, and
could thus not be put to use in real-life organizations.
in this paper, we have presented the prom import framework, which is eﬀec-
tively bridging this gap. it represents a typical enabling technology , connecting
formerly separate areas to their mutual beneﬁt. in its current release, this appli-
cation already features import plug-ins supporting seven process-aware informa-
tion systems. most notably, the support for commercial systems like flow er,
websphere, and staﬀware covers an immense installed base of users. additional
functionality that has been shifted into the framework makes the development
of additional import plug-ins a convenient, time-eﬀective task.
we hold this extension to the process mining tool landscape to be crucial with
respect to the quality and credibility of process mining research. real-life event
log data often exhibits awkward and strange properties, which are unforeseen on
a theoretical level, and which have to be taken into account in order to obtain
meaningful results. it is only after process mining techniques have been proven
to successfully analyze real-life logs, and thus to beneﬁt businesses in their daily
operations, that these techniques can grow into productive tools for business
process optimization.
7 acknowledgements
this research is supported by the technology foundation stw, applied sci-
ence division of nwo and the technology programme of the dutch ministry of
economic aﬀairs.
references
1. w.m.p. van der aalst and k.m. van hee. workﬂow management: models, methods,
and systems . mit press, cambridge, ma, 2002.
2. w.m.p. van der aalst and m. song. mining social networks: uncovering interac-
tion patterns in business processes. in j. desel, b. pernici, and m. weske, editors,
international conference on business process management (bpm 2004) , volume
3080 of lecture notes in computer science , pages 244–260. springer-verlag, berlin,
2004.12
3. w.m.p. van der aalst, b.f. van dongen, j. herbst, l. maruster, g. schimm, and
a.j.m.m. weijters. workﬂow mining: a survey of issues and approaches. data
and knowledge engineering , 47(2):237–267, 2003.
4. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128–1142, 2004.
5. w.m.p. van der aalst, m. weske, and d. gr¨ unbauer. case handling: a new
paradigm for business process support. data and knowledge engineering ,
53(2):129–162, 2005.
6. r. agrawal, d. gunopulos, and f. leymann. mining process models from work-
ﬂow logs. in sixth international conference on extending database technology ,
pages 469–483, 1998.
7. pallas athena. case handling with flower: beyond workﬂow . pallas athena
bv, apeldoorn, the netherlands, 2002.
8. j.e. cook and a.l. wolf. discovering models of software processes from event-
based data. acm transactions on software engineering and methodology ,
7(3):215–249, 1998.
9. b.f. van dongen and w.m.p. van der aalst. multi-phase process mining: building
instance graphs. in p. atzeni, w. chu, h. lu, s. zhou, and t.w. ling, editors, in-
ternational conference on conceptual modeling (er 2004) , volume 3288 of lecture
notes in computer science , pages 362–376. springer-verlag, berlin, 2004.
10. b.f. van dongen, a.k. de medeiros, h.m.w. verbeek, a.j.m.m. weijters, and
w.m.p. van der aalst. the prom framework: a new era in process mining tool
support. in g. ciardo and p. darondeau, editors, proceedings of the 26th interna-
tional conference on applications and theory of petri nets (icatpn 2005) , vol-
ume 3536 of lecture notes in computer science , pages 444–454. springer-verlag,
berlin, 2005.
11. d. grigori, f. casati, m. castellanos, u. dayal, m. sayal, and m.c. shan. business
process intelligence. computers in industry , 53(3):321–343, 2004.
12. j. herbst and d. karagiannis. an inductive approach to the acquisition and
adaptation of workﬂow models. in m. ibrahim and b. drabble, editors, proceed-
ings of the ijcai’99 workshop on intelligent workﬂow and process management:
the new frontier for ai in business , pages 52–57, stockholm, sweden, august
1999.
13. c.a.r. hoare. algorithm 64: quicksort. commun. acm , 4(7):321, 1961.
14. g. keller, m. n¨ uttgens, and a.w. scheer. semantische processmodellierung auf
der grundlage ereignisgesteuerter processketten (epk). ver¨ oﬀentlichungen des
instituts f¨ ur wirtschaftsinformatik, heft 89 (in german), university of saarland,
saarbr¨ ucken, 1992.
15. d.e. knuth. the art of computer programming , volume 3: sorting and searching.
addison wesley, reading, ma, usa, 2 edition, 1998.
16. m. reichert and p. dadam. adeptﬂex: supporting dynamic changes of
workﬂow without loosing control. journal of intelligent information systems ,
10(2):93–129, 1998.