noname manuscript no.
(will be inserted by the editor)
case notion discovery and recommendation
automated event log building on databases
e. gonz ´alez l ´opez de murillas 
h.a. reijers w.m.p. van der aalst
received: date / accepted: date
abstract process mining techniques use event logs as input. when analyzing complex
databases, these event logs can be built in many ways. events need to be grouped into traces
corresponding to a case. different groupings provide different views on the data. building
event logs is usually a time-consuming, manual task. this paper provides a precise view on
the case notion on databases, which enables the automatic computation of event logs. also,
it provides a way to assess event log quality, used to rank event logs with respect to their in-
terestingness. the computational cost of building an event log can be avoided by predicting
the interestingness of a case notion, before the corresponding event log is computed. this
makes it possible to give recommendations to users, so they can focus on the analysis of
the most promising process views. finally, the accuracy of the predictions and the quality
of the rankings generated by our unsupervised technique are evaluated in comparison to ex-
isting regression techniques as well as to state of the art learning to rank algorithms from
the information retrieval ﬁeld. the results show that our prediction technique succeeds at
discovering interesting event logs and provides valuable recommendations to users about
the perspectives on which to focus the efforts during the analysis.
keywords process miningevent logdatabasecase notionrecommendation
ranking
e. gonz ´alez l ´opez de murillash.a. reijersw.m.p. van der aalst
department of mathematics and computer science
eindhoven university of technology, eindhoven, the netherlands
{e.gonzalez,h.a.reijers,w.m.p.v.d.aalst}@tue.nl
edu.gonza.lopez@gmail.com
h.a. reijers
department of information and computing sciences
universiteit utrecht, utrecht, the netherlands
h.a.reijers@uu.nl
w.m.p. van der aalst
department of computer science
rwth aachen university, aachen, germany
wvdaalst@pads.rwth-aachen.de2 e. gonz ´alez l ´opez de murillas et al.
fig. 1 example of database schema types: (a) star, (b) snowﬂake, and (c) arbitrary.
1 introduction
process mining [2] is a ﬁeld of data science devoted to the analysis of process behavior.
this data-driven analysis makes it possible to discover models, analyze performance, detect
deviations, identify bottlenecks and inefﬁciencies, make improvements, monitor the behav-
ior, and make predictions, all related to business processes in a large variety of domains.
to perform these kinds of analyses, process mining techniques require event logs as input.
an event log is a set of process instances or traces, each of which contains a set of events.
events represent occurrences of process tasks or activities at a certain point in time.
obtaining event logs is not a trivial matter. data extraction and preparation are, very
often, the most time-consuming tasks (around 80% of the time) and one of the most costly
(around 50% of the cost) in data analysis projects [30]. this is due to the fact that data comes
in many forms, while a lot of manual work and domain knowledge is needed to obtain
meaningful event logs from it. additionally, not all systems worth analyzing are process-
aware information systems (pais), i.e., event data is not explicitly recorded as a ﬁrst-class
citizen within the system. if that is the case, additional work needs to be performed to obtain
the events required to build logs for analysis. another reason for the high cost in time and
effort of the event log building phase is that, in many cases, domain knowledge about the
system at hand is simply not available. analysts need to interview the business owners and
database managers to understand what parts of the event data can be interesting to look into.
this interaction often requires several iterations and a large time investment from all parties.
the principal idea behind log building is to correlate events in such a way that they
can be grouped into traces to form event logs. classical approaches would use a common
attribute to correlate events. this is a valid method in scenarios where the data schema has
a star shape [16] (figure 1.a): there is a central table and the rest are directly related to it,
with at least one column in common, which can be used as a case notion. however, we
consider the scenario in which some pairs of events may not have any attribute in common.
this is the case for a snowﬂake schema [16] (figure 1.b), which resembles the shape of a
star schema, with the difference that, at the points, we ﬁnd tables that only hold a transitive
relation with the central table. in practice, we often ﬁnd databases which schema presents
a higher complexity than a star or snowﬂake structure (figure 1.c). in that case, there are
many combinations in which events can be grouped. these combinations cannot be arbitrary,
but must obey some criteria with a business meaning, e.g., group the invoice anddelivery
events by means of the invoice idﬁeld present in the former ones. also, more complex
combinations can be deﬁned when transitive relations are considered for the grouping, e.g.,
group the invoice ,delivery , and billevents according to the ﬁeld invoice idin delivery events
and the ﬁeld delivery idin the bill events. each of these examples capture what we will refer
to as a case notion , i.e., a way to look at event data from a speciﬁc perspective.
when dealing with vast datasets from complex databases, the existence of many po-
tential case notions is evident. enterprise resource planning (sap, oracle ebs, dolibarr),case notion discovery and recommendation 3
hospital information systems (chipsoft, ge centricity, agfa integrated care), and cus-
tomer relationship management (salesforce, ms dynamics, sugarcrm) are examples of
systems powered by large databases where multi-perspective analysis can be performed.
according to different case notions, many different event logs can be built. the research
problem we tackle in this paper is how to choose the right perspective on the data, which
is a crucial step in order to obtain relevant insights . it is common practice to perform this
selection by hand-written queries, usually by an analyst with the right domain knowledge
about the system and process under study. however, when facing complex data schemas,
writing such queries can become a very complicated task, especially when many tables are
involved.
a naive way to tackle the exploration of complex databases is to automatically gen-
erate all the possible case notions as combinations of tables. this can lead to many event
log candidates, even for a small database. the combinatorial problem is aggravated in more
complex scenarios, i.e. with hundreds of tables involved. given a weakly connected1data
schema of 90 tables, there exist 4 005 combinations of pairs of tables2. if we consider com-
binations of 3 tables instead, the number increases to 117 480, even before considering the
many different paths that could connect the tables in each combination. in such cases, the
automated building of logs for all possible table combinations may still be possible, but has
proven to be computationally very expensive: in the hypothetical case that building an event
log would take 4 seconds on average, building the event logs for a data schema with 90 ta-
bles and 10 000 possible case notions would take approximately 11 hours. even if we spend
the time to compute all of them, we still need to inspect 10 000 event logs to ﬁnd out which
perspective is both meaningful and interesting.
a way to mitigate the combinatorial explosion is to reduce the case notion search space
as much as possible. identifying the most interesting event logs would help to prioritize the
most promising views on the data for its analysis. the challenge of identifying the most
promising views is related to the log quality problem. the log quality problem is concerned
with identifying the properties that make an event log more suitable to be analyzed, i.e. the
characteristics that increase the probability of obtaining valuable insights from the analysis
of such an event log. the choices made during the log building process have an effect on
the log quality [19]. also, metrics to assess structural log properties have been proposed by
some authors [17], which may be important to assess log quality.
the main contributions of this work are: (a) formally deﬁning complex case notions
to adopt different perspectives on event data; (b) automatically generating candidate case
notions on a dataset; (c) assessing the quality of the resulting event logs; (d) automatically
predicting an event log’s quality before it is built; (e) sorting the case notions according to
their relative quality from the analysis point of view. this drastically reduces the compu-
tational cost avoiding the generation of uninteresting event logs. in order to achieve these
goals, data must be extracted from the original system and transformed to ﬁt into a certain
structure. this structure should be able to capture both the process and the data sides of
the system under study. the techniques proposed in this paper have been implemented in
a framework and evaluated with respect to related ranking algorithms. the approach yields
1weakly connected graph: a directed graph such that, after replacing all of its directed edges with undi-
rected ones, it produces a connected graph. a connected graph is one such that, for any pair of nodes (a, b),
there is a path from a to b.
2for a set of nelements ( ntables), the number of k-combinations (combinations of ktables) is n
k
=
n!
k!(n k)!4 e. gonz ´alez l ´opez de murillas et al.
fig. 2 high level structure of the openslex meta model.
promising results in terms of performance and accuracy on the computation of event log
rankings.
the paper is structured as follows. section 2 introduces some preliminary concepts
about how information contained in databases can be extracted and structured. section 3
introduces a running example. section 4 deﬁnes the concept of case notion and proposes
a formalized way to build event logs. section 5 provides a way to automatically assess the
quality of event logs. section 6 proposes a technique to predict the quality of an event log
before it is computed, reducing the computation time several orders of magnitude. section 7
presents the implementation of all the techniques described in this work. the result of the
evaluation is presented in section 8. related work is discussed in section 9. lastly, sec-
tion 10 presents the conclusions of this study.
2 preliminaries
to enable the application of process mining and the techniques proposed in this work, we
need access to the database of the system under study. this information should be extracted
and transformed to ﬁt into a speciﬁc data structure. an appropriate structure has been pre-
viously deﬁned as a meta model [25] and implemented in a queryable ﬁle format called
openslex. figure 2 shows a high level view of the meta model that describes the open-
slex format. the meta model captures all the necessary aspects to enable the application
of our techniques. this section describes the structure of openslex and provides the nec-
essary background to understand the techniques proposed in the coming sections.
standards of reference like xes [1] are focused on the process view (events, traces, and
logs) of systems. openslex supports all concepts present in xes, but in addition, also
considers the data elements (data model, objects, and versions) as an integral part of its
structure. this makes it more suitable for database environments where only a small part of
the information is process-oriented (i.e. events) with respect to the rest of data objects of dif-
ferent classes that serve as an augmented view of the process information. the openslex
format is supported by a meta model that considers data models andprocesses as the entities
at the highest abstraction level. these entities deﬁne the structure of more granular elements
likelogs,cases , and activity instances with respect to processes, and objects with respect
to classes in the data model. each of these elements at the intermediate level of abstraction
can be broken apart into more granular pieces. this way, cases are formed by events , and
objects can be related to several object versions . both events andobject versions represent
different states of a higher level abstraction ( cases orobjects ) at different points in time.
figure 3 depicts the entity-relation diagram of the openslex format. some elements of
the meta-model have been omitted from the diagram for the sake of simplicity. a full versioncase notion discovery and recommendation 5
fig. 3 er diagram of the openslex meta-model. the entities have been grouped into sectors, delimited by
the dashed lines.6 e. gonz ´alez l ´opez de murillas et al.
of the er diagram is available online3. each of the entities in the diagram, as represented by
a square, corresponds to the basic entities of the meta-model as formalized in deﬁnition 2.
also, these entities, together with their relations (diamond shapes), have been grouped in ar-
eas that we call sectors (delimited by dashed lines). these sectors are: data models ,objects ,
versions ,events ,cases , and process models . these tightly related concepts provide an ab-
breviated representation of the meta-model. as can be observed, the entity-relation diagram
is divided into six sectors. the purpose of each of them is described below:
– data models : this sector is formed by concepts needed to describe the structure of any
database system. many data models can be represented together in this sector, whose
main element is the data model entity. for each data model, several classes can exist.
these classes are abstractions of the more speciﬁc concept of table, which is commonly
found in rdbmss. classes contain attributes , which are equivalent to table columns in
modern databases (e.g., id,name ,address , etc.). the references between classes of the
same data model are represented with the relationship entity. this last entity holds links
between a source and a target class.
– objects : the object entity, part of the objects sector, represents each of the unique data
elements that belong to a class. an example of this can be a hypothetical customer with
customer id= 75. additional details of this object are omitted, given that they belong to
the next sector.
– versions : for each of the unique object entities described in the previous sector, one
or many versions can exist. a version is an instantiation of an object during a certain
period of time, e.g., the customer object with id 75, existed in the database, during a cer-
tain period of time, for example from “2015-08-01 14:45:00” to “2016-09-03 12:32:00”.
during that period of time, the object had speciﬁc values for the attributes of the cus-
tomer class that it belongs to. therefore, there is a version of customer 75, valid between
the mentioned dates, with name “john smith”, address “45, 5th avenue”, and birth date
“1990-01-03”. if at some point, the value of one of the attributes changed (e.g., a new ad-
dress), the end timestamp of the previous version would be set to the time of the change,
and a new version would be created with the updated value for that attribute, and a
start timestamp equal to the end of the previous version, e.g., version 1 =fobject id
= 75, name = “john smith”, address = “45, 5th avenue”, birth date = “1990-01-03”,
start timestamp = “2015-08-01 14:45:00”, end timestamp = “2016-09-03 12:32:00” g,
and version 2 =fobject id = 75, name = “john smith”, address = “ﬂoor 103, empire
state building”, birth date = “1990-01-03”, start timestamp = “2016-09-03 12:32:00”,
endtimestamp = none g. note that the value of endtimestamp for the newly created
object version ( version 2) is none. that means that it is the current version for the
corresponding object ( object id= 75). another entity reﬂected in this sector is the con-
cept of relation . a relation is an instantiation of a relationship, and holds a link between
versions of objects that belong to the source and target classes of the relationship. for
example, a version of a booking object can be related to another version of a customer
object by means of a relation instance, as long as a relationship exists from class booking
to class customer .
– events : this sector collects a set of events, obtained from any available source (database
tables, redo-logs, change records, system logs, etc.). in this sector, events appear as a
collection, not grouped into traces (such grouping is reﬂected in the next sector). in
order to keep process information connected to the data side, each event can be linked
to one or many object versions by means of a label ( eventtoovlabel ). this label allows
3https://github.com/edugonza/openslex/blob/master/doc/meta-model.pngcase notion discovery and recommendation 7
fig. 4 diagram of an instance of the openslex meta-model.
specifying what kind of interaction exists between the event and the referred object
version, e.g., insert ,update ,delete ,read, etc. events hold details such as timestamp ,
life-cycle , and resource information, apart from an arbitrary number of additional event
attributes.
– cases and instances : the entities present in this sector are very important from the pro-
cess mining point of view. the events by themselves do not provide much information
about the control ﬂow of the underlying process, unless they are correlated and grouped
into traces (or cases). first, the activity instance entity should be explained. this entity
is used to group events that refer to the same instance of a certain activity with different
values for its life-cycle, e.g., the execution of an activity generates one event for each
phase of its life-cycle. both events, referring to the same execution of an activity, are
grouped into the same activity instance. next, as in any other event log format, activity
instances can be grouped in cases , and these cases, together, form a log.
– process models : the last sector contains information about processes . several processes
can be represented in the same meta-model. each process is related to a set of activities ,
and each of these activities can be associated with several activity instances, contained
in the corresponding cases and instances sector.
figure 4 shows an example of an instance of the openslex meta-model. for the sake
of clarity the model has been simpliﬁed, but the main structure remains. we see that there
is a global data model . all the classes belong to it: “customer” and “booking”. also, there
are three attributes : “name”, “address”, and “bookingdate”. the ﬁrst two attributes be-
long to the class “customer”. the third one belongs to “booking”. there is a relationship
connecting bookings to customers named “booking tocustomer”. two objects exist. the
ﬁrst object has two versions . each version of the customer object has values for the corre-
sponding attributes. we see that the ﬁrst customer version corresponds to a customer named
“edu” while he lived in “spain”, from 1986 to 2014. the second version corresponds to the
same customer, while he lived in “the netherlands” from 2014 until the present. there is
another object version that belongs to the second object, a booking object. the “booking-
date” value of this version is “2019”. there is a relation (an instance of the relationship
“booking tocustomer”), that connects the second object version of customer 1to the ﬁrst
object version of booking 1. on the left side of the ﬁgure, we see that three events exist.
the ﬁrst event, related to the ﬁrst version of customer 1, is linked to the activity “born”, and8 e. gonz ´alez l ´opez de murillas et al.
happened in 1986. the second event, linked to the activity “move”, happened in 2014 and
is related to the second version of the same customer. finally, the third event is linked to
the activity “book”, and is linked to the ﬁrst version of booking 1. each event belongs to its
own activity instance . all activity instances belong to one case. this case belongs to a log
of the process “life”.
the openslex format makes use of a sql schema to store all the information and a
java api4is available for its integration in other tools. an evaluation of the use of open-
slex [25] in several environments tackles the data extraction and transformation phase
and demonstrates its ﬂexibility and potential to enable standard querying and advanced data
analyses. to keep this paper self-contained and to provide the necessary background for the
understanding of this work, a simpliﬁed version of the meta model is formally presented
below. every database system contains information structured with respect to a data model.
deﬁnition 1 provides a formalization of a data model in the current context.
deﬁnition 1 (data model) a data model is a tuple dm = (cl;at;classofattribute ;
rs;sourceclass ;targetclass )such that
- cl is a set of class names,
- at is a set of attribute names,
-classofattribute2at!clis a function that maps each attribute to a class,
- rs is a set of relationship names,
-sourceclass2rs!clis a function mapping each relationship to its source class,
-targetclass2rs!clis a function mapping each relationship to its target class
data models contain classes (i.e. tables), which contain attribute names (i.e. columns).
classes are related by means of relationships (i.e. foreign keys). deﬁnition 2 formalizes each
of the entities of the meta model and shows the connection between them.
deﬁnition 2 (connected meta model) letvbe some universe of values and ts a
universe of timestamps. a connected meta model is deﬁned as a tuple cmm = (dm;oc;
classofobject ;ovc;objectofversion ;ec;eventtoovlabel ;ic;eventai;pmc;
activityofai ;processoflog )such that
-dm = (cl;at;classofattribute ;rs;sourceclass ;targetclass )is a data model,
-oc is an object collection,
-classofobject2oc!clis a function that maps each object to a class,
-ovc = (ov;attvalue;starttimestamp ;endtimestamp ;rel)is a version collec-
tion where ov is a set of object versions, attvalue2(atov)6!vis a mapping
of pairs of object version and attribute to a value, starttimestamp2ov!tsis a
mapping between object versions and start timestamps, endtimestamp2ov!tsis
a mapping between object versions and end timestamps, and rel(rsovov)
is a set of triples relating pairs of object versions through a speciﬁc relationship,
-objectofversion2ov!oc is a function that maps each object version to an object,
-ec is an event collection such that ec = ( ev;evat;eventtimestamp ;
eventlifecycle ;eventresource ;eventattributevalue )where ev is a set of events,
evat a set of event attribute names, eventtimestamp 2ev!tsmaps events
to timestamps, eventlifecycle2ev! fstart;complete;:::gmaps events to life-
cycle attributes, eventresource2ev!vmaps events to resource attributes, and
eventattributevalue 2(evevat )6!vmaps pairs of event and attribute name
to values,
4https://github.com/edugonza/openslexcase notion discovery and recommendation 9
-eventtoovlabel 2(evov)6!vis a function that maps pairs of an event and
an object version to a label. the existence of a label associated to an event and an object
version, i.e. (ev;ov)2dom(eventtoovlabel ), means that both event and object ver-
sion are linked. the label deﬁnes the nature of the link, e.g “insert”, “update”, “delete”,
etc,
-ic= (ai;cs;lg;aisofcase;casesoflog )is an instance collection where aiis a
set of activity instances, csis a set of cases, lgis a set of logs, aisofcase2cs!
p(ai)is a mapping between cases and sets of activity instances5, and casesoflog2
lg!p (cs)is a mapping between logs and sets of cases,
-eventai2ev!aiis a function that maps each event to an activity instance,
-pmc = (pm;ac;actofproc )is a process model collection where pm is a set of
processes, ac is a set of activities, and actofproc2pm!p (ac)is a mapping
between processes and sets of activities,
-activityofai2ai!acis a function that maps each activity instance to an activity,
-processoflog2lg!pm is a function that maps each log to a process.
a connected meta model provides the functions that make it possible to connect all the
entities in the meta model. however, some constraints must be fulﬁlled for a meta model
to be considered a valid connected meta model (e.g. versions of the same object do not
overlap in time). the details about such constraints are out of the scope of this paper, but
their description can be found in [25]. from now on, any reference to input or extracted data
will assume to be in the form of a valid connected meta model. as we have seen, according
to our meta model description, events can be linked to object versions , which are related to
each other by means of relations . these relations are instances of data model relationships .
in database environments, this would be the equivalent of using foreign keys to relate table
rows and knowing which events relate to each row. for the purpose of this work, we assume
that pairwise correlations between events, by means of related object versions, are readily
available in the input meta model. this means that, prior to the extraction, we know the
data schema, i.e., primary and foreign keys, and how events are stored in each table, e.g.,
which columns contain the timestamp and activity name of each event. the ﬁrst precondition
(knowing the data schema) is fair to assume in most real-life environment. given the lack
of automated approaches in the literature that tackle the challenge of event data discovery,
the second precondition (knowing the events) requires having the right domain knowledge
in order to extract events. the presented meta model formalization sets the ground for the
deﬁnition of case notion andlogthat will be presented in the coming sections.
3 running example
extracting data contained in an information system’s database is a complex task. very of-
ten, we lack the domain knowledge needed to identify business objects and meaningful case
notions. also, understanding complex data schemas can be challenging when the number of
tables is beyond what can be plotted and explored intuitively. consider for example the sap
erp system. this widespread erp system is often a target for process mining analysis, as
it is used in a multitude of organizations, and contains a huge amount of functionalities by
means of conﬁgurable modules. sap can run on different database technologies. and its
instances always maintain a common data model, which is well-known for its complexity.
5p(x)is the powerset of x, i.e.,y2p(x)ifyx.10 e. gonz ´alez l ´opez de murillas et al.
adr2t005
tcurcadrcadrct adrt cskt dd07t
bkpft001
t003t880
bsegekkoekpo
lfa1mara
t001kt001w
t042z
tbslt024et161ebaneinat001l t023 marm
csks
usr02lfbk lfc1
t024dt024
t161s
eine
ekabekbet156
ekes
eket
ekknlfb1makt mbew
lfm1mlan
marct003t
mkpft005t
mseg
rbco
rbkp
reguh
reguprsegt006a t007s t008t t023t t023ut t052u t077y t134t t156t t158w t161tt161u t163c t163f t163i t163m t163y t16ft t460t t681b t685t tbdlsttbslt tcurf tcurr tcurx tinct tka02 tstct tvzbt
fig. 5 general view of the data model of the sap dataset (the table attributes have been omitted).
sap represents a prime example because it is a widely used system. nevertheless, the ap-
proach is highly generic and can be applied in different environments, e.g., alternative erp
tools such as oracle ebs, his solutions such as chipsoft, and crm systems like sales-
force. figure 5 depicts the data model of a sample sap dataset. this dataset, belonging to
sap ides (internet demonstration and evaluation system), is an instance of a ﬁctitious
organization. it contains more than 7m data objects of 87 different classes and more than
26k events corresponding to changes for a subset of the objects present in the database. in
the diagram, classes are represented by squares, while edges show the relationships between
classes. table names in sap are codiﬁed in such a way that it is not easy to identify what
these classes mean without further documentation. also, most of the relevant classes are
connected to many other. this makes it very difﬁcult to plot the graph in such a way that
clusters of classes can be easily identiﬁed.
figure 6 shows in detail a small portion of the graph, where we observe that the ekko
(purchasing document header) class is linked, among others, to the ekpo (purchasing
document item) class. also, the eban (purchase requisition) class is connected to both.
additionally, the class eket (scheduling agreement schedule lines) is linked to eban .
according to the ofﬁcial documentation, both ekko (header table) and ekpo (item ta-
ble) refer to purchasing documents. the eban class contains information about purchase
requisition and the eket class contains schedule lines related to a scheduling agreement.
this could very well be a valid case notion, if we use the connection between the four ta-
bles to correlate the corresponding events in traces. however, there are many ways in whichcase notion discovery and recommendation 11
adr2t005
tcurcadrcadrct adrt cskt dd07t
bkpft001
t003t880
bsegekkoekpo
lfa1mara
t001kt001w
t042z
tbslt024et161ebaneinat001l t023 marm
csks
usr02lfbk lfc1
t024dt024
t161s
eine
ekabekbet156
ekes
eket
ekknlfb1makt mbew
lfm1mlan
marct003t
mkpft005t
mseg
rbco
rbkp
reguh
reguprsegt006a t007s t008t t023t t023ut t052u t077y t134t t156t t158w t161tt161u t163c t163f t163i t163m t163y t16ft t460t t681b t685t tbdlsttbslt tcurf tcurr tcurx tinct tka02 tstct tvzbt
adr2t005
tcurcadrcadrct adrt cskt dd07t
bkpft001
t003t880
bsegekkoekpo
lfa1mara
t001kt001w
t042z
tbslt024et161ebaneinat001l t023 marm
csks
usr02lfbk lfc1
t024dt024
t161s
eine
ekabekbet156
ekes
eket
ekknlfb1makt mbew
lfm1mlan
marct003t
mkpft005t
mseg
rbco
rbkp
reguh
reguprsegt006a t007s t008t t023t t023ut t052u t077y t134t t156t t158w t161tt161u t163c t163f t163i t163m t163y t16ft t460t t681b t685t tbdlsttbslt tcurf tcurr tcurx tinct tka02 tstct tvzbt
fig. 6 detail of the data model of the sap dataset. ekko andekpo tables refer to purchase documents,
while eban contains information about purchase requisitions.
this correlation could be constructed. one-to-many relationships can exist between classes,
which leads to the well known problems of data divergence (several events of the same type
are related to a single case) and data convergence (one event is related to multiple cases), as
described in [21]. this means that the combination of a subset of classes can yield several,
different event logs, depending on the choices made to correlate the events. should all the
purchase items or the same purchase requisition be grouped in the same trace? should one
trace per purchase item exist? would that mean that the same purchase requisition events
would be duplicated in different traces? the fact that these choices exist makes the process
of log building a non-trivial task. section 4 provides a deﬁnition of case notion and presents
a framework to build event logs effectively, taking into account the aforementioned choices
in a formal manner.
4 case notions and log building
as we have discussed earlier, event log building is a job that has been traditionally performed
by analysts. it remains a manual and tedious task, and the time dedicated to it has a large
impact on the cost of process mining projects, especially at the start, when the explorative
analysis is performed.
fig. 7 overview of the approach for case notion discovery and recommendation.
when applying the traditional approach to event extraction and event log building, ana-
lysts need to perform several manual tasks (figure 7). first, a query will be written to extract
events from the dataset, selecting a set of required attributes (timestamp, activity name, case
identiﬁer), and additional attributes (e.g. resource, life-cycle, etc). these events are then
grouped in traces with respect to the value of the chosen case identiﬁer. this method works12 e. gonz ´alez l ´opez de murillas et al.
fig. 8 simple data schema with 5 nodes (tables) and 4 edges (relationships).
well in situations when the case notion is clear, and all the events share a common ﬁeld as
case identiﬁer. this is the case, for example, in databases with a star schema [15], where
a factual table is at the center, being connected to other dimensional tables in a star-like
shape. however, more complex database schemas, like the one exposed in section 3, may
lack a common case-identifying attribute between all the events. in that case, transitive re-
lationships between data elements need to be pursued in order to correlate events that are
not directly linked (e.g., invoices related to orders that are related to customers). in this sit-
uation, queries to extract and correlate events become increasingly complex with respect to
the number of tables involved.
additionally, it may be that we lack the right domain knowledge about the process to be
able to identify the correct case notion. when this happens, analysts are forced to approach
the data in an explorative way. this means applying a trial and error approach, selecting
a speciﬁc case notion, building the log, inspecting the result and, if it is not satisfying,
repeating the process from a different perspective. the problem of this approach is that,
in complex scenarios, it can be extremely time consuming. consider the data schema in
figure 8, where nodes represent tables and edges relationships (foreign keys) between tables.
with only 5 tables and 4 relationships, 17 different combinations, or subgraphs, exist: fa;b;
c;d;e;ab;abc;abcd;abcde;abd;abde;bc;bcd;bcde;bd;bde;de g
the approach to event log building presented in this work aims at automating the process
as much as possible. as shown in figure 7, the goal is to provide input event logs to the user
to be analyzed during the explorative phase of a process mining project, while reducing
the time spent performing manual tasks. first, we rely of previous work [25] to extract the
data from the source database, transforming and storing it in a format suitable for automated
analysis. then, we collect several statistics on different dimensions. these statistics will help
us assess which perspectives (case notions) on the data look more interesting, and are sorted
in a ranking. finally, based on the ranking, the user can choose which of the suggested case
notions to use to automatically obtain an event log for analysis. the methodology that we
propose for event log building is explained in detail along the present and coming sections.
the focus of this section is on deﬁning what a case notion is, in order to build logs from
event data. relying on the meta model structure to correlate events gives us the freedom to
apply our log building technique to data coming from different environments, where sap is
just an example. as long as the existing data elements can be matched to the class, object and
event abstractions, event correlation will be possible. therefore our log building technique
will be feasible. the fact that this kind of data and correlations can be obtained in real-life
environments has been previously demonstrated in [25]. our approach deﬁnes case notions
based on the data model of the dataset (classes and relationships) and projects the data onto
it (objects, object versions, and events) to ﬁnd build traces with correlated events.case notion discovery and recommendation 13
4.1 deﬁning case notions
we deﬁne a case notion (deﬁnition 3) as an annotated rooted tree in which there is always a
root node (root class of the case notion). there can be a set of additional regular class nodes,
together with some converging class nodes, as children of the root node or other nodes of
the subtrees. the root node is the main class of the case notion and triggers the creation of
a new case identiﬁer for each object that belongs to it (e.g. a case identiﬁer for a purchase
order). regular nodes will force the creation of a new case identiﬁer when several of its
objects relate to one root or regular object (e.g. several deliveries of the same order will
result in one case identiﬁer for each delivery). converging nodes are the ones that allow one
case identiﬁer to refer to objects of that same class (e.g., several delivery items linked to the
same delivery will be grouped in under the same case identiﬁer).
deﬁnition 3 (case notion) let us assume a data model dm = ( cl;at;
classofattribute ;rs;sourceclass ;targetclass ). we deﬁne a case notion as a tuple
cn= (c;root;children;conv;idc;rsedge )such that:
-cclis the set of classes involved in the case notion,
-root2cis the root class in the case notion tree,
-children2c!p (c)is a function returning the children of a class in the case notion
tree,
-convcis the set of classes of the case notion for which convergence is applied. if
a classcbelongs to conv , all the members of the subtree of cmust belong to this set,
i.e.,8c2conv :children (c)conv ,
-idc =cnconv is the set of identifying classes that will be used to uniquely identify
cases of this case notion,
-rsedge2(cc)!rsis a function returning the relationship of the edge between
two classes in the tree such that, 8c2c:8c02children (c) :9rs2rs:fc;c0g=
fsourceclass (rs);targetclass (rs)g^rsedge (c;c0) =rs.
table 1 sample object, version and event identiﬁers for the classes involved in the case notion.
class objectid versionid eventid relationid
eket a1 av1 ae1 bv1
eket a1 av2 ae2 bv2
eket a2 av3 ae3 bv3
eban b1 bv1 be1 -
eban b1 bv2 be2 -
eban b2 bv3 be3 -
ekko c1 cv1 ce1 bv2
ekko c2 cv2 ce2 bv2
ekko c3 cv3 ce3 bv3
ekpo d1 dv1 de1 cv1
ekpo d2 dv2 de2 cv1
ekpo d3 dv3 de3 cv2
ekpo d4 dv4 de4 cv3
figure 9 shows an example of a case notion combining classes eban ,eket ,ekko ,
and ekpo . the class eban is the root of the case notion. the class eket is a reg-
ular child of the root node, while the child node ekko is a converging class. by in-14 e. gonz ´alez l ´opez de murillas et al.
fig. 9 sample of a case notion, represented as an
annotated rooted tree.
fig. 10 links between objects of classes eket
(a1, a2 ), eban ( b1, b2 ), ekko (c1, c2, c3), and
ekpo (d1, d2, d3, d4). the objects have been
grouped in two sets, corresponding to the case iden-
tiﬁers computed for the case notion of figure 9.
heritance, the node ekpo is a converging class as well, given that it belongs to a sub-
tree of the converging class ekko . therefore, figure 9 is the graphical representation
of the case notion cnfor whichc=feban;eket;ekko;ekpog,root =
eban ,conv =fekko;ekpog,idc =feban;eketg,children2
c! p (c)such that children (eban ) =feket;ekkog;children (ekko ) =
fekpog;children (ekpo ) =;;andchildren (eket ) =;, and rsedge2
(cc)! rs such that rsedge (eket;eban ) = fkeket toeban6,
rsedge (ekko;eban ) = fkekko toeban;and rsedge (ekpo;ekko ) =
fkekpo toekko . according to this case notion, each trace will contain events belonging
only to one eban object, only one eket object, but to any ekko orekpo objects that
hold a relation with the eban object represented by the trace. this is due to the fact that
ekko andekpo are deﬁned as converging classes in our case notion. the log building
process is described in greater detail below.
4.2 building a log
the process of building an event log can be seen as the projection of a dataset on a certain
case notion. first, a set of case identiﬁers will be constructed, which will determine the
objects that will be correlated per trace. deﬁnition 4 describes in more detail how this set of
case identiﬁers is generated. figure 10 will be used in this section as an example to illustrate
the method.
deﬁnition 4 (case identiﬁers) let us assume a valid connected meta model cmm and
a case notion cn = (c;root;children;conv;idc;rsedge ). we deﬁne cias the
maximal set7of case identiﬁers such that, each case identiﬁer ci2ciis a set of objects
ci=fo2ocjclassofobject (o)2cgand the following properties apply:
-8o2ci:classofobject (o)2idc)(9o02ci:classofobject (o0) =
classofobject (o))o0=o), i.e., cannot exist two objects per identifying class in
each case identiﬁer,
-9o2ci:classofobject (o) =root, i.e., one object of the case identiﬁer belongs to the
root,
-r(cici) =f(o;o0)j9(rs;ov;ov0)2rel :c=classofobject (o)^c0=
classofobject (o0)^objectofversion (ov) =o^objectofversion (ov0) =o0^rs=
rsedge (c;c0)^sourceclass (rs) =c^targetclass (rs) =c0g, i.e.,ris a relation
6fk*stands for “foreign key” , e.g., fkeket toeban represents a foreign key from table eket to table
eban .
7ais a maximal set for property pif: (a)asatisﬁes property pand (b)8basatisfying property p:
b=a.case notion discovery and recommendation 15
between two objects of the case identiﬁer such that both objects have at least one link in
the original data for a relationship considered in the case notion. to improve readability,
we can say that oro0() (o;o0)2r,
-jcij>1)8(o;o0)2(cici) :or+o0, i.e., as long as the case identiﬁer contains
more than one object, any pair of objects must belong to the transitive closure8of the
relationr, i.e. directly or transitively related through objects of the case identiﬁer.
let us consider the sample dataset in table 1. it corresponds to the tables eban ,eket ,
ekko , and ekpo . in total there are 11 objects ( fa1;a2;b1;b2;c1;c2;c3;d1;d2;d3;d4g),
13 object versions ( fav1;av2;av3;bv1;bv2;bv3;cv1;cv2;cv3;dv1;dv2;dv3;dv4g), and
13 events (fae1;ae2;ae3;be1;be2;be3;ce1;ce2;ce3;de1;de2;de3;de4g). additionally,
there are 10 relations between object versions ( fav1!bv1;av2!bv2;av3!
bv3;cv1!bv2;cv2!bv2;cv3!bv3;dv1!cv1;dv2!cv1;dv3!cv2;dv4!
cv3g).
the ﬁrst step to build the event log corresponding to the case notion in figure 9 is to
build the set of case identiﬁers. first, we have to ﬁnd the maximal set of case identiﬁers that
comply with the constrains set by the case notion at hand, i.e. (a) all the objects must belong
to the classes in the case notion, (b) at least one object per case identiﬁer must belong to the
root class of the case notion, (c) two objects of the same case identiﬁer cannot belong to the
same identifying class of the case notion, and (d) all the objects in the same case identiﬁer
must be related, either directly or transitively, by means of the relationships speciﬁed in the
case notion.
going back to our example, we will construct the set of case identiﬁers by looking at the
figure 10. in it we see the relations between objects. knowing that fb1;b2gare the objects
belonging to the eban class and that eban is the root class of the case notion, we know
that exactly one of these objects must be in each of the resulting traces. that means we
will generate, at least, two traces. objects fa1;a2gbelong to the class eket , which is the
other identifying class of the case notion. only one of these objects is allowed per trace. in
this case, each one of them is related to a different eban object. because eket andeban
are the only identifying classes of the case notion, we can combine their objects already to
create a (non-maximal) set of case identiﬁers ci0=fci10;ci20g:
ci10=fa1;b1g
ci20=fa2;b2g
the next class to look at in the case notion hierarchy is ekko . there are three objects
(fc1;c2;c3g) belonging to this class. two of them ( fc1;c2g) are related to the eban object
b1. given that it is a converging class, we can put them in the same case identiﬁer, in this case
ci10. the other object ( c3) is related to the eban objectb2. therefore, it will be inserted in
the case identiﬁer ci20. we proceed analogously with the ekpo objectsfd1;d2;d3;d4g,
given that ekpo is a converging class in our case notion as well. finally, the maximal case
identiﬁers set ci=fci1;ci2gis:
ci1 =fa1;b1;c1;c2;d1;d2;d3g
ci2 =fa2;b2;c3;d4g
once the case identiﬁers have been generated, it is possible to build the log in its ﬁnal
form. first we introduce some useful notation in deﬁnition 5.
8r+is the transitive closure of a binary relation ron a setxif it is the smallest transitive relation on x
containingr.16 e. gonz ´alez l ´opez de murillas et al.
deﬁnition 5 (shorthands i) given a valid connected meta model cmm , a case notion
cn= (c;root;children;conv;idc;rsedge )and a maximal set of case identiﬁers
ci, we deﬁne the following shorthands:
-acto=fact2acj9(e;ov)2dom(eventtoovlabel ) :objectofversion (ov) =
o^activityofai (eventai (e)) = actg, i.e., the set of activities of the activity instances
related to an object through its versions and events,
-actc c=fact2acj9(e;ov)2dom(eventtoovlable ) :objectofversion (ov)
=o^activityofai (eventai (e)) = act^classofobject (o) =cg, i.e., the set of
activities related to a class through its activity instances, events, versions and objects,
-oc=fo2ocjclassofobject (o) =cg, i.e., the set of objects of a certain class
c2c,
-evo o=fe2evj9(e;ov)2dom(eventtoovlabel ) :
objectofversion (ov) =og, i.e., the set of events of a certain object o2oc,
-evc c=fe2evj9(e;ov)2dom(eventtoovlabel ) :
classofobject (objectofversion (ov)) =cg, i.e., set of events of a certain class c2c,
-eai=fe2evjai2ai^eventai (e) =aig, i.e., set of events of a certain activity
instanceai2ai.
in order to build the ﬁnal log, we will map a set of activity instances to each object
and group them per case identiﬁer to form traces. according to the deﬁnition of the open-
slex meta model, an activity instance is a set of events that belong to the same activity and
case, e.g., correlated events with different life-cycle of the same activity ( start andcomplete
events). in our example, for the sake of clarity, we assume that each activity instance is a
singleton with a single event. in fact, we will represent traces as a set of events. deﬁni-
tion 6 provides a formal description of a log and how to build it from a maximal set of case
identiﬁers.
deﬁnition 6 (log) given a valid connected meta model cmm , a case notion cn= (c;
root;children;conv;idc;rsedge )and a maximal set of case identiﬁers ci, we deﬁne
a log l2ci!p (ai)as a deterministic mapping between the set of case identiﬁers and
the powerset of activity instances, such that each of the activity instances in the mapped set
is linked to at least one object of the case identiﬁer, i.e., for all ci2ci:l(ci) =fai2
aij9e2ev:ai=eventai (e)^9ov2ov: (e;ov)2dom(eventtoovlabel )^
objectofversion (ov)2cig.
assuming that, in our example, each activity instance is represented by a single event,
we can build the ﬁnal log las the following mapping:
ci!p (ai)
l:ci1 =fae1;ae2;be1;be2;ce1;ce2;de1;de2;de3g
ci2 =fae3;be3;ce3;de4g
of course, different variations of case notions will lead to different event logs, given
that the grouping rules will change. table 2 shows three different case notions, as well as
the corresponding case identiﬁers and ﬁnal traces. the ﬁrst row (a) is based on the case
notion in figure 9, representing the same example we have just analyzed. case notions (b)
and (c) are variations of the case notion (a). in (b), the ekko class has been promoted to
be an identifying class. this provokes the generation of an additional case identiﬁer, since
objectsfc1;c2gcannot coexist in the case case identiﬁer anymore. in (c), also the ekpo
class has been transformed into an identifying class. this triggers the creation of anothercase notion discovery and recommendation 17
case identiﬁer, since the objects fd1;d2;d3;d4gcannot belong to the same case identiﬁer
either. these examples show the impact of converging and identifying classes in the output
of the log building process.
table 2 case identiﬁers and ﬁnal traces built from the sample dataset, according to each of the three case
notions.
id case notion case identiﬁers & traces
a
trace 1:fae1;ae2;be1;be2;ce1;ce2;de1;de2;de3g
trace 2:fae3;be3;ce3;de4g
b
trace 1:fae1;ae2;be1;be2;ce1;de1;de2g
trace 2:fae1;ae2;be1;be2;ce2;de3g
trace 3:fae3;be3;ce3;de4g
c
trace 1:fae1;ae2;be1;be2;ce1;de1g
trace 2:fae1;ae2;be1;be2;ce1;de2g
trace 3:fae1;ae2;be1;be2;ce2;de3g
trace 4:fae3;be3;ce3;de4g
these deﬁnitions make it possible to create specialized logs that capture behavior from
different perspectives. if all the possible case notions for a data model are generated, au-
tomated analysis techniques could be applied to each of the resulting logs, relieving users
from tedious analysis tasks and enabling process mining on a large scale. however, the com-
binatorial explosion problem makes it practically impossible to explore all the case notions
for large and complex data models. even if the search space could be reduced to discard
irrelevant case notions, the remaining number would be too high in order for humans to in-
terpret the insights for each of the resulting event logs. this means that we must focus our
efforts on the most interesting perspectives to obtain insights without being overwhelmed by
excessive amounts of information. the following section proposes a set of metrics to assess
the interestingness of a case notion, based on measurable quality features of the resulting
event log.18 e. gonz ´alez l ´opez de murillas et al.
5 log quality: is my log interesting?
the log quality problem concerns the identiﬁcation of characteristics that make event logs
interesting to be analyzed. this problem is not new to the ﬁeld. some authors have studied
how the choices made during the log building process can affect the log quality [19] and
have developed procedures to minimize the negative impact. other authors have tried to
deﬁne metrics to assess different log properties from the structural point of view [17]. in
this work, we aim at assessing the quality of an event log in an automated way. for that
purpose, we adopt some metrics from [17], that will give us an idea of the structural and
data properties that a log should possess in order to be an interesting candidate. in the scope
of our meta model and the logs we are able to build, we need to adapt these concepts to
be able to compute them based on our input data, an openslex ﬁle. considering a valid
connected meta model cmm , a case notion cn, a set of case identiﬁers ci, and a logl,
we adapt the following three metrics to match the structure of our meta model:
support (sp) (equation 1): number of traces present in an event log:
sp(l) =jdom(l)j=jcij (1)
level of detail (lod) (equation 2): average number of unique activities per trace:
lod(l) =p
ci2cijs
ai2l(ci)activityofai (ai)j
sp(l)=p
ci2cijs
o2ciactoj
jcij(2)
average number of events (ae) (equation 3): average number of events per trace:
ae(l) =p
ci2cijs
ai2l(ci)eaij
sp(l)=p
ci2cijs
o2cievo oj
jcij(3)
when analyzing processes, intuitively, it is preferable to have event logs with as many
cases as possible, i.e., higher support (equation 1), but not too many activities per case, i.e.,
reasonable level of detail (equation 2). the reason for this is that the complexity of the re-
sulting model, and therefore its interpretation, is closely related to the amount of activities it
needs to represent. however, too few activities results in very simple models that do not cap-
ture any interesting patterns we want to observe. also, we try to avoid cases with extremely
long sequences of events, i.e., large average number of events per trace (equation 3), be-
cause of the difﬁculty to interpret the models obtained when trying to depict the behavior.
however, too short sequences of events will be meaningless if they represent incomplete
cases.
therefore, while we would like to maximize the support value (1), i.e., give priority
to logs with a higher number of traces, we cannot say the same for the level of detail (2)
and average number of events per case (3). these last two metrics will ﬁnd their optimality
within a range of acceptable values, which will depend on the domain of the process and
taste of the user, among other factors. given the differences between the pursued optimal
values for each of the metrics, the need for a scoring function becomes evident. it is required
to be able to effectively compare log metrics. a candidate is the beta distribution. the reason
for our choice is that the beta distribution has two parameters to control its shape, and this
gives us additional freedom to customize the scoring function. choosing the right values
for the parameters of the distribution can seem daunting at ﬁrst. however, it is possible
to estimate their value based on more intuitive parameters that describe the shape of thecase notion discovery and recommendation 19
0.0 0.2 0.4 0.6 0.8 1.00.0 0.5 1.0 1.5 2.0 2.5beta(α,β) distribution
xpdfα =5, β =2
α =4, β =4
α =2, β =5
α =1.47, β =2
α =2, β =1
fig. 11 sample of beta distribution curves for different values of the andparameters.
resulting distribution, e.g., mode and inﬂection points of the curve. in practice, the technique
yields satisfactory results using the default parameters (table 3), and only the advanced user
might need to modify them. note that the choice of the scoring function is not restricted by
the approach and could be replaced by any distribution more appropriate to the setting of
application.
the beta distribution is deﬁned on the interval [0;1]and has two shape parameters,
and. the values of these two parameters determine the shape of the curve, its mean,
mode, variance, etc. also, the skewness of the distribution can be shaped choosing the right
combination of parameters (see figure 11). this allows one to deﬁne a range of values for
which the probability density function (pdf) of the beta distribution (equation 4) will return
higher scores as they approximate to the mode.
betapdf(x;;) =x 1(1 x) 1
b(;);whereb(;)is the euler beta function. (4)
the input values will get a lower score as they get farther from the mode. one advantage
of this distribution is that it is possible to deﬁne a mode value different from the mean, i.e.,
to shape an asymmetric distribution. figure 11 shows examples of beta distributions for
different values of and.
the parameters andcan be estimated based on the mode and approximate inﬂection
points of the desired pdf [27]. we show an example considering only the mode. if we are
interested on event logs with a level of detail close to 7, we need to estimate the values of 
andto obtain a pdf with mode 7. first we scale the value. if the minimum and maximum
values for lod are 1 and 20, then the scaled mode is 0.32. assuming that we are after a
unimodal pdf and ; > 1, we use equation 5 to compute the mode:
mode = 1
+ 2for; > 1 (5)
given the desired mode , we can ﬁx the value of one of the shape parameters, and esti-
mate the other one using equation 5:20 e. gonz ´alez l ´opez de murillas et al.
est(mode ) =8
><
>:= 2;=1
1 mode;ifmode<0:5)positively skewed
= 2;=1 4mode
mode;ifmode>0:5)negatively skewed
;= 2; ifmode = 0:5)symmetric(6)
therefore, for the mode 0.32, the pdf is positively skewed. using equation 6 we evalu-
ateest(0:32)to obtain the values = 2and= 1=(1 0:32) = 1:47. the resulting pdf
can be observed in figure 11 (dotted curve). this is a basic yet effective method to set the
shape parameters of the beta function using domain knowledge, i.e., the optimal value that
we desire to score higher. once the parameters andhave been selected, we can compute
the scores of the previous log metrics. to do so, we provide a score function:
score (f;xi;x;; ) =beta pdf(scaled (f;xi;x);;) (7)
here,fis a function to compute the metric to be scored (e.g., sp,lod orae),xiis the input
of functionf(e.g., a logl),xis the set of elements with respect to which we must scale
the value off(xi)(e.g., a set of logs l),andare the parameters of the beta probability
distribution function, and scaled (f;xi;x)is a rescaling function such that:
scaled (f;xi;x) =f(xi) min
xj2xff(xj)g
max
xj2xff(xj)g min
xj2xff(xj)g(8)
with the score function in equation 7, ﬁrst we perform feature scaling (equation 8).
next, we apply the beta distribution function (equation 4) with the corresponding and
parameters. with respect to the support of the log, the score will be the result of scaling
the support feature ( sp(l)) with respect to the set of possible logs land applying the beta
probability distribution function. as the purpose, in this case, is to give a higher score to
higher support values, we will set the parameters spandspsuch that the probability
distribution function resembles an ascending line (e.g., = 2and= 1in figure 11):
ssp(l;l) =score (sp;l;l;sp;sp) (9)
to score the level of detail , we let the parameters lod andlod to be tuned according
to the preference of the user:
slod(l;l) =score (lod;l;l;lod;lod) (10)
the score of the average number of events per case is computed in the same way, using
the appropriate values for the parameters aeandae:
sae(l;l) =score (ae;l;l;ae;ae) (11)
the interestingness of a log lwith respect to all the logs lcan be deﬁned by the combi-
nation of the score values for each of the previous metrics. in order to combine the scores for
each log metric, a global scoring function gsf2lp(l))rcan be used, which takes
a logland a set of logs l, and returns the score of lwith respect to l. the approach does
not depend on the choice of this function, and it can be replaced by any custom one. for
the purpose of demonstrating the feasibility of this approach, we deﬁne the global scoring
(or “log interestingness”) function as the weighted average of the three previous scores. the
weights (wsp,wsp,wsp) and the parameters of the beta distribution ( sp,sp,lod,case notion discovery and recommendation 21
lod,ae,ae) can be adjusted by the user to balance the features according to their
interest.
gsf(l;l) =wspssp(l;l) +wlodslod(l;l) +waesae(l;l) (12)
it must be noted that it is not necessary to set custom values for the parameters of our
scoring function every time that we want to tackle a different dataset. in most of the cases,
it will be enough to apply the technique using the default parameters in table 3.
table 3 default parameters used to conﬁgure the scoring function for case notions.
metric parameter value description
support spmode - mode of the beta pdf used to score the support
(number of cases). default is null, since we try to
maximize sp.
spmax1 highest value of the desired range used to score the
support value.
spmin 0 highest value of the desired range used to score the
support value.
level of detail lod mode 4 mode of the beta pdf used to score the lod(level of
detail) value.
lod max 10 highest value of the desired range used to score the
lodvalue.
lod min 2 lowest value of the desired range used to score the
lodvalue.
average number of events aemode 8 mode of the beta pdf used to score the ae(average
number of events per trace) value.
aemax 30 highest value of the desired range used to score the
aevalue.
aemin 4 lowest value of the desired range used to score the
aevalue.
global score wsp 0:33 weight of the support score on the ﬁnal global
score.
wlod 0:33 weight of the lodscore on the ﬁnal global score.
wae 0:33 weight of the aescore on the ﬁnal global score.
the “log interestingness” scoring function (equation 12) proposed in this section aims
at giving an indication of how likely it is that a log will be of interest, with respect to
the other candidates, given a set of parameters. table 4 shows the top 8 discovered case
notions of the sample sap dataset, according to the computed score. we see that the tables
involved in the purchase requisition process represent a relevant case notion candidate for
this speciﬁc dataset. the main contribution until now is not the speciﬁc scoring function,
but the framework that enables the assessment and its conﬁguration.
the metrics that we chose (support, level of detail, and average number of events per
trace) represent a baseline set of key indicators to compute an interestingness score per event
log. it can be the case that, in certain scenarios, assessing the potential interestingness of an
event log requires the use of different metrics, e.g., the variety of trace types, some structural
property of a discovered process model, or the ﬁtness score with respect to a normative
model. the framework proposed in this work allows the user to deﬁne any custom metric
and/or global score to be computed for each candidate event log.
however, this framework still requires a log to be generated in order to be subjected to
evaluation. taking into account that the ﬁnal goal is to automatically assess log interesting-22 e. gonz ´alez l ´opez de murillas et al.
table 4 top 8 discovered case notions, sorted by score with parameters ( sp= 2,sp= 1,lod =
4:16,lod = 1 ,ae= 1:28,ae= 1:53,wsp= 0:3,wlod= 0:3, andwae= 0:3). the
andparameters have been estimated based on desired min, max, and mode values for the corresponding
beta distribution ( lodmin = 2,lodmax = 10 ,lodmode = 4,aemin = 4,aemax = 30 , and
aemode = 8). the values for sp, lod, and ae have been scaled.
root tables sp’ lod’ ae’ score
1 eban ekpo, eine, eban, ekko, lfa1 0:54 1:00 0:60 1:90
2 eine ekpo, eine, eban, ekko, lfa1 0:70 0:95 0:65 1:79
3 eban ekpo, eine, eban, mara 0:28 1 0 :69 1:73
4 ekpo ekpo, eine, eban, ekko, lfa1 0:80 0:87 0:63 1:60
5 ekko ekpo, eine, eban, ekko, lfa1 0:55 0:88 0:47 1:53
6 eine ekpo, eine, eban, ekko 0:70 0:85 0:56 1:52
7 eban ekpo, eine, eban, ekko 0:54 0:87 0:48 1:51
8 eine ekpo, eine, eban, mara 0:45 0:89 0:71 1:44
ness at a large scale, we need better ways to score case notions before the corresponding
logs are built. the following section explores this idea, proposing a method to predict log
interestingness based on our baseline metrics and score function.
6 predicting log interestingness
if an event log is completely created from an extracted dataset, then it is straightforward
to assess the actual interestingness. however, as explained before, for large databases it is
infeasible to compute all candidates. in order to mitigate this problem and save computation
time, we aim at approximating the value of the metrics considered in section 5 for a certain
case notion, before the log is computed. to do so, it is important to deﬁne bounds for the log
metrics, given a certain case notion. the purpose is to restrict the range of uncertainty and
improve the prediction accuracy. in fact, at the end of this section, the bounds will be used
to deﬁne a custom predictor for each of the log metrics.
as we mentioned in the previous section, the framework is extensible, allowing the user
to deﬁne additional metrics when necessary. any additional metric used to assess log inter-
estingness will need to be taken into account in the global scoring function (equation 12).
also, in order to take advantage of the log interestingness prediction method, an approxima-
tion function must be provided for any additional metric that the user deﬁnes. the approx-
imation function for a certain metric must be able to compute an approximated value for a
metric, given a certain case notion and the extracted data, without the need to compute the
corresponding event log. as an example, in this section we present upper and lower bounds
of the baseline metrics used in our global scoring function.
first, we try to set bounds to the support of a log. from equation 1 we see that the
support of a log is equal to the domain of the mapping, i.e., the amount of case identiﬁers of
the log. deﬁnition 4 shows that the amount of case identiﬁers depends on the combinations
of objects belonging to the identifying classes of the case notion ( idc ). given that every
case identiﬁer must contain one object of the root class, that only one object of the root class
is allowed per case identiﬁer, and that the set of case identiﬁers is a maximal set, we can
conclude that the set of case identiﬁers will contain at least one case identiﬁer per object in
the root class:
bound 1 (lower bound for the support of a case notion) given a valid connected
meta model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), acase notion discovery and recommendation 23
maximal set of case identiﬁers ci, and the corresponding log lwe see that8ci2ci:9o2
ci:classofobject (o) =root() 8o2oroot:9ci2ci:o2ci)jcijjorootj.
therefore, we conclude that: sp(l)bsp(cn)c=jorootj
for a case identiﬁer to be transformed into an actual trace, at least an event must exist
for the root object involved in it. for the sake of simplicity, bound 1 assumes that at least
one event exists for every object in the root class. this has been taken into account in the
implementation, considering only objects of the root class that contain at least one event.
each of the case identiﬁers is a combination of objects. also, exactly one object of the
root class and no more than one object of each identifying class (classes in idc) can exist
per case identiﬁer. this leads to the following upper bound for support:
bound 2 (upper bound for the support of a case notion) given a valid connected
meta model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), a
maximal set of case identiﬁers ci, and the corresponding log l, we deﬁne a maximal set
ci0for which the following properties hold:
a)8ci2ci0:8o2ci:classofobject (o)2idc)9o02ci:classofobject (o)
=classofobject (o0)()o=o0, i.e., only one object per class belongs to the case
identiﬁer,
b)8ci2ci0:9o2ci:classofobject (o) =root, i.e., one object of the root class must
always belong to the case identiﬁer.
this implies that ci0contains all the possible combinations of one or zero objects of each
class in idc , except for the root class, that must always be represented by an object in
the case identiﬁer. that means that jci0j=jorootjq
c2fcnrootg(jocj+1). given that
ci0is less restrictive than ci, we know that ci0ci)jci0jj cij. therefore:
sp(l)dsp(cn)e=jorootjq
c2fcnrootg(jocj+1)
following the same logic to set a lower bound for support, we know that all the objects
that belong to the root class will be involved in at least one case identiﬁer. however, the
number of traces is still unknown if the log has not been built and we can only consider it
as the maximum possible, i.e. the upper bound of the support. therefore, a lower bound for
the level of detail will be given by the sum of the unique activities per object of the root
class divided by the maximum number of case identiﬁers. if we consider that the additional
case identiﬁers (beyond the number of objects of the root class) will, at least, add a unique
number of activities equal to the minimum number of activities per object of the root class,
we can get a better lower bound as described below:
bound 3 (lower bound for the lod of a case notion) given a valid connected meta
model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), a max-
imal set of case identiﬁers ci, and the corresponding log l, we see that8ci2ci:9o2
ci:classofobject (o) =root() 8o2oroot:9ci2ci:o2ci)8ci2ci:s
o2ciactos
o2(ci\oroot)acto. additionally, we know thatp
ci2cijs
o2(ci\oroot)actoj 
(p
o2orootjactoj) + (jcij jorootj)min
o2orootfjactojg. therefore:
lod(l)blod(cn)c=(p
o2orootjactoj) + (dsp(cn)e jorootj)min
o2orootfjactojg
dsp(cn)e24 e. gonz ´alez l ´opez de murillas et al.
a lower bound for lod is given by the lower bound of the sum of the unique activities
per case, divided by the upper bound on the number of cases. we know that, at least, one
case will exist per object belonging to the root class. that is why the sum of the unique
activities per objects of root is added on the top part of the formula. also, because these
objects could be involved in more than one case, to a maximum of dsp(cn)ecases, we add
the minimum number of unique activities they could have and multiply it by the maximum
number of additional case identiﬁers. this will always be a lower bound given that the
number of activities we add at the upper part for the additional case identiﬁers will always
be equal or lower than the average. not adding these extra case identiﬁers would still result
in a lower bound, but an extremely low one since the divisor is usually an overestimation for
the number of possible case identiﬁers.
with respect to the upper bound for the level of detail, we need to consider the most
extreme situation. this is caused by a case identiﬁer that contains one object per identifying
class and one or more objects per converging class, such that, for each object, the events
related to them represent all the possible activities. for this case identiﬁer, the number of
unique activities will be the sum of the number of unique activities per class involved. how-
ever, there is a way to restrict this bound. if we count the number of unique activities for the
events of each object, and ﬁnd the maximum per class, the upper bound will be given by the
sum of the maximum number of unique activities per object for all the identifying classes,
plus the total of unique activities per converting class involved in the case notion:
bound 4 (upper bound for the lod of a case notion) given a valid connected meta
model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), a max-
imal set of case identiﬁers ci, and the corresponding log l, we know that,8c2c:
8o2oc:jactoj  max
o02ocfjacto0jg. this implies that, 8ci2ci:js
o2ciactoj 
p
c2idcmax
o2ocfjactojg+p
cinconvjactc cj. therefore:
lod(l)dlod(cn)e=jcij(p
c2idcmax
o2ocfjactojg+p
cinconvjactc cj)
jcij=
x
c2idcmax
o2ocfjactojg+x
cinconvjactc cj
the same reasoning used to obtain a lower bound for the level of detail can be applied
in the case of the average number of events per trace. only that, in this case, instead of
counting the number of unique activities, we count the number of events per object:
bound 5 (lower bound for the ae of a case notion) given a valid connected meta
model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), a max-
imal set of case identiﬁers ci, and the corresponding log l, we see that8ci2ci:9o2
ci:classofobject (o) =root() 8o2oroot:9ci2ci:o2ci)8ci2ci:s
o2cievo os
o2(ci\oroot)evo o. additionally, we know thatp
ci2cijs
o2(ci\oroot)evo oj
(p
o2orootjevo oj) + (jcij jorootj)min
o2orootfjevoojg. therefore:
ae(l)bae(cn)c=(p
o2orootjevo oj) + (dsp(cn)e jorootj)min
o2orootfjevoojg
dsp(cn)ecase notion discovery and recommendation 25
a lower bound for ae is given by the lower bound of the sum of the events per case,
divided by the upper bound on the number of cases. at least one case will exist per object
of the root class. therefore, we consider the sum of the number of events per object. these
objects could be involved in more than one case, to a maximum of dsp(cn)ecases. so, we
add the minimum number of events they could have, multiplied by the maximum number
of additional case identiﬁers. this is a lower bound given that the number of events added
at the upper part for the additional case identiﬁers is equal or lower than the average. not
adding these extra case identiﬁers would still result in a lower bound, but an extremely low
one since the divisor is usually an overestimation on the number of possible case identiﬁers.
to deﬁne an upper bound for ae, we use an approach similar to the one used to compute
an upper bound for lod. we need to consider the most extreme case, the case in which the
maximum number of events per object (for the identifying classes) could be included in the
ﬁnal trace. however, if the case notion has converging classes, the most extreme case is the
one in which all the objects of such classes are contained in the case identiﬁer, therefore all
the events belonging to the converging classes would be inserted in the trace:
bound 6 (upper bound for the ae of a case notion) given a valid connected meta
model cmm , a case notion cn = (c;root;children;conv;idc;rsedge ), a max-
imal set of case identiﬁers ci, and the corresponding log l, we know that,8c2c:
8o2oc:jevo oj max
o02ocfjevo o0jg. this implies that, 8ci2ci:js
o2cievo oj
p
c2idcmax
o02ocfjevo o0jg+p
c2convjevc cj. therefore:
ae(l)dae(cn)e=jcij(p
c2idcmax
o02ocfjevo o0jg+p
c2convjevc cj)
jcij=
x
c2idcmax
o02ocfjevo o0jg+x
c2convjevc cj
these bounds deﬁne the limits for our prediction. for each metric ( sp(l),lod(l)and
ae(l)), either the lower or upper bound could be a prediction. however, a better heuristic
can be designed. we deﬁned equations to predict the values as the weighted average of
the corresponding bounds (equations 13, 14, and 15). given a valid connected meta model
cmm and a case notion cn, our prediction for each metric is given by the following
heuristics:
csp(cn) =wlbspbsp(cn)c+wubspdsp(cn)e (13)
dlod(cn) =wlblodblod(cn)c+wubloddlod(cn)e (14)
dae(cn) =wlbaebae(cn)c+wubaedae(cn)e (15)
from these equations we see that, in order to calculate the heuristics for each metric,
we need to collect some features. these features (table 5) can be easily computed once for
each classc2clin the dataset and be reused for every case notion cn we want to assess.
finally, in order to score the predicted values of each metric, the scoring function previ-
ously used (equation 7) must be individually applied. the input parameters are two: a case
notioncn, and a set of case notions cns to compare to. equations 16, 17 and 18 provide
the scores for the predicted metrics given a case notion cn and a set of case notions cns .
cssp(cn;cns ) =score (csp;cn;cns;sp;sp) (16)
dslod(cn;cns ) =score ([lod;cn;cns;lod;lod) (17)26 e. gonz ´alez l ´opez de murillas et al.
table 5 features used to compute upper and lower bounds for each log metric.
feature description
1maxevo c= max
o2ocfjevo ojg maximum # of events per object of a class c
2maxact c= max
o2ocfjactojg maximum # of activities per object of a class c
3minevo c= min
o2ocfjevo ojg minimum # of events per object of a class c
4minact c= min
o2ocfjactojg minimum # of activities per object of a class c
5jevc cj # of events per class c
6jactc cj # of unique activities per class c
7sumevoc=p
o2ocjevo oj total # of events per object for a class c
8sumactc=p
o2ocjactoj total # of unique activities per object for a class c
9jocj # of objects of a class c
csae(cn;cns ) =score (dae;cn;cns;ae;ae) (18)
next, a global scoring function is deﬁned to combine the three of them. we will call this
function the predicted global scoring function ,pgsf2cnsp(cns )!rand it is the
weighted average of the scores of each of the three predicted values:
pgsf(cn;cns ) =wspcssp(cn;cns ) +wloddslod(cn;cns ) +waecsae(cn;cns )
(19)
this function represents our custom predictor for log interestingness. the accuracy of
the predictor will be evaluated in section 8, where it will be compared to alternative tech-
niques.
7 implementation
all the techniques proposed in this paper are part of the event data discovery tools pack-
age ( eddytools9). this tool assists the user at every step from data extraction to event log
building. the eddytools python package provides six commands that cover the main steps
(some of them out of the scope of this paper) of the data extraction and preparation phase.
these steps and their purpose are described below:
1.data exploration : to get a feeling of the size and dimension of the data. also, to look
for any high-level structure that can be extracted from it.
2.data schema discovery : to discover the data relations (primary, unique and foreign
keys) in order to be able to correlate data objects in future steps.
3.data extraction : to obtain an off-line copy of the data that we can transform into a
format suitable for analysis. also, this allows us to complete the data once a schema has
been discovered.
4.event data discovery : event data might be implicitly stored within or across different
tables in the dataset. we need to discover the events and make them explicit.
5.case notion discovery : deﬁning a case notion allows us to correlate events into traces.
many alternative case notions can be deﬁned depending on the perspective we want to
take.
9https://github.com/edugonza/eddytoolscase notion discovery and recommendation 27
table 6 details about the sap dataset used during the evaluation.
tables 87 case notions 10 622
objects 7 339 985 non empty logs 5 180
versions 7 340 650 total log building time 13h 57m
events 26 106 average log building time 4.7s
features computation time 2m
6.event log building : from the discovered events and a case notion we can build an event
log. many case notions can be deﬁned, and the corresponding event logs can be con-
structed in order to analyze different coexisting processes, or the same process from
different perspectives.
we claim that these steps can be executed in a semi-automatic way, given that they allow
for a certain customization depending on the characteristics of the environment to analyze.
in [24] (chapter 8), we provide additional details on the use of the tool in a real-life case
study.
8 evaluation
so far, we proposed a set of metrics to assess the interestingness of an event log once it has
been constructed. also, we provided predictors for these metrics based on (a) the character-
istics of the case notion being considered and (b) features of the dataset under study. the
aim of this section is twofold. (1) to ﬁnd out how good our predictors are at estimating the
value of each log characteristic. (2) to evaluate the quality of the rankings of case notions,
based on their potential interestingness according to certain log metrics, using our custom
predictor and compare them to existing learning to rank algorithms.
the evaluation was carried out on a sap sample dataset (table 6). it contains the data
model, objects, object versions, and events of 87 sap tables. the following steps were
executed using the open source software package eddytools . first, a set of candidate case
notions was generated. to do so, each one of the tables in the data model was taken as the
root node of a potential case notion. next, for each of them, all the possible simple paths
following outgoing arcs were computed, yielding a result of 10,622 case notion candidates.
for each of the candidates, the corresponding event log was generated and the metrics pre-
sented in section 5 were computed. this set of logs and metrics represent the ground truth.
given that we want to predict the metrics in the ground truth set, we need to measure the
features that our predictors require. the following section describes these features.
8.1 features for log quality prediction
section 6 presented our predictors for each of the log characteristics. these predictors es-
timate the values of the support (sp, equation 13), level of detail (lod, equation 14), and
average number of events per trace (ae, equation 15) of a log, given the corresponding case
notion and a set of features. this subsection describes the features used during the evalua-
tion which are (a) the lower and upper bounds of each log property as listed in section 6 and
(b) additional features used to improve the accuracy of the regressors we will compare to.
given a valid connected meta model cmm (i.e., a dataset stored in the openslex
format containing events, objects, versions, and a data model) and a speciﬁc case notion cn,28 e. gonz ´alez l ´opez de murillas et al.
table 7 features used to predict log interestingness.
feature description
1bsp(cn)c lower bound for the support
2dsp(cn)e upper bound for the support
3blod(cn)c lower bound for the level of detail
4dlod(cn)e upper bound for the level of detail
5bae(cn)c lower bound for average number of events per trace
6dae(cn)e upper bound for average number of events per trace
7jcj number of classes in the case notion
8je(cn)j total number of events of all the classes in the case notion
9ir(cn) average number of events per object
we can measure the features enumerated in table 7. the log associated to such case notion
does not need to be built in order to compute these features. actually, many of the features
are the result of an aggregation function over a class property. once the class properties
have been computed, the complexity of calculating these case notion metrics is linear with
respect to the number of classes involved.
8.2 evaluation of predictors’ accuracy
in section 6, upper and lower bounds were given for each log property given a case notion
(cn). these bounds have been used to estimate the value of such log properties by means
of three predictors (one per log property), before the log is actually built. now it is time
to evaluate the accuracy of these predictors. to do so, we compared the predicted value for
each log property ( sp,lod, and ae) with the actual values in the ground truth dataset. this
was done for the predictors for each log property as deﬁned in section 6 (equations 13, 14,
and 15). the combination of the scores of the three individual predictors (equations 16, 17,
and 18) in a single scoring function of log interestingness (equation 19) is what we call our
custom predictor (cp) . additionally, we compared the accuracy of the individual predictors
to three different regressors: (a) multiple linear regressor (mlp), (b) quantile regressor
(qr) [20], and (c) neural network regressor (nn). each of them where trained and tested
using the features in table 7. a 5-fold cross validation was performed in order to determine
the accuracy of the predictors (our predictors, mlp, qr, and nn). to avoid underestimation
of the prediction error, empty logs where ﬁltered out of the dataset, using only 5180 case
notions from the original 10622.
figure 12 shows the mean absolute error (mae) measured per normalized property
for each predictor. we see that our predictors do not perform really well, presenting an
average error of around 1.0 when predicting lod or ae and around 1.1 when predicting
sp. in comparison, the regressors perform better, in particular the quantile regressor with an
average error of around 0.8 for sp and lod, and around 0.9 for ae. this ﬁgure, however,
could be misleading, given that the mae is computed on all the predictions, regardless of
the existence of outliers. to get a better idea of the inﬂuence of extremely bad predictions on
the overall performance, we include figure 13, which shows box-plots for each log property
per predictor. it is important to notice that a logarithmic scale has been used, in order to plot
extreme outliers and still be able to visualize the details of each box.
we see that our predictors ( dsp,[lod , anddae) are the worst performing ones, espe-
cially when it comes to sp. also, they are the ones presenting the most extreme outliers
for the three log properties. quantile regression and neural network regressors presents30 e. gonz ´alez l ´opez de murillas et al.
8.3 evaluation of ranking quality
until now we have evaluated the accuracy of our predictors and compared them to other
existing regressors. however, the goal of predicting log properties is to assess the interest-
ingness of the log before it is built. if we are able to predict the interestingness of the logs
for a set of case notions, we can rank them from more to less interesting and provide a rec-
ommendation to the user. in this section we evaluate how good the predictors are at ranking
case notions according to their interestingness. to do so, we use the metrics on the resulting
event logs as the ground truth to elaborate an ideal ranking (equation 12). then a new rank-
ing is computed using our custom predictor (equation 19) and it is compared to the ideal
one. this comparison is done by means of the metric normalized discounted cumulative
gain at p (ndcg p), widely used in the information retrieval ﬁeld.
dcg p=px
i=1relscore i
log2(i+ 1)=relscore 1+px
i=2reli
log2(i+ 1)(20)
idcg p=jrel scoresjx
i=1relscorei
log2(i+ 1)(21)ndcg p=dcg p
idcg p(22)
thenormalized discounted cumulative gain at p (equation 22) is a metric that assumes
the existence of a relevance score for each result, penalizing the rankings in which a relevant
result is returned in a lower position. this is done by adding the graded relevance value
of each result, that is logarithmically reduced proportional to its position (equation 20).
next, the accumulated score is normalized, dividing it by the ideal score in case of a perfect
ranking (equation 21). this means that the ranking h3;1;2iwill get a lower score than the
rankingh2;3;1ifor an ideal ranking h1;2;3iand a relevance per document of h3;3;1i.
when it comes to ranking, there is a large variety of learning to rank (ltr) algorithms
in the information retrieval ﬁeld [28]. these algorithms are trained on ranked lists of docu-
ments and learn the optimal ordering according to a set of features. a 5-fold cross validation
was performed on the unﬁltered set of case notions (10622 candidates) comparing the im-
plementation10of 10 learning to rank algorithms (mart, ranknet, rankboost, adarank,
coordinate ascent, lambdarank, lambdamart, listnet, random forest, and linear re-
gression) with the predictors evaluated in section 8.2 (quantile regression, multiple linear
regression, neural network regressor, and our custom predictor). two models were trained
for each algorithm: one with the 9 input features in table 7 and another one with 4 extra fea-
tures (the estimated value for sp, lod, ae, i.e., equation 13, 14, and 15). the purpose of
adding these extra features is to ﬁnd out how the estimations made by our predictors affect
the predictions of the other algorithms.
figure 14 shows the result of the evaluation. the 13 algorithms (10 ltr + 3 regressors)
were trained on two different sets of features (9 and 13 input features), 3 different combina-
tions ofandvalues for the log quality function ( (;)2f(2;5);(5;2);(2;1)g), and
with equal weight for the three metrics. that makes a total of 78 models ( (10 + 3)23).
the ndcg@10 metric was measured for each model and the results were grouped per al-
gorithm and feature set. that resulted in 27 categories ((10 ltr algorithms 2 sets of
features) + (3 regressors 2 sets of features) + our custom predictor) with 15 ndcg@10
values each (5 folds the 3 combinations of andvalues). the models trained with 13
10https://sourceforge.net/p/lemur/wiki/ranklib/case notion discovery and recommendation 31
●●
●
●●●●
●●●0.0 0.2 0.4 0.6 0.8 1.0ndcg@10 per ranker for different ( α, β) valuesndcg@10
martmart+ranknetranknet+rankboostrankboost+adarankadarank+
coordinate ascentcoordinate ascent+lambdaranklambdarank+lambdamartlambdamart+listnetlistnet+
random forestrandom forest+linear regressionlinear regression+
neural network (nn)neural network (nn)+
multiple linear regression (mlp)multiple linear regression (mlp)+quantile regression (qr)quantile regression (qr)+custom predictor (cp)random
fig. 14 ndcg@10 per ranker given different combinations of andvalues. the box-plot corresponding
to our custom predictor has been highlighted in red.
features are represented in the ﬁgure with the symbol + at the end of their name. addition-
ally, the ndcg@10 was calculated for a set of random rankings, in order to set a baseline.
in the case of our custom predictor, given that it only takes 6 features (the lower and upper
bounds for sp, lod, and ae) and that it does not need training, only three ndcg@10 val-
ues were computed, one for each pair of values for the andparameters. the horizontal
dashed lines drawn in figure 14 represent the median of the ndcg@10 for our custom
predictor (upper) and the random ordering (lower). any algorithm whose median is above
the upper line, will perform better than our custom predictor at least 50% of the time. any
algorithm whose median is above the lower line, will perform better than random at least
50% of the time. most of the algorithms perform better than random. but only two have
the median above the upper line: mart, and random forest. when trained with 9 input
features, both mart and random forest show very similar behavior. however, when con-
sidering 13 input features, mart’s median is lower. in the case of random forest, using 13
features is better than using 9 in every aspect.
8.4 discussion
the aim of this evaluation has been twofold. first, to assess the precision of our predictors at
estimating the value of each log characteristic. second, to evaluate the quality of the rankings
of case notions, based on their potential “interestingness”, using our custom predictor and
compare them to ltr algorithms. the results (figures 12 and 13) show that our predictors
are not very good at predicting log characteristics with precision. other regressors, like
quantile regression, have shown better results in this aspect. however, when it comes to
ranking quality, the precision in the prediction of the log characteristics is of less importance
than the relative differences between predictions for several case notions (i.e., it is not so
important to predict accurately the log quality of case notions aandb, as long as we can
predict that awill be more interesting than b). in fact, the results obtained from the ranking32 e. gonz ´alez l ´opez de murillas et al.
quality evaluation (figure 14) show that our custom predictor performs better, on average,
than other regressors, even though they showed better prediction accuracy.
we conclude that for the purpose of predicting accurately the value of log characteristics
and when training data are available, the use of regressors such as qr is the best option.
when it comes to ranking candidates, ltr algorithms such as random forest and mart
provide better results. however, unlike our custom predictor, all these techniques require the
existence of training data to build the models. therefore, in the absence of such data, the
proposed custom predictor provides close-to-optimal results when it comes to rankings and
indicative values for the prediction of log characteristics.
9 related work
the ﬁeld of process mining is dominated by techniques for process discovery, conformance,
and enhancement. yet event correlation and log building are crucial since they provide the
data that other process mining techniques require to ﬁnd insights. in fact, the choices made
during the log building phase can drastically inﬂuence the results obtained in further phases
of a process mining project. therefore, it is surprising that there are only a few papers on
these topics. works like the one presented in [19] analyze the choices that users often need
to make when building event logs from databases. also, it proposes a set of guidelines to
ensure that these choices do not negatively impact the quality of the resulting event log. it
is a good attempt at providing structure and a clear methodology to a phase typically sub-
ject to experience and domain knowledge of the user. however, it does not aim at enabling
automated log building in any form. it has been shown that extracting event logs from erp
systems like sap is possible [18]. however, the existing techniques are ad-hoc solutions for
erp and sap architectures and do not provide a general approach for event log building
from databases. another initiative for event log extraction is the onprom project [9–11]. the
focus is on event log extraction by means of ontology-based data access (obda). obda
requires to deﬁne mappings between the source data source and a ﬁnal event log structure
using ontologies. then, the onprom tools perform an automatic translation from the manu-
ally deﬁned mappings to the ﬁnal event log.
event log labeling deals with the problem of assigning case identiﬁers to events from an
unlabeled event log. only a few publications exist that address this challenge. in [12], the au-
thors transform unlabeled event logs into labeled ones using an expectation-maximization
technique. in [29], a similar approach is presented, which uses sequence partitioning to
discover the case identiﬁers. both approaches aim at correlating events that match certain
workﬂow patterns. however, they do not handle complex structures such as loops and paral-
lelism. the approach proposed in [4] makes use of a reference process model and heuristic
information about the execution time of the different activities within the process in order
to deduct case ids on unlabeled logs. another approach called infer case id (ici) is pro-
posed in [3] and [6]. the ici approach assumes that the case id is a hidden attribute inside
the event log. the beneﬁt of this approach is that it does not require a reference process
model or heuristics. the approach tries to identify the hidden case id attribute by measuring
control-ﬂow discovery quality dimensions on many possible candidate event logs. its goal
is to select the ones with a higher score in terms of ﬁtness, precision, generalization, and
simplicity. the mentioned approaches for event log labeling are clearly related to the prob-
lem we try to solve. however, they ignore the database setting, where event correlations are
explicitly deﬁned by means of foreign keys. this means that case identiﬁers do not need to
be discovered. therefore, the challenge of identifying interesting event logs remains open.case notion discovery and recommendation 33
only the ici approach tackles this issue by measuring control-ﬂow metrics to select the
best event log. this is similar to our idea of measuring log “interestingness”. however, the
ici approach requires to build all the candidate event logs in order to measure such prop-
erties. our approach is able to reduce the computational cost by predicting interestingness
properties before the log is built.
other authors have already considered the idea of evaluating event log characteristics.
the metrics proposed in [17] aim at discovering the structural properties of event logs with-
out actually mining the behavior. these metrics have proven to be of great value in order
to develop our automated approach. the approach in [23] focuses on event correlation for
business processes in the context of web services. additionally, it proposes semi-automatic
techniques to generate process views with a certain level of “interestingness”. instead of
focusing on what is interesting, it discards uninteresting correlations based on the variabil-
ity of values on the correlating attributes, or on the ratio of process instances per log. the
approach is certainly of value in the area of event correlation. on the other hand, it does
not provide a framework for automatic case notion discovery. also, the approach chosen by
the authors to deal with the combinatorial explosion problem is search space pruning, which
still requires to compute the event logs, but for a smaller set of candidates.
when it comes to computing rankings, in our case rankings of event logs or case no-
tions, we must consider learning to rank (ltr) algorithms from the information retrieval
ﬁeld. these algorithms are able to learn an optimal ordering of documents with respect to
certain features. three main categories can be distinguished among them: pointwise, pair-
wise, and listwise. pointwise algorithms try to predict the relevance score of each candidate,
one by one. these algorithms are able to give a prediction of the score, but do not consider
the position of a document in the ranking. examples of pointwise algorithms are random
forest [5], linear regression [26], the predictors evaluated in section 8.2, and any other
algorithm that applies regression in general. pairwise algorithms take pairs of candidates
and predict which candidate ranks higher. in this case, the relative position of documents is
taken into account. examples of pairwise algorithms are mart [14], ranknet [7], rank-
boost [13], and lambdarank [8]. listwise algorithms take lists of candidates and learn to
optimize the order. a disadvantage of this type of approach is the difﬁculty to obtain training
sets of full ranked lists of candidates. examples of listwise algorithms are adarank [32],
coordinate ascent [22], lambdamart [31], and listnet [14].
as a summary, event correlation, log building, and process view “interestingness” are
known topics in the ﬁeld. despite the attempts of authors, none of the approaches succeeded
at reaching a satisfactory level of automation. also, none of them proposes a way to recom-
mend process views to the user, neither to rank them by interests.
10 conclusion
applying process mining in environments with complex database schemas and large
amounts of data becomes a laborious task, specially when we lack the right domain knowl-
edge to drive our decisions. this work attempts to alleviate the problem of event log building
by automatically computing case notions and by recommending the interesting ones to the
user. by means of a new deﬁnition of case notion, events are correlated to construct the
traces that form an event log. the properties of these event logs are analyzed to assess their
interestingness. because of the computational cost of building the event logs for a large
set of case notion candidates, a set of features was deﬁned based on the characteristics of
the case notion and the dataset at hand. next, a custom predictor estimates the log metrics34 e. gonz ´alez l ´opez de murillas et al.
used to assess the interestingness. this allows one to rank case notions even before their
corresponding event logs are built. finally, an extensive evaluation of the custom predictor
was carried out, comparing it to different regressors and to state of the art learning to rank
algorithms. we believe that evaluating the approach in comparison to techniques from the
information retrieval ﬁeld has not been considered before in the process mining discipline.
to conclude, this work proposes a framework that covers the log building process from
the case notion discovery phase, to the ﬁnal event log computation, providing the tools to
assess its interestingness based on objective metrics. this assessment can be done on the
case notion itself before the event log is generated. the result of this assessment is used to
provide recommendations to the user.
our framework presents several limitations, however. the most important one has to
do with log interestingness. we are aware that the notion of log “interestingness” proposed
in this work is somewhat superﬁcial. only certain structural properties of the log ( level of
detail ,support ,average number of events per trace ) are taken into account when evaluating
event logs. the current notion of log “interestingness” ignores other important aspects such
as the relevance of the log semantics at the business level, how meaningful the activities
are with respect to the process, as well as the homogeneity of behavior captured in the
event log. our deﬁnition of log “interestingness” is a ﬁrst attempt at providing an objective
score to rank event logs. however, the relation of the proposed “interestingness” metric with
respect to a subjective interestingness score provided by users has not been evaluated. a
study should be carried out involving real business analysts and domain experts to evaluate
the suitability of the metric when applied to different datasets and contexts. also, this study
would be valuable to identify additional measurable aspects that contribute to the notion of
log “interestingness” and have not been considered by our deﬁnition.
another limitation has to do with our prediction results. we proposed certain predic-
tors for the event log metrics used to assess log “interestingness”. it has been shown that
the resulting ranking based on predicted scores resembles, at an acceptable level of accu-
racy, the ranking based on the actual metrics. however, the individual predictions for each
log metric lack accuracy. relative assumptions can still be made, e.g., log a has higher
support than log b. however, accurate predictions would make the technique more robust
to outliers, and beneﬁt the overall quality of the log “interestingness” assessment. finding
stricter upper and lower bounds and designing more accurate predictors for each log metric
would help to improve the quality of event log “interestingness” rankings and provide bet-
ter recommendations to the analyst. this could be combined with sampling techniques that
combine predicted scores on candidate case notions with actual scores on computed event
logs. this would allow to compute event logs only for a limited number of case notions,
while increasing ranking quality introducing some certainty in the scores.
additionally, processing queries expressed on natural language would be a great addi-
tion to the framework, allowing the user to reﬁne the search and insert domain knowledge in
the recommendation process. also, interactive approaches based on feedback provided on
example logs would allow to guide the search using domain knowledge.
references
1. ieee standard for extensible event stream (xes) for achieving interoperability in event logs and
event streams (2016). doi 10.1109/ieeestd.2016.7740858
2. van der aalst, w.m.p., adriansyah, a., de medeiros, a.k.a., arcieri, f., et al.: process mining man-
ifesto, pp. 169–194. springer berlin heidelberg, berlin, heidelberg (2012). doi nurlf10.1007/
978-3-642-28108-2 19gcase notion discovery and recommendation 35
3. andaloussi, a.a., burattin, a., weber, b.: toward an automated labeling of event log attributes. in:
enterprise, business-process and information systems modeling, pp. 82–96. springer (2018)
4. bayomie, d., helal, i.m., awad, a., ezat, e., elbastawissi, a.: deducing case ids for unlabeled event
logs. in: international conference on business process management, pp. 242–254. springer (2015)
5. breiman, l.: random forests. machine learning 45(1), 5–32 (2001)
6. burattin, a., vigo, r.: a framework for semi-automated process instance discovery from decorative
attributes. in: computational intelligence and data mining (cidm), 2011 ieee symposium on, pp.
176–183. ieee (2011)
7. burges, c., shaked, t., renshaw, e., lazier, a., deeds, m., hamilton, n., hullender, g.: learning to
rank using gradient descent. in: 22nd icml, pp. 89–96. acm (2005)
8. burges, c.j., ragno, r., le, q.v .: learning to rank with nonsmooth cost functions. in: advances in
neural information processing systems, pp. 193–200 (2007)
9. calvanese, d., kalayci, t.e., montali, m., santoso, a.: obda for log extraction in process mining. in:
reasoning web international summer school, pp. 292–345. springer (2017)
10. calvanese, d., kalayci, t.e., montali, m., santoso, a.: the onprom toolchain for extracting business
process logs using ontology-based data access. in: proceedings of the bpm demo track and bpm
dissertation award. ceur-ws. org (2017)
11. calvanese, d., kalayci, t.e., montali, m., tinella, s.: ontology-based data access for extracting event
logs from legacy data: the onprom tool and methodology. in: international conference on business
information systems, pp. 220–236. springer (2017)
12. ferreira, d.r., gillblad, d.: discovering process models from unlabelled event logs. in: international
conference on business process management, pp. 143–158. springer (2009)
13. freund, y ., iyer, r., schapire, r.e., singer, y .: an efﬁcient boosting algorithm for combining prefer-
ences. journal of machine learning research 4(nov), 933–969 (2003)
14. friedman, j.h.: greedy function approximation: a gradient boosting machine. annals of statistics pp.
1189–1232 (2001)
15. giovinazzo, w.a.: object-oriented data warehouse design: building a star schema. prentice hall ptr
(2000)
16. gopalkrishnan, v ., li, q., karlapalem, k.: star/snow-ﬂake schema driven object-relational data ware-
house design and query processing strategies. in: datawarehousing and knowledge discovery, pp.
11–22. springer (1999)
17. gunther, c.: process mining in ﬂexible environments. ph.d. thesis, eindhoven university of technology
(2009)
18. ingvaldsen, j.e., gulla, j.a.: preprocessing support for large scale process mining of sap transactions.
in: bpm workshops, pp. 30–41. springer (2008)
19. jans, m., soffer, p.: from relational database to event log: decisions with quality impact. in: bpm
workshops. springer international publishing (2017)
20. koenker, r.: quantile regression. no. 38 in econometric society monographs. cambridge university
press (2005)
21. lu, x., nagelkerke, m., van de wiel, d., fahland, d.: discovering interacting artifacts from erp sys-
tems. ieee trans. services computing 8(6), 861–873 (2015)
22. metzler, d., croft, w.b.: linear feature-based models for information retrieval. information retrieval
10(3), 257–274 (2007)
23. motahari-nezhad, h.r., saint-paul, r., casati, f., benatallah, b.: event correlation for process dis-
covery from web service interaction logs. the vldb journal 20(3), 417–444 (2011). doi
10.1007/s00778-010-0203-9
24. gonz ´alez l ´opez de murillas, e.: process mining on databases: extracting event data from real-life data
sources. ph.d. thesis, department of mathematics and computer science, technische universiteit eind-
hoven (2019)
25. gonz ´alez l ´opez de murillas, e., reijers, h.a., van der aalst, w.m.p.: connecting databases with process
mining: a meta model and toolset. software & systems modeling pp. – (2017)
26. ng, a.y .: feature selection, l1 vs. l2 regularization, and rotational invariance. icml, pp. 78–. acm
(2004). doi 10.1145/1015330.1015435
27. panik, m.j.: advanced statistics from an elementary point of view, vol. 9. academic press (2005)
28. tax, n., bockting, s., hiemstra, d.: a cross-benchmark comparison of 87 learning to rank methods.
information processing & management 51(6), 757 – 772 (2015)
29. walicki, m., ferreira, d.r.: sequence partitioning for process mining with unlabeled event logs. data &
knowledge engineering 70(10), 821–841 (2011)
30. watson, h.j., wixom, b.h.: the current state of business intelligence. computer 40(9), 96–99 (2007).
doi 10.1109/mc.2007.33136 e. gonz ´alez l ´opez de murillas et al.
31. wu, q., burges, c.j., svore, k.m., gao, j.: adapting boosting for information retrieval measures. infor-
mation retrieval 13(3), 254–270 (2010)
32. xu, j., li, h.: adarank: a boosting algorithm for information retrieval. in: sigir, pp. 391–398. acm
(2007)