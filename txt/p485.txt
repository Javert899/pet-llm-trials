process mining towards semantics
a.k. alves de medeiros and w.m.p. van der aalst
eindhoven university of technology,
p.o. box 513, nl-5600 mb, ei ndhoven, the netherlands
{a.k.medeiros,w.m.p.v.d.aalst }@tue.nl
abstract. process mining techniques target the automatic discovery
of information about process models in organizations. the discovery is
based on the execution data registered in event logs . current techniques
support a variety of practical analysis, but they are somewhat limited
because the labels in the log are not linked to any concepts. thus, in this
chapter we show how the analysis provided by current techniques can
be improved by including semantic data in event logs. our explanation
is divided into two main parts. the ﬁrst part illustrates the power of
current process mining techniques by showing how to use the open source
process mining tool prom to answer concrete questions that managers
typically have about business processes. the second part utilizes usage
scenarios to motivate how process mining techniques could beneﬁt from
semantic annotated event logs and deﬁnes a concrete semantic log format
for prom. the prom tool is available at www.processmining.org.
1 introduction
nowadays, most organizations use information systems to support the execu-
tion of their business processes [21]. examples of information systems support-
ing operational processes are workﬂow management systems (wms) [10,15],
customer relationship management (crm) systems, enterprise resource plan-
ning (erp) systems and so on. these information systems may contain an ex-
plicit model of the processes (for instance, workﬂow systems like staﬀware [8],
cosa [1], etc.), may support the tasks i nvolved in the process without neces-
sarily deﬁning an explicit process model (for instance, erp systems like sap
r/3 [6]), or may simply keep track (for auditing purposes) of the tasks that
have been performed without providing any support for the actual execution of
those tasks (for instance, custom-made information systems in hospitals). either
way, these information systems typica lly support logging capa bilities that reg-
ister what has been executed in the organ ization. these produced logs usually
contain data about cases (i.e. process i nstances) that have been executed in the
organization, the times at which the tasks were executed, the persons or systems
that performed these tasks, and other kinds of data. these logs are the starting
point for process mining, and are usually called event logs . for instance, con-
sider the event log in table 1. this log contains information about four process
instances (cases) of a process that handles ﬁnes.
process mining targets the automatic discovery of information from an event
log. this discovered information can be used to deploy new systems that support
t.s. dillon et al. (eds.): advances in web semantics i, lncs 4891, pp. 35–80, 2008.
c/circlecopyrtifip international federation for information processing 200836 a.k.a. de medeiros and w.m.p. van der aalst
table 1. example of an event log
case id
 task name
 event type
 originator
 timestamp
 extra data
1
 file fine
 completed
 anne
 20-07-2004 14:00:00
 ...
2
 file fine
 completed
 anne
 20-07-2004 15:00:00
 ...
1
 send bill
 completed
 system
 20-07-2004 15:05:00
 ...
2
 send bill
 completed
 system
 20-07-2004 15:07:00
 ...
3
 file fine
 completed
 anne
 21-07-2004 10:00:00
 ...
3
 send bill
 completed
 system
 21-07-2004 14:00:00
 ...
4
 file fine
 completed
 anne
 22-07-2004 11:00:00
 ...
4
 send bill
 completed
 system
 22-07-2004 11:10:00
 ...
1
 process payment
 completed
 system
 24-07-2004 15:05:00
 ...
1
 close case
 completed
 system
 24-07-2004 15:06:00
 ...
2
 send reminder
 completed
 mary
 20-08-2004 10:00:00
 ...
3
 send reminder
 completed
 john
 21-08-2004 10:00:00
 ...
2
 process payment
 completed
 system
 22-08-2004 09:05:00
 ...
2
 close case
 completed
 system
 22-08-2004 09:06:00
 ...
4
 send reminder
 completed
 john
 22-08-2004 15:10:00
 ...
4
 send reminder
 completed
 mary
 22-08-2004 17:10:00
 ...
4
 process payment
 completed
 system
 29-08-2004 14:01:00
 ...
4
 close case
 completed
 system
 29-08-2004 17:30:00
 ...
3
 send reminder
 completed
 john
 21-09-2004 10:00:00
 ...
3
 send reminder
 completed
 john
 21-10-2004 10:00:00
 ...
3
 process payment
 completed
 system
 25-10-2004 14:00:00
 ...
3
 close case
 completed
 system
 25-10-2004 14:01:00
 ...
file
finesend
billprocess
paymentclose
case
send
reminder
fig. 1. petri net illustrating the control-ﬂow perspective that can be mined from the
event log in table 1
the execution of business processes or as a feedback tool that helps in auditing,
analyzing and improving already enacte d business processes. the main beneﬁt
of process mining techniques is that information is objectively compiled. in other
words, process mining techniques are helpful because they gather information
about what is actually happening according to an event log of an organization,
and not what people think that is happening in this organization.
the type of data in an event log determines which perspectives of process min-
ing can be discovered. if the log (i) prov ides the tasks that are executed in the
process and (ii) it is possible to infer th eir order of execution and link these taks
to individual cases (or process instances), then the control-ﬂow perspective can
be mined. the log in table 1 has this data (cf. ﬁelds “case id”, “task name”
and “timestamp”). so, for this log, mining algorithms could discover the process
in figure 11. basically, the process describ es that after a ﬁne is entered in the
system, the bill is sent to the driver. if the driver does not pay the bill within
1the reader unfamiliar with petri nets is referred to [17,30,32].process mining towards semantics 37
one month, a reminder is sent. when the bill is paid, the case is archived. if the
log provides information about the persons/systems that executed the tasks,
theorganizational perspective can be discovered. the or ganizational perspective
discovers information like the social network in a process, based on transfer of
work, or allocation rules linked to organizational entities like roles and units. for
instance, the log in table 1 shows that “anne” transfers work to both “mary”
(case 2) and “john” (cases 3 and 4), and “john” sometimes transfers work to
“mary” (case 4). besides, by inspecting the log, the mining algorithm could dis-
cover that “mary” never has to send a reminder more than once, while “john”
does not seem to perform as good. the managers could talk to “mary” and
check if she has another approach to send reminders that “john” could beneﬁt
from. this can help in making good practices a common knowledge in the or-
ganization. when the log contains more details about the tasks, like the values
of data ﬁelds that the execution of a task modiﬁes, the case perspective (i.e. the
perspective linking data to cases) can b e discovered. so, for instance, a forecast
for executing cases can be made based on a lready completed cases, exceptional
situations can be discovered etc. in our particular example, logging information
about the proﬁles of drivers (like age, gender, car etc.) could help in assessing
the probability that they would pay the ir ﬁnes on time. moreover, logging infor-
mation about the places where the ﬁnes were applied could help in improving
the traﬃc measures in thes e places. from this explanation, the reader may have
already noticed that the control-ﬂow perspective relates to the “how?” question,
the organizational perspective to the “w ho?” question, and the case perspective
to the “what?” question. all these three perspectives are complementary and
relevant for process mining.
current process mining techniques ca n address all these three perspec-
tives [9,11,12,16,20,22,28,29,31,33,34,35,38,40]. actually, many of these tech-
niques are implemented in the open-source tool prom [19,39]. as we show in
this chapter, the prom tool can be used to answer common questions about
business processes, like “how is the distribution of all cases over the diﬀerent
paths in through the processes?”, “where are the bottlenecks in the process?”
or “are all the deﬁned rules indeed being obeyed?”. showing how to use the
current process mining techniques implemented in prom to answer these kinds
of questions is the ﬁrst contribution of this chapter.
however, although the process minin g techniques implemented in prom can
answer many of the common questions, these techniques are somewhat limited
because their analysis is purely based on the labels in the log. in other words,
the techniques are unable to reason about concepts in an event log. for in-
stance, if someone wants to get feedback about billing processes in a company
or the webservices that provide a certa in service, this person has to manually
specify all the labels that map to billing processes or webservices providing the
given service. therefore, as supported by [27], we believe that the automatic dis-
covery provided by process mining tec hniques can be augmented if we include
semantic information about the elements in an event log. note that semantic
process mining techniques bring the disc overy to the conceptual (or semantical)38 a.k.a. de medeiros and w.m.p. van der aalst
level. furthermore, because semantic logs will link to concepts in ontologies,
it is possible to embed ontology reasoning in the mining techniques. however,
when supporting the links to the ontologies, it is important to make sure that
the semantically annotated logs can als o be mined by current process mining
techniques. this way we avoid recoding of good existing solutions. thus, the
second contribution of this chapter consists of showing (i) how to extend our
mining format to support links to ontologies and (ii) providing usage scenarios
that illustrate the gains of using such semantic logs.
the remainder of this chapter is organ ized as follows. section 2 shows how
to use prom plug-ins to answer common questions that managers have about
business processes. section 3 explains ho w to extend the current log format used
by prom to support the link to ontologies and discusses usage scenarios based
on this format. section 4 concludes this chapter.
2 process mining in action
in this section we show how to use the prom tool to answer the questions in ta-
ble 2. these are common questions that managers typically have about business
processes. the prom framework [19,39] is an open-source tool specially tailored
to support the development of process mining plug-ins. this tool is currently
at version 4.0 and contains a wide variety of plug-ins. some of them go beyond
process mining (like doing process veriﬁca tion, converting between diﬀerent mod-
elling notations etc). however, since in this chapter our aim is to show how to
use prom plug-ins to answer common questions about processes in companies ,
we focus on the plug-ins that use as input (i) an event log only or (ii) an event
log and a process model. figure 2 illustrates how we “categorize” these plug-ins.
the plug-ins based on data in the event log only are called discovery plug-ins
because they do not use any existing in formation about deployed models. the
plug-ins that check how much the data in the event log matches the prescribed
behavior in the deployed models are called conformance plug-ins. finally, the
plug-ins that need both a model and its logs to discover information that will
enhance this model are called extension plug-ins. in the context of our common
questions, we use (i) discovery plug-ins to answer questions like “how are the
cases actually being executed? are the rules indeed being obeyed?”, (ii) con-
formance plug-ins to questions like “how compliant are the cases (i.e. process
instances) with the deployed process m odels? where are the problems? how
frequent is the (non-)compliance?”, and ( iii) extension plug-ins to questions like
“what are the business rules in the process model?”
the remainder of this section illustrates how to use prom to answer the ques-
tions in table 2. the provided explanations have a tutorial-like ﬂavor because
you should be able to reproduce the results when using prom over the example
in this section or while analyzing other logs. the explanation is supported by
the running example in subsection 2.1. subsection 2.3 describes how you can
inspect and clean an event log before performing any mining. subsection 2.4
shows how to use discovery plug-ins (cf. figure 2) and subsection 2.5 describesprocess mining towards semantics 39
table 2. common questions that managers usually have about processes in organiza-
tions
1. what is the most frequent path for every process model?
2. how is the distribution of all cases over the diﬀerent paths through the process?
3. how compliant are the cases (i.e. process instances) with the deployed process
models? where are the problems? how frequent is the (non-) compliance?
4. what are the routing probabilities for each split/join task?
5. what is the average/minimum/maximum throughput time of cases?
6. which paths take too much time on average? how many cases follow these rout-
ings? what are the critical sub-paths for these paths?
7. what is the average service time for each task?
8. how much time was spent between any two tasks in the process model?
9. how are the cases actually being executed?
10. what are the business rules in the process model?
11. are the process rules/constraints indeed being obeyed?
12. how many people are involved in a case?
13. what is the communication structure and dependencies among people?
14. how many transfers happen from one role to another role?
15. who are important people in the communication ﬂow?
16. who subcontract work to whom?
17. who work on the same tasks?
information 
system
(process)
modelevent
logsmodels
analyzes
discoveryrecords 
events, e.g., 
messages, 
transactions, 
etc.specifies 
configures 
implements
analyzessupports/
controls
extensionconformance“world”
people machines
organizationscomponentsbusiness processes
fig. 2. sources of information for process mining. the discovery plug-ins use only an
event log as input, while the conformance and extension plug-ins also need a (process)
model as input.40 a.k.a. de medeiros and w.m.p. van der aalst
how to mine with conformance andextension plug-ins. finally, we advice you
to have the prom tool at hand while reading this section. this way you can play
with the tool while reading the explanations. subsection 2.2 explains how to get
started with prom.
2.1 running example
the running example is about a process to repair telephones in a company .t h e
company can ﬁx 3 diﬀerent types of phones (“t1”, “t2” and “t3”). the process
starts by registering a telephone device sent by a customer. after registration,
the telephone is sent to the problem de tection (pd) department. there it is
analyzed and its defect is categorized. in total, there are 10 diﬀerent categories
of defects that the phones ﬁxed by this company can have. once the problem is
identiﬁed, the telephone is sent to the repair department and a letter is sent to
the customer to inform him/her about the problem. the repair (r) department
has two teams. one of the teams can ﬁx simple defects and the other team can
repair complex defects. however, some of the defect categories can be repaired
by both teams. once a repair employee ﬁnishes working on a phone, this device
is sent to the quality assurance (qa) d epartment. there it is analyzed by
an employee to check if the defect was indeed ﬁxed or not. if the defect is not
repaired, the telephone is again sent to the repair department. if the telephone is
indeed repaired, the case is archived and the telephone is sent to the customer.
to save on throughput time, the company only tries to ﬁx a defect a limited
number of times. if the defect is not ﬁxed, the case is archived anyway and a
brand new device is sent to the customer.
2.2 getting started
to prepare for the next sections, you need to do:
1. install the prom tool. this tool is freely available at http:// prom.sourceforge.
net. please download and run the installation ﬁle for your operating system.
2. download the two log ﬁles for the running example. these logs ﬁles are
located at (i) http://www.processmining.org/
 media/tutorial/repairexam-
ple.zip and (ii) http://www.processmining.org/
 media/tutorial/repairexam-
plesample2.zip.
2.3 inspecting and cleaning an event log
before applying any mining technique to an event log, we recommend you to
ﬁrst get an idea of the information in this event log. the main reason for this
is that you can only answer certain questions if the data is in the log. for
instance, you cannot calculate the throughput time of cases if the log does not
contain information about the times (timestamp) in which tasks were executed.
additionally, you may want to remove unnecessary information from the log
before you start the mining. for instance, you may be interested in mining
only information about the cases that are completed. for our running exampleprocess mining towards semantics 41
table 3. log pre-processing: questions and pointers to answers
question
 subsection
how many cases (or process instances) are in the log?
2.3.1
how many tasks (or audit trail entries) are in the log?
how many originators are in the log?
are there running cases in the log?
which originators work in which tasks?
how can i ﬁlter the log so that only completed cases are kept?
2.3.2
 how can i see the result of my ﬁltering?
how can i save the pre-processed log so that i do not have to redo
work?
(cf. section 2.1), all cases without an archiving task as the last one correspond
to running cases and should not be considered. the cleaning step is usually a
projection of the log to consider only the data you are interested in. thus, in
this section we show how you can inspect and clean (or pre-process) an event
log in prom. furthermore, we show how you can save the results of the cleaned
log, so that you avoid redoing work.
the questions answered in this sectio n are summarized in table 3. as you can
see, subsection 2.3.1 shows how to answer questions related to log inspection and
subsection 2.3.2 explains how to ﬁlter an event log and how to save your work.
note that the list of questions in table 3 is not exhaustive, but they are enough to
give you an idea of the features oﬀered by prom for log inspection and ﬁltering.
2.3.1 inspecting the log
the ﬁrst thing you need to do to inspect or mine a log is to load it into prom. in
this section we use the log at the location http://www.processmining.org/
 media/
tutorial/repairexample.zip . this log has process instances of the running exam-
ple described in section 2.1.
to open this log, do the following:
1. download the log for the running example and save it at your computer.
2. start the prom framework. you should get a screen like the one in figure 3.
note that the prom menus are context sensitive. for instance, since no log
has been opened yet, no mining algorithm is available.
3. open the log via clicking file→open mxml log , and select your saved copy
of the log ﬁle for the running example. once your log is opened, you should
get a screen like the one in figure 4. note that now more menu options are
available.
now that the log is opened, we can proceed with the actual log inspection. recall
that we want to answer the following questions:
1. how many cases (or process instances) are in the log?
2. how many tasks (or audit trail entries) are in the log?42 a.k.a. de medeiros and w.m.p. van der aalst
fig. 3. screenshot of the main interface of prom. the menu fileallows to open event
logs and to import models into prom. the menu mining provides the mining plug-ins.
these mining plug-ins mainly focus on discovering information about the control-ﬂow
perspective of process models or the social network in the log. the menu analysis gives
access to diﬀerent kinds of analysis plug-ins for opened logs, imported models and/or
mined models. the menu conversion provides the plug-ins that translate between the
diﬀerent notations supported by prom. the menu exports has the plug-ins to export
the mined results, ﬁltered logs etc.
3. how many originators are in the log?
4. are there running cases in the log?
5. which originators work in which tasks?
theﬁrst four questions can be answered by the analysis plug-in log summary .
to call this plug-in, choose analysis→[log name...] →log summary .c a ny o un o w
answer the ﬁrst four questions of the list above? if so, you probably have noticed
that this log has 104 running cases and 1000 completed cases. you see that from
the information in the table “ending log events” of the log summary (cf. fig-
ure 5). note that only 1000 cases end with the task “archive repair”. the last
question of the list above can be answered by the analysis plug-in originator
by task matrix . this plug-in can be started by clicking the menu analysis→[logprocess mining towards semantics 43
fig. 4. screenshot of prom after opening the log of the running example (cf. sec-
tion 2.1)
name...]→originator by task matrix . can you identify which originators per-
form the same tasks for the running example log? if so, you probably have also
noticed that there are 3 people in each of the teams in the repair department
(cf. section 2.1) of the company2. the employees with login “solverc.. . ” deal
with the complex defects, while the employees with login “solvers.. . ” handle
the simple defects.
take your time to inspect this log with these two analysis plug-ins and ﬁnd
out more information about it. if you like, you can also inspect the individual
cases by clicking the button preview log settings (cf. bottom of figure 4) and
then double-clicking on a speciﬁc process instance.
2see the originators working on the tasks “repair (complex)” and “repair (simple)”
in figure 6.44 a.k.a. de medeiros and w.m.p. van der aalst
fig. 5. excerpt of the result of the analysis plug-in log summary
fig. 6. scheenshot with the result of the analysis plug-in originator by task matrix
2.3.2 cleaning the log
in this chapter we use the process mining techniques to get insight about the
process for repairing telephones (cf. section 2.1). since our focus in on the process
as a whole , we will base our analysis on the completed process instances only.
note that it does not make much sense to talk about the most frequent path ifprocess mining towards semantics 45
it is not complete, or reason about throughput time of cases when some of them
are still running. in short, we need to pre-process (or clean or ﬁlter) the logs.
in prom, a log can be ﬁltered by applying the provided log filters .i nf i g u r e4
you can see three log ﬁlters (see bottom-left of the panel with the log): event
types,start event andend event .t h eevent types log ﬁlter allows us to select
the type of events (or tasks or audit trail entries) that we want to consider while
mining the log. for our running example, the log has tasks with two event types:
complete andstart. if you want to (i) keep all tasks of a certain event, you should
select the option “include” (as it is in figure 4), (ii) omit the tasks with a certain
event type from a trace, select the option “ignore”, and (iii) discard all traces
with a certain event type, select the optio n “discard instance”. this last option
may be useful when you have aborted cases etc. the start event ﬁlters the log
so that only the traces (or cases) that start with the indicated task are kept.
theend event works in a similar way, but the ﬁltering is done with respect to
the ﬁnal task in the log trace.
from the description of our running example, we know that the completed
cases are the ones that start with a task to register the phone and end with a
task to archive the instance. thus, to ﬁlter the completed cases, you need to
execute the following procedure:
1. keep the event types selection as in figure 4 (i.e., “include” all the complete
andstartevent types);
2. select the task “register (complete)” as the compulsory start event ;
3. select the task “archive repair (complete)” as the compulsory ﬁnal event .
if you now inspect the log (cf. section 2.3.1), for instance, by calling the analysis
plug-in log summary , you will notice that the log contains fewer cases (can
you say how many?) and all the cases indeed start with the task “register
(complete)” and ﬁnish with the task “archive repair (complete)”.
although the log ﬁlters we have presented so far are very useful, they have
some limitations. for instance, you can only specify one task as the start task for
cases. it would be handy to have more ﬂexibility, like saying “filter all the cases
that start with task x or task y”. for reasons like that, the advanced tab of the
panel with the log (cf. figure 7) provides more powerful log ﬁlters. each log ﬁlter
has a help, so we are not going into details about them. however, we strongly
advise you to spend some time trying them out and getting more feeling about
how they work. our experience shows that the advanced log ﬁlters are especially
useful when handling real-life logs. these ﬁlters not only allow for projecting
data in the log, but also for adding data to the log. for instance, the log ﬁlters
add artiﬁcial start task anda d da r t i ﬁ c i a le n dt a s k support the respective
addition of tasks at the begin and end of traces. these two log ﬁlters are handy
when applying process mining algorithms that assume the target model to have
a single start/end point.46 a.k.a. de medeiros and w.m.p. van der aalst
fig. 7. scheenshot of the advanced log filters in prom
once you are done with the ﬁltering, you can save your results in two ways:
1. export the ﬁltered log by choosing the export plug-in xml log ﬁle . this will
save a copy of the log that contains all the changes made by the application
of the log ﬁlters.
2. export the conﬁgured log ﬁlters themselves by choosing the export plug-in
log filter (advanced) . exported log ﬁlters can be imported into prom at a
later moment and applied to a (same) log. you can import a log ﬁlter by
selecting file→[log name...] →open log filter (advanced) .
if you like, you can export the ﬁltered log for our running example. can you
open this exported log into prom? what do you notice by inspecting this log?
note that your log should only contain 1000 cases and they should all start and
e n dw i t has i n g l et a s k .process mining towards semantics 47
table 4. discovery plug-ins: questions and pointers to answers
question
 subsection
how are the cases actually being executed?
 2.4.1
what is the most frequent path for every process model?
2.4.2
how is the distribution of all cases over the diﬀerent paths through the
process?
how many people are involved in a case?
2.4.3
what is the communication structure and dependencies among people?
how many transfers happen from one role to another role?
who are the important people in the communication ﬂow?
who subcontract work to whom?
who work on the same tasks?
are the process rules/constraints indeed being obeyed?
 2.4.4
2.4 questions answered based on an event log only
now that you know how to inspect and pre-process an event log (cf. subsec-
tion 2.3), we proceed with showing how to answer the questions related to the
discovery prom plug-ins (cf. figure 2). recall that a log is the only input for
these kinds of plug-ins.
the questions answered in this section are summarized in table 3. subsec-
tion 2.4.1 shows how to mine the control-ﬂow perspective of process models.
subsection 2.4.2 explains how to mine information regarding certain aspects
of cases. subsection 2.4.3 describes how to mine information related to the
roles/employees in the event log. subsection 2.4.4 shows how to use temporal
logic to verify if the cases in a log sat isfy certain (requi red) properties.
2.4.1 mining the control-flow perspective of a process
the control-ﬂow perspective of a process establishes the dependencies among its
tasks. which tasks precede which other o nes? are there concurrent tasks? are
there loops? in short, what is the process model that summarizes the ﬂow fol-
lowed by most/all cases in the log? this information is important because it gives
you feedback about how cases are actually being executed in the organization.
as shown in figure 8, prom supports various plug-ins to mine the control-
ﬂow perspective of process models. in this section, we will use the mining plug-in
alpha algorithm plugin . thus, to mine the log of our running example, you should
perform the following steps:
1. open the ﬁltered log that contain s only the completed cases (cf. sec-
tion 2.3.2), or redo the ﬁltering for the original log of the running example.
2. verify with the analysis plug-in log summary if the log is correctly ﬁltered.
if so, this log should contain 1000 process instances, 12 audit trail entries, 1
start event (“register”), 1 end event (“archive repair”), and 13 originators.
3. run the alpha algorithm plugin by choosing the menu mining→[log name...]
→alpha algorithm plugin (cf. figure 8).48 a.k.a. de medeiros and w.m.p. van der aalst
fig. 8. scheenshot of the mining plug-ins in prom
4. click the button start mining . the resulting mined model should look like
the one in figure 9. note that the alpha algorithm plugin uses petri nets3
as its notation to represent process m odels. from this mined model, you can
observe that:
3diﬀerent mining plug-ins can work with diﬀerent notations, but the main idea is
always the same: portray the dependencies between tasks in a model. furthermore,
the fact that diﬀerent mining plug-ins may work with diﬀerent notations does not
prevent the interoperability between these representations because the prom tool
oﬀers conversion plug-ins that translate models from one notation to another [39].process mining towards semantics 49
fig. 9. scheenshot of the mined model for the log of the running example50 a.k.a. de medeiros and w.m.p. van der aalst
–all cases start with the task “register” and ﬁnish with the task “archive
repair”. this is not really surprising since we have ﬁltered the cases in
the log.
–after the task analyze defect completes, some tasks can occur in par-
allel: (i) the client can be informe d about the defect (see task “inform
user”), and(ii) the actual ﬁx of the def ect can be started by executing
the task repair (complete) orrepair (simple) .
–the model has a loop construct involving the repair tasks.
based on these remarks, we can conclude that the cases in our running
example log have indeed been executed as described in section 2.1.
5. save the mined model by choosing the menu option exports→selected petri
net→petri net kernel ﬁle . we will need this exported model in subsec-
tion 2.5.
6. if you prefer to visualize the mined model in another representation, you can
convert this model by invoking one of the menu option conversion .a sa n
example, you can convert the mined petri net to an epc by choosing the
menu option conversion →selected petri net →labeled wf net to epc .
as a ﬁnal note, although in this section we mine the log using the alpha algorithm
plugin , we strongly recommend you to try other plug-ins as well. the main reason
is that the alpha algorithm plugin is not robust to logs that contain noisy data
(like real-life logs typically do). thus, we suggest you have a look at the help
of the other plug-ins before choosing for a speciﬁc one. in our case, we can hint
that we have had good experience while using the mining plug-ins multi-phase
macro plugin ,heuristics miner andgenetic algorithm plugin to real-life logs.
2.4.2 mining case-related information about a process
do you want to know the most frequent path for our running example? or the
distribution of all cases over the diﬀerent paths through the process? then you
should use the analysis plug-in performance sequence diagram analysis .a sa n
illustration, in the context of our running example, one would expect that paths
without the task “restart repair” (i.e., situations in which the defect could
not be ﬁxed in the ﬁrst attempt) should be less frequent than the ones with
this task. but is this indeed the current situation? questions like this will be
answered while executing th e following procedure:
1. open the ﬁltered log that contain s only the completed cases (cf. sec-
tion 2.3.2).
2. run the performance sequence diagram analysis by choosing the menu
analysis→[log name...] →performance sequence diagram analysis .
3. select the tab pattern diagram a n dc l i c ko nt h eb u t t o n show diagram .y o u
should get a screen like the one in figure 10.
take your time to inspect the results (i.e., the sequence patterns and their
throughput times). can you answer our initial questions now? if so, youprocess mining towards semantics 51
fig. 10. scheenshot of the analysis plug-in performance sequence diagram analysis . the conﬁguration options are on the left side,
the sequence diagrams are on the middle and the patterns frequence and throughput times are on the right side.52 a.k.a. de medeiros and w.m.p. van der aalst
have probably notice that the 73,5% of the defects could be ﬁxed in the ﬁrst
attempt4.
4. now, how about having a look at the resources? which employees are in-
volved in the most frequent patterns? in which sequence do they interact?
to see that, just choose “originator” as the component type and click on
the button show diagram .
take your time to have a look at the other options provided by this plug-in. for
instance, by clicking on the button filter options you can select speciﬁc mined
patterns etc.
2.4.3 mining organizational-related information about a process
in this section we answer questions regarding the social (or organizational) aspect
of a company. the questions are:
–how many people are involved in a speciﬁc case?
–what is the communication structure and dependencies among people?
–how many transfers happen from one role to another role?
–who are important people in the communication ﬂow?
–who subcontract work to whom?
–who work on the same tasks?
these and other related questi ons can be answered by using the mining plug-
inssocial network miner andorganizational miner ,a n dt h e analysis plug-in
analyze social network . in the following we explain how to answer each question
in the context of our running example.
to know the people that are involved in a speciﬁc case or all the cases in
the log , you can use the analysis plug-in log summary (cf. section 2.3.1). for
instance, to check which people are involved in the process instance 120of our
example log, you can do the following:
1. open the ﬁltered log (cf. section 2.3.2) for the running example.
2. click the button preview log settings .
3. right-click on the panel process instance and click on find... .
4. in the dialog find, ﬁeld “text to ﬁnd”, type in 120and click “ok”. this
option highlights the process instance in the list.
5. double-click the process instance 120.
6. visualize the log summary for this process instance by choosing the menu
option analysis→previewed selection... →log summary .
you can see who work on the same tasks by using the analysis plug-in originator
by task matrix , or by running the mining plug-in organizational miner .f o r
instance, to see the roles that work for the same tasks in our example log, you
can do the following:
4see patterns 0 to 6, notice that the task “restart repair” does not belong to these
patterns. furthermore, the sum of the occurrences of these patterns is equal to 735.process mining towards semantics 53
1. open the ﬁltered log (cf. section 2.3.2) for the running example.
2. select the menu option mining→filtered... →organizational miner ,c h o o s e
the options “doing similar task” and “correlation coeﬃcient”, and click
onstart mining .
3. select the tab organizational model . you should get a screen like the one in
figure 11. take you time to inspect the information provided at the bottom
of this screen. noticed that the automatically generated organizational model
shows that the people with the role “tester...” work on the tasks “analyze
defect” and “test repair”, and so on. if you like, you can edit these automat-
ically generated organizational model by using the functionality provided at
the other two tabs tasks <->org entity andorg entity <->resource .n o t e
that organizational models can be exported and used as input for other
organizational-related mining and analysis plug-ins.
the other remaining questions of the list on page 52 are answered by using the
mining plug-in social network in combination with the analysis plug-in analyze
social network . for instance, in the context of our running example, we would
like to check if there are employees that outperform others. by identifying these
employees, one can try to make the good pr actices (or way of working) of these
employees a common knowledge in the c ompany, so that peer employees also
beneﬁt from that. in the context of our running example, we could ﬁnd out
which employees are better at ﬁxing defects. from the process description (cf.
section 2.1) and from the mined model in figure 9, we know that telephones
which were not repaired are again sent to the repair department. so, we can
have a look at the handover of work for the tasks performed by the people in
this department. in other words, we can have a look at the handover of work for
the tasks repair (simple) andrepair (complete) . one possible way to do so is
to perform the following steps:
1. open the log for the running example.
2. use the advanced log ﬁlter event log filter (cf. section 2.3.2) to ﬁlter the log
so that only the four tasks “repair (simple) (start)”, “repair (simple) (com-
plete)”, “repair (complex) (start)” and “repair (complex) (complete)” are
kept. (hint: use the analysis plug-in log summary to check if the log is cor-
rectly ﬁltered!).
3. run the social network miner by choosing the menu option mining→fil-
ter e d...→social network miner (cf. figure 8).
4. select the tab handover of work , and click the button start mining .y o u
should get a result like the one in figure 12. we could already analyze this
result, but we will use the analysis plug-in analyze social network to do so.
this analysis plug-in provides a more intuitive user interface. this is done
on the next step.
5. run the analyze social network by choosing the menu option analysis→
sna→analyze social network . select the options “vertex size”, “vertex de-
gree ratio stretch” and set mouse mode to “picking” (so you can use the54 a.k.a. de medeiros and w.m.p. van der aalst
fig. 11. scheenshot of the mining plug-in organizational minerprocess mining towards semantics 55
fig. 12. scheenshot of the mining plug-in social network miner
mouse to re-arrange the nodes in the graph). the resulting graph (cf. fig-
ure 13) shows which employees handed over work to other employees in the
process instances of our running example. by looking at this graph, we can
see that the employees with roles “solvers3” and “solverc3” outperform
the other employees because the telephones these two employees ﬁx always
pass the test checks and, therefore, are not re-sent to the repair department
(since no other employee has to work on the cases involving “solvers3” and
“solverc3”). the oval shape of the nodes in the graph visually expresses
the relation between the inandoutdegree of the connections (arrows) be-
tween these nodes. a higher proportion of ingoing arcs lead to more vertical
oval shapes while higher proportions of outgoing arcs produce more horizon-
tal oval shapes. from this remark, can you tell which employee has more
problems to ﬁx the defects?
take you time to experiment with the plug-ins explained in the procedure above.
can you now answer the other remaining questions?56 a.k.a. de medeiros and w.m.p. van der aalst
fig. 13. scheenshot of the mining plug-in analyzer social network
as a ﬁnal remark, we point out that the results produced by the social net-
work mining plug-in can be exported to more powerful tools like agna5and
netminer6, which are especially tailored to analyze social networks and provide
more powerful user interfaces.
2.4.4 verifying properties in an event log
it is often the case that processes in organizations should obey certain rules or
principles. one common example is the “four-eyes principle” which determines
that some tasks should not be executed by a same person within a process
instance. these kinds of principles or rules are often used to ensure quality of
the delivered products and/or to avoid frauds. one way to check if these rules
5http://agna.gq.nu
6http://www.netminer.com/process mining towards semantics 57
are indeed being obeyed is to audit the log with data about what has happened
in an organization. in prom, auditing is provided by the analysis plug-in default
ltl checker plugin7.
from the description of our running example (cf. section 2.1), we know that
after a try to ﬁx the defect, the telephone should be tested to check if it is
indeed repaired. thus, we could use the default ltl checker plugin to verify
the property: does the task “test repair” always happen afterthe tasks “repair
(simple)” or “repair (complex)” and before t h et a s k“ a r c h i v er e p a i r ” ? we do
so by executing the following procedure:
1. open the ﬁltered log (cf. section 2.3.2) for the running example.
2. run the default ltl checker plugin by selecting the menu option analysis
→[log name...] →default ltl checker plugin . you should get a screen like
the one in figure 14.
3. select the formula “eventually
 activity
 a
then
b
then
c”.
4. give as values: (i) activity a = repair (simple) , (ii) activity b = test
repair and (iii) activity c = archive repair . note that the ltl plug-in is
case sensitive. so, make sure you type in the task names as they appear in
the log.
5. click on the button check . the resulting screen should show the log split
into two parts: one with the cases that satisfy the property (or formula) and
another with the cases that do not satisfy the property. note that the menu
options now also allow you to do mining, analysis etc. over the split log.
for instance, you can apply again the ltl plug-in over the incorrect process
instances to check if the remaining instances refer to situations in which the
task “repair (complex)” was executed. actually, this is what we do in the
next step.
6. run the default ltl checker plugin over the incorrect process instances by
choosing analysis→incorrect instances (573) →default ltl checker plugin .
7. select the same formula and give the same input as in steps 3 and 4 above.
however, this time use activity a = repair (complex) .
8. click on the button check . note that all 573 cases satisfy the formula. so,
for this log, there are not situations i n which a test does not occur after a
repair.
take your time to experiment with the ltl plug-in. can you identify which
pre-deﬁned formula you could use to check for the “four-eyes principle”?
in this section we have shown how to use the pre-deﬁned formulae of the ltl
analysis plug-in to verify properties in a log. however, you can also deﬁne your
own formulae and import them into prom . the tutorial that explains how to do
so is provided together with the documentation for the prom tool8.
7ltl stands for linear temporal logic.
8for windows users, please see start→programs →documentation →all documen-
tation →ltlchecker-manual.pdf .58 a.k.a. de medeiros and w.m.p. van der aalst
fig. 14. scheenshot of the analysis plug-in default ltl checker pluginprocess mining towards semantics 59
table 5. conformance and extension plug-ins: questions and pointers to answers
question
 subsection
how compliant are the cases (i.e. process instances) with the deployed
process models? where are the problems? how frequent is the (non-)
compliance?
2.5.1
what are the routing probabilities for each slipt/join task?
2.5.2
what is the average/minimum/maximum throughput time of cases?
which paths take too much time on average? how many cases follow
these routings? what are the critical sub-paths for these routes?
what is the average service time for each task?
how much time was spent between any two tasks in the process model?
what are the business rules in the process model?
 2.5.3
2.5 questions answered based on a process model plus an event
log
in this section we explain the prom analysis plug-ins that are used to answer the
questions in table 5. these plug-ins diﬀer from the ones in section 2.4 because
they require a log anda (process) model as input (cf. figure 2). subsection 2.5.1
explains a conformance prom plug-in that detects d iscrepancies between the
ﬂows prescribed in a model and the actu al process instances (ﬂows) in a log.
subsections 2.5.2 and 2.5.3 describe extension prom plug-ins that respectively
extend the models with performance ch aracteristics and business rules.
2.5.1 conformance checking
nowadays, companies usually have some process-aware information system [21]
to support their business process. howe ver, these process models may be incom-
plete because of reasons like: one could not think of all possible scenarios while
deploying the model; the world is dynamic and the way employees work may
change but the prescribed process models are not updated accordingly; and so
on. either way, it is always useful to have a tool that provides feedback about
this.
the prom analysis plug-in that checks how much process instances in a log
match a model and highlights discrepancies is the conformance checker .a sa n
illustration, we are going to check the exported mined model (cf. section 2.4.1,
page 50) for the log of the running exam ple against a new log provided by the
company. our aim is to check how compliant this new log is with the prescribed
model. the procedure is the following:
1. open the log “repairexamplesample2.zip”. this log can be downloaded from
http://www.processmining.org/
 media/tutorial/repairexamplesample2.zip .
2. open the exported pnml model that you created while executing the pro-
cedure on page 50.
3. check if the automatically suggested mapping from the tasks in the log to
the tasks in the model is correct. if not, change the mapping accordingly.60 a.k.a. de medeiros and w.m.p. van der aalst
4. run the conformance checker plug-in by choosing the menu option analysis
→selected petri net →conformance checker .
5. deselect the options “precision” and “structure”9, and click the button start
analysis . you should get results like the ones shown in ﬁgures 15 and 16,
which respectively show screenshots of the model and log diagnostic perspec-
tiveof the conformance checker plug-in . these two perspectives provide
detailed information about the problems encountered during the log replay.
themodel perspective diagnoses information about token counter (number
of missing/left tokens), failed tasks (tasks that were not enabled), remaining
tasks (tasks that remained enabled), path coverage (the tasks and arcs that
were used during the log replay) and passed edges (how often every arc in
the model was used during the log replay). the log perspective indicates the
points of non-compliant behavior for every case in the log.
take your time to have a look at the results. can you tell how many traces are
not compliant with the log? what are the problems? have all the devices been
tested after the repair took places? is the client always being informed?
2.5.2 performance analysis
like the conformance checker (cf. section 2.5.1), the plug-in perfomance anal-
ysis with petri net also requires a log and a petri net as input10.t h em a i n
diﬀerence is that this plug-in focuses on analyzing time-related aspects of the
process instances. in other words, this plug-in can answer the questions:
–what are the routing probabilities for each slipt/join task?
–what is the average/minimum/maximum throughput time of cases?
–which paths take too much time on average? how many cases follow these
routings? what are the critical sub-paths for these routes?
–what is the average service time for each task?
–how much time was spent between any two tasks in the process model?
to execute the perfomance analysis with petri net analysis plug-in over the log
of our running example, perform the following steps:
1. open the ﬁltered log (cf. section 2.3.2) for the running example.
2. open the exported pnml model that you created while executing the pro-
cedure on page 50.
3. run the perfomance analysis with petri net analysis plug-in by selecting
the menu option analysis→selected petri net →performance analysis with
petri net .
9these are more advanced features that we do not need while checking for compliance.
thus, we will turn them oﬀ for now.
10if the model you have is not a petri net but another one of the supported formats,
you can always use one of the provided conversion plug-ins to translate your model
t oap e t r in e t .process mining towards semantics 61
fig. 15. scheenshot of the analysis plug-in conformance checker : model view62 a.k.a. de medeiros and w.m.p. van der aalst
fig. 16. scheenshot of the analysis plug-in conformance checker :l o gv i e wprocess mining towards semantics 63
4. set the ﬁeld times measured in to “hours” and click on the button start
analysis . the plug-in will start replaying the log and computing the time-
related values. when it is ready, you should see a screen like the one in
figure 17.
5. take your time to have a look at these results. note that the right-side
panel provides information about the average/minimum/maximum throught-
puttimes. the central panel (the one with the petri net model) shows (i)
thebottlenecks (notice the diﬀerent colors for the places) and (ii) the routing
probabilities for each split/join tasks (for instance, note that only in 27% of
the cases the defect could not the ﬁxed on the ﬁrst attempt). the bottom
panel show information about the waiting times in the places. you can also
select one or two tasks to respectively check for average service times andthe
time spent between any two tasks in the process model . if you like, you can
also change the settings for the waiting time (small window at the bottom
withhigh, medium andlow).
the previous procedure showed how to answer all the questions listed in the
beginning of this section, except for one: which paths take too much time on
average? how many cases follow these routings? what are the critical sub-paths
for these routes? to answer this last question, we have to use the analysis plug-in
performance sequence diagram analysis (cf. section 2.4.2) in combination with
theperformance analysis with petri net . in the context of our example, since
the results in figure 17 indicate that the cases take on average 1.11 hours to be
completed, it would be interesting to analyze what happens for the cases that
take longer than that. the procedure to do so has the following steps:
1. if the screen with the results of the performance analysis with petri net
plug-in is still open, just choose the menu option analysis→whole log →
performance sequence diagram analysis . otherwise, just open the ﬁltered
log for the running example and run step 2 described on page 50.
2. in the resulting screen, select the tab pattern diagram ,s e ttime sort to
hours, and click on the button show diagram .
3. now, click on the button filter options to ﬁlter the log so that only cases
with throughput time superior to 1.11 hours are kept.
4. select the option sequences with throughput time , choose “above” and type
in “1.1” in the ﬁeld “hours”. afterwards, click ﬁrst on the button update and
then on button use selected instances . take your time to analyze the results.
you can also use the log summary analysis plug-ins to inspect the log
selection . can you tell how many cases have throughput time superior to 1.1
hours? note that the performance sequence diagram analysis plug-in shows
how often each cases happened in the log. try playing with the provided
options. for instance, what happens if you now select the component type
as “originator”? can you see how well the employees are doing? once you
have the log selection with the cases with throughput time superior to 1.1
hour, you can check for critical sub-paths by doing the remaining steps in
this procedure.64 a.k.a. de medeiros and w.m.p. van der aalst
fig. 17. scheenshot of the analysis plug-in performance analysis with petri netprocess mining towards semantics 65
5. open exported pnml model that you cr eated while executing the procedure
on page 50, but this time link it to the selected cases by choosing the menu
option file→open pnml ﬁle →with:log selection . if necessary, change the
automatically suggested mappings and click on the button ok.n o wt h e
imported pnml model is linked to the process instances with throughput
times superior to 1.1 hours.
6. run the analysis plug-in performance analysis with petri net to discover
thecritical sub-paths for these cases. take your time to analyze the results.
for instance, can you see that now 43% of the defects could not be ﬁxed on
the ﬁrst attempt?
finally, we suggest you spend some time reading the help documentation of this
plug-in because it provides additional information to what we have explained in
this section. note that the results of this plug-in can also be exported.
2.5.3 decision point analysis
to discover the business rules (i.e. the conditions) that inﬂuence the points of
choice in a model, you can use the decision point analysis plug-in. for instance,
in the context of our running example, we could investigate which defect types
(cf. section 2.1) are ﬁxed by which team. the procedure to do so has the following
steps:
1. open the ﬁltered log (cf. section 2.3.2) for the running example.
2. open the exported pnml model that you created while executing the pro-
cedure on page 50.
3. run the decision point analysis plug-in by selecting the menu option analy-
sis→selected petri net →decision point analysis .
4. double-click the option “choice 4 p2”. this will select the point of choice
between execution the task “repair (complex)” or “repair (simple)”11.
5. select the tab attributes and set the options: (i) attribute selection scope =
“just before”, (ii) change the attribute type of the ﬁeld defecttype to “nu-
meric”. afterwards, click on the button update results . this analysis plug-in
will now invoke a data mining algorithm (called j48) that will discover which
ﬁelds in the log determine the choice between the diﬀerent branches in the
model.
6. select the tab result to visualize the mined rules (cf. figure 18). note that
cases with a defect types12from 1 to 4 are routed to the task “repair (sim-
ple)” and the ones with defect type bigger than 4 are routed to the task
“repair (complex)”. this is the rule that covers the majority of the cases
11note: if your option “choice 4 p2” does not correspond to the point of choice we
refer to in the model, please identify the correct option on your list. the important
thing in this step is to select the correct point of choice in the model.
12from the problem description (cf. section 2.1), we know that there are 10 types of
defects.66 a.k.a. de medeiros and w.m.p. van der aalst
fig. 18. scheenshot of the analysis plug-in decision point analysis :result tabprocess mining towards semantics 67
fig. 19. scheenshot of the analysis plug-in decision point analysis :decision tree/rules tab68 a.k.a. de medeiros and w.m.p. van der aalst
in the log. however, it does not mean that all the cases follow this rule. to
check for this, you have to perform the next step.
7. select the tab decision tree/rules (cf. figure 19). remark the text (695.0/
87.0) inside the box for the task “repair (complex)”. this means that ac-
cording to the discovered business rules, 695 cases should have been routed
to the task “repair (complex)” because their defect type was bi gger than 4.
however, 87 of these cases are misclassiﬁed because they were routed to the
task “repair (simple)” . thus, the automatically discovered business rules
describe the conditions that apply to the majority of the cases, but it does
not mean that all the cases will ﬁt these rules. therefore, we recommend you
to always check for the results in the tab decision tree/rules as well. in our
case, these result makes sense because, from the description of the running
example (cf. section 2.1), we know that some defect types can be ﬁxed by
both teams.
to get more insight about the decision point analysis , we suggest you spend
some time reading its help documentation because it provides additional infor-
mation that was not covered in this section. as for the many prom plug-ins, the
mined results (the discovered rules) can also be exported.
the explanations in this section show that the current process mining tech-
niques can be used to answer the set of typical common questions (cf. table 2)
for the analysis of business processes. however, the level of abstraction and re-
use provided by these techniques for this analysis is quite limited because all
the queries are based on a label-level. f or instance, it is not possible to deﬁne
a generic property that checks for the “fou r-eyes principle”. therefore, the next
section motivates which beneﬁts the use of semantics could bring to current
process mining techniques.
3 semantic process mining
semantic process mining aims at bringing the current process mining techniques
from the level of label-based analysis to the level of concept-based analysis. recall
that the starting point of any mining algorithm is a log and that some techniques
also use a model as input (cf. section 2, figure 2). thus, the core idea in semantic
process mining is to explicitly relate (or annotate) elements in a log with the
concepts that they represent. this can b e achieved by linking these elements to
concepts in ontologies .
as explained in [25], “an ontology is a formal explicit speciﬁcation of a shared
conceptualization”. therefore, ontologies deﬁne (i) a set of concepts used by (a
group of) people to refer to things in the world and (ii) the relationships among
these concepts. furthermo re, because the concepts and relationships are formally
deﬁned, it is possible to automatically reason or infer other relationships among
these concepts. in fact, ontologies are currently been used for modelling and for
performing certain types of analysis in business processes [14,23,24,37]. thus, it
is realistic to base semantic process mining on ontologies.process mining towards semantics 69
to illustrate how ontologies can enhance the analysis provided by process
mining tools, let us return to our running example (cf. section 2.1). for this
example, one could use the ontologies in figure 20 to express the concepts in
the business process for repairing the telephones. this ﬁgure shows three on-
tologies: taskontology ,roleontology andperformerontology . in this ﬁgure, the
concepts are modelled by ellipses and the instances of the concepts by rectangles.
additionally, the arrows deﬁne the relationships between the concepts and the
instances. the arrows go from a concept to a superconcept, or from an instance
to a concept. by looking at these ontologies, it is possible to infer subsump-
tion relations among the concepts and instances. for instance, it is possible to
identify that the elements “repair (complex)” and“repair (simple)” are tasks
forrepairing purposes. the use of ontologies (and the automatic reasoning they
support) enables process mining techniqu es to analyze at diﬀerent abstraction
levels (i.e., at the level of instances a nd/or concepts) and, therefore, promotes
re-use. remark that current process mining techniques only provide for analysis
based on the “instance” level. as an illustration, consider the last procedure on
page 53. in this procedure we had to ﬁlter the log so that only the tasks “repair
(simple)” and “repair (complex)” would be kept in the log. now, assume that
the elements in the log would link to the ontologies as illustrated in figure 20.
in this setting, the ﬁltering performed on page 53 (cf. step 2) could be simpliﬁed
from keep all the tasks “repair (simple)” and “repair (complex)” tokeep all
the tasks linking to the concept “taskontology:repair”13. note that the ﬁlter-
ing now speciﬁes a set of concepts to maintain in the log. therefore, it is up to
the process mining log ﬁlter to analyze the concepts in the log and automat-
ically infer that the tasks “repair (simple)” and “repair (complex)” should
be kept in the log because these two tasks link to subconcepts of the concept
“taskontology:repair”.
the remainder of this section provides an outlook on the possibilities for se-
mantic process mining. our aim is not to d escribe concrete semantic process min-
ing algorithms, but rather motivate the opportunities and identify the elements
that are needed to proceed towards these semantic algorithms. this is done in
subsection 3.1. additionally, as a ﬁrst step for realizing semantic process mining,
subsection 3.2 explains a concrete semantically annotated extension of the input
format for logs mined by the prom tool. the elements in this semantically anno-
tated format support the kind of analysis discussed in the usage scenarios.
3.1 usage scenarios
the usage scenarios presented in this section illustrate the beneﬁts of bringing
the analysis discussed in section 2 to the semantic level. while describing these
scenarios, we assume that (i) the elem ents in event logs and models given as
input link to concepts in ontologies, and (ii) the mining algorithms are able to
load and infer subsumption relationships about the concepts that are referenced
13in this section, we use the notation “ <ontology name >:<ontology concept >”w h e n
referring to concepts of the ontologies illustrated in figure 20.70 a.k.a. de medeiros and w.m.p. van der aalst
taskontology
roleontologytask
register analyze fix notify archive
test repair
complex simpleregisteranalyze
defect
test
repair
repair
(simple)repair
(complex)inform
userarchive
repair
role
complex simpletester fixer
solvers1tester1
solvers2 solvers3 solverc1 solverc2solverc3tester2
tester4
tester5tester3
tester6
performerontology performer
person system
solvers1tester1
solvers2solvers3
solverc1solverc2
solverc3tester2
tester4
tester5tester3
tester6system
fig. 20. example of ontologies that could be deﬁned for the running example in sec-
tion 2.1. concepts are represented by ellipses and instances by rectangles. the root
ellipses specify the most generic concepts and the leaf ellipses the most speciﬁc sub-
concepts.process mining towards semantics 71
(process )
modelevent
logdiscovery
extensionconformance
ontologieslinks to links toreasonerloads,
reasons
fig. 21. sources of information for semantic process mining. the additional elements
(with dashed lines) are necessary to support the mining at the conceptual level(cf.
figure 2).
in these logs/models. both the loading of the ontologies and the respective sub-
sumption inferences are provided by ontology reasoners [3,4,5,26,36]. figure 21
describes the sources of information for semantic process mining. in the following
we elaborate on the usage scenarios. all the scenarios are based on the ontologies
in figure 20 and the running example in subsection 2.1.
scenarios for log inspection. log inspection provides an overview of the
elements in the log (cf. subsection 2.3.1). thus, when a log has links to con-
cepts in ontologies, the log inspection t echniques could also give an overview
of diﬀerent semantic perspectives. as a minimal requirement, these tech-
niques should support the visualizat ion of the concepts that are referenced
in the log, their instances (i.e. the actual elements), and all the supercon-
cepts of these concepts. this is necessary because the user needs to know
which concepts he/she can use while performing the mining. other possi-
bilities would be to automatically ﬁnd out (i) completed cases (or process
instances) in the log (e.g., the ontologies could contain axioms that would
deﬁne when instances of a given proces s were completed), (ii) which roles
are in the log and which tasks were execu ted by originators with these roles,
(iii) which concepts link to which labels, (iv) which tasks are executed by
people and which by systems, and so on.
scenarios for cleaning the log. log cleaning allows for projecting and/or
adding data to a log (cf. subsection 2.3.2). thus, a semantically annotated
log could also be cleaned based on the concepts its elements link to. for
instance, keep in the log only the tasks that are instances (i.e., link to)
concepts x, w and z or any of their subconcepts. note that log ﬁlters deﬁned
over concepts are more gen eric because they can be reused over multiple logs
involving the same concepts. at the current situation, the reuse of log ﬁlters72 a.k.a. de medeiros and w.m.p. van der aalst
only makes sense when the logs have elements with identical labels. thus,
the use of concepts would boost the (re-)use of log ﬁlters.
scenarios for discovery plug-ins. the analysis performed by discovery
plug-ins is based on the log only (cf. subsection 2.4). the control-ﬂow
mining plug-ins (cf. subsection 2.4.1) discover a process model by inferring
ordering relations between tasks in a log. when these tasks link to concepts
in ontologies, these algorithms can mi ne process models at diﬀerent levels
of abstraction by inferring ordering relations between these concepts. the
higher the considered concepts are in the subsumption trees derived from
ontologies, the higher the level of ab straction of the mined models. for
instance, for the ontology “taskontology”, if a control-ﬂow mining algorithm
would use only the concepts at level 1 of its tree (i.e., use only the concepts
“ r e g i s t e r ” ,“ a n a l y z e ” ,“ f i x ” ,“ n o t i fy” and “archive”), a process model
like the one in figure 22 could be discovered. in a similar way, if this same
algorithm would use the instances of the concepts in this ontology, the mined
model could be just like the one in figure 9. note that the mined model
in figure 22 is more compact (i.e., has a higher abstraction level) than the
one in figure 9. in as similar way, the case-related information plug-ins (cf.
subsection 2.4.2) could show most frequent paths with respect to concepts
in the log. for the plug-ins that mine organizational-related information (cf.
subsection 2.4.3), the ontological concepts linked to the originator and tasks
in the log would allow for a more precise analysis of the organizational model
expressed in the log. for instance, co nsider the automatically discovered
organizational model for the running example (cf. figure 11). in this case,
the tasks “analyze defect. ..” and “test repair...” are grouped together
because they are executed by the same originators. however, if the link
to ontologies would be present in the log, the groups in figure 23 could
be automatically inferred. note that the use of ontological concepts would
m ak e i t possi b l e t o ( i ) d i st i n gu i sh be t w e e n t h e t asks “a n al yz e d e f e c t ...”
and “test repair...” and (ii) identify that all originator “tester...” have
two roles: “roleontology:classiﬁer” and “roleontology:tester”. finally,
mining plug-ins for veriﬁcation of properties in the log (cf. subsection 2.4.4)
could also beneﬁt from a reasoning at t he concept level. for instance, in the
procedure on page 57, we have checked if all “repaired” devices would always
be tested before archiving . to do so, we had to explicitly inform the default
ltl checker plugin the labels for the two repair tasks “repair (simple)”
and “repair (complex)”. if there were concepts in the log, this veriﬁcation
could be simpliﬁed to the formula “eventually
 concept
 a
then
b
then
c”,
where a = “taskontology:repair”, b = “taskontology:test” and c =
“taskontology:archive”. by using ontology reasoners, the plug-in would
automatically ﬁnd out the appropriate task labels to verify. furthermore,
like it happens for the log ﬁlters, ltl formulae deﬁned over concepts can be
more easily reused than the ones deﬁned over labels.
scenarios for conformanc e and extension plug-ins. these plug-in en-
hance existing models by adding to them extra information discovered fromprocess mining towards semantics 73
archive analyze registerfix
notify
fig. 22. example of a mined model for the running example when only the concepts at
the level 1 of the tree for the ontology “taskontology” (cf. figure 20) are considered.
note that this mined model is more compact than the one in figure 9.
logs. because they need a log and a model while executing their analysis,
these plug-ins would require the provided (process) models to also contain
links to concepts in ontologies. this w ay they can ﬁnd out relations between
elements in models and logs the surpass the string matching level. there-
fore: (i) the conformance checking plug-ins (cf. subsection 2.5.1) would be
performed at the conceptual level (with subsumption inferences taken into
account), as well as the automatically suggested mapping between tasks
in the log and in the model (cf. step 3, on page 59); (ii) the performance
analysis plug-ins (cf. subsection 2.5.2) would be able to answer questions
like “what are the routing probabilities of split/joint points of a certain
concept ?” or “given tasks of a certain con cept, which subconcepts outper-
form/underperform others in terms of service times?”; and (iii) decision point
analysis plug-ins (cf. subsection 2.5.3) would be able to automatically infer
i fad a t av a l u ei s nominal ornumeric .
the starting point to realize the semantic process mining techniques illustrated
in this subsection is to deﬁne a semantic annotated format for event logs .t h i s
format is the subject of the next subsection.
3.2 semantic annotated mining xml format
the semantic annotated mining extensible markup language (sa-mxml) for-
mat is a semantic annotated version of the mxml format used by the prom
framework. in short, the sa-mxml incorporates the model references (between
elements in logs and concepts in ontologi es) that are necessary to implement our
approach. however, before explaining the sa-mxml, let us ﬁrst introduce the
mxml format.
the mining xml format (mxml) started as an initiative to share a
common input format among diﬀerent mining tools [11]. this way, event
logs could be shared among diﬀerent mining tools. the schema for the
mxml format (depicted in figure 24) is available at is.tm.tue.nl/research/
processmining/workﬂow-log.xsd.74 a.k.a. de medeiros and w.m.p. van der aalst
group
classifiergroup
tester
analyze test
analyze defect
startanalyze defect
completetest repair
starttest repair
completeclassifier testertester1
tester2
tester3
tester4
tester5
tester6
fig. 23. example of an automatically inferred organizational model based on a seman-
tically annotated log for the running example. in this example we assume that in the log
(i) the tasks “analyze defect. ..” link to the concept “taskontology:analyze” and the
tasks “test repair... ” to “taskontology:repair”; and (ii) the originators “tester. ..”
l i nktotheco nce pt“ ro l e on to l o g y: t e s te r” w he ne xe cuti ngtheta s ks“ t e s t re pa i r...”
and to the concept “roleontology:classiﬁer” while performing the task “analyze de-
fect. ..”. note that this model more precisely identiﬁes the groups in the log than the
one in figure 11.
as can be seen in figure 24, an event log (element workﬂowlog )c o n t a i n st h e
execution of one or mor e processes (element process ), and optional information
about the source program that generated the log (element source ) and additional
data elements (element data). every process (element process ) has zero or more
cases or process instances (element processinstance) . similarly, every process
instance has zero or more tasks (element audittrailentry ). every task or audit
trail entry (ate) must at least have a name (element workﬂowmodelelement )
a n da ne v e n tt y p e( e l e m e n t eventtype ). the event type determines the state of
the tasks. there are 13 supported event types: schedule, assign, reassign, start,
resume, suspend, autoskip, manualskip, withdraw, complete, ate
 abort, pi
 abort
and unknown. the other task elements are optional. the timestamp element
supports the logging of time for the task. the originator element records the
person/system that performed the task. the dataelement allows for the logging
of additional information. figure 25 shows an excerpt of the running exampleprocess mining towards semantics 75
(a) process log xml formatreassign 
schedule assign 
start 
resume 
suspend 
autoskip complete manualskip 
ate_abort 
pi_abort withdraw 
(b) transactional model for eventtype
fig. 24. the visual description of the schema for the mining xml (mxml) format
fig. 25. excerpt of the running example log (cf. section 2.1) in the the mxml format76 a.k.a. de medeiros and w.m.p. van der aalst
fig. 26. excerpt of the running example log (cf. section 2.1) in the sa-mxml format. note that the optional ﬁeld modelref-
erence is used to link the elements of the mxml format to concepts in ontologies.process mining towards semantics 77
(cf. subsection 2.1) log in the mxml format. more details about the mxml
format can be found in [18,19].
the sa-mxml format is just like the mxml format plus the addition that
all elements (except for audittrailentry and timestamp) have an optional extra
attribute called modelreference . this attribute links to a list of concepts in ontolo-
gies and, therefore, support the necessary model references for our approach. the
concepts are expressed as uris and the elements in the list are separated by blank
spaces. actually, the use of modelreference in the sa-mxml format is based on
the work for the semantic annotations provided by sawsdl (semantically an-
notated web service deﬁnition language) [7]. the schema for the sa-mxml
format is available at is.tm.tue.nl/research/processmining/samxml.xsd. fig-
ure 3.2 shows an example of a log with semantic annotations for the log of our
running example with respect to the ontologies in figure 20. note that the ﬁelds
“processinstance”, “data”, “workﬂowmodelelement” and “originator” link to
concepts in the ontologies. furthermo re, note that the sa-mxml format is back-
wards compatible with mxml format. this way the process mining techniques
that do not support a semantic treatment can also be directly applied to logs in
sa-mxml.
4 conclusions
in this chapter we have shown (i) how to use the open source process mining tool
prom to get useful feedback about processes, (ii) how the inclusion of semantic
information in event logs can empower process mining techniques, and (iii) we
have deﬁned a concrete semantic log format, the sa-mxml format.
since our focus when illustrating the power of current process mining tech-
niques was on answering a set of common questions that managers usually have
about processes, we have not covered many of the other plug-ins that are in
prom. we hope that the subset we have shown in this chapter will help you in
ﬁnding your way in prom. however, if you are interested, you could have a fur-
ther look in plug-ins to verify (process) models and detect potential problems (by
using the analysis plug-ins check correctness of epc ,woﬂan analysis orpetri
net analysis ),quantify (from 0% until 100%) how much behavior two process
models have in common with respect to a given even log (by using the analysis
plug-in behavioral precision/recall ),create simulation models with the diﬀerent
mined perspectives (by using the export plug-in cpn tools ) etc. the prom tool
can be downloaded at www.processmining.org.
embedding semantic information in event logs brings the process mining tech-
niques from the level of label-based analysis to the concept-based one. this al-
lows for working with diﬀerent levels of abstractions while getting feedback about
processes and properties in a log. furthermore, it also supports easier reuse of
queries deﬁned over logs.
the sa-mxml format is the ﬁrst step t owards creating se mantic process
mining algorithms. this format extends the current mxml format by specifying
that any element present in the mxml format may link to a set of concepts in78 a.k.a. de medeiros and w.m.p. van der aalst
ontologies. following steps will focus o n developing semantic process mining
algorithms to implement the usage scenarios described in this chapter.
acknowledgements
the authors would like to thank ton weijters, boudewijn van dongen, anne
rozinat, christian g¨ unther, minseok song, ronny mans, huub de beer, laura
maruster and peter van den brand for th eir on-going work on process mining
techniques and tools at eindhoven university of technology. additionally, the
authors would like to thank monique jansen-vullers, mariska netjes, irene van-
derfeesten and hajo reijers for their input while selecting the common questions
used in section 2. the work presented in this chapter was partially funded by
the european commission under t he project super (fp6-026850).
references
1. cosa business process management, http://www.cosa-bpm.com/
2. extensible markup language (xml), http://www.w3.org/xml/
3. kaon2, http://kaon2.semanticweb.org/
4. pellet, http://pellet.owldl.com/
5. racer, http://www.racer-systems.com/
6. sap, http://www.sap.com/
7. semantic annotations for web service description language (sa-wsdl),
http://www.w3.org/tr/2006/wd-sawsdl-20060630/
8. staﬀware process suite, http://www.staffware.com/
9. van der aalst, w.m.p., de beer, h.t., van dongen, b.f.: process mining and
veriﬁcation of properties: an approach based on temporal logic. in: meersman,
r., tari, z. (eds.) otm 2005. lncs, vol. 3760, pp. 130–147. springer, heidelberg
(2005)
10. van der aalst, w.m.p., van hee, k.m.: workﬂow management: models, methods,
and systems. mit press, cambridge (2002)
11. van der aalst, w.m.p., van dongen, b.f., herbst, j., maruster, l., schimm, g.,
weijters, a.j.m.m.: workﬂow mining: a survey of issues and approaches. data
and knowledge engineering 47(2), 237–267 (2003)
12. van der aalst, w.m.p., weijters, a.j.m.m., maruster, l.: workﬂow mining: dis-
covering process models from event logs. ieee transactions on knowledge and
data engineering 16(9), 1128–1142 (2004)
13. bussler, c., haller, a. (eds.): bpm 2005. lncs, vol. 3812. springer, heidelberg
(2006)
14. casati, f., shan, m.c.: semantic analysis of business process executions. in:
jensen, c.s., jeﬀery, k., pokorn´ y, j., ˇsaltenis, s., bertino, e., b¨ ohm, k., jarke,
m. (eds.) edbt 2002. lncs, vol. 2287, pp. 287–296. springer, heidelberg (2002)
15. workﬂow management coalition. wfmc home page, http://www.wfmc.org
16. cook, j.e., du, z., liu, c., wolf, a.l.: discovering models of behavior for con-
current workﬂows. computer s in industr y 53(3), 297–319 (2004)
17. desel, j., esparza, j.: free choice petri nets. cambridge tracts in theoretical
computer science, vol. 40. cambridge university press, cambridge (1995)process mining towards semantics 79
18. van dongen, b.f., van der aalst, w.m.p.: a meta model for process mining data.
in: proceedings of the caise 2005 workshops, vol. 2. feup (2005)
19. van dongen, b.f., alves de medeiros, a.k., verbeek, h.m.w., weijters, a.j.m.m.,
van der aalst, w.m.p.: the prom framework: a new era in process mining tool
support. in: ciardo, g., darondeau, p . (eds.) icatpn 2005. lncs, vol. 3536, pp.
444–454. springer, heidelberg (2005)
20. van dongen, b.f., van der aalst, w.m.p.: multi-phase process mining: aggre-
gating instance graphs into epcs and petri nets. in: proceedings of the second
international workshop on applications of petri nets to coordination, workﬂow
and business process management (pncwb) (2005)
21. dumas, m., van der aalst, w.m.p., ter hofstede, a.h. (eds.): process-aware in-
formation systems: bridging people and software through process technology.
john wiley & sons inc., chichester (2005)
22. greco, g., guzzo, a., pontieri, l.: mining hierarchies of models: from abstract
views to concrete speciﬁcations. in: van der aalst, w.m.p., benatallah, b., casati,
f., curbera, f. (eds.) bpm 2005. lncs, vol. 3649, pp. 32–47. springer, heidelberg
(2005)
23. green, p., rosemann, m.: business systems analysis with ontologies. idea group
publishing (2005)
24. grigori, d., casati, f., castellanos, m., dayal, u., sayal, m., shan, m.c.: mining
exact models of concurrent workﬂows 53(3), 321–343 (2004)
25. gruninger, m., lee, j.: special issue on ontology applications and design - intro-
duction. communications of acm 45(2), 39–41 (2002)
26. haarslev, v., m¨ oller, r.: racer: a core inference engine for the semantic web. in:
sure, y., corcho, ´o. (eds.) eon. ceur workshop proceedings, vol. 87 (2003)
ceur-ws.org
27. hepp, m., leymann, f., domingue, j., wahler, a., fensel, d.: semantic business
process management: a vision towards using semantic web services for business
process management. in: ieee international conference on e-business engineer-
ing (icebe 2005), pp. 535–540 (2005)
28. herbst, j., karagiannis, d.: workﬂow mining with inwolve. computers in in-
dustry 53(3), 245–264 (2004)
29. thao ly, l., rinderle, s., dadam, p., reichert, m.: mining staﬀ assignment rules
from event-based data. in: bussler and haller [13], pp. 177–190
30. murata, t.: petri nets: properties, analysis and applications. proceedings of the
ieee 77(4), 541–580 (1989)
31. pinter, s.s., golani, m.: discovering workﬂow models from activities lifespans.
computers in industr y 53(3), 283–296 (2004)
32. reisig, w., rozenberg, g. (eds.): apn 1998. lncs, vol. 1491. springer, heidelberg
(1998)
33. rozinat, a.: decision mining in prom. in: dustdar, s., fiadeiro, j.l., sheth, a.p.
(eds.) bpm 2006. lncs, vol. 4102, pp. 420–425. springer, heidelberg (2006)
34. rozinat, a., van der aalst, w.m.p.: conformance testing: measuring the fit and
appropriateness of event logs and process models. in: bussler and haller [13],
pp. 163–176
35. schimm, g.: mining exact models of concurrent workﬂows. computers in indus-
try 53(3), 265–281 (2004)
36. sirin, e., parsia, b., grau, b.c., kalyanpur, a., katz, y.: pellet: a practical owl-
dl reasoner. web semantics: science, services and agents on the world wide web
5(2), 51–53 (2007)80 a.k.a. de medeiros and w.m.p. van der aalst
37. thomas, m., redmond, r., yoon, v., singh, r.: a semantic approach to monitor
business process. communications of acm 48(12), 55–59 (2005)
38. van der aalst, w.m.p., reijers, h.a., song, m.: discovering social networks from
event logs. computer supp orted cooperative work 14(6), 549–593 (2005)
39. verbeek, h.m.w., van dongen, b.f., mendling, j., van der aalst, w.m.p.: inter-
operability in the prom framework. in: latour, t., petit, m. (eds.) proceedings
of the caise 2006 workshops and doctoral consortium, pp. 619–630. presses
universitaires de namur (june 2006)
40. wen, l., wang, j., sun, j.: detecting implicit dependencies between tasks from
event logs. in: zhou, x., li, j., shen, h.t., kitsuregawa, m., zhang, y. (eds.)
apweb 2006. lncs, vol. 3841, pp. 297–306. springer, heidelberg (2006)