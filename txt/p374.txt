process mining of test processes: a case study
a. rozinat1, i.s.m. de jong2, c.w. g¨ unther1, and w.m.p. van der aalst1
1department of information systems, eindhoven university of technology
p.o. box 513, nl-5600 mb, eindhoven, the netherlands
{a.rozinat,c.w.gunther,w.m.p.v.d.aalst }@tue.nl
2asml, p.o. box 324, nl-5500 ah, veldhoven, the netherlands
ivo.de.jong@asml.com
abstract. process mining techniques attempt to extract non-trivial and
useful information from event logs. for example, there are many process
mining techniques to automatically discover a process model describing
the causal dependencies between activities. moreover, using conformance
checking it is possible to investigate and quantify deviations between the
real process and the modeled process. several successful case studies
have been reported in literature, all demonstrating the applicability of
process mining. however, these case studies refer to rather structured
administrative processes. in this paper, we investigate the applicability
of process mining to less structured processes. we report on a case study
where the prom framework has been applied to the test processes of
asml (the leading manufacturer of wafer scanners in the world). this
case study provides many interesting insights. on the one hand, process
mining is also applicable to the less structured processes of asml. on
the other hand, the case study also shows the need for alternative mining
approaches able to better visualize processes and provide more insights.
1 introduction
asml is the world’s leading manufacturer of chip-making equipment and a
key supplier to the chip industry. asml designs, develops, integrates and ser-
vices advanced systems to produce semiconductors. in short, it makes the wafer
scanners that print the chips. these wafer scanners are used to manufacture
semi-conductors (e.g., processors in devices ranging from mobile phones ad mp3
players to desktop computers). wafer scanners are complex machines consisting
of many building blocks and use a photographic process to image nanometric
circuit patterns onto a silicon wafer, much like a camera prints an image on ﬁlm.
because of competition and fast innovation, the time-to-market is very impor-
tant. there is an ongoing eﬀort to reduce the line widths on silicon wafer to
enhance the performance of the manufactured semi-conductors. every new gen-
eration of wafer scanners is balancing on the border of what is technologically
possible. as a result, the testing of manufactured wafer scanners is an important
but also time-consuming process. every wafer scanner is tested in the factory of
asml. when it passes all tests, the wafer scanner is disassembled and shipped
to the customer where the system is re-assembled. at the customer’s site, the2
wafer scanner is tested again. clearly, testing is a time-consuming process and
takes several weeks at both sites. since time-to-market is very important, asml
is involved in an ongoing eﬀort to reduce the test period. to assist asml in these
eﬀorts, we applied process mining techniques to their test processes.
the basic idea of process mining is to discover, monitor and improve real
processes (i.e., not assumed processes) by extracting knowledge from event logs.
today many of the activities occurring in processes are either supported or
monitored by information systems. consider for example erp, wfm, crm,
scm, and pdm systems to support a wide variety of business processes while
recording well-structured and detailed event logs. however, process mining is not
limited to information systems and can also be used to monitor other operational
processes or systems. for example, we have applied process mining to complex
x-ray machines, high-end copiers, web services, careﬂows in hospitals, etc. all of
these applications have in common that there is a notion of a process and that
the occurrences of activities are recorded in so-called event logs . assuming that
we are able to log events, a wide range of process mining techniques comes into
reach. the basic idea of process mining is to learn from observed executions of
a process and can be used to (1) discover new models (e.g., constructing a petri
net that is able to reproduce the observed behavior), (2) check the conformance
of a model by checking whether the modeled behavior matches the observed
behavior, and (3) extend an existing model by projecting information extracted
from the logs onto some initial model (e.g., show bottlenecks in a process model
by analyzing the event log). all three types of analysis have in common that
they assume the existence of some event log .
wafer scanner
(process)
modelevent
logsmodels
analyzes
discoveryrecords 
eventstest process
extensionconformancetest team
fig. 1. based on the event logs of the wafer scanners, three classes of process mining
techniques are used: (1) “discovery”, (2) “conformance”, and (3) “extension”
at any point in time, asml’s wafer scanners record events that can easily
be distributed over the internet. hence, any event that takes place during the
test process can be recorded easily. the availability of these event logs and the3
desire of asml to improve the testing process, triggered the case study reported
in this paper. in collaboration between eindhoven university of technology and
asml, we analyzed the testing process using the three classes of process mining
techniques mentioned before: (1) “discovery”, (2) “conformance”, and (3) “ex-
tension” (cf. figure 1). using process discovery, we tried to answer the question
“how are the tests actually executed?”, i.e., based on the event logs we auto-
matically constructed process models showing the ordering and frequency of test
activities. this revealed that the real process is much more complicated than the
idealized reference model that asml is using to instruct the test teams. the
reference model shows a rather structured process while in reality the testing
process requires much more ﬂexibility. using conformance checking techniques,
we investigated this further by answering the question “how compliant are the
actual test executions to the reference process?”. through conformance check-
ing we were able to quantify and pinpoint the deviations of the real test process
from the idealized reference model. finally, we used process mining to answer
the question “where is the most time spent in the test process?”. for this we
extended the discovered models and the reference model with additional infor-
mation extracted from the logs. this way, it was possible to project performance
bottlenecks directly on the process model.
in this paper, we show that recently developed process mining techniques
can be used to answer the three questions stated above. this is interesting since,
so far, process mining has only been applied to rather structured processes (e.g.,
administrative processes supported through some information system [3]). for
the case study we used our prom framework . prom is open source and uses a
plug-able architecture, e.g., developers can add new process mining techniques
by adding plug-ins without spending any eﬀorts on the loading and ﬁltering of
event logs and the visualization of the resulting models. version 4.0 of prom
provides 142 plug-ins. for example, there are more than 15 plug-ins to discover
process models from event logs. as we will see, it is not easy to apply traditional
process mining techniques to less structured processes such as the testing of
wafer scanners. therefore, this paper also discusses the limitations of existing
techniques and suggests some alternative approaches. in fact, two new plug-ins
have been developed based on this case study.
the remainder of this paper is organized as follows. section 2 reviews related
work both in process mining and the test process optimization domains. next,
the context of the case study is described in more detail in section 3. section 4
presents the results of this study, and concrete improvement actions for the
asml test process are proposed in section 5. future challenges for process
mining and the two new prom plug-ins are discussed in section 6. section 7
concludes the paper.
2 related work
in this section, we ﬁrst review related work on process mining and then review
related work on test processes.4
since the mid-nineties several groups have been working on techniques for
process mining [5, 6, 12, 15, 19, 17, 31], i.e., discovering process models based on
observed events. in [4] an overview is given of the early work in this domain. the
idea to apply process mining in the context of workﬂow processes was introduced
in [6]. in parallel datta [15] looked at the discovery of business process models.
cook et al. investigated similar issues in the context of software engineering pro-
cesses [12]. herbst [24] was one of the ﬁrst to tackle more complicated processes,
e.g., processes containing duplicate tasks.
most of the classical approaches have problems dealing with concurrency.
the α-algorithm [5] is an example of a simple technique that takes concurrency
as a starting point. however, this simple algorithm has problems dealing with
complicated routing constructs and noise (like most of the other approaches
described in literature). in [19, 17] a more robust but less precise approach is
presented. heuristics [31] or genetic algorithms [2, 16] have been proposed to
deal with issues such as noise. in the case study we used prom’s heuristic
miner [31] to discover suitable process models.
for conformance checking we used the conformance checker [28] and the
ltl checker [1]. conformance checking as used in this paper is closely related
to the work of cook et al. [13, 11] who have introduced the concept of process
validation. they propose a technique comparing the event stream coming from
the process model with the event stream from the execution log based on two
diﬀerent string distance metrics.
process mining can be seen in the broader context of business process in-
telligence (bpi), business activity monitoring (bam), and business process
visualization [30]. in [20, 29] a bpi toolset on top of hp’s process manager is
described. the bpi toolset includes a so-called “bpi process mining engine”. in
[27] zur muehlen describes the pisa tool which can be used to extract perfor-
mance metrics from workﬂow logs. similar diagnostics are provided by the aris
process performance manager (ppm) [25]. it should be noted that bpi tools
typically do not allow for process discovery and conformance checking, and oﬀer
relatively simple performance analysis tools that depend on a correct a-priori
process model.
after providing an overview of process mining literature, we discuss related
work on the improvement of test processes. most test process optimization tech-
niques are currently focused on optimizing the test organization instead of the
processes itself. if the test process is optimized by technology-based optimiza-
tion techniques, then these techniques are either only applicable in a single disci-
pline or still very general. an example of detailed mono-disciplinary optimization
techniques is the use of test coverage measures [26] for software testing. these
detailed measures are used to determine the coverage of a set of test cases on the
system under test. these methods are applied for small software systems where
high quality levels are important. these methods do not scale very well for large
software systems.
more general test process optimization techniques apply risk-based test se-
quencing [7, 22] in combination with a certain stop criterion such that the test5
cases which cover the highest risk (high level test cases) are executed ﬁrst. the
disadvantage of this approach is that these high level test cases are often incon-
clusive about the root cause of a failure. a lengthy diagnosis action is required
when these test cases fail. a sequencing algorithm which optimizes a test phase
including diagnosis and ﬁx actions is described by [10, 9]. the system test model
which is used in [10, 9] models a test problem independent of the discipline.
the case study reported in this paper investigates the diﬀerences between the
actual, executed test sequences and the planned test sequences. the next section,
introduces the case study. then, we start answering the three questions stated
in the introduction. first, we discover the actual process and then we compare
the reference model with the real test sequences. the diﬀerences between the
actual and the planned test sequences are then used to suggest improvements of
the test process (or asml’s organization).
3 case study
this section introduces the case study where process mining was applied to
the test process of asml’s wafer scanners. after providing some background
information (section 3.1), we describe the test process of a wafer scanner in more
detail (section 3.2), and then look at the log data recorded during these tests
(section 3.3). the event logs serve as input for our process mining techniques
and the results of their analysis are described in section 4.
3.1 asml’s wafer scanners
nowadays, semi-conductors can be found in many appliances around us. mobile
phones, mp3-players, television sets and desktop computers contain processors
and computer memory. these semi-conductors are manufactured in twenty-plus
steps, called the semi-conductor manufacturing process. images of a transistor
pattern are placed on a silicon wafer with typical line widths of 90 nm and less.
this imaging process is repeated up to 30 or more times to form a completely
functional integrated circuit. the imaging of the pattern on a silicon wafer is
done by a so-called wafer scanner. the semi-conductor manufacturing process is
explained in detail in [14]. the case study was conducted with asml. asml is
the leading manufacturer of wafer scanners in the world.
a wafer scanner consists of around one thousand building blocks. these build-
ing blocks can be considered a system in itself; they can consist of an entire elec-
tronics rack, thousands of lines of code, or a complete lens system. most of these
building blocks are manufactured at suppliers. together, these building blocks
form a scanner. after assembly of the building blocks, the wafer scanners are
calibrated and tested. the calibration and test sequence of a wafer scanner ends
with a system qualiﬁcation phase. in this system qualiﬁcation phase, the system
performance in terms of throughput, overlay and imaging performance is mea-
sured. the throughput of a wafer scanner is a measure of the wafer production
speed. the overlay of a wafer scanner is a measure of how accurate the diﬀerent6
patterns are placed on top of each other in each next layer. the imaging perfor-
mance of a wafer scanner determines the line width of the structures in the in-
tegrated circuit. the capabilities of a twinscantmxt:1900gi wafer scanner
in terms of throughput, overlay and imaging performance are resp. ≥131 wph,
≤6 nm and ≤40 nm [8]. moore’s law3is forcing companies like asml to con-
stantly innovate and build machines that can handle silicon wafers with smaller
line widths. this requires a continuous balance between performance (smaller
line widths) and reliability (semi-conductors that can be produced without fail-
ures). therefore, asml is working on the boundaries of what is technologically
possible and, hence, it is important to have good test processes in place.
3.2 the test process
the whole test process consists of three phases: (1) the calibration phase, (2) the
test phase (the actual testing), and (3) the ﬁnal qualiﬁcation phase. the whole
process typically takes a few weeks. when ﬁnished, the wafer scanner is partly
taken apart and shipped to a customer. a part of the calibration and test phase
is repeated at the customer site, after re-assembling the wafer scanner.
why is this test process so important for asml? asml operates in a market
where the time-to-market of system enhancements and the time-to-market of
new system types is critical. wafer scanners are continuously enhanced. as a
result, the number of manufactured wafer scanners of a single type is typically
less than 50. and with each new type, parts of the calibration and test phase are
adjusted. on average ﬁve diﬀerent system types are manufactured in parallel.
the short time-to-market, the constant innovation, and the high value of wafer
scanners make testing very important. spending too much time on testing will
result in high inventory costs and lost sales. however, inadequate tests will result
in systems which are malfunctioning.
sets of calibration and test actions are grouped into so called job steps . these
job steps are executed according to a certain sequence. only large changes in the
system design result in changes in the job step sequence, so the job step sequence
can be considered a ﬁxed sequence across diﬀerent systems. some of these job-
steps can be executed independently of each other. an example sequence of
three job steps is depicted in figure 2. note that in asml such structures are
referred to as “sequences”. however, strictly speaking these are process models
rather than sequences. the synchronization point sync forces that both job step
a and job step b must be ﬁnished before job step c can start. each calibration
action or test case can fail. some of the causes for test failure can require a
replacement of a faulty hardware component. the duration of this replacement
can take up to hours or longer. if such a failing test is in the example job step
a, then the independent job step b can be started to ensure maximal progress.
3moore (founder of intel), commenting on the growth of the microelectronics industry
in 1964, noted a doubling of the number of elements on a produced chip once every
12 months. for a decade that meant a growth factor of approximately 1000. today,
when moore’s law is quoted, the time constant typically quoted is 18 months.7
when the replacement hardware becomes available, either job step b is ﬁnished
ﬁrst and then job step a is ﬁnished, or the other way around. if job step b was
already ﬁnished, then the hardware is replaced and job step a is ﬁnished. job
step c is started when job step a and b are both ﬁnished. note that a failure
in a test case in job step c results in no activity on the system (idle time) until
the malfunctioning part of system is ﬁxed and testing can continue.
fig. 2. example sequence of three job steps with a synchronization point
some of the causes for a failure can be ﬁxed immediately. for example, some
parameters in the system can be out of speciﬁcation. this measurement infor-
mation can now be used to adopt the control set-points in the system. after
a second measurement, the parameter can be within speciﬁcation and the test
passes. most of the software which executes the tests is constructed such that
this fast-ﬁx loop is automated. testing, calibration and retesting is performed
in a single test.
finally, a change in low-level machine parameters, because of a hardware
replacement, can cause a re-execution of a previous job-step. for instance, the
proﬁle of some of the mirrors in a wafer scanner are measured and stored in x,y
and z directions. this proﬁle information is used in all positioning calibrations,
such that the error caused by the non-ﬂat mirrors are minimized. replacing
these mirrors results in a new proﬁle. for this reason, a large set of job steps
needs to be redone if a faulty mirror is replaced in one of the last job steps in
the sequence.
in summary, job steps are executed according to a ﬁxed sequence for a set
of machine types. the sequence allows variation of the detailed tests within the
limits of the synchronization points. next to that, the actual execution of tests
results in failing test cases. these failing test cases can result in a lengthy re-test
of parts of the sequence depending on the failure at hand.8
for asml, the goal is to minimize the waiting time for a hardware ﬁx (idle
time) and to reduce the re-execution of parts of the job-step sequence. this goal
could be easily met by testing all components and building blocks thoroughly
before and during system assembly. however, the increase in test eﬀort would
result in an increase of the total test duration and therefore an increase in time-
to-market. this is the main reason that testing everything thoroughly beforehand
is not considered a solution, i.e., the main goal is a reduction of the duration of
the test process and not cutting costs.
the work presented in this paper attempts to shorten the test process by
applying process mining techniques to the existing test processes, i.e., we an-
alyze the test process based on historical data to ﬁnd bottlenecks and ideas
for improvement. historical data helps to provide insight into where the real
bottlenecks are, and, therefore, enables speciﬁc actions to improve future test
executions. for example, failing tests which are often followed by idle time can
be detected and addressed. furthermore, the executed test sequences can be
compared to the given reference sequence , i.e., the “ideal process model” de-
ﬁned by asml. discrepancies can be related to the re-executed parts of the
sequence. for example, failing tests which often cause a re-execution of a part
of the sequence can be addressed and improved.
the next subsection describes the logging characteristics, i.e., the form his-
torical data is recorded during the test process. moreover, we show how this log
data can be converted to apply process mining techniques in the prom frame-
work [18].
3.3 log data and conversion
each wafer scanner in the asml factory produces a log of the software tests
which are executed. the manual assembly and calibration actions are not logged
and appear as idle time in this log. the wafer scanner is calibrated and tested
using calibration and performance software, indicated in the logging as a four-
letter code. the logging contains the start and stop moment of each test. the
idle time, i.e., the time between stop of the previous test and the start of the next
test, is not speciﬁed in detail. this idle time has a number of causes, ranging
from inexperienced operators reading the procedures, the end of the automated
test queue during the night to diagnosing a problem, or waiting for additional
parts. some parts of the test sequence are executed in an automated fashion.
the operator starts a test queue which contains a set of test cases which are
executed in a sequence. this test queue can also contain part of the recovery
and retry sequence for certain failing test cases. the recovery or retry tests are
executed depending on the outcome of a test in the queue.
an example fragment of the test log of one of the wafer scanners is depicted
in figure 3(a). each line corresponds to the execution of one test. the number
at the beginning of the line identiﬁes the machine (i.e., the wafer scanner) that is9
tested. afterwards the start time, the completion time, and the four-letter code
for the executed test are recorded.4
fig. 3. converting the log into the mxml format
to analyze the log data with the prom framework [18], we had to convert
them into the common mxml5format. this was performed by means of a
custom-built converter plug-in for the prom import framework [21], which fa-
cilitates log transformation tasks. in the mxml format, a log is composed of
process instances (i.e., cases) and within each instance there are audit trail en-
tries (i.e., events) with various attributes. these attributes refer to, for example,
data ﬁelds, timestamps, or transactional information (i.e., whether the activity
was scheduled, started, or completed). depending on the kind of information
that is in the log, we may be able to answer diﬀerent questions about the pro-
cess. figure 3(b) depicts the mxml log fragment for the highlighted test from
figure 3(a). one can see that the start and the completion of the test are cap-
tured by separate audit trail entries (including the corresponding timestamps),
and that the enclosing process instance (i.e., the case) corresponds to the tested
machine. because in the original log there were neither data attributes (such
as the outcome of a certain test) nor information about originators (such as
the people performing the tests) recorded, we will focus on the control-ﬂow and
performance dimension.
note that the logging takes place on the test-code level, and that there is no
reference to the job step in which’s context the test is performed. however, in
addition to the log data and the job step reference sequence (i.e., a high-level
process model describing test dependencies as illustrated in figure 2), asml also
provided us with an additional document specifying which test codes should be
executed in which job step. in this mapping, there are a number of tests that
4note that both the actual machine numbers and the four-letter test codes have been
anonymized for conﬁdentiality reasons.
5both the corresponding schema deﬁnition and the prom import framework [21],
which converts logs from a wide variety of systems to the xml format used by
prom, can be downloaded from www.processmining.org .10
appear in more than one job step (i.e., are executed in diﬀerent phases of the
test process).
4 process mining results
after converting the log data of the wafer scanners to mxml, we can start with
the process mining analysis of the test process execution logs. in the remainder of
this section we show to which degree we can already address the questions posed
in section 1 with existing process mining techniques in the prom framework
[18].
first, we look at the log inspection facilities that can be used to obtain
general information, e.g., about activity distributions, test process durations
etc., and apply a number of ﬁltering strategies to prepare the data for further
processing (section 4.1). next, process discovery algorithms are applied to mine
a model of the test process as it really takes place (section 4.2). then, we use
conformance checking to evaluate how good the test process “matches” both
the reference model and the mined models, and apply extension techniques for
further performance analysis (section 4.3). finally, the results are evaluated
from an asml perspective and concrete improvement actions are proposed in
section 5.
4.1 inspection and filtering
the prom framework contains a number of inspection tools that provide an
overview of the log data. for example, the log summary provides general infor-
mation, such as how many cases are captured by the log, how many log events
occurred, how many diﬀerent log events (and how often each of them) occurred,
which log events happened at the very beginning and which at the very end of
a case etc. figure 4 depicts a screenshot of the log summary for the whole log.
if we examine the log summary, it becomes clear that this test process has
very diﬀerent characteristics compared to typical business processes or adminis-
trative processes. in most domains, we typically see a large number of relatively
short log traces, i.e., many process instances (cases) each having just a few
events. for example, when looking at processes related to patient ﬂows, insur-
ance claims, traﬃc ﬁnes, etc., then there are typically thousands of cases each
containing less than 50 events. in this case study, there are just a few cases
(i.e., machines) but for each machine there may be thousands of log events. in
the initial data set we faced process instances that contained more than 50000
log events (each indicating either the start or the completion of a speciﬁc test).
as mentioned earlier, the test process of a wafer scanner lasts for several weeks
and is partly repeated after the machine has been re-assembled at the customer,
thus explaining the huge number of events per machine. from a larger set of
machines we selected 24 machines that fulﬁlled our criteria: (1) the test process
needed to be completed, (2) only include the test period on the asml (and not
the customer) site, (3) belong to the same family (recall that typically not more11
fig. 4. screenshot of the log summary in prom. it indicates that the log contains 24
process instances (i.e., tested machines), and that 360 diﬀerent tests were performed
on these machines. the test code that appears most frequently is ‘appp’, which is a
test that in fact facilitates the execution of another test after some time interval
than 50 wafer scanners of the same type are produced), and (4) not be a pilot
system (as a pilot system is used for development testing and not for manufac-
turing qualiﬁcation). these 24 cases comprise 154966 log events in total, and the
number of log events per process instance (i.e., the length of the test sequence)
ranges from 2820 until 16250. finally, we can see that there are 720 diﬀerent
audit trail entries in the log, which corresponds to 360 diﬀerent four-letter test
codes as each test is captured by both a ‘start’ and ‘complete’ event.
as a next step of log inspection, we can make use of the timestamps in
the log to view the distribution of events over time with the dotted chart
analysis plug-in. figure 5(a) depicts a screenshot of this plug-in, where each
row corresponds to one of the 24 process instances displayed on a time scale.
note that the displayed time scale does not reveal actual time information for
conﬁdentiality reasons, but only shows the logical ordering of events over time.
however, diﬀerent time options (also showing the “real” time) are available. one
can see that some of the test processes were long ﬁnished when the snapshot was
taken, and that systems are being tested in parallel.
the kind of information to be dispersed over the rows can be conﬁgured by
the user. for example, we can see the activity of diﬀerent people over time if
information about originators (i.e., performers) is available in the log. in the
case of our test process, we are interested in analyzing the job steps, i.e., the
test phases that can be associated to the reference sequence. in figure 5(b) each12
fig. 5. dotted chart analysis in the prom framework13
row corresponds to a diﬀerent job step6in the test sequence of one speciﬁc ma-
chine. here, it becomes visible that some of the tests are only performed at the
beginning or at the end, while others are performed throughout the whole pro-
cess. this way, frequently reoccurring test phases can be detected and further
analyzed.
but to be able to analyze the log on the job-step level, we ﬁrst have to apply
certain ﬁltering techniques. recall that there is no information about job steps
recorded in the log, but that we have obtained a document specifying which tests
need to be executed for each job step. in this mapping, there are 184 out of the
360 detected test codes associated to a job step. this means that 176 of the four-
letter codes cannot be connected to a speciﬁc job step (in the remainder of this
paper we call them “unmapped” codes). they mainly correspond to additional
(more speciﬁc) tests that are executed as part of the diagnosis process after a
failure. at the same time, there are 49 out of the 184 mapped test codes that
are associated to more than one job step, i.e., they occur in diﬀerent phases of
the test process (in the remainder we call them “multiple” codes). the rest of
the four-letter codes (i.e., 135 test codes) can be unambiguously mapped onto a
speciﬁc job step.
oswl
startossp
startahzi
startdsna
startoswl
completeossp
completeahzi
completedsna
completee
starte
completee
starte
completee
starte
completeunmapped
startunmapped
complete
oswl ossp ahzi dsnae unmapped
e
starte
completeunmapped
start
test-code leveljob-step level
unmapped
complete
remap filterrepetitions-
to-activity filter
012
fig. 6. a combination of ﬁltering techniques was applied to bring the log data from
the test-code level to the job-step level
in the prom framework, there are various log ﬁlter plug-ins available. they,
for example, remove certain audit trail entries from the log. some log ﬁlters can
be customized by the user, and each of them (or a combination) can be stored and
reused later on. in figure 6 we show as an example how a part of the log fragment
6note that, again, the actual job step names have been replaced by simple letter codes
for conﬁdentiality reasons.14
from figure 3 is transferred to the job-step level using a combination of multiple
log ﬁlters. as a ﬁrst step, a custom remap filter is applied. the remap log ﬁlter
allows for the speciﬁcation of powerful mappings based on regular expressions.
if the name of a log event matches a certain pattern, it can be replaced by an-
other pattern, or the corresponding audit trail entry can be completely removed
from the log. using this ﬁlter, we mapped each of the unambiguous test codes
onto their corresponding job step identiﬁer, or the ‘multiple’ or ‘unmapped’ cate-
gory if this was not possible. for example, figure 6 shows that the tests ‘oswl’,
‘ossp’, and ‘ahzi’ are associated to the job step ‘e’, while the test ‘dsna’ can-
not be mapped to any job step (i.e., ‘unmapped’). as a next step, we abstracted
from all events that occurred between the ﬁrst and the last event belonging to
the same job step in a row using the repetitions-to-activity filter . for
example, in figure 6 only the beginning of the ﬁrst occurrence of a test in job
step ‘e’ (i.e., test ‘oswl’) and the end of the last occurrence in job step ‘e’
(i.e., test ‘ahzi’) is retained. note that using this mapping, now also idle times
within one job step are covered by the overall job-step duration (for example, the
idle time between the completion of test ‘oswl’ and the start of test ‘ossp’).
as a result, only changes between job steps become visible in the log, which we
will use for process discovery on the job-step level later in this paper. for the
analysis of job step-related test occurrences over time in figure 5(b) we used the
event log on level 1 (cf. figure 6), i.e., after the application of the remap filter .
another way to make use of the timestamp information in the log is to in-
spect the overall test process durations (i.e., throughput times), and the activity
durations and idle times of the test process using the basic log statistics
plug-in. the plug-in evaluates the ‘start’ and ‘completion’ times for each activity
in the process. this way, steps in the process that consume much time can be
detected and further analyzed. figure 7(a) depicts a screenshot of the graphical
view on the mean time that is spent within each job step. the results are based
on the whole log mapped to the job-step level (cf. level 2 in figure 6). one can
see that, on average, most time is spent within job step ‘a’.7
while each of the statistical measures, such as mean values, variance and
standard deviation, can be visualized in a graphical way, the plug-in also oﬀers a
textual overview about the results. the measures to be included in the overview
can be conﬁgured by the user, and the table can be sorted by the names of the
activities or any of the available result types. for example, figure 7(b) shows
the textual overview about idle times for the test process of the machine 1596,
sorted by the sum of all measured values. one can see that most of the idle time
accumulated after the execution of test ‘pywz’. this can be explained by the
fact that this test is used to stabilize the wafer scanner, which takes many hours.
therefore, this test is typically started at the beginning of the weekend, and the
7we must keep in mind that a considerable amount of tests could not be uniquely
associated (cf. the “unmapped” and “multiple” test categories), and that they are
here interpreted as a change of job step, which is not necessarily the case. however,
because of the nature of the mapping this is unavoidable.15
fig. 7. basic statistics about activity durations, such as maximum and arithmetic
mean values, can be viewed for both single process instances and the whole log16
wafer scanner enters idle time if stabilization ﬁnishes earlier.
detailed information about idle times, such as in figure 7(b), is essential for
asml. as already mentioned in section 3.2, the goal is to minimize the overall
test duration, and, therefore, to decrease the time-to-market. however, before
we can extract this information from the log, we again need to apply ﬁltering
mechanisms to the initial event log. figure 8 visualizes how the example log
fragment is transformed from representing test durations to idle times after these
tests. as a ﬁrst step, the activity-inversion filter replaces each ‘complete’
event and its succeeding ‘start’ event by a ‘start’ and ‘complete’ event marking
the transition between those tests.8then, a custom remap filter is used to
abstract from the second element in the transition relation, i.e., the test that
was executed afterwards. this is the log that was used as input in figure 7(b).
nevertheless, it can also be useful to analyze the log on level 1 (i.e., directly after
the application of the activity-inversion filter) as the succeeding test code can
provide insight into the nature of the idle time, e.g., whether the re-execution
of parts of the test sequence was necessary, or whether simply the end of an
automatic test queue was reached at the end of a night.
oswl
startossp
startahzi
startdsna
startoswl
completeossp
completeahzi
completedsna
completeoswl-
to-ossp
completeossp-
to-ahzi
startoswl-
to-ossp
startossp-
to-ahzi
completeahzi-
to-dsna
startahzi-
to-dsna
completedsna-
to-ahzi
start
oswl
test durationossp
test durationahzi
test durationdsna
test durationoswl-followed 
idle timeahzi-followed 
idle time
oswl-
followed 
idle time
startahzi-
followed 
idle time
start
test durationsidle times 
after tests
dsna-
followed 
idle time
startremap filter
activity-
inversion filter
012
oswl-
followed 
idle time
completeossp-followed 
idle time
ossp-
followed 
idle time
startossp-
followed 
idle time
completeahzi-
followed 
idle time
completedsna-followed  
idle time
fig. 8. an inversion ﬁlter allows to analyze the idle times instead of the actual test
durations. the remap ﬁlter is used to abstract from the succeeding test
we have seen that using log inspection and ﬁltering tools we can already an-
swer questions about general log characteristics, such as simple frequency mea-
sures, the distribution of events over time, throughput times, and basic statistics
about test durations and idle times. in the remainder of this section, we will fo-
8note that this mechanism only works because we have no interleaving tests in the
log. each test that is started will ﬁrst be completed before the next one is started.17
cus on the control-ﬂow , i.e., the causal dependencies between steps in the test
process. for this, further ﬁltering techniques were applied:
–atomic tests. often, we can abstract from activity durations if we analyze the
control-ﬂow of the process. therefore, all ‘start’ events were removed from
the log and only the ‘complete’ events are kept, representing the occurrence
of a test in an atomic way.
–common behavior. only tests that were performed for all the 24 machines
were selected using the enhanced event log filter . this ﬁlter allows to
remove tests that occur below or above a custom commonality (i.e., in how
many cases) or frequency (i.e., how often in total) threshold.
–mapped test codes. only tests that were contained in our mapping document
were selected using the simple event log filter .
–unique test codes. only unique mapped codes were selected, i.e., those that
occur only in the context of one single job step.
–job step executions. to analyze the changes between the job steps, we built
on the ﬁltering depicted in figure 6, but subsequently abstracted from the
‘unmapped’ and ‘multiple’ categories using the event log filter . after-
wards, repetitions of the same job step were removed using the duplicate
task filter .
4.2 process discovery
as stated in section 3.2, the second goal for asml—next to the minimization of
idle times in the test process—is to reduce the re-execution of parts of the job-
step sequence. in this section, we want to apply process discovery techniques to
gain insight into the actual ﬂow of the test process to ﬁnd out where re-executions
were often necessary.
process discovery algorithms automatically construct a process model based
on the behavior that was observed in the event log, and in the prom framework
there are many diﬀerent discovery plug-ins available. however, the nature of our
test log poses some challenges. on the one hand, there are only a few process
instances available, which at the same time are very long, and contain logged
tests that are executed in diﬀerent phases of the test process (cf. ‘multiple’ codes
in section 4.1). on the other hand, we already know that the process is very
ﬂexible (as parts of it might need to be redone depending on the outcome of the
performed tests), while most of the traditional discovery algorithms described in
literature assume that the underlying process is “structured” (i.e., the number
of possible paths through a process is limited or the paths have some regular
form). most process discovery plug-ins in prom also focus on structured pro-
cesses. fortunately, there are also several plug-ins that are able to deal with less
structured processes [2, 31].
the heuristic miner is an example of a plug-in that can deal with such less
structured processes [31]. it tries to abstract from low-frequent behavior based
on certain heuristics to connect the activities in the process. figure 9(a) depicts18
fig. 9. process models on test-code level discovery using prom19
fig. 10. reference sequence and discovered process model on job-step level20
the initial model that was discovered based on the whole log (as already indi-
cated, from now on we abstract from the ‘start’ events and consider activities
to be atomic). the discovered model is “spaghetti-like”, i.e., it is huge (nodes
corresponding to the 360 tests) and it is unstructured. such spaghetti-like mod-
els are not caused by limitations of the plug-in but by the inherent complexity
of the testing process. therefore, we subsequently applied further ﬁltering tech-
niques (cf. list at the end of section 4.1) to abstract from certain tests in the
process. this helps to yield smaller models. as an example, we show the model
in figure 9(b). this model contains only 70 diﬀerent activities and reﬂects the
view on all common tests in the process (i.e., they were performed for each of
the machines). figure 9(c) depicts a screenshot of the mining result in the prom
framework, whereas the displayed area of the process model is highlighted by
the rectangle in figure 9(b). in the modeling formalism used by the heuristic
miner , which is called heuristics net, boxes correspond to activities (the numbers
within the boxes indicate how often this test occurred) and they are connected
by directed arcs, whereas the lower numbers next to these arcs indicate how of-
ten the connection was observed in the log. one can, for example, see that some
tests are frequently repeated (indicated by the directed arc from a test activity
to itself), which can be explained by the automated test queues that restart
tests after adjusting certain parameters of the wafer scanner as described in sec-
tion 3.2. the processes shown in figure 9 can also be automatically mapped
onto more conventional languages such as event-driven process chains (epcs),
petri nets, yawl, etc.
while it is interesting to visualize dependencies on the test-code level, we
also want to analyze the process on the job-step level to compare the discovered
model to the existing reference sequence. the translated reference sequence is
depicted in figure 10(a), and it reﬂects the normal ﬂow of the test sequence if
nothing goes wrong (i.e., if no test fails). we already know that in reality parts
of the test sequence need to be repeated in certain occasions. this also becomes
visible in the discovered model based on the log ﬁltered for job step executions
(cf. list at the end of section 4.1), which is depicted in figure 10(b). note that the
discovered process model allows for considerably more paths than the reference
model. figure 10(c), again, shows a screenshot of a part of the mining result
in more detail, where one can easily recognize the repetitive nature of the real
(as opposed to the ideal, i.e., reference) test process. note that the numbers
next to the arcs show the importance of the diﬀerent paths (the lower number
indicates how often this connection was observed in the log, while the upper
number indicates the heuristic strength of the corresponding connection). what
we can see in figure 10(c) is a highly connected group around job step ‘f’, which
is bi-directionally connected to the job steps ‘e’, ‘b’, ‘g’, and ‘o’. a reason for
this eﬀect can be that many executions of the job steps ‘e’, ‘b’, ‘g’, and ‘o’ result
in a re-execution of job step ‘f’.21
4.3 conformance and extension
so far, we have seen that it is possible to automatically discover models which
represent the behavior that was observed in the event log. but how well is the
actual process represented by these models? and to which extent does the ob-
served process comply with the behavior speciﬁed in the reference model? where
in the process do most of the deviations occur? these are questions that are ad-
dressed by conformance techniques, such as the conformance checker plug-in
in the prom framework. this plug-in measures and visualizes the discrepancies
between an event log and a given process model. another conformance technique
is the ltl checker [1], which veriﬁes the compliance of an event log not with
respect to a complete process model, but a set of requirements (e.g., business
rules).
in the following we show in more detail how the conformance checker can
be used to analyze the conformance of both the reference model and the discov-
ered model on the job-step level (cf. figure 10) with respect to our test log. as a
preparation step, we need to translate the process models from a heuristics net,
as discovered by the heuristic miner , to a petri net, which is the modeling
language that is supported by the conformance checker . this can be achieved
with the help of one of the various conversion plug-ins in the prom framework.
prom supports diﬀerent process modeling techniques, e.g., epcs (the notation
used by systems such as sap and aris), diﬀerent classes of petri nets, heuristics
nets, diﬀerent workﬂow languages (e.g., yawl and bpel), etc. these models
can be imported, exported, and in most cases it is possible to convert one type
of model to another type. the conversion from heuristics nets to petri nets is
just one of many conversion plug-ins.
figure 11(a) depicts a screenshot of the conformance checker , where the
discrepancies between the reference model and the event log (ﬁltered for job
step executions as described earlier) are visualized from the model perspective.
alog replay9is performed, i.e., each case in the log is “parsed” by the model
to analyze the match. during this log replay, tokens may be missing (indicated
by a−sign), which indicates that the corresponding task was not ready to be
executed according to the model at the time it was performed. for example, in
figure 11(a) one row in the diagnostic feedback for the place before job step ‘e’
is highlighted. it means that in 4 out of the 24 cases (# instances = 4) job step
‘e’ was executed 10 times when it was not possible according to the reference
sequence (# t okens =−10). such a perceived conformance problem may point
us to a repeated execution of a part of the test process (due to a failing test later
in the sequence). similarly, remaining tokens (indicated by a + sign) hint that
the following activity was expected to be executed according to the model (but
in fact was not).
the log view of the conformance checker helps to further narrow down
potential root causes of the discrepancy. figure 11(b) shows a screenshot of
the log view (highlighting those log events that do not comply with the process
9the interested reader is kindly referred to [28] for further details about the described
conformance checking techniques.22
fig. 11. the conformance checker measures and visualizes discrepancies between the
log and a given process model23
description). one can see that in the top-most log trace the ﬁrst mismatch occurs
after the initial ‘zero’ step and the executions of job step ‘a’ and ‘c’. in the log
we then ﬁnd a second execution of job step ‘a’, which in not allowed according
to the model. however, job step ‘a’ is not causally related to job step ‘c’, and as
described in section 3.2, it is in fact allowed to change between such parallel job
steps within the test process, which is not captured by the process model. this
shows that not every detected conformance problem necessarily corresponds to
a re-execution of a part of the test sequence. domain knowledge is needed to
investigate the discrepancies between the actual test process and the reference
model in more detail, and we will report on the evaluation of our ﬁndings from
an asml perspective in section 5.
however, the conformance checker also measures the degree of ﬁtness based
on the amount of missing and remaining tokens during log replay [28], i.e., it
quantiﬁes to which degree the log traces comply with a given process model.
this ﬁtness analysis clearly indicates that the discovered model is much more
representative for the observed test process than the reference model (cf. ﬁtness
values in table 1).
table 1 contains the ﬁtness values for each of the test instances with respect
to both the reference sequence and the discovered model on the job-step level
as depicted in figure 10, whereas possible values range from 0 .0 (corresponds
to the case where the model and the log do not ﬁt at all) to 1 .0 (i.e., model
and log ﬁt to 100%). furthermore, it shows how many job step executions were
contained in the ﬁltered log for each machine (column before the last column
in table 1), and how many test code events were originally recorded for this
machine (last column in table 1). finally, in the bottom row average values are
given for all the 24 machines. we can see that, although the discovered process
model does not completely “match” the behavior observed in the log, it clearly
ﬁts much better than the reference sequence. this is not surprising as we already
know that—in contrast to the discovered model—the reference model does not
capture the possible repetitions in the process at all, but it describes the ideal
ﬂow of the process if nothing goes wrong. so, the discovered model is a much
better representation of the test process as it took place, which demonstrates
that process mining can provide insight into how processes are really executed.
finally, if we have a good process model (which may be handcrafted or ob-
tained through process discovery), extension techniques can be used to project
information which was extracted from the log onto the existing model. for ex-
ample, performance bottlenecks can be visualized directly in the process model.
figure 12 depicts a screenshot of the performance analysis with petri net
plug-in in prom, which graphically highlights performance-related information,
such as the waiting time between two activities, directly in the given process
speciﬁcation. according to the severity, places are colored in red (high), yellow
(medium), or blue (low waiting time). however, for this kind of analysis the
plug-in assumes that the model and the log are perfectly ﬁtting each other. this
is not the case for any of our models of the test process, and therefore we can
only use it to display performance information with respect to activities, such24
table 1. fitness values indicating the degree of compliance for each of the test in-
stances with respect to both the reference sequence and a discovered process model.
clearly, the discovered model ﬁts much better than the overall model
machine fitness reference fitness discovered no. of filtered no. of original
id process model process model job-step events test-code events
0431 f= 0 .309 f= 0 .751 238 6504
0278 f= 0 .385 f= 0 .828 270 6136
0185 f= 0 .376 f= 0 .717 206 5710
0466 f= 0 .356 f= 0 .745 422 8162
0391 f= 0 .384 f= 0 .727 159 3902
1722 f= 0 .334 f= 0 .760 301 6270
1694 f= 0 .397 f= 0 .782 526 10408
1256 f= 0 .410 f= 0 .744 222 5722
1343 f= 0 .399 f= 0 .701 130 5360
1981 f= 0 .357 f= 0 .667 551 12670
1754 f= 0 .402 f= 0 .776 192 16250
1662 f= 0 .414 f= 0 .769 182 3830
1453 f= 0 .405 f= 0 .596 164 6410
1298 f= 0 .378 f= 0 .424 170 3852
1876 f= 0 .356 f= 0 .753 150 4538
1656 f= 0 .368 f= 0 .656 126 2820
1099 f= 0 .424 f= 0 .672 193 3946
1919 f= 0 .337 f= 0 .727 205 5048
1348 f= 0 .410 f= 0 .638 184 5240
1596 f= 0 .410 f= 0 .581 224 5784
1164 f= 0 .376 f= 0 .672 499 10860
1032 f= 0 .324 f= 0 .706 301 6896
1794 f= 0 .394 f= 0 .734 114 2972
1160 f= 0 .405 f= 0 .770 186 5676
average f= 0 .375 f= 0 .711 246 .458 6456 .917
as the time which is spent between two user-deﬁned activities. in figure 12, the
job steps ‘a’ and ‘e’ are selected, and the average, minimum, etc. time between
them is displayed.
5 evaluation and improvement suggestions
to identify concrete improvement suggestions, we evaluated the presented pro-
cess mining results from an asml perspective. in a ﬁrst analysis eﬀort, we
mainly investigated the diﬀerences between the reference model and the discov-
ered model. first, the order of job steps was analyzed (section 5.1). the job-step
order is the sequence in which job steps are executed in the factory. some vari-
ation is allowed, but not too much. we investigated whether—according to the
discovered model as in figure 10—the test process followed the reference process
(including the allowed variations). second, the dominant feedback loops in the
test process were analyzed (section 5.2). feedback loops in the process indicate25
fig. 12. the performance analysis with petri-net plug-in graphically projects time
information (such as waiting time between two activities) onto a given process model
that a job step fails and a previous job step needs to be re-done. after the pre-
vious job step is re-done, often parts of the already executed sequence must also
be re-executed. this is taking valuable time. finally, possible root causes for
some of the idle times are given (section 5.3). the reduction of idle times can
help to decrease the overall test process duration, and, therefore, decrease the
time-to-market.
5.1 order of job steps in the test process
when we investigated whether the real process followed the reference process,
considering the allowed variations, we obtained three types of results: (1) job
steps that are actually executed on a diﬀerent place in the reference sequence
(i.e., deviations from the process model shown in figure 10(a)), (2) groups of
highly connected job steps, and (3) job steps that are not in the reference se-
quence but in the test log. in the following, we describe them in more detail.
1. it appeared that job step ‘i’ was positioned in 81% of the cases just after
the ‘zero’ job step, i.e., at the beginning of the discovered process model,
while—according to the reference sequence—it should be executed in the
middle of the test process. while looking for possible root causes for this
diﬀerence, we realized that a newer version of the reference sequence was
released in the end of 2006. the main change in the new reference sequence
was that job step ‘i’ and ‘j’ were positioned just after the ‘zero’ job step
at the beginning of the test sequence. the analyzed systems were build up
according to the new sequence for job step ‘i’. interestingly, job step ‘j’ was
still found in the original position. if job step ‘j’ is really to be executed in
the beginning of the sequence, then active steering should take place to align
the test execution. note that we also re-checked the conformance of the test26
log with respect to the updated reference sequence, but the ﬁtness values
did not change signiﬁcantly (on average f≈0.45).
2. two highly connected groups of job steps are discovered in the discovered
process model. the ﬁrst group is depicted in figure 10(c), a strong connec-
tion between job step ‘f’ and a number of other job steps: ‘e’, ‘b’, ‘g’ and
‘o’. these connections are bi-directional between ‘f’ and the other job steps.
a reason for this eﬀect could be that any execution of the job steps ‘e’, ‘b’,
‘g’ and ‘o’ results in a re-execution of job step ‘f’. job step ‘f’ is a relatively
short job step which can be executed automatically. as a result, the entire
test set is executed. speciﬁc (parts) of the test set in job step ‘f’ could be
faster when job step ‘f’ needs to be executed after job step ‘e’, ‘b’, ‘g’ and
‘o’ are executed. in general, speeding up job step ‘f’ is beneﬁcial because it
is executed multiple times in the entire sequence.
fig. 13. highly connected group of job steps, which has been identiﬁed based on the
process mining results presented in section 4
the second highly connected group is centered around job steps ‘r’, ‘s’, ‘t’
and ‘j’. the mined process showed the following pattern (see figure 13).
job step ‘r’ and ‘t’ are bi-directionally connected. job steps ‘r’, ‘j’ and ‘t’
are illumination steps, while job step ‘s’ is a non-illumination step. the root
cause of a failure of job step ‘t’ is solved by job step ‘r’. a re-execution
of job step ‘r’ causes a re-execution of job step ‘s’ (and possibly ‘j’). an
improvement proposal would be to introduce a more thorough test in job step
‘r’ (i.e., add a similar test to the one in job step ‘t’) which causes that, if the
failure occurs, it already occurs in job step ‘r’ and can be immediately ﬁxed
in job step ‘r’. this prevents the re-execution of job step ‘s’ (and possibly
‘j’).
3. one of the feedback loops revealed that job step ‘d’ is executed, although it is
not in the reference sequence. job step ‘d’ is currently not investigated to be
improved to decrease the cycletime, because this job step is not supposed to27
be executed. the process mining results revealed that job step ‘d’ is executed
as part of a recovery plan. job step ‘d’ could be further investigated for cycle
time reduction.
5.2 dominant feedback loops
feedback loops in the mined process indicate that a certain job step failed and
caused that a job step, which was positioned earlier in the sequence, needs to
be re-executed. for example: job step 10 fails – a repair takes place (hardware,
software or machine parameters change) – job step 5 must be re-executed to
calibrate the hardware/software/machine parameters – job steps 6 to 10 need to
be re-executed as well.
ideally, failures, ﬁxes, and re-execution of test cases are performed in the job
step that failed. given technological constraints related to the construction of
a wafer scanner, this is not always feasible. moreover, many diﬀerent faults can
cause a particular failure. process improvement in this area should start with the
most important failures and feedback loops. the following dominant feedback
loops (→) have been observed in the discovered process model: (1) z →(via d)
→a, (2) z →(via d) →l, (3) t →a, and (4) v →f.
in general, this shows that the job steps ‘z’, ‘t’, and ‘v’ are job steps which are
capable of ﬁnding the errors that were missed in the previous job steps. errors
detected in these steps cause the test process to be “rolled back”. test cases that
fail in these job steps should be placed in the lower level job steps as additional
test case to test the overall performance. this allows an early failure and an
early ﬁx. this might be the reason that job step ‘i’ and ‘j’ (both illumination job
steps) were positioned earlier in the updated reference sequence (cf. section 5.1).
process improvement should focus on these four dominant feedback loops for
these systems. more speciﬁcally, the ﬁrst and second feedback loop visit job step
‘d’. this is possibly to determine if either job step ‘a’ or job step ‘l’ needs to
be executed to ﬁx this problem. the test cases in job step ‘d’ that perform this
diagnosis could be useful as a standard test case in job step ‘a’ and ‘l’.
5.3 idle times during test procedure
using idle time for scheduling automatic test actions is one of the possibilities to
reduce the overall test duration. in section 4.1 we described how the idle time
accumulating after a certain type of test can be determined using a combination
of ﬁltering techniques and the application of the basic log statistics plug-
in. figure 7(b) depicts the analysis results for one of the 24 machines. most of
the idle time on this machine accumulated after executing the tests (1) ‘pywz’,
(2) ‘dsna’, and (3) ‘iwow’, which are also among the 4 tests that accumulated
most of the idle time for all the 24 machines together.
(1) the ‘pywz’ test is used to stabilize the wafer scanner, which takes hours.
therefore, this test is started at the beginning of the weekend at the end of the
‘zero’ job step. the wafer scanner enters idle time if stabilization ﬁnishes earlier.
this can easily be solved by adding the test cases of the next job step to this28
test set. in the case of the machine depicted in figure 7(b), at least 97 minutes,
and on average 61 hours accumulated after 7 executions of this test.
(2) the ‘dsna’ test ensures the reliability of the wafer handler, one of the
sub-systems in a wafer scanner. this test is executed during the available night
hours throughout the entire job step sequence. sometimes, the weekend is too
long for the test queue. this can be resolved by adding additional ‘dsna’ test
cases to the test set. for the machine in figure 7(b), at least 0 .01 minutes, and
on average 1 .14 hours accumulated after 129 executions of this test.
(3) after the execution of the test ‘iwow’, at least 1 minute, and on average
13 hours accumulated after 8 executions on the same machine. the test was ex-
ecuted twice as the last test before the weekend, and once before the christmas
holiday.
we have demonstrated that current process mining techniques can already
answer many questions, even yield concrete suggestions for process improve-
ment also in as complex environments as the wafer stepper qualiﬁcation phase
of asml. however, due to the rapid technological advancements, the analysis
results presented in this paper are likely to be outdated already for the next
series of wafer steppers than the ones that we analyzed. to enable a continuous
improvement of the test process, process analysis should be best carried out in
an iterative manner.
at the same time, we also face interesting challenges posed by the size of the
log (initially more than 150,000 log entries) and the few process instances (log
data from only 24 tested wafer scanners were available for analysis). even more
challenging is the extremely ﬂexible nature of the process, where people testing
these machines may change back and forth between job steps if they do not need
to be synchronized (to use the otherwise idle time in the most eﬃcient way), and
where whole parts of the process may need to be repeated due to a failing test
later in the test sequence. this is the reason why—in contrast to other case
studies (e.g., a municipality [16] and the national public works department [3]
in the netherlands)—our current process mining algorithms discover “spaghetti-
like” process models. consider, for example, the discovered process model in
figure 9(a). in fact, there is nothing wrong with discovering such a spaghetti-like
model as it correctly shows that the underlying process behavior is very diverse.
but it is not very helpful for gaining insight into the main process, distinguishing
certain scenarios, and answering speciﬁc questions. in such a case, it is crucial to
be able to abstract from some of the details. in section 4.1, we achieved diﬀerent
levels of abstraction using various ﬁltering techniques. however, ﬁltering the log
as shown requires some work, is not interactive, and we lose the connection to the
previous stage of ﬁltering (e.g., we cannot decide to expand one of the job steps
to view this part on the test-code level). therefore, the next section discusses
some research challenges related to the mining of processes like the test processes
of asml and other spaghetti-like processes encountered in other domains (e.g.,
health-care processes).29
6 challenges for process mining
in section 4 it was shown that with existing process mining techniques present
in the prom framework [18], already compelling results can be achieved. note
that the logs considered in this paper are more challenging than the logs of more
structured processes (e.g., workﬂows in administrative organizations). hence, it
is fair to conclude that applying process mining to real-life logs is a viable option.
however, sometimes the resulting models are overly complex and confusing (i.e.,
“spaghetti-like”), which makes them hard to extract useful information from. in
this section, we report on two prom plug-ins whose development was inspired
by the case study reported in this paper: the cloud chamber miner and the
frequency abstraction miner . these new plug-ins have been developed with
the test processes of asml in mind.
to motivate the development of the two new plug-ins, we ﬁrst summarize
limitations and assumptions of existing tools. it can be argued that the current
practice of process mining is founded on a, partially implicit, set of assumptions
which need to be adjusted to succeed in less structured, more ﬂexible situations
as found in real life (e.g., asml’s test processes and health-care processes).
–noise , i.e. incorrect or corrupted information in logs, is already expected
and dealt with in a number of mining approaches. however, recent analyses
of real-life logs have shown that the concrete quantity and quality of noise
diﬀers signiﬁcantly from initial, clean-room assumptions, and also between
distinct situations.
– current algorithms focus strongly on a precise description of the observed
behavior, preferably using formalized and exact models. in practice, the ob-
served reality is often very complex. in many cases one would thus prefer a
less precise description, where one can abstract from less relevant details.
– when faced with unknown log data, one needs extensive time to explore the
structure and particularities of that speciﬁc data set, so that mining tools
can be correctly adjusted to the situation. for this task, the complex and
inﬂexible nature of current mining techniques is often a burden. interactive
tools for data exploration could dramatically improve the quality of results
and reduce the eﬀort necessary, by leveraging on user knowledge.
as introduced in section 4.1, ﬁltering is a powerful means for improving
the speciﬁc quality of raw log data. however, the ﬁlters currently available are
not suﬃcient to tackle the types of noise found in real-life logs. thus, in many
practical applications proprietary ﬁlters have to be developed, which are tar-
geted towards a speciﬁc (type of) log ﬁle. lessons learned from these custom
implementation eﬀorts include:
– filtering is not limited to deleting events; it is also often necessary to modify
events or to introduce new events .
– more advanced ﬁlters need to have a more global view on the log data , rather
than basing decisions on local events considered in isolation.30
– providing ﬁne-grained customization options to the user, while picking useful
default conﬁgurations tremendously increases the eﬃciency of using a ﬁlter.
the development of more eﬀective and versatile log ﬁlters is thus a necessity
for the successful interpretation of real-life log data. before log ﬁlters can be
conﬁgured to remove noise from the data, however, one needs to determine what
unwanted noise exactly is in the ﬁrst place. this usually involves a back-and-forth
cycle between ﬁltering and mining, adjusting ﬁlters based on the observation of
unwanted artifacts in mining results.
we are currently experimenting with techniques to visualize event log data
appropriately for pre-processing tasks. the goal is to ﬁnd a visualization that
enables practitioners to spot data anomalies quickly, and to determine eﬀective
means for removing them. one technique for data visualization which relies
heavily on the human brain’s ability to spot geometric patterns are dotplots [23].
the data is laid out on both axes of a raster (from top-left to top-right, and from
top-left to bottom-left); for every coordinate where the xand ycoordinates
represent the same event class, the pixel is painted with a color symbolizing the
frequency of the respective event. otherwise, the pixel is painted black. to be
more precise: both xand yrange over all possible events, i.e., the concatenation
of all events in all cases. a pixel ( x, y) is painted black if xand yrefer to diﬀerent
events. only if xand y“match”, pixel ( x, y) is colored. this way, the diagonal
axis from the top-left towards the bottom-right represents the self-similarity of
the data (by deﬁnition). but every pattern that appears oﬀ the axis reﬂects some
kind of similarity pattern (see [23] for more information and practical examples,
such as detecting duplicated code in a software system).
fig. 14. prom’s cloud chamber miner enables log exploration by dotplot visualization31
figure 14 shows the cloud chamber miner , a plug-in recently added to
prom, which uses the idea of dotplots and applies this to process mining. fig-
ure 14 shows only a fraction of the entire dotplot. both the xand ycoordinates
refer to all events for all machines (i.e., the whole log). the log’s self-similarity
can be clearly seen as a diagonal line extending from the upper left corner, i.e.,
all coordinates ( x, x) are colored. several repetitive patterns are visible in the
screenshot in figure 14. for example, the biggest square around the diagonal
axis shows the repeated executions of the ‘hqle’ test. the green color indicates
that this is a test having a moderate frequency (about 66% relative frequency
compared to the most-frequent test in the log). this test is one of the conver-
gence test cases, where the performance of the projection system is measured for
a number of typical user settings. the log contains a number of event classes,
which are repeated frequently. these short loops, which refer to the automatic
re-starts of test steps, show up as square patterns in the dotplot visualization.
an interesting feature of dotplots is that it is possible to see patterns when fo-
cusing on a single machine but also between diﬀerent machines, e.g., if the same
sequence occurs in multiple machines, then diagonal lines are shown. while the
dotplot visualization is still very close to the log, the cloud chamber miner
provides means for navigating to speciﬁc process instances, and for exploring
the log in real time. this feature is important, in that it enables and encourages
experimentation, which is essential in the process mining domain.
fig. 15. the frequency abstraction miner in prom can aggregate low-frequent behav-
ior in clusters
the most challenging task in process mining, however, remains to ﬁnd suit-
able techniques for the high-level analysis of process log data. one line of re-
search focuses on making these techniques more ﬂexible, less precise, and more
interactive. figure 15 shows prom’s frequency abstraction miner , an exper-32
imental technique that attempts to satisfy these requirements. it features one
single control, a slider that allows the user to specify a frequency threshold value.
whenever this slider is moved, the plug-in will cluster all events below the fre-
quency threshold. clusters are visualized as green octagonal nodes, while less
frequently observed relations are depicted using a lighter greytone. this behav-
ior signiﬁcantly simpliﬁes the displayed process model, allowing the user to focus
on the most frequent behavior in the log.
in contrast to ﬁltering as described in section 4.1, the abstraction provided by
thefrequency abstraction miner is both dynamic andnon-destructive , i.e. it
can be adjusted interactively and does not modify the original data. clusters
can also be “expanded”, i.e. when the user clicks on a cluster node a subgraph
will be shown, detailing the primitive nodes the cluster is composed of and their
respective relations.
both the cloud chamber miner and the frequency abstraction miner
represent but ﬁrst steps on the road to more explorative tools for process mining.
future work in this direction will complement the present set of more precise
algorithms, in that it enables users to interactively learn about the log data.
using this knowledge, the data can be ﬁltered appropriately, so that also more
precise techniques can be applied while avoiding “spaghetti results”.
the development of these techniques is driven and inspired by our experience
in practical applications, like the asml case described in this paper and the
analysis of the event logs of several dutch hospitals. we expect that an improved
and extended toolkit will allow us to perform case studies like this in a much
shorter timeframe, while providing more insightful results.
7 conclusion
to conclude the paper, we ﬁrst reﬂect on our ﬁndings in the case study and then
make some more general comments on the applicability of process mining.
analyzing the event log of a test process of asml’s wafer scanners using
process mining techniques revealed potential improvements in the following two
areas:
1.conformance of the actual executed sequence with the reference sequence.
the mined process revealed that another process was executed than the ref-
erence sequence, i.e., the ﬁtness of the reference process compared to reality
was only 37.5 percent. an updated version of the reference sequence was re-
leased in the middle of the observed period. however, the discovered process
model revealed that the new reference sequence was not followed entirely
and that there are many deviations.
2.concrete improvement proposals for the failures which disturb the test pro-
cess the most. improvement proposals were based on a number of diﬀerent
observations. first, groups of strongly connected job steps were discovered.
this resulted in speciﬁc suggestions that could make the process more trans-
parent and faster. second, one job step that was found in the log was not
contained in either of the reference sequences. this job step was executed33
as part of the diagnosis and analysis feedback loops, and could be further
analyzed for optimization. third, four dominant feedback loops were identi-
ﬁed in the discovered model of the test process. case-by-case analysis is still
required to see how these feedback loops can be removed. finally, possible
root causes for some of the idle times were given. further investigation of
idle times can lead to a more eﬃcient usage and reduction of the overall test
duration.
the typical characteristics of the test process of an asml wafer scanner are
that a highly variable reference sequence is allowed and a relatively small number
of wafer scanners of one system type are made. the application of process mining
techniques in asml should therefore start with the ﬁrst system of a new wafer
scanner type. analysis and adjustment of the sequence should then be done in
a “closed loop” after each new system is tested and delivered. a closed-loop
process would look like the following:
1. determine the reference sequence.
2. test the wafer scanner.
3. analyze the log and reference sequence for each tested wafer scanner.
4. determine the most dominant feedback loops and job step groups.
5. determine improvement actions for the next untested wafer scanner (more
wafer scanners can be in the pipeline already). typical improvement actions
are: (a) moving test cases to other job steps / duplicating test cases, (b) mov-
ing test cases from job steps to sub-system level, (c) updating the reference
sequence, and (d) the normal actions (parts quality, test process, machine
status monitoring, etc. but with focus on the dominant feedback loops).
6. continue until all systems are delivered.
additional information sources that could be used to improve the data set
are available, but were not used in this case study. sap data is available with
(manually entered) job step start and stop data, and the start and stop mo-
ment of the entire test sequence. this data is less reliable, because it is manually
entered and it contains job steps that overlap in time. however, together with
the other log data, better process models could be discovered. the individual
test results (pass, fail, out of speciﬁcation) could be used to determine why a
job step is stopped and a feedback loop is started. furthermore, the start and
stop moments of test queues can be added as information. this information is
available in the asml systems, but needs conversion to the prom format.
this paper describes the ﬁrst case study that applies process mining to less
structured processes. all prior applications of process mining reported in liter-
ature have been focusing on rather structured administrative processes . in our
case study, we applied our prom framework to the test process of asml. as de-
scribed, the testing of asml’s wafer scanners is a highly dynamic process and a
real challenge for existing process mining techniques. moreover, there are many
other domains where similar logs can be obtained (e.g., health-care processes).
therefore, our ﬁndings are also relevant to these other domains. the case study34
shows that ﬁltering is very important, e.g., for many questions we had to map
low level events (test codes) onto aggregate events (job steps). after doing the
appropriate ﬁltering we were able to demonstrate the applicability of all three
types of process mining: (1) “discovery”, (2) “conformance”, and (3) “exten-
sion” (cf. figure 1). using these three types of process mining, we could answer
all three questions raised in the introduction: (1) “how are the tests actually
executed?”, (2) “how compliant are the actual test executions to the reference
process?”, and (3) “where is the most time spent in the test process?”.
despite the positive results, the case study also showed that the mining
of less structured processes is far from trivial and requires expert knowledge
and perseverance. therefore, using the case study, we discussed limitations of
existing algorithms and presented two new process mining techniques inspired
by the mining of asml’s test processes. both techniques have been implemented
as plug-ins in prom and are subject to further research.
acknowledgements
this research is supported by the technology foundation stw, eit, the eu
project super, and the iop program of the dutch ministry of economic af-
fairs. furthermore, the authors would like to thank all prom developers for their
on-going work on process mining techniques.
references
1. w.m.p. van der aalst, h.t. de beer, and b.f. van dongen. process mining and
veriﬁcation of properties: an approach based on temporal logic. in r. meers-
man and z. tari et al., editors, on the move to meaningful internet systems
2005: coopis, doa, and odbase: otm confederated international conferences,
coopis, doa, and odbase 2005 , volume 3760 of lecture notes in computer sci-
ence, pages 130–147. springer-verlag, berlin, 2005.
2. w.m.p. van der aalst, a.k. alves de medeiros, and a.j.m.m. weijters. genetic
process mining. in g. ciardo and p. darondeau, editors, applications and theory
of petri nets 2005 , volume 3536 of lecture notes in computer science , pages
48–69. springer-verlag, berlin, 2005.
3. w.m.p. van der aalst, h.a. reijers, a.j.m.m. weijters, b.f. van dongen, a.k.
alves de medeiros, m. song, and h.m.w. verbeek. business process mining: an
industrial application. information systems , 32(5):713–732, 2007.
4. w.m.p. van der aalst, b.f. van dongen, j. herbst, l. maruster, g. schimm, and
a.j.m.m. weijters. workﬂow mining: a survey of issues and approaches. data
and knowledge engineering , 47(2):237–267, 2003.
5. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workﬂow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128–1142, 2004.
6. r. agrawal, d. gunopulos, and f. leymann. mining process models from work-
ﬂow logs. in sixth international conference on extending database technology ,
pages 469–483, 1998.35
7. s. amland. risk-based testing: risk analysis fundamentals and metrics for software
testing including a ﬁnancial application case study. journal of systems software ,
53(3):287–295, 2000.
8. asml. website asml nv, veldhoven, the netherlands, www.asml.com, 2007.
9. r. boumen, i.s.m. de jong, j.w.h. vermunt, j.m. van de mortel-fronczak, and
j.e. rooda. a risk-based stopping criterion for test sequencing. internal report
se 420460, eindhoven university of technology, january 2006. submitted to ieee
transactions on systems,man,and cybernetics-part a: systems and humans.
10. r. boumen, i.s.m. de jong, j.w.h. vermunt, j.m. van de mortel-fronczak, and
j.e. rooda. test sequencing in complex manufacturing systems. accepted for
ieee transactions on systems,man,and cybernetics-part a: systems and hu-
mans , 2006.
11. j.e. cook, c. he, and c. ma. measuring behavioral correspondence to a timed
concurrent model. in proceedings of the 2001 international conference on soft-
ware mainenance , pages 332–341, 2001.
12. j.e. cook and a.l. wolf. discovering models of software processes from event-
based data. acm transactions on software engineering and methodology ,
7(3):215–249, 1998.
13. j.e. cook and a.l. wolf. software process validation: quantitatively measuring
the correspondence of a process to a model. acm transactions on software
engineering and methodology , 8(2):147–176, 1999.
14. chipworks corporation. advanced semiconductor manufacturing handbook. tech-
nical report, chipworks corporation, januari 2000.
15. a. datta. automating the discovery of as-is business process models: proba-
bilistic and algorithmic approaches. information systems research , 9(3):275–301,
1998.
16. a.k. alves de medeiros. genetic process mining . phd thesis, department of
technology management, technical university eindhoven, 2006.
17. b. van dongen and w.m.p. van der aalst. multi-phase mining: aggregating
instances graphs into epcs and petri nets. in d. marinescu, editor, proceedings of
the second international workshop on applications of petri nets to coordination,
workﬂow and business process management , pages 35–58. florida international
university, miami, florida, usa, 2005.
18. b. van dongen, a.k. alves de medeiros, h.m.w. verbeek, a.j.m.m. weijters, and
w.m.p. van der aalst. the prom framework: a new era in process mining tool
support. in g. ciardo and p. darondeau, editors, application and theory of petri
nets 2005 , volume 3536 of lecture notes in computer science , pages 444–454.
springer-verlag, berlin, 2005.
19. b.f. van dongen and w.m.p. van der aalst. multi-phase process mining: building
instance graphs. in p. atzeni, w. chu, h. lu, s. zhou, and t.w. ling, editors, in-
ternational conference on conceptual modeling (er 2004) , volume 3288 of lecture
notes in computer science , pages 362–376. springer-verlag, berlin, 2004.
20. d. grigori, f. casati, m. castellanos, u. dayal, m. sayal, and m.c. shan. business
process intelligence. computers in industry , 53(3):321–343, 2004.
21. c.w. g¨ unther and w.m.p. van der aalst. a generic import framework for process
event logs. in j. eder and s. dustdar, editors, business process management
workshops, workshop on business process intelligence (bpi 2006) , volume 4103
oflecture notes in computer science , pages 81–92. springer-verlag, berlin, 2006.
22. m.j. harrold, d. rosenblum, g. rothermel, and e. weyuker. empirical studies
of a prediction model for regression test selection. ieee transactions on software
engineering , 27(3):248–263, 2001.36
23. j. helfman. dotplot patterns: a literal look at pattern languages. tapos ,
2(1):31–41, 1995.
24. j. herbst. a machine learning approach to workﬂow management. in proceedings
11th european conference on machine learning , volume 1810 of lecture notes in
computer science , pages 183–194. springer-verlag, berlin, 2000.
25. ids scheer. aris process performance manager (aris ppm): measure, ana-
lyze and optimize your business process performance (whitepaper). ids scheer,
saarbruecken, gemany, http://www.ids-scheer.com, 2002.
26. british computer society special interest group in software testing. standard
for software component testing . british computer society, 2001.
27. m. zur m¨ uhlen and m. rosemann. workﬂow-based process monitoring and con-
trolling - technical and organizational issues. in r. sprague, editor, proceedings
of the 33rd hawaii international conference on system science (hicss-33) , pages
1–10. ieee computer society press, los alamitos, california, 2000.
28. a. rozinat and w.m.p. van der aalst. conformance testing: measuring the fit
and appropriateness of event logs and process models. in c. bussler et al., editor,
bpm 2005 workshops (workshop on business process intelligence) , volume 3812
oflecture notes in computer science , pages 163–176. springer-verlag, berlin,
2006.
29. m. sayal, f. casati, u. dayal, and m.c. shan. business process cockpit. in pro-
ceedings of 28th international conference on very large data bases (vldb’02) ,
pages 880–883. morgan kaufmann, 2002.
30. a. streit, b. pham, and r. brown. visualisation support for managing large
business process speciﬁcations. in w.m.p. van der aalst, a.h.m. ter hofstede,
and m. weske, editors, international conference on business process management
(bpm 2005) , volume 2678 of lecture notes in computer science , pages 205—219.
springer-verlag, berlin, 2005.
31. a.j.m.m. weijters and w.m.p. van der aalst. rediscovering workﬂow models
from event-based data using little thumb. integrated computer-aided engi-
neering , 10(2):151–162, 2003.