finding suitable activity clusters for
decomposed process discovery
b.f.a. hompes, h.m.w. verbeek, and w.m.p. van der aalst
department of mathematics and computer science
eindhoven university of technology, eindhoven, the netherlands
{b.f.a.hompes,h.m.w.verbeek,w.m.p.v.d.aalst}@tue.nl
abstract. event data can be found in any information system and pro-
vide the starting point for a range of process mining techniques. the
widespread availability of large amounts of event data also creates new
challenges. existing process mining techniques are often unable to handle
\big event data" adequately. decomposed process mining aims to solve
this problem by decomposing the process mining problem into many
smaller problems which can be solved in less time, using less resources,
or even in parallel. many decomposed process mining techniques have
been proposed in literature. analysis shows that even though the de-
composition step takes a relatively small amount of time, it is of key
importance in nding a high-quality process model and for the compu-
tation time required to discover the individual parts. currently there is
no way to assess the quality of a decomposition beforehand. we dene
three quality notions that can be used to assess a decomposition, before
using it to discover a model or check conformance with. we then propose
a decomposition approach that uses these notions and is able to nd a
high-quality decomposition in little time.
keywords: decomposed process mining, decomposed process discovery,
distributed computing, event log
1 introduction
process mining aims to discover, monitor and improve real processes by extract-
ing knowledge from event logs readily available in today's information systems
[1]. in recent years, (business) processes have seen an explosive rise in support-
ing infrastructure, information systems and recorded information, as illustrated
by the term big data. as a result, event logs generated by these information
systems grow bigger and bigger as more event (meta-)data is being recorded and
processes grow in complexity. this poses both opportunities and challenges for
the process mining eld, as more knowledge can be extracted from the recorded
data, increasing the practical relevance and potential economic value of process
mining. traditional process mining approaches however have diculties coping
with this sheer amount of data (i.e. the number of events), as most interesting
algorithms are linear in the size of the event log and exponential in the number
of dierent activities [2].2 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
in order to provide a solution to this problem, techniques for decomposed
process mining [2{4] have been proposed. decomposed process mining aims to
decompose the process mining problem at hand into smaller problems that can be
handled by existing process discovery and conformance checking techniques. the
results for these individual sub-problems can then be combined into solutions for
the original problems. also, these smaller problems can be solved concurrently
with the use of parallel computing. even sequentially solving many smaller prob-
lems can be faster than solving one big problem, due to the exponential nature
of many process mining algorithms [2]. several decomposed process mining tech-
niques have been developed in recent years [2{4, 9, 10, 12, 16]. though existing
approaches have their merits, they lack in generality. in [3], a generic approach
to decomposed process mining is proposed. the proposed approach provides
a framework which can be combined with dierent existing process discovery
and conformance checking techniques. moreover, dierent decompositions can
be used while still providing formal guarantees, e.g. the fraction of perfectly
tting traces is not inuenced by the decomposition.
when decomposing an event log for (decomposed) process mining, several
problems arise. in terms of decomposed process discovery, these problems lie in
the step where the overall event log is decomposed into sublogs, where submod-
els are discovered from these sublogs, and/or where submodels are merged to
form the nal model. even though creating a decomposition is computationally
undemanding, it is of key importance for the remainder of the decomposed pro-
cess discovery process in terms of the overall required processing time and the
quality of the resulting process model.
the problem is that there is currently no clear way of determining the quality
of a given decomposition of the events in an event log, before using that decom-
position to either discover a process model or check conformance with.
the current decomposition approaches do not use any quality notions to
create a decomposition. thus, potential improvements lie in nding such qual-
ity notions and a decomposition approach that uses those notions to create a
decomposition with.
the remainder of this paper is organized as follows. in section 2 related
work is discussed. section 3 introduces necessary preliminary denitions for de-
composed process mining and the generic decomposition approach. section 4
introduces decomposition quality notions to grade a decomposition upon, and
two approaches that create a high quality decomposition according to those no-
tions. section 5 shows a (small) use case. the paper is concluded with views on
future work in section 6.
2 related work
process discovery aims at discovering a process model from an event log while
conformance checking aims at diagnosing the dierences between observed and
modeled behavior (resp. the event log and the model). various discovery algo-
rithms and many dierent modeling formalisms have been proposed in literature.finding suitable activity clusters for decomposed process discovery 3
as very dierent approaches are used, it is impossible to provide a complete
overview of all techniques here. we refer to [1] for an introduction to process
mining and an overview of existing techniques. for an overview of best practices
and challenges, we refer to the process mining manifesto [5]. the goal of this
paper is to improve decomposed process discovery, where challenging discovery
problems are split in many smaller problems which can be solved by existing
discovery techniques.
in the elds of data mining and machine learning many eorts have been
made to improve the scalability of existing techniques. most of these techniques
can be distributed [8, 17], e.g. distributed clustering, distributed classication,
and distributed association rule mining. to support this, several distributed data
processing platforms have been created and are widely used [14, 20]. some exam-
ples are apache hadoop [23], spark [26], flink [21], and tez [19]. specic data
mining and machine learning libraries are available for most of these platforms.
however, these approaches often partition the input data and therefore cannot
be used for the discovery of process models. decomposed process mining aims
to provide a solution to this problem.
little work has been done on the decomposition and distribution of process
mining problems [2{4]. in [18] mapreduce is used to scale event correlation
as a preprocessing step for process mining. more related are graph-partitioning
based approaches. by partitioning a causal dependency graph into partially over-
lapping subgraphs, events are clustered into groups of events that are causally
related. in [11] it is shown that region-based synthesis can be done at the level
of synchronized state machine components (smcs). also a heuristic is given to
partition the causal dependency graph into overlapping sets of events that are
used to construct sets of smcs. other region-based decomposition techniques
are proposed in [9, 10]. however, these techniques are limited to discovering petri
nets from event logs. in [16] the notions of single-entry single-exit (sese) and
rened process structure trees (rpsts) are used to hierarchically partition a
process model and/or event log for decomposed process discovery and confor-
mance checking. in [6], passages are used to decompose process mining problems.
in [3] a dierent (more local) partitioning of the problem is given which,
unlike [11], decouples the decomposition approach from the actual conformance
checking and process discovery approaches. it is indicated that a partitioning of
the activities in the event log can be made based on a causal graph of activities.
it can therefore be used together with any of the existing process discovery
and conformance checking techniques. the approach presented in this paper is
an extension of the approach presented in [3], though we focus on discovery.
where [3] splits the process mining problem at hand into subproblems using a
maximal decomposition of a causal dependency graph, our approach rst aims
to recombine the many created activity clusters into better and fewer clusters,
and only then splits the process mining problem into subproblems. as a result,
fewer subproblems remain to be solved.
the techniques used to recombine clusters are inspired by existing, well-
known software quality metrics and the business process metrics listed in [22],4 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
i.e.cohesion and coupling . more information on the use of software engineering
metrics in a process mining context is described in [22]. however, in this instance,
these metrics are used to measure the quality of the decomposition itself rather
than the process to be discovered. as such, the quality of the decomposition can
be assessed before it is used to distribute the process mining problem.
3 preliminaries
this section introduces the notations needed to dene a better decomposition
approach. a basic understanding of process mining is assumed [1].
3.1 multisets, functions, and sequences
denition 1 (multisets).
multisets are dened as sets where elements may appear multiple times. b(a)
is the set of all multisets over some set a. for some multiset b2b(a), and
elementa2a,b(a)denotes the number of times aappears inb.
for example, take a=fa;b;c;dg:b1= [] denotes the empty multiset, b2= [a;b]
denotes the multiset over awhereb2(c) =b2(d) = 0 andb2(a) =b2(b) = 1,b3=
[a;b;c;d ] denotes the multiset over awhereb3(a) =b3(b) =b3(c) =b3(d) = 1,
b4= [a;b;b;d;a;c ] denotes the multiset over awhereb4(a) =b4(b) = 2 and
b4(c) =b4(d) = 1, andb5= [a2;b2;c;d] =b4. the standard set operators can be
extended to multisets, e.g. a2b2,b5nb2=b3,b2]b3=b4=b5,jb5j= 6
denition 2 (function projection).
letf2x6!ybe a (partial) function and qx.fqdenotes the projection
offonq:dom(fq) =dom(f)\qandfq(x) =f(x)forx2dom(fq).
the projection can be used for multisets. for example, b5fa;bg= [a2;b2].
denition 3 (sequences).
a sequence is dened as an ordering of elements of some set. sequences are used
to represent paths in a graph and traces in an event log. s(a)is the set of all
sequences over some set a.s=ha1;a2;:::;ani2s (a)denotes a sequence s
overaof lengthn. furthermore: s1=hiis the empty sequence and s1s2is
the concatenation of two sequences.
for example, take a=fa;b;c;dg:s1=ha;b;bi,s2=hb;b;c;di,s1s2=ha;b;b;
b;b;c;di
denition 4 (sequence projection).
letabe a set and qaa subset. q2s(a)!s(q)is a projection function
and is dened recursively: (1) hiq=hiand (2) for s2s(a)anda2a:
(hais)q=(
sq ifa =2q
haisqifa2q
soha;a;b;b;c;d;difa;bg=ha;a;b;bi.finding suitable activity clusters for decomposed process discovery 5
3.2 event logs
event logs are the starting point for process mining. they contain information
recorded by the information systems and resources supporting a process. typ-
ically, the executed activities of multiple cases of a process are recorded. note
that only example behavior is recorded, i.e. event logs only contain information
that has been seen. an event log often contains only a fraction of the possible
behavior [1]. a trace describes one specic instance (i.e. one case) of the process
at hand, in terms of the executed activities. an event log is a multiset of traces,
since there can be multiple cases having the same trace. for the remainder of
this paper, we let uabe some universe of activities.
denition 5 (event log).
letauabe a set of activities. a trace s2s(a)is a sequence of activities.
letl2b(s(a))be a multiset of traces over a.lis an event log over a.
an example event log is l1= [ha;b;c;di5;ha;b;b;c;di2;ha;c;di3]. there are three
unique traces in l1, and it contains information about a total of 10 cases. there
are 45+52+33 = 39 events in total. the projection can be used for event logs
as well. that is, for some log l2b(s(a)) and setqa:lq= [sqjs2l].
for example l1fa;b;cg= [ha;b;ci5;ha;b;b;ci2;ha;ci3]. we will refer to these
projected event logs as sublogs .
3.3 activity matrices, graphs, and clusters
in [3] dierent steps for a generic decomposed process mining approach have
been outlined. we decompose the overall event log based on a causal graph of
activities. this section describes the necessary denitions for this decomposition
method.
denition 6 (causal activity matrix).
letauabe a set of activities. m(a) = (aa)![ 1:0;1:0]denotes the set
of causal activity matrices over a. fora1;a22aandm2m (a),m(a1;a2)
denotes the \causal relation strength" from a1toa2.
a value close to 1 :0 signies that we are quite condent there exists a causal
relation (e.g. directly follows relation) between two activities while a value close
to 1:0 signies that we are quite sure there is no relation. a value close to
0:0 indicates uncertainty, i.e., there may be a relation, but there is no strong
evidence for it.
for example, table 1 shows an example causal activity matrix for the event
logl1. it shows that we are condent that casual relations exists from atob,
fromatoc, frombtoc, and from ctod, that we are uncertain about a causal
relation from btob, and that we are condent that other causal relations do not
exist.6 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
table 1: example causal activity matrix m1for event log l1.
fromnto a b c d
a -0.46 0.88 0.75 -1.00
b -1.00 0.00 0.88 -1.00
c -1.00 -1.00 -0.90 1.00
d -1.00 -1.00 -1.00 -0.67
denition 7 (causal activity graph).
letauabe a set of activities. g(a)denotes the set of causal activity graphs
overa. a causal activity graph g2g(a)is a 3-tuple g= (v;e;w )whereva
is the set of nodes, e(vv)is the set of edges, and w2e!(0:0;1:0]
is a weight function that maps every edge onto a positive weight. g= (v;e;
w)2 g(a)is the causal activity graph based on m2 m (a)and a specic
causality threshold 2[ 1:0;1:0)i
e=f(a1;a2)2aajm(a1;a2)>g,
v=s
(a1;a2)2efa1;a2g, and
w((a1;a2)) =m(a1;a2) 
1 for(a1;a2)2e.
that is, for every pair of activities (a1;a2)2a, there's an edge with a positive
weight from a1toa2ingi the value for a1toa2in the causal activity matrix
mexceeds some threshold . note that vasince some activities in amight
not be represented in g.
fig. 1: example causal activity graph g1for causal activity matrix m1.
for example, figure 1 shows the causal activity graph that was obtained from
the causal activity matrix m1using= 0.
denition 8 (activity clustering).
letauabe a set of activities. c(a)denotes the set of activity clusters over
a. an activity cluster c2c(a)is a subset of a, that is,ca.bc(a)denotes
the set of activity clusterings over a. an activity clustering bc2bc(a)is a
set of activity clusters, that is, bcp (a)1. ak-clusteringbc2bc(a)is a
1p(a) denotes the powerset over afinding suitable activity clusters for decomposed process discovery 7
clustering with size k, i.e.jbcj=k. the number of activities in bcis denoted by
jjbcjj=js
c2bccj, i.e.jjbcjjsignies the number of unique activities in bc.
fig. 2: example activity clustering bc1for causal activity graph g1.
for example, figure 2 shows the activity clustering bc1with size 2 for the causal
activity graph g1. cluster 0 contains the activities c(as input) and d(as output),
whereas cluster 1 contains a(as input), b(as input and as output), and c(as
output). note that the inputs and output are not part of the denition, but they
are included here to better illustrate the fabric of the clusters. also note that
jjbc1jj=jfa;b;cg[fc;dgj=jfa;b;c;dgj= 4.
3.4 process models and process discovery
process discovery aims at discovering a model from an event log while confor-
mance checking aims at diagnosing the dierences between observed and modeled
behavior (resp. the event log and the model). various discovery algorithms have
been proposed in literature. literature suggests many dierent notations for
models. we abstract from any specic model notation, but will dene the set of
algorithms that discover a model from an event log. these discovery algorithms
are often called mining algorithms , orminers in short.
denition 9 (process model).
letauabe a set of activities. n(a)denotes the set of process models over
a, irrespective of the specic notation (petri nets, transition systems, bpmn,
uml asds, etc.) used.
denition 10 (discovery algorithm).
letauabe a set of activities. d(a) =b(s(a))!n (a)denotes the set
of discovery algorithms over a. a discovery algorithm d2d(a)discovers a
process model over afrom an event log over a.
for example, figure 3 shows the petri net n1which was discovered from the
event logl1using the \ilp-based process discovery" algorithm2.
2this algorithm is available in prom 6.5, see subsection 4.3.8 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
fig. 3: example petri net n1discovered from the event log l1.
3.5 decomposed process discovery
as discussed, in [3], a general approach to decomposed process mining is pro-
posed. in terms of decomposed process discovery, this approach can be explained
as follows: let auabe a set of activities, and let l2b(s(a)) be an event log
overa. in order to decompose the activities in l, rst a causal activity matrix
m2m (a) is discovered (cf. table 1). any causal activity matrix discovery
algorithmdca2b(s(a))!m (a) can be used. from ma causal activity
graphg2g(a) isltered (using a specic causality threshold, cf. figure 1). by
choosing the value of the causality threshold carefully, we can lter out uncom-
mon causal relations between activities or relations of which we are unsure, for
example those relations introduced by noise in the event log.
once the causal activity graph ghas been constructed, an activity clus-
teringbc2bc(a) is created (cf. figure 2). any activity clustering algorithm
ac2 g(a)!bc(a) can be used to create the clusters. eectively, an ac-
tivity clustering algorithm partitions the causal activity graph into partially
overlapping clusters of activities. many graph partitioning algorithms have been
proposed in literature [7]. most algorithms however partition the graph into
non-overlapping subgraphs, while in our case some overlap is required in order
to merge submodels later on in the process. in [3], the so-called maximal decom-
position is used, where the causal activity graph is cut across its vertices and
each edge ends up in precisely one submodel, according to the method proposed
in [15]. this leads to the smallest possible submodels.
next, for every cluster in the clustering, lisltered to a corresponding
sublog by projecting the cluster to l, i.e., for all c2bca subloglcis created.
for example, based on the activity cluster array bc1(cf. figure 2), the event
logl1would be split into sublogs l1fa;b;cgandl1fc;dg, referring to clusters
1 and 0 respectively. a process model is discovered for each sublog. these are
the submodels. any discovery algorithm d2d(a) can be used to discover the
submodels. for example, figure 4 shows the submodels that may be discovered
from the sublogs of l1as mentioned above.
finally, the submodels are merged into an overall model (cf. figure 3). any
merging algorithm in b(n(a))!n (a) can be used for this step. currently,
submodels are merged based on activity labels. note that we have jbcjclusters,
sublogs and submodels, and jjbcjjactivities in the nal, merged model. figure 5
shows the general decomposed process discovery workow.finding suitable activity clusters for decomposed process discovery 9
fig. 4: submodels discovered for the sublogs of l1.
this workow claries the generality of the approach. the necessary steps
in the approach are dened by their input and output, but any applicable algo-
rithm can be used to perform each step. this also shows the strength of a generic
decomposed process mining approach. it might be of interest to only discover a
small fragment of a particular process, e.g. only show the part of a medical pro-
cess where repeated treatment is necessary, or only show the activities (events)
performed by some selected resource (provided resource-data is stored in the
log). the same holds for decomposed conformance checking. clustering the ac-
tivities such that the activities belonging to the interesting part of a process
are clustered together will lead to a \ltered" sublog. applying a fast discovery
algorithm to non-interesting parts of the process or only discovering or checking
conformance of the submodel of interest can greatly reduce calculation time.
fig. 5: the general decomposed process discovery workow, using a more com-
plex event log l2as an example. finding a suitable activity clustering is of key
importance.10 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
4 a better decomposition
it is apparent that the manner in which activities are clustered has a substantial
eect on required processing time, and it is possible for similarly sized clusterings
(in the average cluster size) to lead to very dierent total processing times. as
a result of the vertex-cut ( maximal ) decomposition approach [3], most activities
will be in two (or more) activity clusters, leading to double (or more) work, as the
clusters have a lot of overlap and causal relations between them, which might not
be desirable. from the analysis results in [13] we can see that this introduces a
lot of unwanted overhead, and generally reduces model quality. also, sequences
or sets of activities with high causal relations are generally easily (and thus
quickly) discovered by process discovery algorithms, yet the approach will often
split up these activities over dierent clusters.
model quality can potentially suer from a decomposition that is too ne-
grained. it might be that the sublogs created by the approach contain too little
information for the process discovery algorithm to discover a good, high quality
submodel from, or that a process is split up where it shouldn't be. merging these
low-quality submodels introduces additional problems.
in order to achieve a high quality process model in little time, we have to
nd a decomposition that produces high quality submodels with as little over-
lap as possible (shared activities) and causal relations between them. also, the
submodels should preferably be of comparable size, because of the exponential
nature of most process discovery algorithms [2]. hence, a good decomposition
should (1) maximize the causal relations between the activities within each clus-
ter in the activity clustering, (2) minimize the causal relations and overlap across
the clusters and (3) have approximately equally sized clusters. the challenge lies
in nding a good balance between these three clustering properties. in subsec-
tion 4.1, we formally dene these properties and provide metrics in order to be
able to asses the quality of a given decomposition before using it to discover a
model or check conformance with.
a clustering where one cluster is a subset of another cluster is not valid as
it would lead to double work, and would thus result in an increase in required
processing time without increasing (or even decreasing) model quality. note
that this denition of a valid clustering allows for disconnected clusters, and
that some activities might not be in any cluster. this is acceptable as processes
might consist of disconnected parts and event logs may contain noise. however,
if activities are left out some special processing might be required.
denition 11 (valid clustering).
letauabe a set of activities. let bc2bc(a)be a clustering over a.bcis a
valid clustering i: bc6=;^8c1;c22bc^c16=c2c16c2.bcv(a)denotes the set of
valid clusterings over a.
for example, the clustering shown in figure 2 is valid, as fa;b;cg6fc;dgand
fc;dg6fa;b;cg.finding suitable activity clusters for decomposed process discovery 11
4.1 clustering properties
we dene decomposition quality notions in terms of clustering properties.
the rst clustering property we dene is cohesion . the cohesion of an ac-
tivity clustering is dened as the average cohesion of each activity cluster in
that clustering. a clustering with good cohesion (cohesion 1) signies that
causal relations between activities in the same cluster are optimized, whereas
bad cohesion (cohesion 0) signies that activities with few causal relations are
clustered together.
denition 12 (cohesion).
letauabe a set of activities. let g= (v;e;w )2g(a)be a causal activity
graph over a, and letbc2bcv(a)be a valid clustering over a. the cohesion of
clusteringbcin graphg, denotedcohesion (bc;g), is dened as follows:
cohesion (bc;g) =p
c2bccohesion (c;g)
jbcj
cohesion (c;g) =p
(a1;a2)2e\(cc)w((a1;a2))
jccj
for example:
cohesion (fa;b;cg;g1) = (0:88 + 0:75 + 0:88)=9 = 0:28,
cohesion (fc;dg;g1) = 1:00=4 = 0:25, and
cohesion (bc1;g1) = (0:28 + 0:25)=2 = 0:26.
the second clustering property is called coupling , and is also represented
by a number between 0 and 1. good coupling (coupling 1) signies that
causal relations between activities across clusters are minimized. bad coupling
(coupling0) signies that there are a lot of causal relations between activities
in dierent clusters.
denition 13 (coupling).
letauabe a set of activities. let g= (v;e;w )2g(a)be a causal activity
graph over a, and letbc2bcv(a)be a valid clustering over a. the coupling of
clusteringbcin graphg, denotedcoupling (bc;g), is dened as follows:
coupling (bc;g) =8
<
:1 ifjbcj1
1 p
c1;c22bc^c16=c2coupling (c1;c2;g)
jbcj(jbcj 1)ifjbcj>1
coupling (c1;c2;g) =p
(a1;a2)2e\((c1c2)[(c2c1))w((a1;a2))
2jc1c2j12 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
for example:
coupling (fa;b;cg;fc;dg;g1) = (0:75 + 0:88 + 1:00)=12 = 0:22,
coupling (fc;dg;fa;b;cg;g1) = (0:75 + 0:88 + 1:00)=12 = 0:22, and
coupling (bc1;g1) = 1 (0:22 + 0:22)=2 = 0:78.
note that the weights of the causal relations are used in the calculation of
cohesion and coupling. relations of which we are not completely sure of (or that
are weak) therefore have less eect on these properties than stronger ones.
the balance of an activity clustering is the third property. a clustering with
good balance has clusters of (about) the same size. like cohesion and coupling,
balance is also represented by a number between 0 and 1, where a good balance
(balance1) signies that all clusters are about the same size and a bad balance
(balance0) signies that the cluster sizes dier quite a lot. decomposing
the activities into clusters with low balance (e.g. a k-clustering with one big
cluster holding almost all of the activities and ( k 1) clusters with only a few
activities) will not speed up discovery or conformance checking, rendering the
whole decomposition approach useless. at the same time nding a clustering
with perfect balance (all clusters have the same size) will most likely split up
the process / log in places that \shouldn't be split up", as processes generally
consist out of dierent-sized natural parts.
this balance formula utilizes the standard deviation of the sizes of the clusters
in a clustering to include the magnitude of the dierences in cluster sizes. a
variation of this formula using squared errors or deviations could also be used
as a clustering balance measure.
denition 14 (balance).
letauabe a set of activities. let bc2bcv(a)be a valid clustering over a.
the balance of clustering bcdenotedbalance (bc)is dened as follows:
balance (bc) = 1 2(bc)
jjbcjj
where(bc)signies the standard deviation of the sizes of the clusters in the
clusteringbc.
for example, balance (bc1) = 1 (20:5)=2 = 0:5.
in order to assess a certain decomposition based on the clustering properties,
we use a weighted scoring function, which grades an activity clustering with a
score between 0 (bad clustering) and 1 (good clustering). a weight can be set
for each clustering property, depending on their relative importance. a cluster-
ing with clustering score 1 therefore has perfect cohesion, coupling and balance
scores, on the set weighing of properties.finding suitable activity clusters for decomposed process discovery 13
denition 15 (clustering score).
letauabe a set of activities. let g2g(a)be a causal activity graph over
a, and letbc2bcv(a)be a valid clustering over a. the clustering score (score)
of clusteringbcin graphg, denotedscore (bc;g), is dened as follows:
score (bc;g) =cohesion (bc;g)cohw
cohw+couw+balw
+coupling (bc;g)couw
cohw+couw+balw
+balance (bc)balw
cohw+couw+balw
wherecohw,couw, andbalware the weights for cohesion, coupling, and
balance.
for example, if we take all weights to be 10, that is, cohw=couw=balw=
10, thenscore (bc1;g1) = 0:26(10=30) + 0:78(10=30) + 0:5(10=30) = 0:51.
4.2 recomposition of activity clusters
as described in subsection 3.5, creating a good activity clustering is essentially
a graph partitioning problem. the causal activity graph needs to be partitioned
in parts that have (1) good cohesion, (2) good coupling and (3) good balance.
the existing maximal decomposition approach [3] often leads to a decomposition
that is too decomposed, i.e. too ne-grained. cohesion and balance of cluster-
ings found by this approach are usually quite good, since all clusters consist of
only a few related activities. however, coupling is inherently bad, since there's a
lot of overlap in the activity clusters and there are many causal relations across
clusters. this decomposition approach leads to unnecessary and unwanted over-
head and potential decreased model quality. we thus want to nd a possibly
non-maximal decomposition which optimizes the three clustering properties.
instead of applying or creating a dierent graph partitioning algorithm, we
recompose the activity clusters obtained by the vertex-cut decomposition, since it
is maximal (no smaller valid clustering exists [3]). the idea is that it is possible
to create a clustering that has fewer, larger clusters, requiring less processing
time to discover the nal model, because overhead as well as cluster overlap are
reduced. additionally, model quality is likely to increase because of the higher
number of activities in each cluster and the lower coupling between clusters.
for example, if we put all activities from event log l1in a single cluster
fa;b;c;dg, yielding clustering bc0
1, and use the same weights, then we would get
the following scores:
cohesion (bc0
1;g1) = (0:88 + 0:75 + 0:88 + 1:00)=16 = 0:22,
coupling (bc0
1;g1) = 1,
balance (bc0
1) = 1, and
score (bc0
1;g1) = 0:22(10=30) + 1(10=30) + 1(10=30) = 0:74.14 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
clearly, as 0 :74>0:51, the chosen weights would lead to the situation where a
single clusterfa;b;c;dgwould be preferred over two clusters fa;b;cgandfc;dg.
there are often many ways in which a clustering can be recomposed to the
desired amount of clusters, as shown in figure 6. we are interested in the high-
est quality clustering of the desired size k, i.e. thek-clustering that has the
best cohesion, coupling and balance properties. a k-clustering that has a high
clustering score will very likely lead to such a decomposition.
in order to nd a good decomposition in the form of a high-scoring clustering
quickly, we propose two agglomerative hierarchical recomposition approaches,
which iteratively merge clusters, reducing the size of the clustering by one each
iteration. as the amount of k-clusterings for a given causal activity graph is
nite, it is possible to exhaustively nd the best k-clustering. however, for even
moderately-sized event logs (in the number of activities) this is too resource-
and time-consuming, as shown in [13]. also, a semi-exhaustive \random" recom-
position approach was implemented that randomly recomposes the clustering to
kclusters a given amount of times and returns the highest-scoring k-clustering
found. this method is used as a benchmark for the hierarchical recomposition
approaches.
proximity-based approach we propose an hierarchical recomposition ap-
proach based on proximity between activity clusters, where cluster coupling is
used as the proximity measure (algorithm 1). the starting point is the clustering
as created by the vertex-cut approach. we repeatedly merge the clusters closest
to one another (i.e. the pair of clusters with the highest coupling) until we end
up with the desired amount of clusters ( k). after the k-clustering is found, it is
made valid by removing any clusters that are a subcluster of another cluster, if
such clusters exist. it is therefore possible that the algorithm returns a clustering
with size smaller than k.
by merging clusters we are likely to lower the overall cohesion of the cluster-
ing. this drawback is minimized, as coupling is used as the distance measure.
coupling is also minimized. the proximity-based hierarchical recomposition ap-
proach however is less favored towards the balance property, as it is possible
that -because of high coupling between clusters- two of the larger clusters are
merged. in most processes however, coupling between two \original" clusters will
be higher than coupling between \merged" clusters. if not, the two clusters cor-
respond to parts of the process which are more dicult to split up (e.g. a loop,
a subprocess with many interactions and/or possible paths between activities,
etc.). model quality is therefore also likely to increase by merging these clusters,
as process discovery algorithms don't have to deal with missing activities, or
incorrect causal relations introduced in the corresponding sublogs. a possible
downside is that as the clustering might be less balanced, processing time can
be slightly higher in comparison with a perfectly-balanced decomposition.
for example, figure 7 shows the 2-clustering that is obtained by this ap-
proach when we start from the 4-clustering for event log l2shown earlier. cluster
0 is merged with cluster 3, and cluster 1 with cluster 2.finding suitable activity clusters for decomposed process discovery 15
fig. 6: 2 possible recompositions for event log l2(cf. figure 5) from 4 (top)
to 2 clusters (bottom left and right). finding a good recomposition is key to
creating a coarser clustering which could potentially decrease processing time
and increase model quality.
score-based approach we propose a second hierarchical recomposition algo-
rithm that uses the scoring function in a look-ahead fashion (algorithm 2). in
essence, this algorithm, like the proximity-based variant, iteratively merges two
clusters into one. for each combination of clusters, the score of the clustering that
results from merging those clusters is calculated. the clustering with the highest
score is used for the next step. the algorithm is nished when a k-clustering is
reached. like in the proximity-based approach, after the k-clustering is found, it
is made valid by removing any clusters that are a subcluster of another cluster,
if such clusters exist.
the advantage of this approach is that specic (combinations of) clustering
properties can be given priority, by setting their scoring weight(s) accordingly.
for example, it is possible to distribute the activities over the clusters near
perfectly, by choosing a high relative weight for balance. this would likely lead
to a lower overall processing time. however, it might lead to natural parts of the16 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
algorithm 1: proximity-based agglomerative hierarchical recomposition
input :bc2bcv(a),g2g(a),k2[1;jbcj]
output :bc02bcv(a),jbc0jk
result : the clustering bc0recomposed from bc, into maximal kclusters.
begin
bc02bc(a)  bc
whilejbc0j> kdo
highestcoupling  0
ca2bc(a)  ;
cb2bc(a)  ;
foreach c12c0do
foreach c22c0nfc1gdo
ifcoupling (c1; c2; g)> highestcoupling then
highestcoupling  coupling (c1; c2; g)
ca  c1
cb  c2
bc0  bc0nfca; cbgsfca[cbg
bc002bc(a)  bc0
foreach c2bc00nfca[cbgdo
ifcca[cbthen
bc0  bc0nfcg
return bc0
process being split over multiple clusters, which could negatively aect model
quality. a downside of this algorithm is that, as the algorithm only looks ahead
one step, it is possible that a choice is made that ultimately leads to a lower
clustering score, as that choice cannot be undone in following steps.
for example, figure 8 shows the 2-clustering that is obtained by this ap-
proach when we start from the 4-clustering for event log l2shown earlier where
all weights have been set to 10. cluster 0 is now merged with cluster 1, and
cluster 2 with cluster 3.
4.3 implementation
all concepts and algorithms introduced in this paper are implemented in release
6.5 of the process mining toolkit prom3[24], developed at the eindhoven uni-
versity of technology. all work can be found in the activityclusterarraycreator
package, which is part of the divideandconquer package suite. this suite is
installed by default in prom 6.5.
3prom 6.5 can be downloaded from http://www.promtools.orgfinding suitable activity clusters for decomposed process discovery 17
fig. 7: best 2-clustering found when starting from the 4-clustering shown at the
top of figure 6 using the proximity-based approach.
fig. 8: best 2-clustering found when starting from the 4-clustering shown at the
top of figure 6 using the score-based approach with all weights set to 10.
the plug-in to use in prom is modify clusters , which takes an activity cluster
array (clustering) and a causal activity graph as input. the user can decide to
either set the parameters of the action using a dialog, or to use the default
parameter values. figure 9 shows both options, where the top one will use the
dialog and the bottom one the default parameter values. figure 10 shows the
dialog. at the top, the user can select which approach to use:
brute force:
use a brute force approach to nd the optimal k-clustering.
incremental using best coupling:
use the proximity-based approach to nd a k-clustering.18 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
algorithm 2: score-based agglomerative hierarchical recomposition
input :bc2bcv(a),g2g(a),k2[1;jbcj]
output :bc02bcv(a),jbc0jk
result : the clustering bc0recomposed from bc, into maximal kclusters.
begin
bc02bc(a)  bc
whilejbc0j> kdo
highestscore  0
foreach c12bc0do
foreach c22bc0nfc1gdo
bc00  bc0nfc1; c2gsfc1[c2g
ifscore (bc00; g)> highestscore then
highestscore  score (bc00; g)
bc0002bc(a)  bc00
bc0  bc000
bc0  bc0nfca; cbgsfca[cbg
foreach c12bc00do
foreach c22bc00nfc1gdo
ifc1c2then
bc0  bc0nfc1g
return bc0
incremental using best coupling (only overlapping clusters):
use the proximity-based approach to nd a k-clustering, but allow only to
merge clusters that actually overlap (have an activity in common). this is
the default approach.
incremental using best score:
use the score-based approach to nd a k-clustering.
incremental using best score (only overlapping clusters):
use the score-based approach to nd a k-clustering, but allow only to merge
clusters that actually overlap (have an activity in common).
random:
randomly merge clusters until a k-clustering is reached.
below, the user can set the value of k, which is set to 50% of the number
of existing clusters by default. at the bottom, the user can set the respective
weights to a value from 0 to 100. by default, all weights are set to 100.finding suitable activity clusters for decomposed process discovery 19
fig. 9: the modify clusters actions in prom 6.5.
fig. 10: the dialog that allows the user to set parameter values.20 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
5 use case
the proposed recomposition techniques are tested using event logs of dierent
sizes and properties. results for an event log consisting of 33 unique activities,
and 1000 traces are shown in this section. for this test the ilp miner process
discovery algorithm was used [25]. discovering a model directly for this log will
lead to a high quality model, but takes 25 minutes on a modern quad-core sys-
tem [13]. the vertex-cut decomposed process mining approach is able to discover
a model in roughly 90 seconds, however the resulting model suers from discon-
nected activities (i.e. a partitioned model). the goal is thus to nd a balance
between processing times and model quality.
we are interested in the clustering scores of each algorithm when recompos-
ing the clustering created by the vertex-cut approach to a smaller size. exhaus-
tively nding the best possible clustering proved to be too time- and resource-
consuming. therefore, besides the two approaches listed here, a random recom-
position approach was used which recomposes clusters randomly one million
times. the highest found score is shown as to give an idea of what the best
possible clustering might be. equal weights were used for the three clustering
properties in order to compute the clustering scores. all clustering scores are
shown in figure 11. as can be seen, the vertex-cut approach creates 22 clusters.
we can see that all algorithms perform very similarly in terms of clustering score.
only for very small clustering sizes the proximity-based approach performs worse
than the other approaches, due to its tendency to create unbalanced clusters.
besides clustering scores, we are even more interested in how each decompo-
sition method performs in terms of required processing time and quality of the
resulting process model. in figure 12 we can see that decomposing the event log
drastically reduces processing times. for an event log this size, the decomposi-
tion steps relatively takes up negligible time (see base of bars in gure), as most
time is spent discovering the submodels (light blue bars). processing times are
reduced exponentially (as expected), until a certain optimum decomposition (in
terms of speed) is reached, after which overhead starts to increase time linearly
again.
we have included two process models (petri nets) discovered from the event
log. figure 14 shows the model discovered when using the vertex-cut decomposi-
tion. figure 15 shows the model discovered when using the clustering recomposed
to 11 clusters with the proximity-based agglomerative hierarchical approach. we
can see that in figure 14, activity \10" is disconnected (marked blue). in fig-
ure 15, this activity is connected, and a structure (self-loop) is discovered. we
can also see that activities \9" and \12" now are connected to more activi-
ties. this shows that the vertex-cut decomposition sometimes splits up related
activities, which leads to a lower quality model. indeed, figure 13 shows that
the activities \9", \10", and \12" were split over two clusters in the vertex-cut
decomposition (top), and were regrouped (bottom) by our recomposition. by re-
composing the clusters we rediscover these relations, leading to a higher quality
model. processing times for these two models are comparable, as can be seen in
figure 12.finding suitable activity clusters for decomposed process discovery 21
0,50,550,60,650,70,75
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22clustering score
# clustersclustering scores
agglomerative hierarchical w/ proximity
 agglomerative hierarchical w/ scoring
 random (max)
fig. 11: clustering score per recomposition algorithm.
01234
0100200300400500600700800900
2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22
partitions in modeltime in seconds
recomp osition method -# clusterstime per step & p artitions in mode l -ilp m iner discov ery algorithm
matrix graph clustering sublogs submodels merge partition s in modelagglomerative hierarchical w/ proximity max. decomp. agglomerative hierarchical w/ scoring
fig. 12: time per step & partitions in model using the agglomerative hierarchi-
cal recomposition approaches and the ilp miner process discovery algorithm.22 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
fig. 13: activities that were split over multiple clusters by the vertex-cut decom-
position (top), that were regrouped by our recomposition (bottom).
the proposed agglomerative hierarchical recomposition algorithms are able
to create activity clusterings that have good cohesion ,coupling andbalance prop-
erties in very little time. often times the scores of these clusterings are almost
as good as the scores of clusterings created by the much slower exhaustive ap-
proaches [13]. the results show that by creating such good clusterings, indeed a
better decomposition can be made. because cluster overlap and the amount of
clusters are reduced, overhead can be minimized, reducing required processing
time. this results in a comparable total required processing time from event log
to process model, even though an extra step is necessary. the larger the event
log, the higher the time gain. by recomposing the clusters, processes are split
up less in places where they shouldn't be, leading to better model quality. be-
cause the discovered submodels are of higher quality and have less overlap and
coupling, the merging step introduces less or no unnecessary (implicit) or double
paths in a model, which leads to improvements in precision, generalization and
simplicity of the nal model.finding suitable activity clusters for decomposed process discovery 23
fig. 14: process model discovered using the vertex-cut decomposition. some ac-
tivities are disconnected in the nal model.24 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
fig. 15: process model discovered using the vertex-cut clustering recomposed to
11 clusters. previously disconnected activities are connected again, improving
model quality.finding suitable activity clusters for decomposed process discovery 25
6 conclusions and future work
in decomposed process discovery, large event logs are decomposed by somehow
clustering their events (activities), and there are many ways these activity clus-
terings can be made. hence, good quality notions are necessary to be able to
assess the quality of a decomposition before starting the time-consuming actual
discovery algorithm. being able to nd a high-quality decomposition plays a key
role in the success of decomposed process mining, even though the decomposition
step takes relatively very little time.
by using a better decomposition, less problems arise when discovering sub-
models for sublogs and when merging submodels into the overal process model.
we introduced three quality notions in the form of clustering properties: cohe-
sion,coupling and balance . it was shown that nding a non-maximal decom-
position can potentially lead to a decrease in required processing time while
maintaining or even improving model quality, compared to the existing vertex-
cutmaximal decomposition approach. we have proposed two variants of an
agglomerative hierarchical recomposition technique, which are able to create a
high-quality decomposition for any given size, in very little time.
even though the scope was limited to decomposed process discovery, the
introduced quality notions and decomposition approaches can be applied to
decomposed conformance checking as well. however, more work is needed to
incorporate them in a conformance checking environment.
besides nding a better decomposition, we believe improvements can be
gained in nding a better, more elaborate algorithm to merge submodels into the
overal process model. by simply merging submodels based on activity labels it is
likely that implicit paths are introduced. model quality in terms of tness, sim-
plicity, generality or precision could suer. an additional post-processing step
(potentially using causal relations) could also solve this issue.
even though most interesting process discovery algorithms are exponential
in the number of dierent activities, adding an infrequent or almost unrelated
activity to a cluster might not increase computation time for that cluster as much
as adding a frequent or highly related one. therefore, besides weighing causal
relations between activities in the causal activity matrix, activities themselves
might be weighted as well. frequency and connectedness are some of the many
possible properties that can be used as weights. it might be possible that one
part of a process can be discovered easily by a simple algorithm whereas another,
more complex part of the process needs a more involved discovery algorithm to
be modeled correctly. further improvements in terms of processing time can be
gained by somehow detecting the complexity of a single submodel in a sublog,
and choosing an adequate discovery algorithm.
finally, as discussed, the proposed recomposition algorithms expect the de-
sired amount of clusters to be given. even though the algorithms were shown to
provide good results for any chosen number, the approach would benet from
some method that determines a tting clustering size for a given event log. this
would also mean one less potentially uncertain step for the end-user.26 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
references
[1] van der aalst, w.m.p.: process mining: discovery, conformance and en-
hancement of business processes. springer, berlin (2011) 1, 3, 4, 5
[2] van der aalst, w.m.p.: distributed process discovery and conformance
checking. in: de lara, j., zisman, a. (eds.) fase. lecture notes in com-
puter science, vol. 7212, pp. 1{25. springer (2012) 1, 2, 3, 10
[3] van der aalst, w.m.p.: a general divide and conquer approach for process
mining. in: computer science and information systems (fedcsis), 2013
federated conference on. pp. 1{10. ieee (2013) 2, 3, 5, 8, 10, 13
[4] van der aalst, w.m.p.: decomposing petri nets for process mining: a
generic approach. distributed and parallel databases 31(4), 471{507 (2013)
2, 3
[5] van der aalst, w.m.p., adriansyah, a., de medeiros, a.k.a., arcieri, f.,
baier, t., blickle, t., bose, r.p.j.c., van den brand, p., brandtjen, r.,
buijs, j.c.a.m., et al.: process mining manifesto. in: business process man-
agement workshops. pp. 169{194. springer (2012) 3
[6] van der aalst, w.m.p., verbeek, h.m.w.: process discovery and confor-
mance checking using passages. fundamenta informaticae 131(1), 103{138
(2014) 3
[7] bulu c, a., meyerhenke, h., safro, i., sanders, p., schulz, c.: recent ad-
vances in graph partitioning. corr abs/1311.3144 (2013), http://arxiv.
org/abs/1311.3144 8
[8] cannataro, m., congiusta, a., pugliese, a., talia, d., truno, p.: dis-
tributed data mining on grids: services, tools, and applications. systems,
man, and cybernetics, part b: cybernetics, ieee transactions on 34(6),
2451{2465 (2004) 3
[9] carmona, j.: projection approaches to process mining using region-based
techniques. data min. knowl. discov. 24(1), 218{246 (2012), http://dblp.
uni-trier.de/db/journals/datamine/datamine24.html 2, 3
[10] carmona, j., cortadella, j., kishinevsky, m.: a region-based algorithm for
discovering petri nets from event logs. in: business process management
(bpm2008). pp. 358{373. springer (2008) 2, 3
[11] carmona, j., cortadella, j., kishinevsky, m.: divide-and-conquer strategies
for process mining. in: business process management, pp. 327{343. springer
(2009) 3
[12] goedertier, s., martens, d., vanthienen, j., baesens, b.: robust process
discovery with articial negative events. journal of machine learning
research 10, 1305{1340 (2009) 2
[13] hompes, b.f.a.: on decomposed process mining: how to solve a jigsaw
puzzle with friends. master's thesis, eindhoven university of technology,
eindhoven, the netherlands (2014), http://repository.tue.nl/776743
10, 14, 20, 22
[14] kambatla, k., kollias, g., kumar, v., grama, a.: trends in big data an-
alytics. journal of parallel and distributed computing 74(7), 2561{2573
(2014) 3finding suitable activity clusters for decomposed process discovery 27
[15] kim, m., candan, k.: sbv-cut: vertex-cut based graph partitioning using
structural balance vertices. data & knowledge engineering 72, 285{303
(2012) 8
[16] munoz-gama, j., carmona, j., van der aalst, w.m.p.: single-entry single-
exit decomposed conformance checking. information systems 46, 102{122
(2014), http://dx.doi.org/10.1016/j.is.2014.04.003 2, 3
[17] park, b.h., kargupta, h.: distributed data mining: algorithms, systems,
and applications (2002) 3
[18] reguieg, h., toumani, f., motahari-nezhad, h.r., benatallah, b.: using
mapreduce to scale events correlation discovery for business processes min-
ing. in: business process management, pp. 279{284. springer (2012) 3
[19] saha, b., shah, h., seth, s., vijayaraghavan, g., murthy, a., curino, c.:
apache tez: a unifying framework for modeling and building data process-
ing applications. in: proceedings of the 2015 acm sigmod international
conference on management of data. pp. 1357{1369. acm (2015) 3
[20] shukla, r.k., pandey, p., kumar, v.: big data frameworks: at a glance
(2015) 3
[21] the apache software foundation: apache flink: scalable batch and stream
data processing. http://flink.apache.org/ (7 2015), (website) 3
[22] vanderfeesten, i.t.p.: product-based design and support of workow pro-
cesses (2009) 3, 4
[23] vavilapalli, v.k., murthy, a.c., douglas, c., agarwal, s., konar, m.,
evans, r., graves, t., lowe, j., shah, h., seth, s., et al.: apache hadoop
yarn: yet another resource negotiator. in: proceedings of the 4th annual
symposium on cloud computing. p. 5. acm (2013) 3
[24] verbeek, h.m.w., buijs, j.c.a.m., van dongen, b.f., van der aalst,
w.m.p.: prom 6: the process mining toolkit. in: proc. of bpm demon-
stration track 2010. vol. 615, pp. 34{39. ceur-ws.org (2010), http:
//ceur-ws.org/vol-615/paper13.pdf 16
[25] van der werf, j.m.e.m., van dongen, b.f., hurkens, c.a.j., serebrenik,
a.: process discovery using integer linear programming. in: applications
and theory of petri nets, pp. 368{387. springer (2008) 20
[26] zaharia, m., chowdhury, m., franklin, m.j., shenker, s., stoica, i.: spark:
cluster computing with working sets. in: proceedings of the 2nd usenix
conference on hot topics in cloud computing. vol. 10, p. 10 (2010) 3