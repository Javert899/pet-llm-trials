connecting databases with process mining:
a meta model and toolset
e. gonz ´alez l ´opez de murillas1;3, h.a. reijers1;2, and w.m.p van der aalst1
1department of mathematics and computer science
eindhoven university of technology, eindhoven, the netherlands
2department of computer science
vu university amsterdam, amsterdam, the netherlands
3lexmark enterprise software, gooimeer 12, 1411de naarden, the netherlands
{e.gonzalez,h.a.reijers,w.m.p.v.d.aalst}@tue.nl
abstract. process mining techniques require event logs which, in many cases, are
obtained from databases. obtaining these event logs is not a trivial task and requires
substantial domain knowledge. in addition, the result is a single view on the database
in the form of a specific event log. if we desire to change our view, e.g. to focus on
another business process, and generate another event log, it is necessary to go back to
the source of data. this paper proposes a meta model to integrate both process and data
perspectives, relating one to the other and allowing to generate different views from
it at any moment in a highly flexible way. this approach decouples the data extraction
from the application of analysis techniques, enabling its use in different contexts.
keywords: process mining ,database ,data schema ,meta model ,event extraction .
1 introduction
the field of process mining offers a wide variety of techniques to analyze event data. process
discovery, conformance and compliance checking, performance analysis, process monitoring
and prediction, and operational support are some of the techniques that process mining
provides in order to better understand and improve business processes. however, most of
these techniques rely on the existence of an event log.
anyone who has dealt with obtaining event logs in real-life scenarios knows that this is
not a trivial task. it is not common to find logs exactly in the right form. in many occasions,
they simply do not exist and need to be extracted from some sort of storage, like databases. in
these situations, when a database exists, several approaches are available to extract events. the
most general is the classical extraction in which events are manually obtained from the tables
in the database. to do so, a lot of domain knowledge is required in order to select the right data.
some work has been done in this field to assist in the extraction and log generation task [2].
also, studies have been performed on how to extract events in very specific environments
like sap [5, 6, 13] or other erp systems [8]. a more general solution to extract events from
databases, regardless of the application under study, is presented in [10], which describes
how to automatically obtain events from database systems that generate redo logs as a way
to recover from failure. the mentioned approaches aim at, eventually, generating an event log ,
i.e. a set of traces, each of them containing a set of events . these events represent operations
or actions performed in the system under study, and are grouped in traces following some kind
of criteria. however, there are multiple ways in which events can be selected and grouped2 connecting databases with process mining
erp
platformcrm
system
bpm
work ow
managerdatabasesprocess
aware
meta
model
process
miningredo logs
fig. 1: data gathering from several systems to a meta model
into traces. depending on the perspective we want to have on the data, we need to extract
event logs differently. also, a database contains a lot more information than just events. the
extraction of events and its representation as a plain event log can be seen as a “lossy” process
during which valuable information can get lost. considering the prevalence of databases as
a source for event logs, it makes sense to gather as much information as possible, combining
the process view with the actual data.
we see that process mining techniques grow more and more sophisticated. y et, the most
time-consuming activity, event log extraction, is hardly supported. this paper provides mature
support to tackle the problem of obtaining, transforming, organizing and deriving data and
process information from databases. this makes easier to connect the registration system of
enterprises with analysis tools, generating different views on the data in a flexible way. also,
this work presents a comprehensive integration of process and data information in a consistent
and unified format. all of this is formally supported with automated techniques. moreover,
the provided solution has the benefit of being universal, being applicable regardless of the
specific system in use. figure 1 depicts an environment in which the information is scattered
over several systems from a different nature, like erps, crms, bpm managers, database
systems, redo logs, etc. in such a heterogeneous environment, the goal is to extract, transform
and derive data from all sources to a common representation that is able to connect all the
pieces together such that analysis techniques like process mining can be readily applied.
the remainder of this paper is structured as follows: section 2 presents a running example
used through the paper. section 3 explains the proposed meta model and the formalization.
implementation details are presented in section 4. the approach is evaluated in section 5, and
section 6 shows the related work. finally, section 7 presents the conclusions and future work.
2 running example
in this section we propose a running example to explain and illustrate our approach. assume
we want to analyze a setting where concerts are organized and concert tickets are sold. to do
so, a database is used to store all the information related to concerts, concert halls ( hall), seats,
tickets, bands, performance of bands in concerts ( band playing ), customers and bookings.
figure 2 shows the data schema of the database. in it we see many different elements of the
involved process represented. let us consider now a complex question that can be of interest
from a business point of view: what is the interaction with the process of customers between
18 and 25 years old who bought tickets for concerts of band x? this question represents a
challenge starting from the given database for several reasons:
1.the database does not provide an integrated view of process and data. therefore, ques-
tions related to the execution of the underlying process cannot be answered with a query.
2.the current database schema fits the purpose of storing the information in this specific
setting, but it does not have enough flexibility to extend its functionality allocating new
kinds of data such as events or objects of a different nature.a meta-model and toolset 3
fig. 2: data schema of the example database
3.the setting lacks execution information in an accessible way (events, traces and logs are
missing so one cannot apply process mining without a lot of extra work), and there is
no assistance on how to extract or derive this information from the given data.
4.if we plan to use the data as it is, it requires to adapt to the way it is stored for every
question we want to answer.
all these reasons make the analysis complex, if not impossible, in many settings. at best,
such an analysis can only be carried out by either the extraction of a highly specialized event
log or the creation of a complex ad hoc query.
3 meta model
as has been shown before, a need exists for a way to store execution information in a struc-
tured way, something that accepts data from different sources and allows to build further
analysis techniques independently from the origin of this data. efforts in this field have already
been made as can be observed in [14] with the openxes standard. this standard defines
structure to manage and manipulate logs, containing events and traces and the corresponding
attributes. therefore, xes is a good target format to represent behavior. however, a xes
file is just one view on the data and, despite being an extensible format, it does not provide
a predefined structure to store all the linked information we want to consider.
because of this, it seems necessary to define a structured way to store additional infor-
mation that can be linked to the classical event log. this new way to generalize and store
information must provide sufficient details about the process, the data types and the relations
between all the elements, making it possible to answer questions at the business level, while
looking at two different perspectives: data and process.
3.1 requirements
to be able to combine the data and process perspectives in a single structure, it is important
to define a set of requirements that a meta model must fulfill. it seems reasonable to define
requirements that consider backwards-compatibility with well established standards, support
of additional information, its structure and the correlation between process and data views:4 connecting databases with process mining
1.the meta model must be compatible with the current meta model of xes, i.e. any xes
log can be transformed to the new meta model and back without loss of information,
2.it must be possible to store several logs in the new meta model, avoiding event duplication,
3. logs stored in the same meta model can share events and belong to different processes,
4. it must be possible to store some notion of process in the meta model,
5.the meta model must allow to store additional information, like database objects, together
with the events, traces and processes, and the correlation between all these elements,
6. the structure of additional data must be precisely modeled,
7.all information mentioned must be self contained in a single storage format, easy to
share and exchange, similarly to the way that xes logs are handled.
the following section describes the proposed meta model which complies with these
requirements, providing a formalization of the concepts along with explanations.
3.2 formalization
considering the typical environments subject to study in the process mining field, we can
say that it is common to find systems backed up by some sort of database storage system.
regardless of the specific technology behind these databases, all of them have in common
some kind of structure for data. we can describe our meta model as a way to integrate process
and data perspectives, providing flexibility on its inspection and assistance to reconstruct the
missing parts. figure 3 shows a high level representation of the meta model. on the right hand
side, the data perspective is considered, while the left models the process view. assuming
that the starting point of our approach is data, we see that the less abstract elements of the
meta model, events andversions , are related, providing the connection between the process
and data view. these are the basic blocks of the whole structure and, usually, the rest can be
derived from them. however, in section 5 we will see that, given enough information, we
can also derive any of these two basic blocks from the other.
the data side considers three elements: data model ,objects andversions . the data model
provides a schema describing the objects of the database. the objects represent the unique
entities of data that ever existed or will exist in our database, while the versions represent the
specific values of the attributes of an object during a period of time. v ersions represent the
evolution of objects through time. the process side considers events ,instances andprocesses .
processes describe the behavior of the system. instances are traces of execution for a given
process, being sets of events ordered through time. these events represent the most granular
kind of execution data, denoting the occurrence of an activity or action at a certain point in time.
the remainder of this section proposes a formalization of the elements in this meta model,
starting from the data and continuing with the process side. as has been mentioned before,
fig. 3: diagram of the meta model at a high levela meta-model and toolset 5
we can assume a way to classify elements in types or classes exists. looking at our running
example, we can distinguish between a ticket class and a customer class. this leads to the
definition of data model as a way to describe the schema of our data.
definition 1 (data model) a data model is a tuple dm =(cl;at;classofattribute ;rs;
sourceclass ;targetclass )such that
–cl is a set of class names,
–at is a set of attribute names,
–classofattribute2at!clis a function that maps each attribute to a class,
–rs is a set of relationship names,
–sourceclass2rs!clis a function that maps each relationship to its source class,
–targetclass2rs!clis a function that maps each relationship to its target class
each of these elements belonging to a class represents a unique entity, something that can
be differentiated from the other elements of the same class, e.g. customer a andcustomer
b. we will call them objects , being unique entities according to our meta model.
definition 2 (object collection) assume obj to be the set of all possible objects. an object
collection oc is a set of objects such that ocobj .
something we know as well is that, during the execution of a process, the nature of these
elements can change over time. modifications can be made on the attributes of these objects .
each of these represents mutations of an object, modifying the values of some of its attributes,
e.g. modifying the address of a customer. as a result, despite being the same object, we will
be looking at a different version of it. the notion of object v ersion is therefore introduced
to show the different stages in the life-cycle of an object .
during the execution of a process, operations will be performed and, many times, links
between elements are established. these links allow to relate tickets toconcerts , orcustomers
tobookings , for example. these relationships are of a structured nature and usually exist at the
data model level, being defined between classes . therefore, we know upfront that elements of
the class ticket can be related somehow to elements of the class concert .relationships is the
name we use to call the definition of these links at the data model level. however, the actual
instances of these relationships appear at the object v ersion level, connecting specific versions
of objects during a specific period of time. these specific connections are called relations .
definition 3 (version collection) assume v to be some universe of values, ts a universe
of timestamps and dm = (cl;at;classofattribute ;rs;sourceclass ;targetclass )a
data model. a version collection is a tuple ovc = (ov;attvalue ;starttimestamp ;
endtimestamp ;rel )such that
–ov is a set of object versions,
–attvalue2(atov)6!vis a function that maps a pair of object version and attribute
to a value,
–starttimestamp2ov!tsis a function that maps each object version to a start
timestamp,
–endtimestamp2ov!tsis a function that maps each object version to an end
timestamp such that 8ov2ov c :endtimestamp (ov)starttimestamp (ov),
–relrsovovis a set of triples relating pairs of object versions through a
specific relationship.6 connecting databases with process mining
at this point, it is time to consider the process side of the meta model. the most basic
piece of information we can find in a process event log is an event. these are defined by some
attributes, among which we find a few typical ones like timestamp ,resource orlifecycle .
definition 4 (event collection) assume v to be some universe of values and ts a universe
of timestamps. an event collection is a tuple ec = (ev;evat ;eventattributevalue ;
eventtimestamp ;eventlifecycle ;eventresource )such that
–evis a set of events,
–evat is a set of event attribute names,
–eventattributevalue 2evevat6!vis a function that maps a pair of an event and
event attribute name to a value,
–eventtimestamp2ev!tsis a function that maps each event to a timestamp,
–eventlifecycle2ev!fstart;complete ;:::gis a function that maps each event to a
value for its life-cycle attribute,
–eventresource2ev!vis a function that maps each event to a value for its resource
attribute.
when we consider events of the same activity but relating to a different lifecycle, we
gather them under the same activity instance . for example, two events that belong to the
activity make booking could have different lifecycle values, being start the one denoting the
beginning of the operation (first event) and complete the one denoting the finalization of the
operation (second event). therefore, both events belong to the same activity instance . each
of these activity instances can belong to different cases ortraces . at the same time, cases
can belong to different logs, that represent a whole set of traces on the behavior of a process.
definition 5 (instance collection) an instance collection is a tuple ic= (ai;cs;lg;
aisofcase ;casesoflog )such that
–aiis a set of activity instances,
–csis a set of cases,
–lgis a set of logs,
–aisofcase2cs!p (ai)is a function that maps each case to a set of activity instances,
–casesoflog2lg!p (cs)is a function that maps each log to a set of cases.
the last piece of the our meta model is the process model collection . this part stores
process models on an abstract level, i.e. as sets of activities . anactivity can belong to different
processes at the same time.
definition 6 (process model collection) a process model collection is a tuple pmc =
(pm;ac;actofproc )such that
–pmis a set of processes,
–acis a set of activities,
–actofproc2pm!p (ac)is a function that maps each process to a set of activities.
now we have all the pieces of our meta model, but it is still necessary to wire them together.
aconnected meta model defines the connections between these blocks. therefore, we see that
versions belong to objects (objectofv ersion ) and objects belong to a class ( classofobject ).
in the same way, events belong to activity instances (eventai ),activity instances belong to
activities (activityofai ) and can belong to different cases (aisofcase ),cases to different logs
(casesoflog ) and logsto process ( processoflog ). connecting both data and process views, wea meta-model and toolset 7
fig. 4: er diagram of the meta model
findevents andversions . they are related ( eventtoovlabel ) in a way that can be interpreted
as a causal relation between events andversions , i.e. when events happen they trigger the
creation of versions as a result of modifications on data (the update of an attribute for instance).
another possibility is that the event represents a read access or query of the values of a version .
definition 7 (connected meta model) assume v to be some universe of values, dm =
(cl;at;classofattribute ;rs;sourceclass ;targetclass )a data model, oc an object
collection, ovc = (ov;attvalue ;starttimestamp ;endtimestamp ;rel )a version
collection, ec = (ev;evat ;eventattributevalue ;eventtimestamp ;eventlifecycle ;
eventresource )an event collection, ic= (ai;cs;lg;aisofcase ;casesoflog )an in-
stance collection and pmc = (pm;ac;actofproc )a process model collection. a con-
nected meta model is a tuple cmm = (dm;oc;classofobject ;objectofversion ;ovc;
ec;eventtoovlabel ;ic;eventai ;pmc ;activityofai ;processoflog )such that
–classofobject2oc!clis a function that maps each object to a class,
–objectofversion2ov!ocis a function that maps each object version to an object,
–eventtoovlabel2evov6!vis a function mapping pairs of an event and an
object version to a label. if a pair (ev;ov )2domain (eventtoov label ), this means
that both event and object version are linked. the label itself defines the nature of such
link, e.g “insert”, “update”, “read”, “delete”, etc,
–eventai2ev!aiis a function that maps each event to an activity instance,
–activityofai2ai!acis a function that maps each activity instance to an activity,
–processoflog2lg!pm is a function that maps each log to a process.
an instantiation of this meta model fulfills the requirements set in section 3.1 in terms
of storage of data and process view. some characteristics of this meta model that enable full
compatibility with the xes standard have been omitted in this formalization for the sake of
brevity. in addition to this formalization, an implementation has been made. this was required
in order to provide tools that assist in the exploration of the information contained within the
meta model. more details on this implementation are explained in the following section.
4 implementation
the library openslex, based on the meta model proposed in this work, has been imple-
mented in java1. this library provides an interface to insert data in the meta model, and to
1http://www.win.tue.nl/ ˜egonzale/projects/openslex/8 connecting databases with process mining
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
input
unknown or
not required
derivedlegend (a) (b) (c) (d) (e) (f)
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
fig. 5: input scenarios to complete meta model elements
access it in a similar way to how xes logs are managed by openxes [14]. however, under
the hood it relies on sql technology. specifically, the meta model is stored in an sqlite2file.
this provides some advantages like an sql query engine, a standardized format as well as stor-
age in self contained single data files that benefits its exchange and portability. figure 4 shows
an er diagram of the internal structure of the meta model. however, it represents a simplified
version to make it more understandable and easy to visualize. the complete class diagram
of the meta model can be accessed in the tool’s website3. in addition to the library mentioned
earlier, an inspection tool3has been developed. this tool allows to explore the content of
openslex files by means of a gui in an exploratory fashion, which lets the user dig into the
data and apply some basic filters on each element of the structure. the tool presents a series of
blocks that contain the activities ,cases ,activity instances ,events ,event attribute values ,data
model ,objects ,object versions ,object version attribute values andrelations in the meta model.
some of the lists in the inspector ( cases ,activity instances ,events andobjects ) have tabs that
allow to filter the content they show. for instance, if the tab “per activity” in the cases list
is clicked, only cases that contain events of such activity will be shown. in the same way, if
the tab “per case” in the events list is clicked, only events contained in the selected case will
be displayed. an additional block in the tool displays the attributes of the selected event.
5 evaluation
the development of the meta model presented in this paper has been partly motivated by the
need of a general way to capture the information contained in different systems combining the
data and process view. these systems, usually backed by a database, use very different ways
to internally store their data. therefore, in order to extract this data, it is necessary to define a
translation mechanism tailored to the wide variety of such environments. because of this, the
evaluation aims at demonstrating the possibility of transforming information from different en-
vironments to the proposed meta model . specifically, three source environments are analyzed:
1.database redo logs: files generated by the dbms in order to maintain the consistency
of the database in case of failure or rollback operations.
2.in-table version storage: application-specific schema to store new versions of objects
as a new row in each table.
3.sap-style change table: changes on tables are recorded in a ”redo log” style as a separate
table, the way it is done in sap systems.
the benefit of transforming the data to a common representation is that it allows for
decoupling the application of techniques for the analysis from the sources of data. in addition,
2http://www.sqlite.org/
3http://www.win.tue.nl/ ˜egonzale/projects/meta-model/a meta-model and toolset 9
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
events
cases
data
model
process
modelobjectsversions
database
with
sap-style
change log(d)
(c) (f)database
with
redo logs
events
cases
data
model
process
modelobjectsversions
database
with in-table
versioning(e)
fig. 6: meta model completion in the three evaluated environments
a centralized representation allows to link data from different sources. however, the source
of data may be incomplete. in this case, when transforming the data to fit in our meta model
is not enough, we need to apply some inference techniques. this allows to derive the missing
information and create a complete and fully integrated view.
the first part of this evaluation (section 5.1) presents the different scenarios that we can
find when transforming data. each of these scenarios start from data that corresponds to
different parts of the meta model. then, it shows how to derive the missing elements from
the given starting point. sections 5.2, 5.3 and 5.4 analyze the three realistic environments
mentioned before. we will demonstrate that data extraction is possible and that the meta
model can be used to apply process mining instantly. section 5.5 shows an example of the
corresponding output meta model for the three environments.
5.1 meta model completion scenarios
it is rare to find an environment that explicitly provides the information to fill every cellof our
meta model. this means that additional steps need to be taken to evolve from an incomplete
meta model to a complete one. to do so, figure 5 presents several scenarios in which, starting
from a certain input, it is possible to infer other elements. applying these steps consecutively
will lead us, in the end, to a completely filled meta model:
aone of the most basic elements we require in our meta model to be able to infer other
elements is the event collection . starting from this input and applying schema, primary
key and foreign key discovery techniques [12, 15], it is possible to obtain a data model
describing the structure of the original data.
bthe events, when combined with a data model, constitute one of the basic components
and allows to infer objects. to do so, it is necessary to know the attributes of each class
that identify the objects (primary keys). finding the unique values for such attributes in the
events corresponding to each class results in the list of unique objects of the meta model.
calso, we can derive cases from the combination of events and a data model. the event
splitting technique described in [10], which uses the transitive relations between events
defined by the data model, allows to generate different sets of cases, or event logs.
dthe events of each object can be processed to infer the object versions as results of the
execution of each event. to do so, events must contain the values of the attributes of
the object they relate to at a certain point in time or, at least, the values of the attributes
that were affected (modified) by the event. then, ordering the events by (ascending)
timestamp allows to reconstruct the versions of each object.10 connecting databases with process mining
table 1: fragment of a redo log: each line corresponds to the occurrence of an event
#time + op + table redo undo
12014-11-27 15:57:08.0 +
insert+customerinsert into "sampledb".
"customer" ("id", "name",
"address", "birth_date")
values (’17299’, ’name1’,
’address1’, to_date( ’01-aug-06’,
’dd-mon-rr’));delete from "sampledb".
"customer" where "id" =
’17299’ and "name" = ’name1’
and "address" = ’address1’
and "birth_date" = to_date(
’01-aug-06’, ’dd-mon-rr’) and
rowid = ’1’;
22014-11-27 16:07:02.0 +
upda te+customerupdate "sampledb". "customer" set
"name" = ’name2’ where "name" =
’name1’ and rowid = ’1’;update "sampledb". "customer" set
"name" = ’name1’ where "name" =
’name2’ and rowid = ’1’;
32014-11-27 16:07:16.0 +
insert+bookinginsert into "sampledb". "booking"
("id", "customer_id") values
(’36846’, ’17299’);delete from "sampledb". "booking"
where "id" = ’36846’ and
"customer_id" = ’17299’ and rowid
= ’2’;
42014-11-27 16:07:16.0 +
upda te+ticketupdate "sampledb". "ticket" set
"booking_id" = ’36846’ where
"booking_id" is null and rowid
= ’3’;update "sampledb". "ticket"
set "booking_id" = null where
"booking_id" = ’36846’ and rowid
= ’3’;
52014-11-27 16:07:17.0 +
insert+bookinginsert into "sampledb". "booking"
("id", "customer_id") values
(’36876’, ’17299’);delete from "sampledb". "booking"
where "id" = ’36876’ and
"customer_id" = ’17299’ and rowid
= ’4’;
62014-11-27 16:07:17.0 +
ticket+upda teupdate "sampledb". "ticket" set
"id" = ’36876’ where "booking_id"
is null and rowid = ’5’;update "sampledb". "ticket" set
"id" = null where "booking_id" =
’36876’ and rowid = ’5’;
ethe inverse of scenario dis the one in which events are inferred from object versions.
looking at the attributes that differ between consecutive versions it is possible to create
the corresponding event for the modification.
ffinally, a set of cases is required to discover a process model using any of the multiple
miners available in the process mining field.
the following three sections analyze realistic environments and relate them to these
scenarios to demonstrate that the complete meta model structure can be derived in each of
them. the goal is to create an integrated view of data and process, specially when event logs
are not directly available.
5.2 database redo logs
the first environment focuses on database redo logs , a mechanism present in many dbmss to
guarantee consistency, as well as providing additional features such as rollback, point-in-time
recovery, etc. redo logs have already been considered in previous works [1, 10] as a source of
event data for process mining. table 1 shows an example of a redo log obtained from an oracle
dbms. after its processing, explained in [10], these records are transformed into events.
figure 6 shows a general overview of how the meta model elements are completed
according to the starting input data and the steps taken to derive the missing ones. in this case,
the analysis of database redo logs allows to obtain a set of events, together with the objects
they belong to and the data model of the database. these elements alone are not sufficient
to do process mining without the existence of an event log (a set of traces). in addition, the
versions of the objects of the database need to be inferred from the events as well.
fortunately, a technique to build logs using different perspectives (trace id patterns) is
presented in [10]. the existence or definition of a data model is required for this technique to
work. figure 6 shows a diagram of the data transformation performed by the technique, and
how it fits in the proposed meta model structure. the data model is automatically extracted
from the database schema and is the one included in the meta model. this data model, togethera meta-model and toolset 11
customer (original)  
(pk) id integer  
(pk) load_timestamp  timestamp  
name  string  
address  string  
birth_date  date  customer (derived)  
(pk) id integer  
name  string  
address  string  
birth_date  date  
id name  address  birth_date  
17299  name2  address1  01-aug -06 
17300  name3  address3  14-jun-04 id name  address  birth_date  
17299  name2  address1  01-aug -06 
17300  name3  address2  14-jun-04 id name  address  birth_date  
17299  name2  address1  01-aug -06 id name  address  birth_date  
17299  name1  address1  01-aug -06 id name  address  birth_date  t0 : 
t1 : 
t2 : 
t3 : 
t4 : obj1 : v1 
obj1 : v2 
obj1 : v2 
obj2 : v1 
obj1 : v2 
obj2 : v2 database states through time  
id load_  
timestamp  name  address  birth_date  
17299  2014 -11-27 
15:57:08.0  name1  address1  01-aug -06 
17299  2014 -11-27 
16:07:02.0  name2  address1  01-aug -06 
17300  2014 -11-27 
17:48:09.0  name3  address2  14-jun-04 
17300  2014 -11-27 
19:06:12.0  name3  address3  14-jun-04 e1 
e2 
e3 
e4 
fig. 7: example of in-table versioning and its transformation into objects and versions
with the extracted events, allows to generate both cases (c) and object versions (d). then,
process discovery completes the meta model with a process (f). once the meta model structure
is filled with data, we can make queries on it taking advantage of the established connections
between all the elements and apply process mining to do the analysis.
5.3 in-table versioning
it is not always possible to get redo logs from databases. sometimes they are disabled or not
supported by the dbms. also, we simply may not be able to obtain credentials to access
them. whatever the reason, we often face a situation in which events are not explicitly stored.
this enormously limits the analysis that can be performed on the data. the challenge in this
environment is to obtain, somehow, an event log to complete our data.
it can be the case that we encounter an environment such that, despite of lacking events,
versioning of objects is kept in the database, i.e. it is possible to retrieve the old value for any
attribute of an object at a certain point in time. this is achieved by means of duplication of the
modified versions of rows. the table at the bottom left corner of figure 7 shows an example
of an in-table versioning of objects. we see that the primary key is formed by the fields id
andload timestamp . each row represents a version of an object and every new reference
to the same idat a later load timestamp represents an update. therefore, if we order rows
(ascending) by idandload timestamp , we get sets of versions for each object. the first one
(with older load timestamp ) represents an insertion, and the rest updates on the values.
looking at figure 7 it is clear that, ordering by timestamp the versions in the original set
(bottom left), we can reconstruct the different states of the database (right). each new row in
the original table represents a change in the state of the database. performing this process for all
the tables, allows to infer the events in a setting where they were not explicitly stored. figure 6
shows that, thanks to the meta model proposed it is possible to derive events starting from a
data model, a set of objects, and their versions as input (figure 5.e). the next step is to obtain
cases from the events and data model applying the technique from [10] to split event collections
into cases selecting an appropriate trace id pattern (scenario c). finally, process discovery
will allow us to obtain a process model to complete the meta model structure (scenario f).
as a result of the whole procedure, we have a meta model completely filled with data (orig-
inal and derived) that enables any kind of analysis available nowadays in the process mining
field. moreover, it allows for extended analysis combining data and process perspectives.12 connecting databases with process mining
cdhdr  
objectclas  string  
changenr  integer  
objectid  integer  
username  string  
udate  date  
utime  time  
tcode  integer  customer  
(pk) id integer  
name  string  
address  string  
birth_date  date  objectclas  changenr  objectid  username  udate  utime  tcode  
cust  0001  0000001  user1  2014 -11-27 15:57:08.0  0001  
cust  0002  0000001  user2  2014 -11-27 16:07:02.0  0002  
cust  0003  0000002  user2  2014 -11-27 17:48:09.0  0003  
cust  0004  0000002  user1  2014 -11-27  19:06:12.0  0004  
... ... ... ... ... ... ... 
objectclas  changenr  tabname  tabkey  fname  value_new  value_old  
cust  0001  customer  17299  name  name1  
cust  0001  customer  17299  address  address1  
cust  0001  customer  17299  birth_date  01-aug -06 
cust  0002  customer  17299  name  name2  name1  
cust  0003  customer  17300  name  name3  
cust  0003  customer  17300  address  address2  
cust  0003  customer  17300  birth_date  14-jun-04 
cust  0004  customer  17300  address  address3  address2  
... ... ... ... ... ... ... cdpos  
objectclas  string  
changenr  integer  
tabname  string  
tabkey  string  
fname  string  
value_new  string  
value_old  string  
fig. 8: example of sap change tables cdhdr and cdpos
5.4 sap-style change table
the last environment we will consider is related to very widespread erp systems such as
sap . these systems provide a huge amount of functionalities to companies by means of
configurable modules. they can run on various platforms and rely on databases to store all
their information. however, in order to make them as flexible as possible, the implementation
tries to be independent of the specific storage technology running underneath. we see sap
systems running on mssql, oracle or other technologies but they do not make intensive
use of the features that the database vendor provides. therefore, data relations are often not
defined in the database schema, but managed at the application level. this makes the life of the
analyst who would be interested in obtaining event logs a bit more complicated. fortunately,
sap implements its own redo log like mechanism to store changes in data, and it represents
a valid source of data for our purposes.in this setting we lack event logs, object versions, a
complete data model and processes. without some of these elements, performing any kind
of process mining analysis becomes very complicated, if not impossible. for instance, the
lack of an event log does not allow for the discovery of a process and, without it, performance
or conformance analysis are not possible. to overcome this problem, we need to infer the
lacking elements from the available information in the sap database.
first, it must be noted that, despite the absence of an explicitly defined data model, sap
uses a consistent naming system for their tables and columns, and there is lots of documen-
tation available that describes the data model of the whole sap table landscape. on the other
hand, to extract the events we need to process the change log. this sap-style change log,
as can be observed in figure 8, is based on two change tables: cdhdr andcdpos . the
first table ( cdhdr ) stores one entry per change performed on the data with a unique change
id (changenr ) and other additional details. the second table ( cdpos ) stores one entry
per field changed. several fields in a data object can be changed at the same time and will
share the same changenr . for each field changed, the table name is recorded ( tabname )
together with the field name ( fname ), the key of the row affected by the change ( tabkey )
and the old and new values of the field ( value old ,value new ).
as can be seen in figure 6, after processing the change log and providing an sap data
model, we are in a situation in which the events, objects and data model are known. then,
we can infer the versions of each object (d), split the events in cases (c) and finally, discover
a process model (f). with all these ingredients it becomes possible to perform any process
mining analysis and answer complex questions combining process and data perspectives.a meta-model and toolset 13
customer  
(pk) id integer  
name  string  
address  string  
birth_date  date  id start_  
timestamp  end_  
timestamp  name  address  birth_date  object_id  
17299  2014 -11-27 
15:57:08.0  2014 -11-27 
16:07:02.0  name1  address1  01-aug -06 1 
17299  2014 -11-27 
16:07:02.0  name2  address1  01-aug -06 1 
17300  2014 -11-27 
17:48:09.0  2014 -11-27 
19:06:12.0  name3  address2  14-jun-04 2 
17300  2014 -11-27 
19:06:12.0  name3  address3  14-jun-04 2 id class_id  
1 1 
2 1 
id timestamp  lifecycle  resource  activity_instance_id  
1 2014 -11-27 
15:57:08.0  name1  address1  1 
2 2014 -11-27 
16:07:02.0  name2  address1  2 
3 2014 -11-27 
17:48:09.0  name3  address2  3 
4 2014 -11-27 
19:06:12.0  name3  address3  4 id activitiy_id  
1 1 
2 2 
3 1 
4 3 id case_name  
1 casea  
2 caseb  
id name  pid 
1 customer -creation  1 
2 customer -update -name  1 
3 customer -update -address  1 id process_name  
1 process 1  versions  
events  cases  
activity instances  
activities  objects  
data model  
processes  
fig. 9: sumarized example of resulting meta model
5.5 resulting meta model
in the three previous sections we have explored different environments in which, from a given
starting input, we can derive the missing blocks to completely fill our meta model. the three
environments are different, but based on the running example presented in section 2. there-
fore, we assume that the same information can be accessed in all three of them. as a result, the
resulting meta model will contain the same information, but different parts have been derived
through a different procedure depending on the starting input data. figure 9 provides a simpli-
fied view on the final content of the meta model for any of the three environments. however, a
full meta model considering all the tables presented in the running example is available online4
and can be explored using our tool shown in section 4. now we have all the information we
need in a centralized storage. however, it cannot be used yet to do process mining analysis.
most of these techniques require an event log in xes format. fortunately, our meta model
allows full compatibility with this target format. in addition, our meta model has the advantage
of allowing to generate many different event logs depending on the view we want to obtain of
our data. let us reconsider the question proposed in section 2: what is the interaction with the
process of customers between 18 and 25 years old who bought tickets for concerts of band x?
in order to answer this question, we need to make a proper selection on the data and
transform it to a xes event log. the openslex library provides the functionality required
to programmatically select the desired view of the data in our meta model and instantly
export it to xes. the great advantage is that, regardless of the source environment from
which we obtained the data, the structure in which to make this selection is always the same.
figure 10 shows the result of mining the selected event log. we see that customers make two
kind of operations: ticket booking and customer updates. in order to buy a ticket, customers
make a booking ( booking+insert ) and, afterward, the corresponding ticket is updated
(ticket+update ). this means that the ticket is assigned to the booking to avoid it from be-
ing sold twice. the other operation, customer updates ( customer+update ), is performed
quite often and it corresponds with changes in the details of the customer, e.g. a change of
address. the dashed lines denote deviations of the log respect to the discovered model. in this
image we see that, in 5 cases, the ticket+update operation was skipped after executing
booking+insert . this could mean that the booking process does not finish or it is empty.
therefore, no ticket is ever booked. also, we see some deviations represented by a dashed
4http://www.win.tue.nl/ ˜egonzale/projects/meta-model/mm-01.zip14 connecting databases with process mining
fig. 10: process model of the selected events
loop line on top of an activity or place, for example the one in activity booking+insert
that reads a counter with the value 14. this means that a certain amount of “move on log”
operations took place, i.e. an event happened in the log that cannot be aligned with the model.
at this point we have shown that, from the proposed meta model, it is possible to generate
views on the data. then, exporting them to the target xes format allows to perform process
mining analysis to get more insights on the behavior of the system under study.
6 related work
several efforts have been made trying to provide structure to the representation of execution
data in the information systems field. workflow management systems are an example of
environment in which the community has focused on providing models to describe their
functioning and allow for the analysis of their behavior. papers like [9, 11] provide meta
models to give structure to audit trails on workflows. however, they focus mainly on the
workflow or process perspective. process mining has different needs and the desire to store
event data in a unified way is obvious. in [3], the authors provide a meta model to define
event logs, which would evolve later in the openxes format [14]. this format represents a
great achievement from the point of view of standardization, and allows to exchange logs and
develop mining techniques assuming a common representation of the data. the xes format
is, in fact, a target format from our perspective and not a source of information, i.e. we aim at,
from a richer source, generate different views on data in xes format to enable process mining.
the main flaw of these approaches resides in the way they force the representation of
complex systems by means of a flat event log. the data perspective is missing, only allowing
to add attributes at the event, trace or log level. more recent works try to improve the situation,
analyzing data dependencies [7] in business models with the purpose of improving them, or
even observing changes on object states to improve their analysis [4]. however, none of the
existing approaches provides a generic and standard way of gathering, classifying, storing
and connecting process and data perspectives on information systems, specially when dealing
with databases where the concept of structured process can be fuzzy or nonexistent.
7 conclusion
in this paper, a meta model has been proposed that provides a bigger picture of the reality of
business information systems. this meta model aligns the data and process perspectives and
enables the application of existing process mining techniques (at the same time that unleashes
a new way to query data and historical information). this is possible thanks to the combination
of data and process perspectives on the analysis of information systems. the applicability
of the technique has been demonstrated by means of the analysis of several real-life envi-
ronments. also, an implementation of the proposed solution has been developed and tested.
however, from the authors’ point of view, the main contribution of this work is not only ona meta-model and toolset 15
what new features it enables, but on the universality of the proposed solution. its applicability
to so many different environments provides a common ground to separate data extraction and
analysis as different problems, creating an interface that is much richer and powerful than
the current existing standards. several challenges remain open. for instance, a system that
enhances the query building experience, allowing for a more natural and user friendly way is
desirable. also, mechanisms to exploit the benefits of the process part of the structure when
combined to the data will make the solution really beneficial in comparison to the regular
queries that can be performed in the source database systems. in addition, the development
of techniques to incorporate data from more varied sources and systems will certainly make
the proposed meta model a real candidate for extended use and standardization.
references
1.van der aalst, w.m.p .: extracting event data from databases to unleash process mining. in: vom
brocke, j., schmiedel, t. (eds.) bpm - driving innovation in a digital world, pp. 105–128.
management for professionals, springer international publishing (2015)
2.buijs, j.: mapping data sources to xes in a generic way. master’s thesis, technische universiteit
eindhoven, the netherlands (2010)
3.van dongen, b.f., van der aalst, w.m.p .: a meta model for process mining data. emoi-interop
160, 30 (2005)
4.herzberg, n., meyer, a., weske, m.: improving business process intelligence by observing object
state transitions. data & knowledge engineering 98, 144–164 (2015)
5.ingvaldsen, j.e., gulla, j.a.: preprocessing support for large scale process mining of sap
transactions. in: business process management workshops. pp. 30–41. springer (2008)
6.mahendrawathi, e., astuti, h.m., wardhani, i.r.k.: material movement analysis for warehouse
business process improvement with process mining: a case study. in: asia pacific business
process management, pp. 115–127. springer (2015)
7.meyer, a., pufahl, l., fahland, d., weske, m.: modeling and enacting complex data dependencies
in business processes. in: proceedings of the 11th international conference on business process
management. pp. 171–186. springer-v erlag (2013)
8.mueller-wickop, n., schultz, m.: erp event log preprocessing: timestamps vs. accounting logic.
in: design science at the intersection of physical and virtual design, lecture notes in computer
science, vol. 7939, pp. 105–119. springer berlin heidelberg (2013)
9.zur muhlen, m.: evaluation of workflow management systems using meta models. in: systems
sciences. hicss-32. proceedings of the 32nd annual hawaii international conference on (1999)
10.gonz ´alez-l ´opez de murillas, e., van der aalst, w.m.p ., reijers, h.a.: process mining on databases:
unearthing historical data from redo logs. in: business process management. springer (2015)
11.rosemann, m., zur muehlen, m.: evaluation of workflow management systems-a meta model
approach. australian journal of information systems 6(1) (1998)
12.sismanis, y ., brown, p ., haas, p .j., reinwald, b.: gordian: efficient and scalable discovery of
composite keys. in: proceedings of the 32nd international conference on v ery large data bases.
pp. 691–702. vldb endowment (2006)
13.ˇstolfa, j., kopka, m., ˇstolfa, s., kob ˇersk`y, o., sn ´aˇsel, v .: an application of process mining to
invoice verification process in sap. in: innovations in bio-inspired computing and applications,
pp. 61–74. springer (2014)
14.v erbeek, h., buijs, j.c., v an dongen, b.f., van der aalst, w.m.p .: xes, xesame, and prom 6.
in: information systems evolution, pp. 60–75. springer (2011)
15.zhang, m., hadjieleftheriou, m., ooi, b.c., procopiuc, c.m., srivastava, d.: on multi-column
foreign key discovery. proceedings of the vldb endowment 3(1-2), 805–814 (2010)