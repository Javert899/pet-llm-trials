simplifying discovered process models
in a controlled manner
dirk fahland, wil m.p. van der aalst
eindhoven university of technology, the netherlands
abstract
process models discovered from a process log using process mining tend to be
complex and have problems balancing between overtting and undertting. an
overtting model allows for too little behavior as it just permits the traces in the
log and no other trace. an undertting model allows for too much behavior as it
permits traces that are signicantly dierent from the behavior seen in the log.
this paper presents a post-processing approach to simplify discovered process
models while controlling the balance between overtting and undertting. the
discovered process model, expressed in terms of a petri net, is unfolded into a
branching process using the event log. subsequently, the resulting branching
process is folded into a simpler process model capturing the desired behavior.
keywords: process mining, model simplication, petri nets, branching
processes
1. introduction
information systems are becoming more and more intertwined with the op-
erational processes they support. while supporting these processes, multitudes
of events are recorded, cf. audit trails, database tables, transaction logs, data
warehouses. the goal of process mining is to use such event data to extract
process-related information. the most prominent problem of process mining
isprocess discovery , that is, to automatically discover a process model by ob-
serving events recorded by some information system. the discovery of process
models from event logs is a relevant, but also challenging, problem [1{3].
input for process discovery is a collection of traces. each trace describes the
life-cycle of a process instance (often referred to as case). output is a process
model that is able to reproduce these traces. the automated discovery of process
models based on event logs helps to jump-start process improvement eorts and
provides an objective up-to-date process description. there are two other kinds
email addresses: d.fahland@tue.nl (dirk fahland), w.m.p.v.d.aalst@tue.nl (wil
m.p. van der aalst)
preprint submitted to information systems april 24, 2012p_12 
p_10 p_40 p_38 
p_32 
p_34 p_36 
p_24 p_22 
p_3 p_11 p_37 p_31 p_23 
p_6 
p_19 p_16 
p_2 p_5 
p_13 
p_18 p_35 p_27 
p_15 p_9 
p_39 p_14 
p_8 
p_25 p_26 p_29 
p_1 p_4 
p_30 p_7 
p_17 p_20 p_28 
p_21 
p_33  
 
  
  
 
  
   
   
p_14 p_10 
  
p_26 p_13 p_6 p_17 p_9 
p_25 p_38 
    
 
  
  
   figure 1: hospital patient treatment process after process discovery (left) and after subsequent
simplication using the approach presented in this paper (right).
p_28 
p_27 p_5 p_18 
p_10 
p_17 p_16 
p_7 p_25 
p_26 p_22 
p_21 
p_4 
p_8 p_19 
p_13 p_23 
p_1 
 p_15 
p_14  
p_12 p_11 p_20 
p_9 p_3 
p_29 p_6 p_24 p_2 
   
  
 
  
  
 
 
 
    
    
 
 
p_3 
p_20 
p_4 p_8 p_22 
p_12 
p_10 p_19 p_25 p_16 
p_18 p_21 
p_28 
figure 2: municipality complaint process after process discovery (left) and after subsequent
simplication (right).
of process mining. process extension extends a given (handmade or discovered)
process model with information from the log, for instance, by projecting a log on
a discovered model to show bottlenecks and deviations. conformance checking
is the problem of measuring how well a handmade or discovered process model
describes behavior in a given log [1].
the main problem of process discovery from event logs is to balance between
overtting and undertting . a model is overtting if it is too specic, i.e.,
the example behavior in the log is included, but new instances of the same
process are likely to be excluded by the model. for instance, a process with
10 concurrent activities has 10! = 3628800 potential interleavings. however,
event logs typically contain fewer cases. moreover, even if there are 3628800
cases in the log, it is extremely unlikely that all possible variations are present.
hence, an overtting model (describing exactly these cases) will not capture the
underlying process well. a model is undertting when it over-generalizes the
example behavior in the log, i.e., the model allows for behaviors very dierent
from what was seen in the log. process discovery is challenging because (1)
the log typically only contains a fraction of all possible behaviors, (2) due to
concurrency, loops, and choices the search space has a complex structure , and
(3) there are no negative examples (i.e., a log shows what has happened but
does not show what could not happen) [1, 3].
a variety of approaches has been proposed to address these challenges [1, 2].
technically, all these approaches extract ordering constraints on activities which
are then expressed as control-ow constructs in the resulting process model.
provided that enough event data are available and variability is low, today's
approaches are able to discover the underlying process adequately. however,
processes with more variability are more dicult to discover and numerous
2figure 3: the hospital process (fig. 1) discovered by [12] (left) can be simplied (right).
approaches have been proposed to deal with this.
several approaches try to abstract from infrequent behavior and construct
models that capture only the \highways" in processes. examples are heuristic
mining1[4], fuzzy mining [5], and genetic process mining [6]. the resulting
models are relatively simple, but may not able to reproduce all traces seen in
the log. these techniques exploit the fact that for many processes the so-called
\80/20-rule" holds, i.e., 80% of the observed cases can be explained by 20% of
the paths in the process whereas the remaining 20% of cases is responsible for
80% of the variability. although the techniques proposed in [4{6] can simplify
models, parts of the event log are no longer explained by the model and the
model is often not executable because split-join behavior is either unspecied
(fuzzy mining) or implicit (heuristic mining and genetic process mining) [7].
other approaches try to deal with variability by constructing an over-general
model . instead of leaving out infrequent behavior, everything is allowed unless
there is strong evidence that it is not possible. this can easily be understood
in terms of a petri net. a petri net with transitions tand without any places
can reproduce any event log over a set of activities t. adding a place to a
petri net corresponds to adding a constraint on the behavior. techniques based
onlanguage-based regions [8, 9] use this property. for example, as shown in
[9] it is possible to solve a system of inequations to add places that do not
inhibit behavior present in the event log. in [10], an approach based on convex
polyhedra is proposed. here the parikh vector of each prex in the log is seen as
a polyhedron. by taking the convex hull of these convex polyhedra one obtains
an over-approximation of the possible behavior. in [11], the authors resort to the
use of or-splits and or-joins to create an over-general model that guarantees
that all traces in the log can be reproduced by the model. surprisingly, these
over-general process models tend to be convoluted as illustrated by fig. 1 and
2.
in [12] an approach to balance overtting and undertting is proposed. first,
1historically, process discovery and process mining were used synonymously, as discovery
was the rst and most prominent process mining problem that was addressed. thus, various
discovery techniques are called \mining techniques" although they just focus on discovery.
we will use their original name, but use the term \discovery" to refer to the problem.
3a transition system is constructed from the log; the user may balance general-
ization by inuencing how states are generated from the log. then, a petri net
is derived from this transition system. the approach requires expert knowledge
to specify the right abstraction that balances overtting and undertting. if ap-
plied correctly, this technique yields simpler models (compare fig. 3 (left) and
fig. 1 (left)), but even these models are still convoluted and can be simplied
as shown by fig. 3 (right).
the problem that we address in this paper is to structurally simplify a mined
process model nwhile preserving that ncan replay the entire log lfrom which
it was generated ; a model is simpler if it shows less interconnectedness between
its nodes, see figs. 1-3. we propose a set of techniques for re-adjusting the
generalization done by process discovery algorithms, and to cut down involved
process logic to the logic that is necessary to explain the observed behavior.
note that our approach is notintended as a replacement for existing discovery
techniques. it does not infer causal dependencies between activities. instead,
it can be used as a post-optimization for any process discovery technique whose
results can be represented as petri nets . also, our approach is not limited to
discovered process models: also a hand-made model can be \post-processed"
with respect to a given log.
original
event log discovery 
algorithm
process 
modelalign log
overfitting 
modelfiltered 
overfitting 
model
aligned
event logunfoldfilterrefold
process 
modelremove 
implicit 
places
abstract 
chains
split flower 
placesgeneralize 
and 
simplify
final
process 
model
figure 4: overview of the approach to simplify mined process models.
figure 4 shows the overall approach proposed in this paper. starting point
for our approach is an event log land a discovered process model n=m(l).
mis some conventional process discovery algorithm able to produce a petri net,
e.g., [8{10, 13{15]. results by other algorithms such as heuristic mining [4] or
genetic mining [6] can be converted into a petri net as shown in [7, 16].
some discovery algorithms mguarantee a tting model, i.e., all traces in l
can be replayed on n. however, using the approach described in [17, 18] we
can align log and model when the discovery algorithm itself does not ensure
this. the basic idea is that the non-tting traces are massaged to t the model.
the resulting tting log l0and the discovered model nare used to generate an
unfolded overtting model. technically speaking, we construct the so-called
branching process ofnonly allowing for the observed behavior in the aligned
4event log. the branching process also shows frequencies and can be used to
remove infrequent behavior (if desired). the resulting ltered overtting model
can be folded into a process model. during folding the observed behavior is gen-
eralized in a controlled manner. after folding, we apply further generalization
and simplication techniques. for example, we reduce superuous control-ow
structures by removing implicit places from the model and dene abstraction
operations to simplify the structure and generalize the described behavior in a
disciplined manner.
figures 1-3 illustrate the eectiveness of our approach. interestingly, it can
be combined with any of the existing process discovery techniques. moreover,
as fig. 4 already suggests, the user can inuence the result. this is important
as only the user can decide on the degree of simplication and generalization
needed. the whole approach is supported by the process mining toolkit prom
which can be downloaded from www.processmining.org . we validated the
feasibility of our technique in a number of experiments to simplify benchmark
processes as well as process models from industrial case studies.
in the remainder of this paper, we rst introduce preliminaries regarding
event logs, petri nets, partially ordered runs, and branching processes (sect. 2).
the subsequent sections describe the dierent steps depicted in fig. 4. sect. 3
shows the steps to come to a (ltered)f overtting model. basically, the model
is projected onto the behavior actually observed while addressing issues related
to infrequent or non-tting behavior. in sect. 4 it is shown how refolding can be
used to generalize behavior in a controlled manner. sect. 5 denes the operations
for further simplifying the folded model. we report on experimental results in
sect. 6. sect. 7 discusses related work and sect. 8 concludes the paper.
2. causal behavior of process models w.r.t. an event log
this section introduces the notion of an event log and recalls some standard
notions from petri net theory. in particular the notion of a branching process
will be vital for our approach.
2.1. event logs
process mining aims to discover, monitor and improve real processes by
extracting knowledge from event logs available in today's information systems.
starting point for process discovery is an event log . each event in such a log
refers to an activity (i.e., a well-dened step in some process) and is related
to a particular case (i.e., a process instance ). the events belonging to a case
areordered and can be seen as one \run" of the process. event logs may
store additional event attributes. in fact, whenever possible, process mining
techniques use attributes such as the resource (i.e., person or device) executing
or initiating the activity, the timestamp of the event, or data elements recorded
with the event (e.g., the size of an order).
in this paper we abstract from these additional attributes and focus on
control-ow discovery. each case is described by a trace, i.e., a sequence of
5/mt97
/mt120
/mt99
/mt122 /mt117/mt118/mt98
/mt121
/mt119/mt100 /mt101
/mt102
/mt103 /mt104figure 5: a net system n.
activity names. dierent cases may have the same trace. therefore, an event
log is a multiset of traces (rather than a set).
denition 1 (event log) .letadenotes some universe of activities , i.e., actions
that can be recorded in a log. l2ais a trace, i.e., a sequence of activities.
l2i b(a) is an event log , i.e., a multiset of traces. ( l) is the set of activities
used inl.
for example, l= [xzy;xzy;yy;yy;yy] is an event log with ve cases, two cases
follow trace xzyand three follow trace yy. (l) =fx;y;zg. the fact that multi-
ple cases have the same trace is important for process discovery. frequencies are
used as a basis for removing outliers and detecting incompleteness. neverthe-
less, we will often refer to a log las an ordinary set of traces. from the context,
it will be clear whether lrefers tol= [xzy;xzy;yy;yy;yy] orl=fxzy;yyg.
2.2. petri nets
a process discovery algorithm mreturns for a log la petri net n=m(l).
ideally,nis able to reproduce the event log, i.e., elements of lcorrespond to
occurrence sequences of n.
denition 2 (petri net) .a petri net ( p;t;f ) consists of a set pofplaces , a
settoftransitions disjoint from p, and a set of arcs f(pt)[(tp).
amarkingmofnassigns each place p2pa natural number m(p) oftokens ;
technically, m2i b(p) is a bag of marked places of p. a net system n=
(p;t;f;m 0) is a petri net ( p;t;f ) with an initial markingm0.
we writey:=fxj(x;y)2fgandy:=fxj(y;x)2fgfor the pre-and the
post-set ofy, respectively. fig. 5 shows a slightly involved net system nwith
the initial marking [ a;b].nwill serve as our running example as its structural
properties are typical for results of a discovery algorithm.
the semantics of a net system nare typically given by a set of sequential
runs. a transition tofnisenabled at a marking mofnim(p)1, for
allp2t. iftis enabled at m, thentmay occur in the step mt  !mtofn
6that reaches the successor marking mtwithmt(p) =m(p) 1 ifp2tnt,
mt(p) =m(p) + 1 ifp2tnt, andmt(p) =m(p) otherwise, for each place
pofn. a sequential run of nis a sequence m0t1 !m1t2 !m2:::of steps
miti+1  !mi+1;i= 0;1;2;:::ofnbeginning in the initial marking m0ofn.
the sequence t1t2:::is an occurrence sequence ofn. for example, in the net
nof fig. 5 transitions xand yare enabled at the initial marking [ a;b]; the
occurrence of xresults in marking [ c;f;d;b] where z,u, and yare enabled;
xzyuwyz is a possible occurrence sequence of n.
occurrence sequences of a net system ncorrespond to traces in the event
log. whereas traces in the log have a clear begin and end, this is less clear for
occurrence sequences. occurrence sequences start in the initial marking, but
in a net system termination is undened. therefore, it is sometimes useful to
dene a set of nal markings . in this case, only occurrence sequences leading
to a nal marking correspond to traces in the event log. for example, wf-nets
have a designated source and sink place to model the begin and end of the life-
cycle of a process instance [1]. in this paper, we will not enforce such a structure
and allow models such as the net nof fig. 5.
2.3. partially ordered runs and branching processes
in the following, we study the behavior of nin terms of its partially ordered
runs [19]. we will use so-called branching processes [20] to represent sets of
partially ordered runs, e.g., an event log will be represented as a branching
process. we rst illustrate the idea of a partially ordered run of nby an
example and then dene the branching processes of n.
partially ordered runs. a partially ordered run orders occurrences of tran-
sitions by a partial order | in contrast to a sequential run where occurrences are
totally ordered. a partially ordered run is again represented as a petri net.
such a petri net is labeled and, since it describes just one run of the process, the
preset (postset) of a place contains at most one element. the net 1in fig. 6
describes a partially ordered run of the net nof fig. 5. a partially ordered run
of a net system nhas the following properties:
each place of is called condition and is labeled with a place of n, each
transition of is called an event and is labeled with a transition of n.
a condition bofwith labelpdescribes a token onp, the conditions of 
with an empty pre-set describe the initial marking of n.
an eventeofwith labeltdescribes an occurrence of transition twhich
consumes the tokensefrom the placestand produces the tokens eon
the placest.
for example, event e2of1in fig. 6 describes an occurrence of yconsuming
tokenb2from place band producing token b5oneand a new tokenb6onb.
the events of 1are partially ordered: e5depends on e2whereas neither e1
depends on e2nore2one1. that is,e1ande2areconcurrent . the partially
7/mt97 /mt98
/mt120 /mt121
/mt99 /mt100
/mt122
/mt103/mt117
/mt104
/mt119
/mt99/mt102/mt98/mt101
/mt121
/mt98/mt101
/mt122
/mt103/mt101/mt49/mt98/mt49 /mt98/mt50
/mt101/mt50
/mt98/mt51 /mt98/mt52 /mt98/mt53 /mt98/mt54
/mt101/mt51 /mt101/mt52 /mt101/mt53
/mt98/mt55/mt98/mt56
/mt98/mt57 /mt98/mt49/mt48 /mt98/mt49/mt49
/mt101/mt54
/mt98/mt49/mt50
/mt101/mt55
/mt98/mt49/mt51figure 6: a partially ordered run 1ofnof
fig. 5.
/mt97 /mt98
/mt120 /mt121
/mt99 /mt100
/mt122
/mt103/mt117
/mt104
/mt119
/mt99/mt102
/mt118/mt98/mt101
/mt121
/mt98/mt101
/mt100/mt102
/mt117
/mt104
/mt119
/mt99/mt122
/mt103
/mt122
/mt103/mt101/mt49/mt98/mt49 /mt98/mt50
/mt101/mt50
/mt98/mt51 /mt98/mt52 /mt98/mt53 /mt98/mt54
/mt101/mt51 /mt101/mt52 /mt101/mt53
/mt98/mt55/mt98/mt56
/mt98/mt57 /mt98/mt49/mt48 /mt98/mt49/mt49
/mt101/mt54 /mt101/mt56
/mt98/mt49/mt50 /mt98/mt49/mt52 /mt98/mt49/mt53
/mt101/mt57
/mt98/mt49/mt54
/mt101/mt49/mt48
/mt98/mt49/mt55/mt101/mt55
/mt98/mt49/mt51
/mt98/mt49/mt56/mt101/mt49/mt49figure 7: a branching process the petri net
nof fig. 5.
8ordered run 1describes the occurrence sequence xzyuwyz | and several other
sequences that order concurrent events dierently such as yyxuzwz .
branching processes. the partial order behavior of a net system nis the set
of its partially ordered runs. a branching process represents a set of partially
ordered runs in a single structure. this notion has been studied extensively in
the last three decades [20{23] and we will use it to reason about the behavior
ofn.
a branching process ofnresembles an execution tree: each path of an
execution tree denotes a run, all runs start in the same initial state, and when-
ever two runs diverge they never meet again. in a \path" denotes a partially
ordered run of nand we can read as a special union of partially ordered runs
ofn: all runs start in the same initial marking, and whenever two runs diverge
(by alternative events), they never meet again (each condition of has at most
one predecessor). fig. 7 depicts an example of a branching process representing
two partially ordered runs 1and2.1is shown in fig. 6, 2consists of the
white nodes of fig. 7. both runs share b1-b11ande1-e5, and diverge at the
alternative events e6ande8which compete for b9(i.e., a token in h) and also
forb8andb5.
a branching process of nis formally a labeled petri net= (b;e;g; );
eachb2b(e2e) is called condition (event ),2(b[e)!(p[t) assigns
each node of to a node of nsuch that(b)2pifb2band(e)2tif
e2e.
here, we give the constructive denition of the branching processes of n[22].
to begin with, we need some preliminary notions. two nodes x1;x2ofare in
causal relation , writtenx1x2, i there is path from x1tox2along the arcs
gof.x1andx2are in conict , writtenx1#x2, i there exists a condition
b2bwith distinct post-events e1;e22b;e16=e2ande1x1ande2x2.
x1andx2areconcurrent , writtenx1jjx2i neitherx1x2, norx2x1, nor
x1#x2. for example in fig. 7 e2ande9are in causal relation ( e2e9),e7
ande9are in conict ( e7#e9), ande3ande9are concurrent ( e3jje9).
the branching processes of a petri net n= (p;t;f;m 0) are dened induc-
tively:
base. letb0:=s
p2pfb1
p;:::;bk
pjm0(p) =kgbe a set of conditions such
that(bi
p) =pforbi
p2b0.b0represents the initial marking ofn. then
:= (b0;;;;;) is a branching process of n.
assumption. let= (b;e;g; ) be a branching process of n. lett2t
witht=fp1;:::;pkg. letfb1;:::;bkgbbe pair-wise concurrent conditions
(i.e.,bijjbj, for all 1i < jk) with(bi) =pi, fori= 1;:::;k . the
conditionsb1;:::;bktogether represent tokens in the pre-set of t.
step. if there is no post-event eofb1;:::;bkthat represents an occurrence of t,
then a new occurrence of tcan be added to . formally, if there is no post-event
e2tk
i=1biwith(e) =t, thentisenabled atfb1;:::;bkg. we callfb1;:::;bkg
enabling location oftin. then the petri net 0= (b[c;e[feg;g0;0) is
9obtained from by adding
a fresh event e(not in) with label 0(e) =twithe=fb1;:::;bkg, and
a fresh post-condition for each of the output places of t, i.e., fort=
fq1;:::;qmg, the set of conditions c=fc1;:::;cmgis added to 0such
thatc\b=;and fori= 1;:::;m :0(ci) =qi,ci=feg.
0is a branching process of n. for example, assume the branching process 
of fig. 7 without e10;b17;e11;b18to be given. the conditions fb7;b14;b16;b10g
are pair-wise concurrent and represent tokens inwofnof fig. 5. appending
e10(labeled w) andb17(labeled c) represents an occurrence of w; evente11ofz
is added in the same way.
the arcs of a branching process ofnform a partial order, and any two
nodesx1andx2are either in causal relation, in conict, or concurrent [22].
moreover, every petri net nhas a unique, possibly innite, maximal branching
process(n) which contains every other branching process of nas a prex [20].
3. reconsider generalization: create an overtting model
returning to our original problem setting, we now consider the behavior of
a petri net n=m(l) that was discovered from an event log lby some con-
ventional discovery algorithm m. in this section, we show how to construct an
overtting model that can be generalized and simplied in a controlled manner.
ideally, model ngenerated by the discovery algorithm is able to reproduce the
log. however, in general, this does not need to be the case because of outliers
(i.e., deliberate abstraction) or imperfections of the discovery technique. there-
fore, we rst align event log and model (sect. 3.1). then, we show how to create
an overtting model by restricting the branching process to actually observed
behavior (sect. 3.2). we can further restrict the branching process by removing
infrequent behavior (sect. 3.3).
3.1. aligning event log and model
letn=m(l) be a net system discovered for an event log lwhile using
algorithmm. there are dozens of discovery algorithms that directly produce a
net system [8{10, 12{15]. however, it is also possible to rst discover a model
using a dierent representation (e.g., [4{6]) and then convert it to a petri net
(cf. [7, 16]).
some approaches, e.g., the techniques based on language-based regions [8, 9],
guarantee that each trace l2lis an occurrence sequence of the discovered net
systemn. however, most algorithms will not guarantee that all traces of l
t perfectly. this implies that there may be a trace l2lthat cannot be
reproduced by n. alsoncould be hand-made and not t the log l. this is
a problem for the techniques described in this paper. for example, it is not
possible to unfold a net based on an event log that does not match the model.
there are basically two ways to address this problem.
10remove all non-tting traces from l, i.e.,lis removed from lif it is not
an occurrence sequence of n.
massage the non-tting traces such that all t, i.e., if l2lis not an
occurrence sequence of n, it is transformed into the \closest" occurrence
sequencel0.
in most cases, the second approach is preferable. for larger processes with
lots of variability there may be just a few cases in the log that t the model
from begin to end. it does not make sense to remove the majority of cases.
therefore, we elaborate on aligning model and log . consider net nof fig. 5
andl=xywzu2l.ldoes not t n. therefore, we consider alternative
alignments such as:
1=xyw zu
xy? zuand2=xy?? w zu
xyz uw z?
alignment 1aligns trace l=xywzu with occurrence sequence xyzu. ideally
event log and model make the same \moves". for example, the rst two moves
of tracelcan be mimicked by n. however, move win the log cannot be followed
by the model. therefore, there is a \move in log only", denoted ( w;?), in the
third position of alignment 1. the next two positions in 1show that after this,
event log and model can make identical moves again. alignment 2aligns trace
lwith occurrence sequence xyzuwz . again model and log \agree" on the rst
two moves. this is followed by two \moves in model only" (( ?;z) and (?;u)),
i.e., in the model zand uoccur without a matching move in the log. then
model and log \agree" on the next two moves. however, as position seven in
the alignment shows, there is a \move in log only", denoted ( u;?), becauseuis
not enabled. given a trace like lthere are many possible alignments. however,
as shown in [17, 18] it is possible to associate costs to the dierent \moves" and
select an \optimal" alignment. for example, when assuming unit costs for all
moves where model and log disagree, 1(cost 1) is a better alignment than 2
(cost 3).
by selecting optimal alignments and replacing each trace of lwith its \op-
timally aligned" occurrence sequence, we can convert log linto a logl0such
that anyl2l0is an occurrence sequence of n. this allows us to only con-
sider perfectly tting logs independent of the discovery algorithm used . in the
remainder, lwill always refer to the log after alignment.
3.2. branching process restricted to observed behavior
the maximal branching process (n) ofnintroduced in sect. 2.3 describes
all behavior of n, not only the cases recorded in l. this additional behavior
was introduced by the discovery algorithm mwhich discovered nfroml. to
re-adjust the generalization, we restrict the behavior (n) toland derive an
overtting process model n(l) that exhibits exactly l.
the restriction of (n) to the cases lis the branching process (l) that we
obtain by restricting the inductive denition of the branching processes of nto
11the cases in l. beginning with = (b0;;;;;0), iterate the following steps for
each casel=t1t2:::tn2l. initially, let m:=b0,i:= 1.
1. letfp1;:::;pkg=ti.
2. if there exists fb1;:::;bkgmwith(bj) =pj,j= 1;:::;k ande2tk
j=1bjwith(e) =ti, thenm:= (mne)[e.
[the occurrence eoftiis already represented at fb1;:::;bkg; compute the
successor marking of mby consuming the tokensefrom the pre-places
tiand producing the tokens eonti.]
3. otherwise, choose fb1;:::;bkgmwith(bj) =pj,j= 1;:::;k , and
append a new event e;(e) =tito allbj;j= 1;:::;k , and append a new
conditionctoe(with(c) =q) for eachq2t.m:= (mne)[e.
[add a new occurrence eoftiatfb1;:::;bkgand compute the successor
marking.]
4.i:=i+ 1, and return to step 1 if in.
this procedure replays each l2l. this is possible because lis also an occur-
rence sequence of n. if not, use the preprocessing step described in sect. 3.1.
by construction, (l) is a smallest prex of (n) that represents each l2l.
we call(l) thel-induced unfolding ofn. step 3 is non-deterministic when
markingmputs more than one token on a place. the results in this paper
were obtained by treating mas a queue: the token that is produced rst is also
consumed rst.
for example, the branching process of fig. 7 is the branching process =
(l) of netnof fig. 5 for the log l= [xzuywz;xzuyvuywz;xzyuwz;xyzuvuywz;
xuzywz;xuzyywz;yyxuvuzwz;:::].
(l) not only succinctly represents l,but also all cases that dier from
lby reordering concurrent actions . the discovery algorithm that returned n
determines whether two actions are concurrent. further, (l) already denes a
petri net that exhibits the log l. by putting a token on each minimal condition
bof(l) withb=;, we obtain a labeled petri net n(l) = (b;e;g;;m 0)
that exhibits exactly (l), i.e.,n(l) restricts the behavior of ntol.
3.3. removing infrequent behavior from the branching process
the procedure just described constructs n(l) without considering the fre-
quencies of traces in l. whether a trace appears once or many times will yield
the same overtting process model. however, it is easy to assign counters to all
events when constructing the branching process restricted to log l.
formally, we create a counter 2e!i n. in step 2 of the procedure
described in section 3.2, we increment the counter as follows (e) :=(e)+1. in
step 3 of the procedure we initialize the counter (e) := 1. after constructing
(l), counterindicates how often an event was executed in the branching
process after replaying the entire log. note that the frequencies of traces matter
for this counter. therefore, we dened an event log to be a multiset rather than
a set.
to illustrate function consider the event log l= [xzuywz20;xzuyvuywz30;
xzyuwz15]. the superscripts indicate how frequent traces appear in the event
12(a) (b)1
11
1 0
0
1
1 0
0
065
6565
65 30
30
35
35 30
30
30figure 8: counter : (a) after processing the rst trace xzuywz and (b) after processing the
whole event log l= [xzuywz20;xzuyvuywz30;xzyuwz15].
log. loglcontains 65 cases and trace xzuywz was observed 20 times. applying
the procedure described in section 3.2 to this lwill result in the branching
process of fig. 7. let us rst apply the procedure and update for only one
trace: xzuywz . this results in the shown in fig. 8(a), e.g., (e6) = 1 and
(e8) = 0. after processing all 65 cases we get the shown in fig. 8(b). event
e1is executed for all cases, therefore, (e1) = 65. event e6is executed for the
20 cases following xzuywz and the 15 cases following xzyuwz , hence,(e6) = 35.
functioncan be extended to conditions. for a condition b2b0,(b) :=
jlj. all other conditions bhave precisely one pre-event eand(b) :=(e). this
means that, while replaying the event log, condition bwas marked (b) times.
values tend to decrease towards the leaves of the branching process. if two
nodesx1;x2are causally related, i.e., x1x2, then by denition (x1)(x2).
functioncan be used to prune the overtting model (l). we distinguish
two possible reasons for removing an event efrom(l).
eventeis removed because (e)<  1. here1is an absolute threshold .
13for example, for 1= 33, events e5,e8,e9,e10, ande11will be removed
from(l).
eventeis removed because eis a post-event of some condition band
(e)=(b)<  2. here2is an relative threshold . consider for example
2= 0:50. based on this threshold, event e8would be removed because
(e8)=(b5) = 30=65 = 0:46<0:5, i.e., of the 65 tokens produced for b5
only 30 (i.e., 46%) were consumed by e8.
it is also possible to use a mixture of both or to take into account the distance
to the initial conditions. when removing an event e, also all causally dependent
nodesfx2b[ejexgneed to be removed.
note that the representation shown in fig. 8(b) can also be used to manually
prune the overtting branching process. in fact, a direct inspection of can
provide interesting insights that cannot be obtained when looking at the log.
note, for example, that traces xzuywz and xzyuwz are obviously dierent when
directly inspecting the event log. however, from the viewpoint of the branching
process(l) and function , these two traces are equivalent (modulo reordering
concurrent activities).
the overtting model (l) and corresponding function can be used to
remove infrequent behavior also referred to as \noise" or \outliers". when
removing events from (l) the non-tting parts of the event log need to be
removed as they have been classied as being too infrequent.
in the remainder, we will not consider frequencies and assume an l,n,
(l), andn(l) such that each trace l2lis an occurrence sequence of both
the original net nand the overtting process model n(l) (which in turn is
based on(l)). however, as demonstrated, our overall approach can deal with
non-tting and infrequent traces.
4. generalize an overtting model by folding
the algorithm described in the preceding section yields for a petri net n
discovered from a log l, an overtting net n(l) that exhibits exactly the (l-
tered) branching process (l), i.e., the cases l(modulo reordering of concur-
rent actions). in the following, we present our main contribution: a number
of operations that generalize n(l) (introduce more behavior) and simplify the
structure compared to n. each operation addresses generalization and simpli-
cation in a dierent way and is independent of the other operations. so, a
user may balance between the overtting model n(l) and the complex model
nby choosing from the available generalization and simplication operations.
we provide three kinds of operations which are typically executed in the given
order.
1.n(l) describes the cases lin an explicit form, i.e., only observed behavior
is captured. we foldn(l) to a more compact petri net by identifying
loops, and by merging similar behavior after an alternative choice. this
14partly generalizes behavior of n(l); the folded net is as most as complex
asn(and typically much simpler).
2. then we further simplify the folded net by removing implicit places. an
implicit place does not constrain the enabling of transitions and hence can
be removed [24]. repeatedly removing implicit places can signicantly
simplify the net.
3. finally, the net may have specic structures such as chains of actions of the
same kind or places with a large number of incoming and outgoing arcs.
we provide techniques to replace such structures by simpler structures.
this allows us to generalize the behavior of n(l) in a controlled way.
the structural complexity ofnis its simple graph complexity c(n) =jfj
jpj+jtj
which correlates with the perceived complexity of the net, e.g., the complexities
in fig. 1 are 4.01 (left) and 1.46 (right). each mentioned operation transforms
a netn0into a netn00guaranteeing that (1) n0exhibits at least each case of
n00(generalization), and (2) c(n00)c(n0) (simplication).
this section describes the folding of n(l); removing implicit places and
other structural simplications are explained in sect. 5.
4.1. folding an overtting model
our rst step in creating a simplied process model is to foldthe overtting
netn(l) to a petri net nf(l).nf(l) exhibits more behavior than n(l)
(generalization) and has a simpler structure than the original net n.
technically, we fold the underlying branching process (l) = (b;e;g; )
ofn(l) by an equivalence relation onb[ethat preserves labels of nodes,
and the local environments of events. we write hxi:=fx0jxx0gfor the
equivalence class of node x.hxi=fhxijx2xgis a set of equivalence
classes.
denition 3 (folding equivalence) .letbe a branching process of n. an
equivalence relation on the nodes of is afolding equivalence i
1.x1x2implies(x1) =(x2), for all nodes x1;x2of, and
2.e1e2implieshe1i=he2iandhe1i=he2i, for all events e1;e2
of.
trivial folding equivalences are (1) the identity, and (2) the equivalence
induced by the labeling :x1x2i(x1) =(x2). sect. 4.2 will present a
folding equivalence tailored towards process discovery. every folding equivalence
of a branching process induces a folded petri net which is in principle the
quotient of under.
denition 4 (folded petri net) .letbe a branching process of n, let
be a folding equivalence of . the folded petri net (w.r.t.) is:=
(p;t;f;m) wherep:=fhbijb2bg,t:=fheije2eg,
f:=f(hxi;hyi)j(x;y)2fg, andm(hbi) :=jfb02hbijb0=;gj, for
allb2b.
15/mt97
/mt117/mt98
/mt97 /mt99/mt118
/mt99
/mt122
/mt98/mt122
/mt98/mt120
/mt97/mt101/mt49/mt98/mt49 /mt98/mt50
/mt98/mt51 /mt98/mt52 /mt98/mt53
/mt98/mt56 /mt98/mt55 /mt98/mt54/mt101/mt50
/mt101/mt51 /mt101/mt52 /mt101/mt53
/mt97
/mt99
/mt122
/mt98/mt120/mt98
/mt117 /mt118
/mt97
/mt97
/mt99/mt122 /mt120/mt98
/mt117 /mt118figure 9: the branching process 2(left) can be folded to dierent nets n2(middle) and n0
2
(right) using dierent folding equivalences.
for example, on 2of fig. 9 we can dene a folding equivalence b6b1,b4
b5,e4e5, ,b7b8(and each node equivalent to itself). the corresponding
folded net2
isn2of fig. 9. the coarser folding equivalence dened by the
labeling, i.e.,xyi(x) =(y), yields the net n0
2of fig. 9 (right). this
example indicates that choosing a ner equivalence than the labeling equivalence
yields a more explicit process model. regardless of its explicitness, each folded
net exhibits at least the original behavior (l).
lemma 1. letnbe a petri net. let be a branching process of nwith a
folding equivalence . letn2:=be the folded petri net of w.r.t.. then
the maximal branching process (n2)containsas a prex.
proof (sketch). by def. 3, all nodes of n2carry the same label, and the pre-set
(post-set) of each transition tofn2is isomorphic to the pre-set (post-set) of
each event of deningt. thus,(n2) is built from the same events as . by
induction follows that n2can mimic the construction of : for each event ewith
post-set that is added when constructing , the transition t=heiofn2leads
to an isomorphic event e2that is added when constructing (n2). thus, we can
reconstruct (up to isomorphism) in (n2).n2may allow to add more events
to(n2) than represented in . these additional events are always appended
to, sois a prex of (n2). see [25] for the full formal proof.
4.2. the future equivalence
the following procedure future () constructs a folding equivalence (def. 3)
that specically suits the simplication of discovered process models. the prin-
ciple idea is to make all conditions that represent a token on a nal place of the
process model n(i.e., with an empty post-set) equivalent, and then to extend
the equivalence as much as possible. to this end, we assume to be nite which
is the case for (l) introduced in sect. 3.2.
1. begin with the identity x1x2ix1=x2, for all nodes x1;x2of.
2. whilechanges:
for any two conditions b1;b2ofwith(b1) =(b2) andb1=b2=;,
setb1b2.
16/mt97 /mt98
/mt120 /mt121
/mt99 /mt100
/mt122
/mt103/mt117
/mt104
/mt119
/mt99/mt102
/mt118/mt98/mt101
/mt121
/mt98
/mt122
/mt103/mt101/mt49/mt98/mt49 /mt98/mt50
/mt101/mt50
/mt98/mt51 /mt98/mt54
/mt101/mt51 /mt101/mt53
/mt98/mt55/mt98/mt56/mt44
/mt98/mt49/mt52
/mt98/mt49/mt49
/mt101/mt56/mt98/mt52/mt44
/mt98/mt49/mt53/mt98/mt53/mt44
/mt98/mt49/mt48
/mt101/mt52/mt44
/mt101/mt57
/mt98/mt57/mt44
/mt98/mt49/mt54
/mt101/mt54/mt44
/mt101/mt49/mt48
/mt98/mt49/mt50/mt44
/mt98/mt49/mt55
/mt101/mt55/mt44
/mt101/mt49/mt49
/mt98/mt49/mt51/mt44
/mt98/mt49/mt56
/mt97
/mt120
/mt99
/mt122 /mt117
/mt118/mt98
/mt121
/mt119/mt100 /mt101
/mt103/mt104
/mt99/mt101/mt98
/mt121
/mt101/mt98
/mt122
/mt103
/mt97
/mt120
/mt99
/mt122 /mt117 /mt118/mt98
/mt121
/mt119/mt100 /mt101
/mt103/mt104
/mt99
/mt122
/mt103figure 10: folding the branching process of fig. 7 by future () yields the petri net nf()
(left). removing places of the implicit conditions b8andb14yields the petri net ni()
(middle). abstracting the chain of ytransitions yields the net nc() (right).
3. whilechanges:
for any two events e1;e2ofwith(e1) =(e2) ande1=fy1;:::;ykg,
e2=fz1;:::;zkgwithyizi, fori= 1;:::;k , sete1e2, and set
uv, for any two pre-conditions u2e1,v2e2with the same label
(u) =(v).
4. return future () :=.
foldingalong=future () merges the maximal conditions of , i.e., rebuilds
the nal places of the process model of n, and then winds up backwards as
much as possible. this way, we also identify loops in the process model as
illustrated in fig. 10.
takingof fig. 7 as input, the algorithm sets b13b18in step 2,b11
remains singleton. in the third step, rst e7e11andb12b17are set because
ofb13b18; thene6e10andb7b7,b8b14,b9b16;b5b10. the
equivalence b9b16introduces a loop in the folded model. step 3 continues
withe4e9andb4b15, so thate8(v) has nowb4(d) in the post-set. folding
by this equivalence yields the net nf() of fig. 10. it diers from nof fig. 5
primarily in representing action ztwice in dierent contexts. this example
illustrates the main eect of future (): to make process ow w.r.t. termination
more explicit.
complexitywise, future () has at mostjejsteps where events are merged;
merginge1with another event requires to check at most jejeventse2; whether
e1ande2are merged depends on the equivalence classes of their post-sets.
hence, future () runs ino(jej2k) wherekis the size of the largest post-set
of an event.
the folded model exhibits the behavior and possibly additional be-
havior. some of this additional behavior may be problematic: if the original
modelnreaches an unsafe marking (i.e., a place has more than two tokens), the
17/mt97
/mt121
/mt98/mt120
/mt99
/mt100/mt117 /mt118/mt119
/mt101/mt122
/mt97
/mt120
/mt99 /mt117
/mt97/mt98
/mt121
/mt97/mt100
/mt99
/mt117
/mt97
/mt122
/mt101/mt118
/mt101
/mt119
/mt101/mt101/mt49/mt98/mt49
/mt98/mt55
/mt98/mt49/mt49/mt101/mt55/mt98/mt52
/mt101/mt52
/mt98/mt56/mt98/mt50
/mt98/mt51
/mt98/mt53/mt101/mt50
/mt101/mt51
/mt98/mt54/mt101/mt54
/mt98/mt49/mt48
/mt101/mt53
/mt98/mt57
/mt97
/mt120
/mt99/mt98
/mt121/mt100
/mt117
/mt97
/mt122/mt118/mt97
/mt119
/mt101/mt101/mt49/mt98/mt49
/mt98/mt52
/mt101/mt52/mt98/mt50
/mt101/mt51/mt101/mt54/mt98/mt51/mt44
/mt98/mt55/mt101/mt50/mt44
/mt101/mt55
/mt98/mt49/mt49/mt44
/mt98/mt53
/mt98/mt57/mt44/mt98/mt54/mt44/mt98/mt49/mt48/mt101/mt53/mt98/mt56figure 11: the unsafe net n3(left) has among others the non-deterministic branching process
3(middle); a deterministic future equivalence merges transitions and results in a determin-
istic netnd(3) =3
det(future (3))(right).
folded model nf() =may reach a corresponding marking which enables
two transitions t16=t2with the same label a2(l). however, when replaying
l2lone can select the wrong transition, potentially resulting in a deadlock.
fig. 11 illustrates the situation.
the netn3of fig. 11 and the log l3=fxuzyw;xyvuw;xyuzwgyield the
branching process 3=(n3) shown in the middle. the future equivalence
future (3) would only join b9b6b10. when replaying the third case xyuzw
in3, we have to choose whether e7ore2shall occur; the choice determines
whether the net can complete the case with zor ends in a deadlock.
we can solve the problem by determinizing the equivalenceusing the
following procedure det():
whilechanges do, for any two events e1;e2of,e16e2with(e1) =(e2),
if there exist conditions b1b2withb12e1;b22e2, then
1. sete1e2,
2. setc1c2, for allc12e1;c22e2with(c1) =(c2),
3. setc1c2, for allc12e1;c22e2with(c1) =(c2).
the resulting equivalence relation det(future ()) is a folding equivalence that
is coarser than the trivial equivalence dened by the identity on and ner
than the equivalence dened by the labeling of . for example, determinizing
future (3) of fig. 11 sets additionally b7b3,e7e2,b11b5. note that
we can merge the two ulabeled events because b2b2,b22e7, andb22e2
the resulting folded net nd(3) =3
det(future (3))of fig. 11 (right) is indeed
deterministic and can replay the entire log l3.
the folded net det(future ())exhibits(by lem. 1) and possibly more be-
havior because the folding infers loops from and merges nondeterministic
transitions, which merges branches of unobservable choices until an observable
choice. for our running example ( in fig. 7),nf() is already deterministic
(cf. fig. 10), i.e., nd() =nf().
18the preceding operations of unfolding a mined net nto its log-induced
branching process (l) and refolding (l) tond(l) :=(l)det(future ((l)))
yields a net that can replay all cases in l(by lem. 1). the structure of nd(l)
is at most as complex as the structure of the original net n| when(l) com-
pletely folds back to n. we observed in experiments that this operation typically
reduces the complexity of nby up to 30%.
5. controlled generalization and simplication
in section 4 we showed how to fold a (ltered) log-inducted branching process
(l) back to a petri net ndthat exhibits (l) (and possibly more behavior).
ndmay still be structurally complex. we can further simplify ndwith dierent
techniques that we present in this section: (1) remove implicit places, (2) ab-
stract a complex sub-structures to a more simple sub-structure, and (3) split a
complex sub-structure into several simpler structures. we show in the following
how to remove, abstract, or split the \right" places and sub-structures, so that
the behavior of ndis preserved or generalized in a controlled way.
5.1. removing implicit places
a standard technique for structurally simplifying a petri net nwhile pre-
serving its behavior is to remove places that do not restrict the enabling of a
transition. such places are called implicit .
denition 5 (implicit place) .letnbe a petri net. a pre-place pof a transi-
tiontofnisimplicit i whenever nreaches a marking mwith tokens on each
pre-placetnfpg, then also phas a token.
in other words, whether tis enabled only depends ontnfpg. removing an
implicit place pfromnpreserves the behavior of nup to tokens on p[24]. in
the running example of fig. 5, place fis implicit. removing an implicit place
pfromnreduces its structural complexity because at least two adjacent arcs
are removed as well. this yields an idea for our rst simplication operation:
remove as many implicit places from the folded net nd() as possible.
note however that not all implicit places of a net can be removed, as an im-
plicit place pcan cease to be implicit when another implicit place gets removed.
finding a maximal set of implicit places is a well-known problem that can be
solved by solving a system of linear (in-)equations [24] using an ilp-solver.
5.2. removing places based on implicit conditions
we found in initial experiments that petri nets discovered by process discov-
ery algorithms contain only few implicit places according to def. 5; the model
simplication is only marginal. for example the model of fig. 12(top left) con-
tains no implicit place.
fortunately, def. 5 requires more than we do in our setting. the refolded
netnd() which we want to simplify generalizes the behavior recorded in the
loglto the occurrence sequences of nd(). removing implicit places of nd()
19t1
t2
t3
t4
t5t6
t7
t8
t9t10
t11
t12
t13
t14
t1
t2
t3t4
t5t6
t7t8
t9
t10t11
t12
t13
t14
t1
t2
t3
t4t5
t6 t7
t8t9t10t11
t12t13
t14figure 12: the net at the top left has no implicit conditions; it can be simplied by removing
dierent variants of places based on implicit conditions.
preserves all occurrence sequences ofnd() whereas we are only interested in
preservingl, or more precisely, . in the following, we introduce a family of
techniques to simplify nd() by removing places based on implicit conditions
of the log-induced unfolding ; the results of the dierent techniques are shown
in fig. 12.
5.2.1. implicit conditions
by focusing on the l-induced unfolding , we can discover which places in
nd() can be removed while preserving the behavior l. the basic idea is to
consider the implicit conditions of the net. removing any implicit condition
bofpreserves the behavior of (up tob), and hence preserves l.
denition 6 (implicit conditions) .letbe a branching process. let imp()
be the set of all implicit places of according to def. 5. we call a set b0
imp() aconsistent subset of implicit conditions of i for each b2b0:bis
implicit inwithoutb0nfbg.
in contrast to implicit places, an implicit condition can easily be detected
on the structure of : a condition bwith pre-eventfeg=bis implicit i for
each post-event f2bthere exists a path from etofthat does not contain b.
for example in fig. 7, conditions b8andb14are implicit.
consider two implicit conditions b1;b22imp(). after removing b1it may
be thatb2is no longer implicit. therefore, we dene the notion of a consistent
subset of implicit conditions in def. 6. all places in a consistent subset can
be removed without changing the behavior whereas the removal of imp() may
enable more behavior.
recall that the places in a folded petri net have identiers corresponding
to equivalence classes of conditions in the branching process (cf. def 4). let
p=fb1;:::;bkgbe such a place obtained using folding relation . removing
20/mt97
/mt119
/mt120 /mt121 /mt122/mt99 /mt98
/mt100 /mt101
/mt97
/mt119
/mt99 /mt98
/mt120
/mt100
/mt121
/mt98
/mt122
/mt101/mt122
/mt101/mt101/mt49/mt98/mt49
/mt98/mt50 /mt98/mt51
/mt101/mt50
/mt98/mt52
/mt101/mt52
/mt98/mt53
/mt101/mt53
/mt98/mt54/mt101/mt54
/mt98/mt55
/mt97
/mt119
/mt98
/mt120
/mt100
/mt121
/mt98
/mt122
/mt101/mt122
/mt101
/mt120
/mt100/mt101/mt49/mt98/mt49
/mt98/mt51
/mt101/mt50
/mt98/mt52
/mt101/mt52
/mt98/mt53
/mt101/mt53
/mt98/mt54/mt101/mt54
/mt98/mt55figure 13: the net n(left) has no implicit places; the c-labeled condition b2of its log-induced
unfolding(middle) is implicit; removing cfromnallows for more behavior \after" (right).
pfromnd() corresponds to removing b1;:::;bkfrom, and if all conditions
b1;:::;bkare implicit, then the behavior of is preserved. in other words, we
may consider pas implicit w.r.t. .
denition 7 (im1-implicit places) .letbe a branching process, a folding
equivalence, and n=the folded net. let b0imp() be a consistent
subset of implicit conditions of . a placepofnisim1-implicit (w.r.t.b0) i
pb0.
by denition, removing all im1-implicit places from nsimplies the net and
preserves the behavior of (and hence of l). the places highlighted in the net
of fig. 12(top left) are classied as im1-implicit, but not as classically implicit;
removing these places results in the net shown in fig. 12(bottom left). we
observed in experiments that this notion of implicit places identies on average
twice as many implicit places as the classical notion (def. 5).
figure 13 illustrates the dierence between classically implicit places and
im1-implicit places. assume the net nand the log l= [wxyz;wz] to be given.
thel-induced unfolding ofnis shown in fig. 13(middle). folding yields
nagain. condition b2is implicit in which makes can im1-implicit place of
n. removing ccorresponds to removing b2: it preserves the behavior of n
w.r.t., but it generalizes the behavior \after" . the net without chas the
occurrence sequence wxyx which cannot occur in nasyconsumed the token on
cneeded by x. the branching process of nwithout cshown in fig. 13(right)
illustrates the situation.
5.2.2. limit generalization
removing im1-implicit places usually generalizes the behavior of nbeyond
, i.e., behavior that is not described by the original log l. this generalization
21can be limited by considering more behavior of nto be preserved, i.e., by
extendingwith more events of nto a branching process +and then remove
places from nbased on implicit conditions of +. experiments have shown that
the branching process \after" tends to grow very quickly for nets discovered
by process discovery algorithms. we found the \single extension by all enabled
events" to be feasible.
denition 8 (im1+-implicit places) .letbe a branching process, a folding
equivalence, and n=the folded net.
1. for each transition t2tn, let en(t;) be the set of all enabling locations
oftin(locations in wheretis enabled but did not occur, see sect. 2.3).
let+be the branching process obtained by extending , for eacht2tn
and eachb2en(t;) with a fresh t-labeled event e;e=b.
2. letb0imp(+) be a consistent subset of implicit conditions of .
3. a place pofnisim1+-implicit ipb0.
each im1+-implicit place is also an im1-implicit place, but not vice versa.
an additional event ein+turns an im1-implicit condition b2einto a non-
implicit condition if bis really needed to enable e, e.g., ife=fbg. also, there
are im1+-implicit places that are not classically implicit.
figure 12(top right) shows the net from top left after removing im1+-implicit
places, the remaining im1-implicit places are highlighted red. we could conrm
that removing im1+-implicit places generalizes the behavior of nless than re-
moving im1-implicit places. yet, removing im1-implicit or im1+-implicit places
yields almost the same reduction in terms of model complexity.
5.2.3. stronger simplication
the two previous notions require all conditions that constitute a place to be
implicit. alternatively, we may classify a place as implicit as soon as some of
its constituting conditions is implicit.
denition 9 (im2-implicit places) .letbe a branching process, a folding
equivalence, and n=the folded net. let b0imp() be a consistent
subset of implicit conditions of .
1. a place pofnisim2-implicit ip\b06=;.
2. a place pofnisim2 -implicit ip\imp()6=;.
im2-implicit places are much more liberal than im1-implicit places because
a single implicit condition suces to characterize the place as implicit. the idea
for this notion is to superimpose the fact that pis implicit in some situations to
all situations of p. im2 -places are even more general as the implicit condition
does not even have to be from a consistent subset of implicit conditions.
removing all im2-implicit (im2 -implicit) places from ncould yield tran-
sitions without pre-place (which then can occur unboundedly) or transitions
without post-place (which then have no eect on the net). to preserve a mini-
mum of discovered process logic in n, we remove im2-implicit (im2 -implicit)
places from n1-by-1 as follows.
22lethp1;:::;pkibe an enumeration of the im2-implicit places (im2 -implicit)
ofn.
fori= 1;:::;k , ifpiis the only pre-place of its post-transitions or the
only post-place of its pre-transitions, then keep piinn, otherwise remove
pifromn.
applying this procedures usually generalizes behavior in (in contrast to re-
moving im1-implicit places). however, we observed in experiments signicant
structural simplications that outweigh generalization by large. in some cases,
the structure simplied by up to 72%; up to 95% of the places were implicit.
figure 12(bottom right) shows the net from the top left after removing all im2-
implicit places; also the bottom left net highlights the im2-implicit places that
are not im1-implicit.
5.3. abstracting substructures: chains of unrestricted transitions
the previously presented two operators, unfolding/refolding and removing
implicit places, generalized and simplied nalong the structure of nas it was
dened by the discovery algorithm mthat returned n. next, we present two
operators to generalize nbychangingn's structure.
petri nets discovered through process discovery often contain several unre-
stricted transitions which are always enabled such as transition yin fig. 5. the
branching process then contains a chain of occurrences of these transitions that
often cannot be folded to a more implicit structure as illustrated by e2ande5
of fig. 10.
yet, we can abstract such a chain t1:::tnof unrestricted transitions with
the same label ato a loop of length 1: (1) replace t1:::tnwith a new transition
tlabeleda, (2) add a new place pin the pre- and post-set of t, and (3) for
each placepwhich had a tiin its pre-set and no other transition tj6=tiin its
post-set, add an arc ( t;p). fig. 10 illustrates the abstraction: abstracting the
chain of y-labeled transition of ni() (middle) yields the net nc() (right); we
observed signicant eects of this abstraction in industrial case studies.
the new transition tcan mimic the chain t1:::tn:tis always enabled and
an occurrence of thas the combined eect of all t1;:::;tn. for this reason,
a chain-abstracted net exhibits at least the behavior of the original net and
possibly more. for longer chains this results in a signicant reduction in size.
5.4. splitting ower places
the last operation in this paper deals with a specic kind of places that
are introduced by some discovery algorithms and cannot be abstracted with the
previous techniques. we deal with these places by splitting them into several
places.
aower place pis place which has many transitions that contain pin their
pre- and their post-set. mostly, ponly sequentializes occurrences of these tran-
sitions as can be seen in fig. 14 to the left. here, fcan be viewed as a ower
23/mt119/mt97
/mt98/mt99
/mt120
/mt121
/mt100/mt122
/mt101/mt102
/mt119/mt97
/mt98/mt99
/mt120
/mt121
/mt100/mt122
/mt101/mt102/mt102/mt44/mt122figure 14: the ower place fin the net on the left sequentializes occurrences of wand z.
splitting fand removing self-loops yields the structurally simpler net on the left with more
behavior.
place. for example, zmay occur arbitrarily often before or after w, though only
after xand before yoccurred. similarly, wcan only occur after xand before y.
however, as wcan occur only once, the restrictive eect of ower place fonw
is limited.
based on this observation we may (1) remove self-loops of transitions that
are still restricted by another pre-place such as w, and (2) split the ower place
for a transition tthat has no other pre-place, i.e., to create a new place pin
the pre- and post-set of t. the net in fig. 14 to the right shows the result of
this abstraction. the resulting net exhibits more behavior than the original
net. some of this behavior may even be not explained by the log. for example,
wmay occur now before xand after y. yet, the transformation may help to
signicantly reduce the number of synchronizing arcs in the discovered process
model. we observed a removal of up to 95% of the arcs leading to structural
simplication of the same amount.
5.5. a parameterized process model simplication algorithm
all operations presented in the preceding sections together yield the following
algorithm for simplifying a petri net n=m(l) that was discovered from a log
lby a process discovery algorithm m(see also fig. 4).
1. construct the branching process (l) ofnthat represents all cases l2l;
optionally lter infrequent behavior from (l) (see sect. 3.3); construct
the folding equivalence =det(future ((l))); foldnd:=(l).
2. remove implicit places from nd(using one of the notions presented in
sect. 5.1 and 5.2).
3. abstract chains of unrestricted actions from nd.
4. split ower-places of nd.
the technique is modular . by design, the result of each transformation step is a
net that is structurally simpler than the preceding net and can replay the entire
logl, i.e., the resulting model n0recalls each case of l. moreover, starting from
(l) which is an overtting model of l, each step also generalizes (l) towards
an undertting model of l. the degree to which n0allows more behavior than l
is measured by a precision metric [26, 27]. each of the four steps can be applied
24figure 15: the conguration screen of the prom plugin \uma" and an example of a simplied
model.
selectively. this way it is possible to balance between precision, generalization,
and complexity reduction.
the algorithm described above assumes a perfectly aligned event log where
all behavior is considered to be relevant. in sect. 3.1, we showed how to align
log and model by massaging the event log. alignment is considered to be a
preprocessing step conducted before executing the above algorithm.
6. experimental results
this section describes the implementation of the techniques introduced in
the preceding sections and reports on experimental results. all result were
obtained using our implementation in prom.
6.1. implementation in prom
we implemented our approach as a plugin for the process mining toolkit
prom. prom as well as the package \uma" that contains the plugin are available
athttp://www.processmining.org/ .
to use the plugin called \simplify mined models", the user picks as input a
log and a petri net that was discovered from this log. the plugin then shows
a panel (fig. 15), where the user can congure the simplication of the net by
disabling any of the steps as discussed in sect. 5.5 and by choosing the amount
of simplication and generalization to be applied when removing implicit places.
by default, all steps are enabled requiring no petri net knowledge from the user
for model simplication; the default setting for implicit places is \im2" (def. 9).
25the plugin then runs fully automatically and returns the simplied petri net
which can be inspected and analyzed further as shown in fig. 15.
our implementation applies a number of optimizations to increase perfor-
mance. the log-induced unfolding is built using data structures similar to the
ones used for constructing classical unfoldings of petri nets [22]. the unfolding
is only refolded after the entire folding relation has been constructed; this way
we only have to handle equivalent sets of nodes rather than change the structure
of a petri net. because the unfolding can become very large, we save memory
by detecting implicitness of a condition by depth-rst search on the unfolding's
structure (see sect. 5.2); intermediate results are cached to improve search per-
formance. a canonical order of nodes in the unfolding allows to truncate parts of
the search space that contain no predecessor of a node. our current implemen-
tation does not search for a largest consistent set of implicit conditions (def. 6)
and picks conditions in a greedy way until no other condition is implicit; future
improvements are possible. we use an extension library of the vip-tool [28] to
identify classically implicit places using an ilp solver.
6.2. eect and comparison of simplication steps
using this plugin, we validated our approach in a series of experiments on
benchmark logs, and logs obtained in industrial case studies. for each exper-
iment, we generated from a given log la petri net nwith the ilp miner [9]
using its default settings; the log was notpre-processed beforehand. we then
applied the simplication algorithm of sect. 5.5 on nusing the original log
l. figs. 1 and 2 illustrate the eect of our algorithm on industrial processes:
the algorithm balances the control-ow logic of a discovered process model by
removing up to 85% of the places, and up to 92% of the arcs.
6.2.1. eect of complete simplication procedure
table 1 gives some more details when applying all reduction steps and remov-
ing im2-implicit places (which we consider the most suitable setting for model
simplication). the logs named a xnyare benchmark logs having xdierent
activities; the a xn0 are logs of highly structured processes. yis the percentage
of random noise events introduced into the log a xn0. the remaining logs were
obtained in case studies in the health care domain (hc) and from municipal ad-
ministrations (m). we compared the nets in terms of the numbers jpj,jtj, and
jfjof places, transition and arcs, and their simple graph complexity c=jfj
jpj+jtj
which roughly correlates with the perceived complexity of the net. the eect of
the algorithm is measured as the percentage of places jpjand arcsjfjremoved
from (or added to) the original net, and the percentage by which the graph
complexity was reduced.
the numbers show that almost all models could be reduced signicantly in
terms of places and arcs (up to 85% and 92% with 53% and 67% in average). we
observed that some models (a32n0, m1) grew slightly in size, i.e., more places
and transitions were introduced. we found unrolled loops of length greater
than 2 that occur only once in the branching process to be responsible for the
26table 1: experimental results using all reduction steps and removing im2-implicit places.
original simplied dierence runtime
jpj/jtj/jfj/cjpj/jtj/jfj/cjpj/jfj/c in sec
a22n00 21/ 22/ 60/ 1.40 20/ 22/ 54/ 1.29 -5%/ -10%/ -8% 0.819
a22n05 38/ 22/204/ 3.40 19/ 22/ 77/ 1.89 -50%/ -62%/ -45% 2.574
a22n10 52/ 22/428/ 5.78 14/ 22/ 78/ 2.17 -73%/ -82%/ -63% 29.2
a22n20 74/ 22/569/ 5.93 14/ 22/ 58/ 1.61 -81%/ -90%/ -73% 131.9
a22n50 91/ 22/684/ 6.05 15/ 22/ 52/ 1.41 -84%/ -92%/ -77% 163.5
a32n00 32/ 32/75/ 1.17 32/ 32/ 74/ 1.16 0%/ -1%/ -1% 0.243
a32n05 44/ 32/225/ 2.96 34/ 32/ 107/ 1.62 -23%/ -52%/ -45% 5.1
a32n10 68/ 32/543/ 5.43 24/ 32/ 111/ 1.98 -65%/ -80%/ -63% 83.6
a32n20 90/ 32/612/ 5.02 28/ 32/ 98/ 1.63 -69%/ -84%/ -67% 108.8
a32n50 110/ 32/868/ 6.11 27/ 32/ 102/ 1.73 -75%/ -88%/ -72% 173.4
hc1 41/ 15/224/ 4.00 10/ 15/ 44/ 1.76 -76%/ -80%/ -56% 0.029
hc2 20/ 14/139/ 4.09 8/ 14/ 48/ 2.18 -60%/ -65%/ -47% 0.003
hc3 23/ 14/122/ 3.30 11/ 14/ 42/ 1.68 -52%/ -66%/ -49% 0.003
hc4 43/ 17/224/ 3.73 11/ 17/ 47/ 1.69 -74%/ -79%/ -55% 0.028
hc5 89/ 26/959/ 8.34 13/ 26/ 79/ 2.03 -85%/ -92%/ -76% 0.362
m1 58/ 55/358/ 3.17 64/ 81/ 196/ 1.35 10%/ -45%/ -57% 0.163
m2 31/ 23/255/ 4.72 17/ 23/ 64/ 1.60 -45%/ -75%/ -66% 0.038
avg 54/ 26/ 385/ 4.4 21/ 27/ 78/ 1.7 -53%/ -67%/ -54% 41.2
max -85%/ -92%/ -77%
growth in size. our algorithm cannot fold back these singular loops; though the
algorithm could be extended to handle such patterns. yet, in all cases where
the logs contained noisy behavior, our technique reduced each petri net's graph
complexity cby 45% to 77% (with 54% in average). a modeler is able to inspect
models of this complexity and gain an understanding of the modeled process as
illustrated by figs. 1 and 2 which show the models of hc1 and m2.
6.2.2. eect of individual simplication steps
we also investigated the eect of the individual simplication steps and
compared the dierent notions of implicit places introduced in section 5.2. fig-
ure 16 shows how model complexity changed through the dierent stages of the
algorithm in comparison to the notion of implicit places that was removed in
the corresponding step; the diagrams show minimum, average, and maximum
graph complexity in our data set.
it turned out that unfolding/refolding in most cases only yields a reduction
of up to 5%, with one exception from the industrial logs yielding about 30%
reduction. the dierent modes of removing implicit places have dierent eects:
1. removing classical implicit places (im) reduces complexity by at most 5%.
in average, 3 places were removed from the model.
2. removing im1- and im1+-implicit places reduces complexity by about 5%.
about 6 places were removed by im1+and 6.5 places by im1 in average.
3. removing im2- and im2 -implicit places yields signicant reduction for
all logs that contain noise, ranging from 40% up to 60% reduction in
complexity. in average, 50% of the places (23 places) were removed.
chain reduction removed between 12% and 51% of the transitions in the refolded
27im
im1+
im1
im2
im2−
originalfoldedimpliedchainsflower01248
complexity
im
im1+
im1
im2
im2−
originalfoldedimpliedchainsflower01248
im
im1+
im1
im2
im2−
originalfoldedimpliedchainsflower01248
(min) (avg) (max)figure 16: accumulated eect of model simplication depending on the kind of implicit places
being reduced.
model of logs with 5% noise and in m1; as this reduction removes transitions,
places, and arcs, it has no measurable eect in the graph complexity.
from models obtained by removing classically/im1/im1+-implicit places,
splitting ower places allows to remove up to 95% of the arcs. in particular,
models with a high level of noise tend to have a large number of arcs connected
to ower places. after removing im2- and im2 -implicit places, splitting ower
places can remove about 10% of the arcs in logs with a noise level of 10% or
more, other arcs already had been removed beforehand by removing their adja-
cent implicit places.
runtimes correlate with the size of the branching processes constructed by
the algorithm, we observed branching processes of up to 192,000 nodes and
360,000 arcs in the benchmarks and 4,800 nodes and 9,800 arcs in the case
studies. runtime is dominated by computing the folding equivalence: roughly
2/3 of the runtimes shown in tab. 1 are used for folding. the runtimes for
unfolding, chain abstractions, and splitting ower are neglible. runtimes for
removing implicit places depend on the notion:
1. classically implicit places can be removed in at most 3 sec (1sec in avg.).
2. removing im1-implicit places succeeds in milliseconds.
3. removing im1+-implicit places takes signicantly more time as the un-
folding needs to be extended rst (about 50% of the time is required for
folding, i.e., max. 30 secs, avg. 3 secs).
4. removing im2-implicit places takes runtimes up to 54 secs, 9 secs in av-
erage. here we have to rst nd a consistent set of implicit conditions
before removing places. for im1- and im1+-implicit places this set can
be built much faster by exploiting that implicit conditions of interest are
folding equivalent.
5. im2 -implicit places avoid computing a consistent set and hence compu-
tation nishes in less than a second.
compared to an earlier version of the paper [29] optimizations in the implemen-
tation (see sect. 6.1) reduced runtime by about factor 6.
28(min)
originalunfolding foldedimplied chains flower−0.200.20.40.6
change in precision im
im1+
im1
im2
im2-(avg)
originalunfolding foldedimplied chains flower−0.200.20.40.6
im
im1+
im1
im2
im2-(max)
originalunfolding foldedimplied chains flower−0.200.20.40.6
im
im1+
im1
im2
im2-figure 17: change in precision depending on the kind of implicit places being reduced.
6.2.3. generalization of behavior
finally, we measured the amount of behavioral generalization that is intro-
duced by the dierent simplication steps. in particular, we compared the eect
of the dierent notions of implicit places. we measured generalization by com-
paring how much more behavior a model allows compared to its original log,
this measure is known as precision . in our experiment, we applied the measure
proposed in [27, 30]. a precision value of 1 :0 means the model allows for exactly
the behavior in the model; the lower the precision, the more behavior does the
model allow.
in the diagram in fig. 17 the x-axis shows the dierent stages of the simpli-
cation algorithm (including the log-induced unfolding), the y-axis the dierent
notions of implicit places, and the z-axis the relative change of precision com-
pared to the precision of the original model, for the worst case, the average case,
and the best case.
as expected, we could observe a signicant increase in precision for the log-
induced unfolding (reaching +0 :5 in average and up to +0 :61 in the best case).
the unfolding presents just the behavior of the log and additional linearizations
of concurrent transitions that are not represented in the log (thus, an unfolding
usually does not have perfect precision of 1 :0). refolding the unfolding increased
precision by up to 0 :1 in the best case and just slightly in average, compared
to the original model. when removing implicit places, precision drops. the
amount of generalization (i.e., loss in precision) depends on the notion of implicit
places.
for the classical notion (def. 5), precision was preserved in all cases. re-
moving only im1+-implicit places still yielded increased precision in the average
case and a loss of about  0:05 in the worst case. the more liberal the notion of
implied places, the more precision is lost, with  0:15 ( 0:19) in the worst case
for im2- (im2 )-implicit places in the worst case.
when applying alls steps, the eect for dierent notions of implicit places
converge, ranging from  0:14 precision for classically implicit places to  0:21
for im2 -implicit places in the worst case. most importantly, in average we
observed only a very small loss of precision by  0:02 to 0:07 whereas precision
did increase for some cases between +0 :06 and +0:09.
29figure 18: filtering the model of fig. 2 with thresholds f= 0;0:05;0:15;0:3, and removing
im1+-implicit places.
6.3. eect of filtering infrequent behavior
we just presented the eects of the simplication algorithm that preserved
all behavior of the log. our approach also allows to remove infrequent behavior
from the branching process as described in sect. 3.3 in order to remove behavior
that requires complex model structures. to measure the eect of the ltering,
we implemented the following relative lter: for a lter threshold f, remove from
the branching process every event e(and its successors) where (e)=(b)< f,
whereb2eis the pre-condition of ewith the smallest -value. the rationale
for this lter is that the least-frequent pre-condition of edetermines how often
ecould have occurred at most.
in the experiment, models were unfolded, ltered, folded, and nally im1+-
implicit places were removed. we measured the eect of this lter for values
f= 0:05;0:1;0:15;0:2;0:3.
figure 18 illustrates the eect of the ltering on the model of fig. 2; we depict
the unltered model f= 0 and the models for f= 0:05;0:15;0:3. the ltering
of infrequent behavior (noise) immediately results in less complex models. for
f= 0:05 some structured process logic is recognizable in the model; for f= 0:15
the model is rather structured with some complex logic in the middle; f= 0:30
lters all behavior but the most frequent path, the model has no alternative
executions.
figure 19 shows how model complexity changes in the entire data set through
the dierent stages of the simplication algorithm. we depict average (left)
and maximal model complexities (right) for lter values f= 0:05 (top) and
f= 0:30 (bottom); the results for the other lter values are linearly distributed
between these two extremes. for models that already were structurally simple,
no dierence could be noted between the unltered and the ltered cases.
model complexity falls as more behavior is ltered from the branching pro-
cess (f= 0:05 vsf= 0:30). filtering is particularly eective on models discov-
ered from logs with much infrequent behavior (e.g., benchmark logs with noise
10%). the main eect of ltering can be seen before ower places are split.
after ower places are split, the complexities converge in all cases to similar val-
ues. this suggests that all behavior (including the frequent behavior) requires
a structure of certain complexity that cannot be simplied without generalizing
the model's behavior.
30im
im1+
im1
im2
im2−
original foldedimplied chains flower01248
complexity
im
im1+
im1
im2
im2−
original foldedimplied chains flower01248
complexityim
im1+
im1
im2
im2−
original foldedimplied chains flower01248
im
im1+
im1
im2
im2−
original foldedimplied chains flower01248
(5%,avg) (5%,max)
(30%,avg) (30%,max)figure 19: change in complexity by ltering infrequent behavior.
when comparing simplication by removing implicit places with simplica-
tion by ltering, the resulting models dier signicantly depending on how the
structure is simplied, i.e., compare fig. 18(ltering) and fig. 2(right, removing
implicit places and ower places). we observed growing model complexity for
some models when implicit places were removed after ltering, see fig. 19(right).
this is mostly due to places with few incoming or outgoing arcs that are ob-
tained by ltering (low complexity) and that are then classied as implicit places
afterwards and removed (rising complexity).
filtering removes infrequent behavior from the branching process. the l-
tered behavior is (most likely) no longer a behavior of the refolded model, i.e.,
the model cannot replay the complete original log but only its frequent behavior.
to measure the deviation introduced by ltering, we also measured the tness of
each ltered, simplied model to its original log using the technique of [17, 18].
figure 20 shows how tness of the resulting model changed depending on the
lter threshold and the notion of implicit places; the least tness (left) and the
average tness (right) of the data set are shown; a model that can replay all
traces has tness 1 :0.
in all models discovered from logs with noise, ltering results in models that
cannot replay the entire log anymore (falling tness). for low ltering thresholds
(< :1) log and model deviate only in 1 or 2 events per non-tting cases which
still leads to high tness values of approx :95. the more behavior is ltered, the
more cases cannot be replayed on the model. we observed a steep decline in
tness for lter thresholds of f= 0:3 and more. here also the frequent behavior
31im
im1+
im1
im2
im2−0%5%10%15%20%30%0.20.40.60.81
im
im1+
im1
im2
im2−0%5%10%15%20%30%0.20.40.60.81
fitness(min) (avg)figure 20: change in tness depending on thresholds for ltering infrequent behavior.
gets ltered. the notion of implicit places has no signicant impact on tness.
throughout the experiment, most eective results were achieved for logs
with many cases. in these logs, frequent and infrequent behavior can clearly be
distinguished which leads to eective ltering as demonstrated in fig. 18. for
logs that consist of only few or many unique cases, we observed that all behavior
was ltered for lter thresholds >0:05 (as the branching process contained no
frequent behavior). for these logs, ltering is inapplicable.
6.4. discussion
in sight of these observations, we can conclude that our proposed technique
for simplifying discovered process models succeeds. typically, the structure
of a discovered model can be simplied dramatically while tness is preserved
completely .
at the same time, we could conrm that this simplication comes with a
reasonable model generalization w.r.t. the behavior recorded in the log. in
average, model behavior generalizes only slightly, though in some cases the
model can be generalized considerably, depending on the chosen parameters.
the choice in the notion of implicit places allows to trade simplication vs.
generalization (i.e., im1 vs. im2) as well as generalization vs. runtime (i.e.,
im1+vs. im1 and im2 vs. im2 ).
the model can also be simplied by removing infrequent behavior through
ltering. this allows to trade simplication for specialization: the ltered model
is signicantly simpler but only replays a subset of the log. we also observed
ltered models to be more structured than unltered models (i.e., compare fig. 2
and fig. 18).
altogether, we found a new way to let a user balance between the 4 com-
peting quality criteria of process models: tness, simplicity, generalization (not
overtting) and precision (not undertting) [1]. in particular, our technique
allows to improve one criterion (simplicity) while balancing the other criteria in
a controlled way. however, our approach should not be seen as a black box: a
user still has to choose which criteria she wants to optimize for, e.g., simplicity
vs. precision.
327. related work
in the last decade, process mining emerged as a new research disciple com-
bining techniques from data mining, process modeling, and process analysis.
process discovery, i.e., constructing a process model based on sample behavior,
is the most challenging process mining task [1, 3] and many algorithms have
been proposed [2]. examples are the algorithm [13], variants of the algo-
rithm [14, 15], heuristic mining [4], genetic process mining [6, 7], fuzzy mining
[5], process mining techniques based on language-based regions [8, 9], mining
based using convex polyhedra [10], and various multi-phase approaches [11, 12].
the approach presented in this paper can be used to simplify the models
generated by all of the approaches mentioned above. however, there are two
relevant characteristics. the rst characteristic is whether the discovery algo-
rithm produces a petri net or a process model in some other notation (e.g.,
heuristic nets, c-nets, epcs, or plain transition systems). the techniques de-
scribed in [8{10, 13{15] produce directly a petri net. for the other approaches
some conversion is needed [7, 16]. the second characteristic is whether the dis-
covery algorithm guarantees a tness of 1. most techniques do not provide such
a guarantee. notable exceptions are [8{12]. however, these techniques cannot
deal with noise. therefore, we showed that it is possible to align log and model
in sect. 3.1. this way we can also handle process models that are not perfectly
tting. for aligning traces without an initial model we refer to [31].
the approach of [12] allows to balance between overtting and undertting of
mined process models, controlled by the user. however, this approach requires
expert knowledge to nd the right balance. our approach is easier to congure,
and yields signicant simplication in the fully automatic setting. moreover,
our approach even simplies models produced by [12] as illustrated by fig. 3.
our approach can improve results of existing discovery algorithms because
all reasoning on how to improve a model is done on the log-induced unfolding .
this unfolding provides a global view on behavior that is directly related to
the model's structural features and contains information about frequencies and
redundancies of model elements w.r.t. the given log. such combined information
isnot available to most process discovery algorithms during the discovery.
conformance checking techniques [17, 18, 26, 27, 30], like the post-processing
approach presented in this paper, use a log and a model as input. in [26], the
log is replayed on the petri net to see where the model and reality diverge. in
[27, 30], the behavior of the model restricted to the log is computed. the border
between the log's and model's behavior highlights the points where the model
deviates from the log. in [17, 18], the problem of aligning model and log is
translated into an optimization problem to nd the \closest" path in the model.
the goal of this paper is to simplify and structure discovered process mod-
els. this is related to techniques discovering block structures in graph-based
models [32, 33] and transforming unstructured models into structured ones [34].
these techniques focus on and preserve concurrency in models as in our ap-
proach. in particular the future equivalence used in this paper to fold the log-
based unfolding preserves fully concurrent bisimulation [35] that is also used
33in [34]. however, these techniques just consider the model as is and not the
behavior that has been observed in reality and recorded in a log. moreover, [34]
focuses on equivalent transformations whereas we allow to generalize or to lter
behavior to trade competing quality criteria if needed.
the problem coped with in this paper resembles the problem of restricting a
system (here n) to admissible behaviors (here l) by means of a controller , e.g.,
[36]. however, these approaches require nto have a nite state space, which
usually does not hold for discovered process models. additionally, our aim is
also to structurally simplify n, not only to restrict it to l.
this paper is an extended version of our bpm 2011 paper [29]. compared to
[29] we extended our approach to be able to deal with an initial model that does
not t completely. moreover, we showed that the overtting branching process
can be extended with frequencies to support the removal of noise and outliers.
we also improved the identication of implicit places using a notion tailored
towards unfoldings. for example, we showed how dierent notions of implicit
place allow to trade model simplication and generalization. in addition, we
presented various new experimental results that also document an acceptable
generalization by our technique despite signicant model simplication. these
results also show that we were able to further improve the approach already
presented in [29].
8. conclusion
the approach presented in this paper can be combined with any process
discovery technique that produces a petri net that can reproduce the event
log. extensive experimentation using real-life event logs show that our post-
processing approach is able to dramatically simplify the resulting models. more-
over, the approach allows users to balance between overtting and undertting.
unnecessary generalization is avoided and the user can guide the simplica-
tion/generalization process.
our decisive technical contribution to process mining is the adaptation of
classical branching processes to log-induced unfoldings . the latter provides a
global view on the behavior in the log that is related to the model structure and
contains information about frequencies, redundancies, and equivalences. this
allows to to balance quality criteria for process models in a dierent way than
before.
acknowledgements. the research leading to these results has received funding from
the european community's seventh framework programme fp7/2007-2013 under
grant agreement no257593 (acsi).
references
[1] w. m. p. van der aalst, process mining: discovery, conformance and
enhancement of business processes, springer, 2011.
34[2] b. f. van dongen, a. k. alves de medeiros, l. wen, process mining:
overview and outlook of petri net discovery algorithms, topnoc 2
(2009) 225{242.
[3] ieee task force on process mining, process mining manifesto, in: bpm
workshops, vol. 99 of lnbip, springer, 2011.
[4] a. j. m. m. weijters, w. m. p. van der aalst, rediscovering workow
models from event-based data using little thumb, integrated computer-
aided engineering 10 (2) (2003) 151{162.
[5] c. w. g unther, w. m. p. van der aalst, fuzzy mining: adaptive process
simplication based on multi-perspective metrics, in: bpm 2007, vol.
4714 of lncs, springer, 2007, pp. 328{343.
[6] a. k. alves de medeiros, a. j. m. m. weijters, w. m. p. van der aalst,
genetic process mining: an experimental evaluation, data mining and
knowledge discovery 14 (2) (2007) 245{304.
[7] w. m. p. van der aalst, a. k. alves de medeiros, a. j. m. m. weijters,
genetic process mining, in: g. ciardo, p. darondeau (eds.), applications
and theory of petri nets 2005, vol. 3536 of lncs, springer, 2005, pp.
48{69.
[8] r. bergenthum, j. desel, r. lorenz, s. mauser, process mining based on
regions of languages, in: bpm 2007, vol. 4714 of lncs, springer, 2007,
pp. 375{383.
[9] j. m. van der werf, b. f. van dongen, c. a. j. hurkens, a. serebrenik,
process discovery using integer linear programming, fundamenta infor-
maticae 94 (2010) 387{412.
[10] j. carmona, j. cortadella, process mining meets abstract interpretation,
in: j. balcazar (ed.), ecml/pkdd 210, vol. 6321 of lecture notes in
articial intelligence, springer, 2010, pp. 184{199.
[11] b. f. van dongen, w. m. p. van der aalst, multi-phase process mining:
building instance graphs, in: er 2004, vol. 3288 of lncs, springer, 2004,
pp. 362{376.
[12] w. m. p. van der aalst, v. rubin, h. m. w. verbeek, b. f. van dongen,
e. kindler, c. w. g unther, process mining: a two-step approach to balance
between undertting and overtting, software and system modeling 9 (1)
(2010) 87{111.
[13] w. m. p. van der aalst, a. j. m. m. weijters, l. maruster, workow
mining: discovering process models from event logs, ieee transactions
on knowledge and data engineering 16 (9) (2004) 1128{1142.
35[14] l. wen, w. m. p. van der aalst, j. wang, j. sun, mining process models
with non-free-choice constructs, data mining and knowledge discovery
15 (2) (2007) 145{180.
[15] l. wen, j. wang, w. m. p. van der aalst, b. huang, j. sun, mining pro-
cess models with prime invisible tasks, data and knowledge engineering
69 (10) (2010) 999{1021.
[16] w. m. p. van der aalst, a. adriansyah, b. f. van dongen, causal nets:
a modeling language tailored towards process discovery, in: j. katoen,
b. koenig (eds.), 22nd international conference on concurrency theory
(concur 2011), lncs, springer, 2011, pp. 28{42.
[17] a. adriansyah, b. f. van dongen, w. m. p. van der aalst, towards robust
conformance checking, in: m. muehlen, j. su (eds.), bpm 2010 work-
shops, proceedings of the sixth workshop on business process intelligence
(bpi2010), vol. 66 of lnbip, springer, 2011, pp. 122{133.
[18] a. adriansyah, b. van dongen, w. m. p. van der aalst, conformance
checking using cost-based fitness analysis, in: ieee international en-
terprise computing conference (edoc 2011), ieee computer society,
2011.
[19] u. goltz, w. reisig, processes of place/transition-nets, in: icalp'83,
vol. 154 of lecture notes in computer science, springer, 1983, pp. 264{
277.
[20] j. engelfriet, branching processes of petri nets, acta informatica 28 (6)
(1991) 575{591. doi:http://dx.doi.org/10.1007/bf01463946.
[21] m. nielsen, g. d. plotkin, g. winskel, petri nets, event structures and
domains, part i, theor. comput. sci. 13 (1981) 85{108.
[22] j. esparza, s. r omer, w. vogler, an improvement of mcmillan's unfolding
algorithm, formal methods in system design 20 (3) (2002) 285{310.
[23] j. esparza, k. heljanko, unfoldings: a partial-order approach to model
checking, springer, 2008. doi:10.1007/978-3-540-77426-6.
[24] j. colom, m. silva, improving the linearly based characterization of p/t
nets, in: advances in petri nets 1990, vol. 483 of lncs, springer, 1991,
pp. 113{145.
[25] d. fahland, from scenarios to components, ph.d. thesis, humboldt-
universit at zu berlin and technische universiteit eindhoven (2010).
[26] a. rozinat, w. m. p. van der aalst, conformance checking of processes
based on monitoring real behavior, information systems 33 (1) (2008)
64{95.
36[27] j. mu~ noz-gama, j. carmona, a fresh look at precision in process con-
formance, in: bpm'10, vol. 6336 of lncs, springer, 2010, pp. 211{226.
[28] r. bergenthum, j. desel, s. mauser, comparison of dierent algorithms to
synthesize a petri net from a partial language, in: transactions on petri
nets and other models of concurrency iii, vol. 5800 of lecture notes
in computer science, springer berlin / heidelberg, 2009, pp. 216{243,
10.1007/978-3-642-04856-2 9.
[29] d. fahland, w. m. p. van der aalst, simplifying mined process models:
an approach based on unfoldings, in: s. rinderle, f. toumani, k. wolf
(eds.), business process management (bpm 2011), vol. 6896 of lncs,
springer, 2011, pp. 362{378.
[30] j. mu~ noz-gama, j. carmona, enhancing precision in process confor-
mance: stability, condence and severity, in: n. chawla, i. king, a. sper-
duti (eds.), ieee symposium on computational intelligence and data
mining (cidm 2011), ieee, paris, france, 2011.
[31] r. p. j. c. bose, w. m. p. van der aalst, trace alignment in process
mining: opportunities for process diagnostics, in: r. hull, j. mendling,
s. tai (eds.), business process management (bpm 2010), vol. 6336 of
lncs, springer, 2010, pp. 227{242.
[32] j. vanhatalo, h. v olzer, j. koehler, the rened process structure tree,
data knowledge engineering 68 (9) (2009) 793{818.
[33] a. polyvyanyy, j. vanhatalo, h. v olzer, simplied computation and gen-
eralization of the rened process structure tree, in: ws-fm'10, vol. 6551
of lecture notes in computer science, 2010, pp. 25{41.
[34] a. polyvyanyy, l. garc a-ba~ nuelos, m. dumas, structuring acyclic process
models, inf. syst. 37 (6) (2012) 518{538.
[35] e. best, r. r. devillers, a. kiehn, l. pomello, concurrent bisimulations
in petri nets, acta inf. 28 (3) (1991) 231{264.
[36] a. l uder, h.-m. hanisch, synthesis of admissible behavior of petri nets
for partial order specications, in: wodes'00, kluwer, 2000, pp. 409 {
431.
37