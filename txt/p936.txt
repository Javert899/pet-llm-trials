responsible data science: using event
data in a “people friendly” manner
wil m.p. van der aalst(b)
department of mathematics and computer science,
eindhoven university of technology,
po box 513, 5600 mb eindhoven, the netherlands
w.m.p.v.d.aalst@tue.nl
http://www.vdaalst.com/
abstract. the omnipresence of event data and powerful process mining
techniques make it possible to quickly learn process models describing
what people and organizations really do. recent breakthroughs in processmining resulted in powerful techniques to discover the real processes, to
detect deviations from normative process models, and to analyze bottle-
necks and waste. process mining and other data science techniques canbe used to improve processes within any organization. however, there are
also great concerns about the use of data for such purposes. increasingly,
customers, patients, and other stakeholders worry about “irresponsible”forms of data science. automated data decisions may be unfair or non-
transparent. conﬁdential data may be shared unintentionally or abused
by third parties. each step in the “data science pipeline” (from raw datato decisions) may create inaccuracies, e.g., if the data used to learn a
model reﬂects existing social biases, the algorithm is likely to incorpo-
rate these biases. these concerns could lead to resistance against thelarge-scale use of data and make it impossible to reap the beneﬁts of
process mining and other data science approaches. this paper discusses
responsible process mining (rpm) as a new challenge in the broader
ﬁeld of responsible data science (rds). rather than avoiding the use of
(event) data altogether, we strongly believe that techniques, infrastruc-
tures and approaches can be made responsible by design . not addressing
the challenges related to rpm/rds may lead to a society where (event)
data are misused or analysis results are deeply mistrusted.
keywords: data science
·process mining ·big data ·fairness ·
accuracy ·conﬁdentiality ·transparency
1 introduction
big data is changing the way we do business, socialize, conduct research, and
govern society. data are collected on anything, at any time, and in any place[5]. organizations are investing heavily in big data technologies and data sci-
ence has emerged as a new scientiﬁc discipline providing techniques, methods,
c/circlecopyrtspringer international publishing ag 2017
s. hammoudi et al. (eds.): iceis 2016, lnbip 291, pp. 3–28, 2017.doi: 10.1007/978-3-319-62386-3
14 w.m.p. van der aalst
and tools to gain value and insights from new and existing data sets. data
abundance combined with powerful data science techniques has the potentialto dramatically improve our lives by enabling new services and products, while
improving their eﬃciency and quality. big data is often considered as the “new
oil” and data science aims to transform this into new forms of “energy”: insights,diagnostics, predictions, and automated decisions. however, the process of trans-
forming “new oil” (data) into “new energy” (analytics) may negatively impact
citizens, patients, customers, and employees. systematic discrimination basedon data, invasions of privacy, non-transparent life-changing decisions, and inac-
curate conclusions occur regularly and show that the saying “with great power
comes great responsibility” also applies to data science.
data science techniques may lead to new forms of “pollution”. technological
solutions that aim to avoid the negative side eﬀects of using data, can be char-
acterized by the term “green data science” (gds) ﬁrst coined in [ 4]. the term
refers to the collection of techniques and approaches trying to reap the beneﬁts
of data science and big data while ensuring fairness, accuracy, conﬁdentiality,and transparency. citizens, patients, customers, and employees need to be pro-
tected against irresponsible uses of data (big or small). therefore, we need to
separate the “good” and “bad” of data science. compare this with environmen-tally friendly forms of green energy (e.g. solar power) that overcome problems
related to traditional forms of energy. data science may result in unfair deci-
sion making, undesired disclosures, inaccuracies, and non-transparency. theseirresponsible uses of data can be viewed as “pollution”. abandoning the system-
atic use of data may help to overcome these problems. however, this would be
comparable to abandoning the use of energy altogether. data science is used tomake products and services more reliable, convenient, eﬃcient, and cost eﬀec-
tive. moreover, most new products and services depend on the collection and
use of data. therefore, we argue that the “prohibition of data (science)” is not
a viable solution. instead we believe that technological solutions can be used to
avoid pollution and protect the environment in which data is collected and used.
in this paper we use the term “responsible data science” (rds) rather
than “green data science” (gds). our notion of responsible is inspired by the
emerging ﬁeld of responsible innovation [15,21]. from the overall “responsibility”
notion, we distill four main challenges speciﬁc to data science:
–fairness: data science without prejudice - how to avoid unfair conclusions
even if they are true?
–accuracy: data science without guesswork - how to answer questions with
a guaranteed level of accuracy?
–conﬁdentiality: data science that ensures conﬁdentiality - how to answer
questions without revealing secrets?
–transparency: data science that provides transparency - how to clarify
answers such that they become indisputable?
this paper discusses these so-called “fact” challenges while emphasizing the
need for technological solutions that enable individuals, organizations and societyresponsible data science: using event data in a “people friendly” manner 5
to reap the beneﬁts from the widespread availability of data while ensuring
fairness, accuracy, conﬁdentiality, and transparency (fact).
the “fact” challenges are fairly general. therefore, the second part of this
paper focuses on a speciﬁc subdiscipline of data science: process mining [5].
process mining can be used to discover what people actually do, check compli-ance, and uncover bottlenecks. process mining reveals the behaviors of work-
ers, customers, and other people involved in the processes being analyzed. the
unique capabilities of process mining also create a range of “fact” challenges.for example, analysis may reveal that workers taking care of the most diﬃcult
cases are slower than others or cause more deviations. moreover, the ﬁltering of
event data may be used to inﬂuence the outcomes in such a way that decisionmakers are not aware of this. these examples illustrate the negative side-eﬀects
thatresponsible process mining (rpm) aims to avoid.
this paper extends the iceis 2016/enase 2016 keynote paper [ 4]b yi n t r o -
ducing the data science discipline and by elaborating on rds and rpm. the
remainder of this paper is organized as follows. section 2introduces the ﬁeld of
data science and uses the example of photography to illustrate the impact of dig-
itization in our daily lives. in sect. 3we elaborate on the four general “fact”
challenges. section 4introduces process mining as a technology to analyze the
behavior of people and organizations. in this more speciﬁc setting, we revisit
the four “fact” challenges and mention possible solution directions (sect. 5).
finally, sect. 6concludes the paper.
2 data science
many deﬁnitions have been proposed for data science [ 11,24]. here, we use a
deﬁnition taken from [ 5]:
data science is an interdisciplinary ﬁeld aiming to turn data into real
value. data may be structured or unstructured, big or small, static or
streaming. value may be provided in the form of predictions, automated
decisions, models learned from data, or any type of data visualizationdelivering insights. data science includes data extraction, data prepara-
tion, data exploration, data transformation, storage and retrieval, comput-
ing infrastructures, various types of mining and learning, presentation ofexplanations and predictions, and the exploitation of results taking into
account ethical, social, legal, and business aspects.
the deﬁnition shows that the data science ﬁeld is quite broad. data science has
it roots in diﬀerent ﬁelds. like computer science emerged from mathematics,data science is now emerging from a range of disciplines (see fig. 1).
within statistics, one of the key areas in mathematics, there is a long tradi-
tion in data analysis. statistics developed over four centuries starting with thework of john graunt (1620–1674). although data science can be seen as a con-
tinuation of statistics, the recent progress in data science cannot be attributed
to traditional statisticians that tend to focus more on theoretical results rather6 w.m.p. van der aalst
than real-world analysis problems. the computational aspects, which are critical
for larger data sets, are typically ignored by statisticians [ 5,27]. the focus is on
generative modeling rather than prediction and dealing with practical challenges
related to data quality and size. it was the data mining community that realized
major breakthroughs in the discovery of patterns and relationships (e.g., eﬃ-ciently learning decision trees and association rules). data science is also closely
related to data processing. turing award winner peter naur (1928–2016) used
the term “data science” long before it was in vogue [ 5]. in 1974, naur wrote:
“a basic principle of data science , perhaps the most fundamental that may be
formulated, can now be stated: the data representation must be chosen with
due regard to the transformation to be achieved and the data processing toolsavailable” [ 19].
as fig. 1shows, the roots of data science extend beyond mathematics and
computer science. other areas include ethics, law, economics, and operations
management.
mathema ɵcs
data science
computer science
fig. 1. just like computer science emerged from mathematics, data science is now
emerging from multiple disciplines.
to illustrate the relevance of data science, let us consider the development
of photography over time as sketched in fig. 2. photography emerged at the
beginning of the 19th century. until 1975 photos were analog and for a longtime kodak was the undisputed market leader. at the peak of its success kodak
developed the ﬁrst digital camera. it could make 0.01 megapixel black and white
pictures and marked both the beginning of the digital photography and thedecline of kodak as a company (see fig. 2). in 2003, the sales of digital cameras
exceeded the sales of traditional cameras for the ﬁrst time. today, we make pho-
tographs using smartphones and tablets rather than cameras. the remarkable
transition from analog to digital photography illustrated by fig. 2has had an
impact that goes far beyond the photos themselves. the digitization of photog-raphy enabled new applications. for example, photos can be shared online (e.g.
flickr, instagram, facebook, and twitter) and changed the way we communicate
and socialize (see the uptake of the term “selﬁe”). smartphone apps can even beresponsible data science: using event data in a “people friendly” manner 7
used to detect eye cancer, melanoma, and other diseases by analyzing photos.
photos capture “events” showing what is really happening. this is enabling newforms of data analysis.
world's earliest 
surviving camera 
photograph (1826)
kodak box camera 
developed by 
george eastman   
(1888)
first digital camera 
by steve sasson 
from kodak (1975)
sales digital 
cameras exceeds 
analog cameras 
(2003)
release of 
ﬁrst iphone 
(2007)
release of ipad 2
 (2011)
2.2 million apps in 
google play and 2.0 
million apps in apple 
app store (2016)around 1800, thomas wedgwood 
attempted to capture the image in a camera 
obscura by means of a light-sensitive 
substance. the earliest remaining photo 
dates from 1826. 
george eastman founded kodak around 
1890 and produced “the kodak” box camera that was sold for $25, thus making 
photography accessible for a larger group 
of people. 
in 1976, kodak was responsible for 90% of film 
sales and 85% of camera sales in the united 
states. kodak developed the first digital 
camera in 1975, i.e., at the peak of its success.
in 2003, the sales of digital cameras exceeded 
the sales of traditional cameras for the first 
time. kodak and others could not adapt.
soon after their introduction,
smartphones with built-in 
cameras overtook dedicated 
cameras.
the first ipad having a 
camera (ipad 2) was 
presented on march 2nd,
2011 by steve jobs.
today, most photos are made using mobile phones and 
tablets. photos can be shared online (e.g. flickr,
instagram, facebook, and twitter) and changed the way 
we communicate and socialize. smartphone apps can 
detect eye cancer, melanoma, and other diseases by 
analyzing photos. a photo created using a smartphone may generate to a wide range of events (e.g., sharing)
having data attributes (e.g., location) that reach far 
beyond the actual image.analogdigital
fig. 2. example of digitization: digital photography changed the way we make and use
photos. moreover, the digitization of photos enabled new forms of analysis.
similar developments can be witnessed in all economic sectors. consider for
example the music industry. the transition from analog to digital music hasquite some similarities with fig. 2.
looking at the timeline in fig. 2, one can easily see why data science is now
emerging as a new discipline. the exponential growth of data over the last decadeshas now reached a “tipping point” dramatically changing the way we do business
and socialize. after explaining why and how data science emerged as a new disci-
pline, we now use fig. 3to introduce the three main aspects of data science:8 w.m.p. van der aalst
infrastructure analysis eﬀect
networks & sensors
distributed systems (e.g. hadoop)
databases (nosql)
programming (mapreduce)
security...staɵsɵcs
data/process mining
machine learning
operaɵons research 
algorithmsvisualizaɵon
...ethics & privacy
human technology interac ɵon
operaɵons management
business models
entrepreneurship
...
fig. 3. the data science landscape composed of three main aspects: infrastructure,
analysis, and eﬀect.
–infrastructure: how to collect, store, and process (large amounts of) data?
the infrastructure provides the basis for analysis. data need to be collected
and stored. systems may need to be distributed to cope with larger amounts ofdata. databases may need to be tailored towards the application and special
programming models may need to be employed.
–analysis: how to turn data into insights, answers, ideas, and decisions?
using the infrastructure diﬀerent types of approaches can be used to extract
value from data. this includes machine learning, data/process mining, sta-
tistics, visual analytics, predictive analytics, decision support, etc.
–eﬀect: how to positively impact reality? the application of data science may
impact individuals, processes, organizations, and society. there may be trade-
oﬀs between diﬀerent goals and stakeholders. for example, privacy concernsmay conﬂict with business targets.
figure 4provides yet another view on the data science landscape by sketch-
ing the “data science pipeline”. individuals interact with a range of hard-
ware/software systems (information systems, smartphones, websites, wearables,etc.) ➊. data related to machine and interaction events are collected ➋and
preprocessed for analysis ➌. during preprocessing data may be transformed,
cleaned, anonymized, de-identiﬁed, etc. models may be learned from data or
made/modiﬁed by hand ➍. for compliance checking, models are often norma-
tive and made by hand rather than discovered from data. analysis results basedon data (and possibly also models) are presented to analysts, managers, etc. ➎
or used to inﬂuence the behavior of information systems and devices ➏. based on
the data, decisions are made or recommendations are provided. analysis resultsmay also be used to change systems, laws, procedures, guidelines, responsibili-
ties, etc. ➐.responsible data science: using event data in a “people friendly” manner 9
3 responsible data science (rds)
figure 4also lists the four “fact” challenges mentioned in the introduction.
each of the challenges requires an understanding of the whole data pipeline.
flawed analysis results or bad decisions may be caused by diﬀerent factors suchas a sampling bias, careless preprocessing, inadequate analysis, or an opinionated
presentation. we use the term responsible data science (rds) for data science
approaches that try to exploit data while avoiding negative side-eﬀects. rds issynonymous with “green data science” (gds) [ 4]. the latter term is based on
the metaphor that “data is the new oil” and that we should develop technologies
to avoid the “pollution” caused by irresponsible uses of data.
data in a 
variety of 
systems
data used as 
input for 
analyticsinformation 
systems, 
devices, etc.
resultsmodelsextract, load, 
transform, clean, 
anonymize, de-
identify, etc.
report, discover, 
mine, learn, check, 
predict, recommend, etc.interaction with individuals
interpretation by analysts , managers, etc.12
34
56
7- data science 
without prejudice: how to 
avoid unfair conclusions 
even if they are true?
- data 
science that ensures 
confidentiality: how to 
answer questions without 
revealing secrets?
 - data science 
without guesswork : how to 
answer questions with a 
guaranteed level of accuracy? - data science 
that provides transparency: how 
to clarify answers such that they 
become indisputable?
fig. 4. the “data science pipeline” facing the four “fact” challenges.
rds advocates taking the third aspect (“eﬀect”) in fig. 3as leading when
designing or using the ﬁrst two aspects (“infrastructure” and “analysis”). when-
ever possible, infrastructures and analysis techniques should be responsible by
design.
the remainder of this section elaborates on the four “fact” challenges:
fairness, accuracy, conﬁdentiality, and transparency.
3.1 fairness - data science without prejudice: how to avoid
unfair conclusions even if they are true?
data science techniques need to ensure fairness : automated decisions and
insights should not be used to discriminate in ways that are unacceptable from
a legal or ethical point of view. discrimination can be deﬁned as “the harmful
treatment of an individual based on their membership of a speciﬁc group or10 w.m.p. van der aalst
category (race, gender, nationality, disability, marital status, or age)”. however,
most analysis techniques aim to discriminate among groups. banks handing out
loans and credit cards try to discriminate between groups that will pay their
debts and groups that will run into ﬁnancial problems. insurance companies
try to discriminate between groups that are likely to claim and groups that areless likely to claim insurance. hospitals try to discriminate between groups for
which a particular treatment is likely to be eﬀective and groups for which this
is less likely. hiring employees, providing scholarships, screening suspects, etc.can all be seen as classiﬁcation problems: the goal is to explain a response vari-
able (e.g., person will pay back the loan) in terms of predictor variables (e.g.,
credit history, employment status, age, etc.). ideally, the learned model explainsthe response variable as well as possible without discriminating on the basis of
sensitive attributes (race, gender, etc.).
to explain discrimination discovery anddiscrimination prevention , let us
consider the set of all (potential) customers of some insurance company special-
izing in car insurance. for each customer we have the following variables:
– name,
– birthdate,
– gender (male or female),– nationality,
– car brand (alfa, bmw, etc.),
– years of driving experience,– number of claims in the last year,
– number of claims in the last ﬁve years, and
– status (insured, refused, or left).
the status ﬁeld is used to distinguish current customers (status =insured) from
customers that were refused (status =refused) or that left the insurance company
during the last year (status =left). customers that were refused or that left morethan a year ago are removed from the data set.
techniques for discrimination discovery aim to identify groups that are dis-
criminated based on sensitive variables, i.e., variables that should not matter.
for example, we may ﬁnd that “males have a higher likelihood to be rejected
than females” or that “foreigners driving a bmw have a higher likelihood to be
rejected than dutch bmw drivers”. discrimination may be caused by human
judgment or by automated decision algorithms using a predictive model. the
decision algorithms may discriminate due to a sampling bias, incomplete data,or incorrect labels. if earlier rejections are used to learn new rejections, then
prejudices may be reinforced. similar “self-fulﬁlling prophecies” can be caused
by sampling or missing values.
even when there is no intent to discriminate, discrimination may still occur.
even when the automated decision algorithm does not use gender and uses only
non-sensitive variables, the actual decisions may still be such that (fe)males orforeigners have a much higher probability to be rejected. the decision algorithm
may also favor more frequent values for a variable. as a result, minority groups
may be treated unfairly.responsible data science: using event data in a “people friendly” manner 11
discrimination prevention aims to create automated decision algorithms that
do not discriminate using sensitive variables. it is not suﬃcient to remove thesesensitive variables: due to correlations and the handling of outliers, uninten-
tional discrimination may still take place. one can add constraints to the deci-
sion algorithm to ensure fairness using a predeﬁned criterion. for example, theconstraint “males and females should have approximately the same probabil-
ity to be rejected” can be added to a decision-tree learning algorithm. next to
adding algorithm-speciﬁc constraints used during analysis one can also use pre-processing (modify the input data by resampling or relabeling) or postprocessing
(modify models, e.g., relabel mixed leaf nodes in a decision tree). in general there
is often a trade-oﬀ between maximizing accuracy and minimizing discrimination
(see fig. 5). by rejecting fewer males (better fairness), the insurance company
may need to pay more claims.
discrimination prevention often needs to use sensitive variables (gender, age,
nationality, etc.) to ensure fairness. this creates a paradox , e.g., information on
gender needs to be used to avoid discrimination based on gender.
the ﬁrst paper on discrimination-aware data mining appeared in 2008 [ 22].
since then, several papers mostly focusing on fair classiﬁcation appeared: [ 8,
14,26]. these examples show that unfairness during analysis can be actively
prevented. however, unfairness is not limited to classiﬁcation and more advanced
forms of analytics also need to ensure fairness.
3.2 conﬁdentiality - data science that ensures conﬁdentiality:
how to answer questions without revealing secrets?
the application of data science techniques should not reveal certain types of
personal or otherwise sensitive information. often personal data need to be keptconﬁdential . the general data protection regulation (gdpr) (see also sect. 6)
focuses on personal information [ 10]:“the principles of data protection should
apply to any information concerning an identiﬁed or identiﬁable natural person.
personal data which have undergone pseudonymisation, which could be attributed
fairness
accuracylow accuracy highest accuracy 
possible using all data 
without constraintsanalysis results 
and model are 
non-discrimina ɵng
analysis results and 
model are created 
without considering 
discrimina ɵonpossible compromise 
between fairness and 
accuracyideal 
situation 
(impossible)
fig. 5. tradeoﬀ between fairness and accuracy.12 w.m.p. van der aalst
to a natural person by the use of additional information should be considered to
be information on an identiﬁable natural person. to determine whether a naturalperson is identiﬁable, account should be taken of all the means reasonably likely
to be used, such as singling out, either by the controller or by another person to
identify the natural person directly or indirectly. to ascertain whether means arereasonably likely to be used to identify the natural person, account should be taken
of all objective factors, such as the costs of and the amount of time required for
identiﬁcation, taking into consideration the available technology at the time ofthe processing and technological developments. the principles of data protection
should therefore not apply to anonymous information, namely information which
does not relate to an identiﬁed or identiﬁable natural person or to personal datarendered anonymous in such a manner that the data subject is not or no longer
identiﬁable.”
conﬁdentiality is not limited to personal data. companies may want to hide
sales volumes or production times when presenting results to certain stakehold-
ers. one also needs to bear in mind that few information systems hold informa-tion that can be shared or analyzed without limits (e.g., the existence of personal
data cannot be avoided). the “data science pipeline” depicted in fig. 4shows
that there are diﬀerent types of data having diﬀerent audiences. here we focuson: (1) the “raw data” stored in the information system ➋, (2) the data used
as input for analysis ➌, and (3) the analysis results interpreted by analysts and
managers ➎. whereas the raw data may refer to individuals, the data used for
analysis is often (partly) de-identiﬁed, and analysis results may refer to aggre-
gate data only. it is important to note that conﬁdentiality may be endangered
along the whole pipeline and includes analysis results.
consider a data set that contains sensitive information. records in such a
data set may have three types of variables:
–direct identiﬁers : variables that uniquely identify a person, house, car, com-
pany, or other entity. for example, a social security number identiﬁes a person.
–key variables : subsets of variables that together can be used to identify some
entity. for example, it may be possible to identify a person based on gender,
age, and employer. a car may be uniquely identiﬁed based on registration
date, model, and color. key variables are also referred to as implicit identiﬁers
orquasi identiﬁers .
–non-identifying variables : variables that cannot be used to identify some
entity (direct or indirect).
conﬁdentiality is impaired by unintended or malicious disclosures. we con-
sider three types of such disclosures:
–identity disclosure : information about an entity (person, house, etc.) is
revealed. this can be done through direct or implicit identiﬁers. for example,the salaries of employees are disclosed unintentionally or an intruder is able
to retrieve patient data.
–attribute disclosure : information about an entity can be derived indirectly. if
there is only one male surgeon in the age group 40–45, then aggregate data
for this category reveals information about this person.responsible data science: using event data in a “people friendly” manner 13
–partial disclosure : information about a group of entities can be inferred.
aggregate information on male surgeons in the age group 40–45 may disclosean unusual number of medical errors. these cannot be linked to a particular
surgeon. nevertheless, one may conclude that surgeons in this group are more
likely to make errors.
de-identiﬁcation of data refers to the process of removing or obscuring
variables with the goal to minimize unintended disclosures. in many cases re-
identiﬁcation is possible by linking diﬀerent data sources. for example, the com-
bination of wedding date and birth date may allow for the re-identiﬁcation of a
particular person. anonymization of data refers to de-identiﬁcation that is irre-
versible: re-identiﬁcation is impossible. a range of de-identiﬁcation methods is
available: removing variables, randomization, hashing, shuﬄing, sub-sampling,
aggregation, truncation, generalization, adding noise, etc. adding some noise toa continuous variable or the coarsening of values may have a limited impact on
the quality of analysis results while ensuring conﬁdentiality.
there is a trade-oﬀ between minimizing the disclosure of sensitive information
and the usefulness of analysis results (see fig. 6). removing variables, aggrega-
tion, and adding noise can make it hard to produce any meaningful analysis
results. emphasis on conﬁdentiality (like security) may also reduce convenience.note that personalization often conﬂicts with fairness and conﬁdentiality .d i s -
closing all data, supports analysis, but jeopardizes conﬁdentiality.
access rights to the diﬀerent types of data and analysis results in the “data
science pipeline” (fig. 4) vary per group. for example, very few people will have
access to the “raw data” stored in the information system ➋. more people
will have access to the data used for analysis and the actual analysis results.poor cybersecurity may endanger conﬁdentiality. good policies ensuring proper
authentication (are you who you say you are?) and authorization (what are
you allowed to do?) are needed to protect access to the pipeline in fig. 4. cyber-
security measures should not complicate access, data preparation, and analysis;
otherwise people may start using illegal copies and replicate data. see [ 18,20,23]
for approaches to ensure conﬁdentiality.
conﬁdenɵality
data uɵlityno meaningful 
analysis possiblefull use of data 
potenɵal possiblefull disclosure 
of sensiɵve 
datano sensiɵve 
data disclosed
possible compromise 
between confidentiality 
and utilityideal 
situation 
(impossible)
fig. 6. tradeoﬀ between conﬁdentiality and utility.14 w.m.p. van der aalst
3.3 accuracy - data science without guesswork: how to answer
questions with a guaranteed level of accuracy?
increasingly decisions are made using a combination of algorithms and data
rather than human judgement. hence, analysis results need to be accurate and
should not deceive end-users and decision makers. yet, there are several factors
endangering accuracy.
first of all, there is the problem of overﬁtting the data leading to “bogus
conclusions”. there are numerous examples of so-called spurious correlations
illustrating the problem. some examples (taken from [ 28]):
– the per capita cheese consumption strongly correlates with the number of
people who died by becoming tangled in their bedsheets.
– the number of japanese passenger cars sold in the us strongly correlates
with the number of suicides by crashing of motor vehicle.
– us spending on science, space and technology strongly correlates with suicides
by hanging, strangulation and suﬀocation.
– the total revenue generated by arcades strongly correlates with the number
of computer science doctorates awarded in the us.
when using many variables relative to the number of instances, classiﬁcation
may result in complex rules overﬁtting the data. this is often referred to as the
curse of dimensionality : as dimensionality increases, the number of combina-
tions grows so fast that the available data become sparse. with a ﬁxed numberof instances, the predictive power reduces as the dimensionality increases. using
cross-validation most ﬁndings (e.g., classiﬁcation rules) will get rejected. how-
ever, if there are many ﬁndings, some may survive cross-validation by sheer luck.
in statistics, bonferroni’s correction is a method (named after the italian
mathematician carlo emilio bonferroni) to compensate for the problem of multi-
ple comparisons. normally, one rejects the null hypothesis if the likelihood of theobserved data under the null hypothesis is low [ 9]. if we test many hypotheses,
we also increase the likelihood of a rare event. hence, the likelihood of incorrectly
rejecting a null hypothesis increases [ 17]. if the desired signiﬁcance level for the
whole collection of null hypotheses is α, then the bonferroni correction suggests
that one should test each individual hypothesis at a signiﬁcance level of
α
kwhere
kis the number of null hypotheses. for example, if α=0.05 and k= 20, then
α
k=0.0025 is the required signiﬁcance level for testing the individual hypotheses.
next to overﬁtting the data and testing multiple hypotheses, there is the
problem of uncertainty in the input data and the problem of not showing uncer-
tainty in the results .
uncertainty in the input data is related to the fourth “v” in the four “v’s of
big data” (volume, velocity, variety, and veracity). veracity refers to the trust-
worthiness of the input data. sensor data may be uncertain, multiple users may
use the same account, tweets may be generated by software rather than people,etc. these uncertainties are often not taken into account during analysis assuming
that things “even out” in larger data sets. this does not need to be the case and the
reliability of analysis results is aﬀected by unreliable or probabilistic input data.responsible data science: using event data in a “people friendly” manner 15
according to bonferroni’s principle we need to avoid treating random obser-
vations as if they are real and signiﬁcant [25]. the following example, inspired
by a similar example in [25], illustrates the risk of treating completely random
events as patterns.
adutch government agency is searching for terrorists by examining hotel
visits of all of its 18 million citizens (18 ×106). the hypothesis is that terrorists
meet multiple times at some hotel to plan an attack. hence, the agency looks
for suspicious “events” {p1,p2}†{d1,d2}where persons p1andp2meet on
daysd1andd2. how many of such suspicious events will the agency ﬁnd if the
behavior of people is completely random? to estimate this number we need
to make some additional assumptions. on average, dutch people go to a hotel
every 100 days and a hotel can accommodate 100 people at the same time. we
further assume that there are18×106
100×100= 1800 dutch hotels where potential
terrorists can meet.the probability that two persons ( p
1andp2) visit a hotel on a given day d
is1
100×1
100=1 0−4. the probability that p1andp2visit the same hotel on
daydis 10−4×1
1800=5.55×10−8. the probability that p1andp2visit the
same hotel on two diﬀerent days d1andd2is (5.55×10−8)2=3.086×10−15.
note that diﬀerent hotels may be used on both days. hence, the probability
of suspicious event {p1,p2}†{d1,d2}is 3.086×10−15.
how many candidate events are there? assume an observation period of 1000
days. hence, there are 1000 ×(1000 −1)/2 = 499 ,500 combinations of days d1
andd2. note that the order of days does not matter, but the days need to be
diﬀerent. there are (18 ×106)×(18×106−1)/2=1.62×1014combinations
of persons p1andp2. again the ordering of p1andp2does not matter, but
p1/negationslash=p2. hence, there are 499 ,500×1.62×1014=8.09×1019candidate events
{p1,p2}†{d1,d2}.
the expected number of suspicious events is equal to the product of the num-
ber of candidate events {p1,p2}†{d1,d2}and the probability of such events
(assuming independence): 8 .09×1019×3.086×10−15= 249 ,749. hence, there
will be around a quarter million observed suspicious events {p1,p2}†{d1,d2}
in a 1000 day period!
suppose that there are only a handful of terrorists and related meetings in
hotels. the dutch government agency will need to investigate around a quarter
million suspicious events involving hundreds of thousands innocent citizens.
using bonferroni’s principle, we know beforehand that this is not wise: there
will be too many false positives.
example 1: bonferroni’s principle explained using an example taken from [ 5].
to apply the principle, compute the number of observations of some phenomenaone is interested in under the assumption that things occur at random. if this
number is signiﬁcantly larger than the real number of instances one expects,
then most of the ﬁndings will be false positives.16 w.m.p. van der aalst
when we say, “we are 95% conﬁdent that the true value of parameter x
is in our conﬁdence interval [ a,b]”, we mean that 95% of the hypothetically
observed conﬁdence intervals will hold the true value of parameter x. averages,
sums, standard deviations, etc. are often based on sample data. therefore, it is
important to provide a conﬁdence interval. for example, given a mean of 35 .4t h e
95% conﬁdence interval may be [35 .3,35.6], but the 95% conﬁdence interval may
also be [15 .3,55.6]. in the latter case, we will interpret the mean of 35 .4 as a “wild
guess” rather than a representative value for true average value. although we areused to conﬁdence intervals for numerical values, decision makers have problems
interpreting the expected accuracy of more complex analysis results like decision
trees, association rules, process models, etc. cross-validation techniques like k-
fold checking and confusion matrices give some insights. however, models and
decisions are often presented unequivocally thus hiding uncertainties. explicit
vagueness or more explicit conﬁdence diagnostics may help to better interpret
analysis results. parts of models should be kept deliberately “vague” if analysis
is not conclusive.
3.4 transparency - data science that provides transparency: how
to clarify answers such that they become indisputable?
data science techniques are used to make a variety of decisions. some of these
decisions are made automatically based on rules learned from historic data.
for example, a mortgage application may be rejected automatically based on adecision tree. other decisions are based on analysis results (e.g., process models
or frequent patterns). for example, when analysis reveals previously unknown
bottlenecks, then this may have consequences for the organization of work andchanges in staﬃng (or even layoﬀs). automated decision rules ( ➏in fig. 4) need
to be as accurate as possible (e.g., to reduce costs and delays). analysis results
(➎in fig. 4) also need to be accurate. however, accuracy is not suﬃcient to
ensure acceptance and proper use of data science techniques. both decisions ➏
and analysis results ➎also need to be transparent .
figure 7illustrates the notion of transparency. consider an application sub-
mitted by john evaluated using three data-driven decision systems. the ﬁrst
system is a black box: it is unclear why john’s application is rejected. the sec-
ond system reveals it’s decision logic in the form of a decision tree. applications
from females and younger males are always accepted. only applications from
older males get rejected. the third system uses the same decision tree, but alsoexplains the rejection (“because male and above 50”). clearly, the third system
is most transparent. when governments make decisions for citizens it is often
mandatory to explain the basis for such decisions.
deep learning techniques (like many-layered neural networks) use multiple
processing layers with complex structures or multiple non-linear transformations.
these techniques have been successfully applied to automatic speech recognition,image recognition, and various other complex decision tasks. deep learning meth-
ods are often looked at as a “black box”, with performance measured empirically
and no formal guarantees or explanations. a many-layered neural network is notresponsible data science: using event data in a “people friendly” manner 17
gender
ageaccept
rejectgender
ageaccept
rejectgender
ageaccept
rejectblack box
data-driven 
decision system 2
data-driven 
decision system 3data-driven 
decision system 1
your claim is 
rejected
because you 
are male 
and above 50 
.. . 
fig. 7. diﬀerent levels of transparency.
as transparent as for example a decision tree. such a neural network may make
good decisions, but it cannot explain a rule or criterion. therefore, such black
box approaches are non-transparent and may be unacceptable in some domains.
transparency is not restricted to automated decision making and explaining
individual decisions, it also involves the intelligibility, clearness, and compre-
hensibility of analysis results (e.g., a process model, decision tree, regression
formula). for example, a model may reveal bottlenecks in a process, possiblefraudulent behavior, deviations by a small group of individuals, etc. it needs
to be clear for the user of such models (e.g., a manager) how these ﬁndings
where obtained. the link to the data and the analysis technique used should beclear. for example, ﬁltering the input data (e.g., removing outliers) or adjusting
parameters of the algorithm may have a dramatic eﬀect on the model returned.
storytelling is sometimes referred to as “the last mile in data science”. the
key question is: how to communicate analysis results with end-users? storytelling
is about communicating actionable insights to the right person, at the right time,
in the right way. one needs to know the gist of the story one wants to tell
to successfully communicate analysis results (rather than presenting the whole
model and all data). one can use natural language generation to transform
selected analysis results into concise, easy-to-read, individualized reports.
to provide transparency there should be a clear link between data and analy-
sis results/stories. one needs to be able to drill-down and inspect the data from
the model’s perspective. given a bottleneck one needs to be able to drill down
to the instances that are delayed due to the bottleneck. this related to data
provenance : it should always be possible to reproduce analysis results from the
original data.18 w.m.p. van der aalst
the four “fact” challenges depicted in fig. 4are clearly interrelated. there
may be trade-oﬀs between them. for example, to ensure conﬁdentiality we mayadd noise and de-identify data thus possibly compromising accuracy and trans-
parency.
4 process mining
the goal of process mining is to turn event data into insights and actions [ 5].
process mining is an integral part of data science, fueled by the availabilityof data and the desire to improve processes. process mining can be seen as a
means to bridge the gap between data science and process science. data science
approaches tend to be process agonistic whereas process science approaches tendto be model-driven without considering the “evidence” hidden in the data.
4.1 what is process mining?
figure 8shows the “process mining pipeline” and can be viewed as a spe-
cialization of the fig. 4. process mining focuses on the analysis of event
dataand analysis results are often related to process models . process min-
ing is a rapidly growing subdiscipline within both business process manage-
ment (bpm) [ 2] and data science [ 3]. mainstream business intelligence (bi),
data mining and machine learning tools are not tailored towards the analy-sis of event data and the improvement of processes. fortunately, there are
dedicated process mining tools able to transform event data into actionable
process-related insights. for example, prom (www.processmining.org )i sa n
data in a 
variety of 
systems
data used as 
input for 
analyticsinformation 
systems, 
devices, etc.
resultsmodelsextract, load, 
transform, clean, 
anonymize, de-
identify, etc.
report, discover, 
mine, learn, check, 
predict, recommend, etc.interaction with individuals
interpretation by 
analysts, 
managers, etc.12
34
56
7event data 
(e.g., in xes 
format)data in databases , 
files, logs, etc. 
having a temporal 
dimension 
process models (e.g., 
bpmn, uml ad/sds, petri 
nets, workflow models) 
techniques for process discovery , 
conformance checking, and 
performance analysis results include process models 
annotated with frequencies, 
times, and deviationsoperational support, e.g., 
predictions, recommendations, 
decisions, and alerts 
people and devices 
generating a 
variety of events 
fig. 8. the “process mining pipeline” relates observed and modeled behavior.responsible data science: using event data in a “people friendly” manner 19
open-source process mining tool supporting process discovery, conformance
checking, social network analysis, organizational mining, clustering, decisionmining, prediction, and recommendation (see fig. 9). moreover, in recent years,
several vendors released commercial process mining tools. examples include:
celonis process mining by celonis gmbh ( www.celonis.de ),disco by fluxicon
(www.ﬂuxicon.com ),interstage business process manager analytics by fujitsu
ltd. ( www.fujitsu.com ),minit by gradient ecm ( www.minitlabs.com ),myin-
venio by cognitive technology ( www.my-invenio.com ),perceptive process min-
ingby lexmark ( www.lexmark.com ),qpr processanalyzer by qpr ( www.
qpr.com ),rialto process by exeura ( www.exeura.eu ),snp business process
analysis by snp schneider-neureither & partner ag ( www.snp-bpa.com ),
andppm webmethods process performance manager by software ag
(www.softwareag.com ).
4.2 creating and managing event data
process mining is impossible without proper event logs [1]. an event log contains
event data related to a particular process. each event in an event log refers tooneprocess instance , called case. events related to a case are ordered. events can
have attributes. examples of typical attribute names are activity, time, costs,
and resource. not all events need to have the same set of attributes. however,typically, events referring to the same activity have the same set of attributes.
figure 9(a) shows the conversion of an csv ﬁle with four columns (case, activity,
resource, and timestamp) into an event log.
most process mining tools support xes (extensible event stream) [ 13]. in
september 2010, the format was adopted by the ieee task force on process
mining and became the de facto exchange format for process mining. the ieeestandards organization is currently evaluating xes with the aim to turn xes
into an oﬃcial ieee standard.
to create event logs we need to extract, load, transform, anonymize, and de-
identify data from a variety of systems (see ➌in fig. 8). consider for example
the hundreds of tables in a typical his (hospital information system) like chip-
soft, mckesson and epic or in an erp (enterprise resource planning) system
like sap, oracle, and microsoft dynamics. non-trivial mappings are needed to
extract events and to relate events to cases. event data needs to be scoped tofocus on a particular process. moreover, the data also needs to be scoped with
respect to conﬁdentiality issues.
4.3 process discovery
process discovery is one of the most challenging process mining tasks [ 1]. based
on an event log, a process model is constructed thus capturing the behavior
seen in the log. dozens of process discovery algorithms are available. figure 9(c)
shows a process model discovered using prom’s inductive visual miner [16]. tech-
niques use petri nets, wf-nets, c-nets, process trees, or transition systems as
a representational bias [ 5]. these results can always be converted to the desired20 w.m.p. van der aalst
case acɵvity resource ɵmestampeach row 
corresponds 
to an event
each dot 
corresponds 
to an event208 cases
ɵme
process model 
discovered for the most 
frequent ac ɵviɵes
conformance 
checking view
performance 
analysis viewacɵvity was 
skipped 16 ɵmes
average wai ɵng 
ɵme is 18 days
queue length is 
currently 22 (a) (b)
(c)
(d)
(e)
(f)5987 
events
tokens refer to 
real cases 
fig. 9. six screenshots of prom while analyzing an event log with 208 cases, 5987
events, and 74 diﬀerent activities. first, a csv ﬁle is converted into an event log (a).
then, the event data can be explored using a dotted chart (b). a process model isdiscovered for the 11 most frequent activities (c). the event log can be replayed on the
discovered model. this is used to show deviations (d), average waiting times (e), and
queue lengths (f).responsible data science: using event data in a “people friendly” manner 21
notation, for example bpmn (business process model and notation), yawl,
or uml activity diagrams.
4.4 conformance checking
using conformance checking discrepancies between the log and the model can be
detected and quantiﬁed by replaying the log [ 6]. for example, fig. 9(c) shows an
activity that was skipped 16 times. some of the discrepancies found may expose
undesirable deviations, i.e., conformance checking signals the need for a better
control of the process. other discrepancies may reveal desirable deviations and
can be used for better process support. input for conformance checking is a
process model having executable semantics and an event log.
4.5 performance analysis
by replaying event logs on process model, we can compute frequencies and wait-
ing/service times. using alignments [ 6] we can relate cases to paths in the model.
since events have timestamps, we can associate the times in-between events along
such a path to delays in the process model. if the event log records both start and
complete events for activities, we can also monitor activity durations. figure 9(d)
shows an activity that has an average waiting time of 18 days and 16 h. note
that such bottlenecks are discovered without any modeling.
4.6 operational support
figure 9(e) shows the queue length at a particular point in time. this illustrates
that process mining can be used in an online setting to provide operational sup-port. process mining techniques exist to predict the remaining ﬂow time for a
case or the outcome of a process. this requires the combination of a discovered
process model, historic event data, and information about running cases. thereare also techniques to recommend the next step in a process, to check confor-
mance at run-time, and to provide alerts when certain service level agreements
(slas) are (about to be) violated.
5 responsible process mining (rpm)
this section discusses challenges related to fairness, accuracy, conﬁdentiality,and transparency in the context of process mining. the goal is not to provide
solutions, but to illustrate that the more general challenges discussed before
trigger concrete research questions in the process mining domain.22 w.m.p. van der aalst
5.1 classiﬁcation of rpm challenges
tables 1and2map the four generic “fact” challenges introduced in sect. 3onto
the ﬁve key ingredients of process mining brieﬂy introduced in subsects. 4.2–4.6.
using both dimensions we obtain a classiﬁcation consisting of 4 ×5 = 20 possible
problem areas.
it is impossible to discuss all 20 potential problem areas listed in tables 1
and2. therefore, we discuss four selected problem areas in more detail.
5.2 example: conﬁdentiality and creating and managing event
data
let us now explore one of the cells in table 2. event data may reveal conﬁden-
tial information as highlighted in fig. 10. the class model shows the informa-
tion found in event logs using xes [ 13], mxml, or some other logging format.
process mining tools exploit such information during analysis. in fig. 10three
levels are identiﬁed: process model level ,case/instance level ,a n d event level .
the case/instance level consists of cases andactivity instances that connect
processes andactivities in the model to events in the event log. see [ 5]f o ra
detailed description of the typical ingredients of an event log. for rpm it is
important to note that events and cases often refer to individuals. a case maycorrespond to a customer, patient, student, or citizen. events often refer to the
person executing the corresponding activity instance (e.g., an employee).
event data are notoriously diﬃcult to fully anonymize. in larger processes,
most cases follow a unique path. in the event log used in fig. 9, 198 of the
208 cases follow a unique path (focusing only on the order of activities). hence,
knowing the order of a few selected activities may be used to de-anonymize orre-identify cases. the same holds for (precise) timestamps. for the event log in
fig.9, several cases can be uniquely identiﬁed based on the day the registration
activity (ﬁrst activity in process) was executed. if one knows the timestampsof these initial activities with the precision of an hour, then almost all cases
can be uniquely identiﬁed. this shows that the ordering and timestamp data in
event logs may reveal conﬁdential information unintentionally. therefore, it isinteresting to investigate what can be done by adding noise (or other transforma-
tions) to event data such that the analysis results do not change too much. for
example, we can shift all timestamps such that all cases start in “week 0”. most
process discovery techniques will still return the same process model. moreover,
the average ﬂow/waiting/service times are not aﬀected by this. however, if oneis investigating queueing or resource behavior, then one cannot consider cases in
isolation and shift cases in time.
moreover, event data can also be stored in aggregated form as is done for
streaming process mining where one cannot keep track of all events and all
cases due to memory constraints and the need to provide answers in real-time
[5,7,29]. aging data structures, queues, time windows, sampling, hashing, etc.
can be used to keep only the information necessary to instantly provide answers
to selected questions. such approaches can also be used to ensure conﬁdentiality,
often without a signiﬁcant loss of accuracy.responsible data science: using event data in a “people friendly” manner 23table 1. relating the four challenges to process mining speciﬁc tasks (1/2).
creating and managing
event dataprocess discovery conformance checking performance analysis operational support
fairnessdata science without
prejudice: how to avoid
unfair conclusions evenif they are true?the input data may bebiased, incomplete or
incorrect such that the
analysis reconﬁrmsprejudices. byresampling or relabeling
the data, undesirable
forms of discriminationcan be avoided. notethat both cases and
resources (used to
execute activities) mayrefer to individuals
having sensitive
attributes such as race,gender, age, etc.the discovered modelmay abstract from
paths followed by
certainunder-representedgroups of cases.
discrimination-aware
process-discoveryalgorithms can be usedto avoid this. for
example, if cases are
handled diﬀerentlybased on gender, we
may want to ensure
that both are equallyrepresented in themodel’conformance checkingcan be used to “blame”
individuals, groups, or
organizations fordeviating from somenormative model.
discrimination-aware
conformance checking(e.g., alignments) needsto separate (1)
likelihood, (2) severity
and (3) blame.deviations may need to
be interpreted
diﬀerently for diﬀerentgroups of cases andresourcesstraightforwardperformance
measurements may be
unfair for certain classesof cases and resources(e.g., not taking into
account the context).
discrimination-awareperformance analysisdetects unfairness and
supports process
improvements takinginto account trade-oﬀs
between internal
fairness (worker’sperspective) andexternal fairness (citi-
zen/patient/customer’s
perspective)process-relatedpredictions,
recommendations and
decisions maydiscriminate(un)intentionally. this
problem can be tackled
using techniques fromdiscrimination-awaredata mining
accuracydata science withoutguesswork: how toanswer questions with a
guaranteed level of
accuracy?event data (e.g., xesﬁles) may have all kindsof quality problems.attributes may be
incorrect, imprecise, or
uncertain. for example,timestamps may be toocoarse (just the date) or
reﬂect the time of
recording rather thanthe time of the event’soccurrenceprocess discoverydepends on manyparameters andcharacteristics of the
event log. process
models should bettershow the conﬁdencelevel of the diﬀerent
parts. moreover,
additional informationneeds to be used better(domain knowledge,
uncertainty in event
data, etc.)often multipleexplanations arepossible to interpretnon-conformance. just
providing one alignment
based on a particularcost function may bemisleading. how robust
are the ﬁndings?in case of ﬁtness
problems (processmodel and event logdisagree), performance
analysis is based on
assumptions and needsto deal with missingvalues (making results
less accurate)inaccurate process
models may lead toﬂawed predictions,recommendations and
decisions. moreover, not
communicating the(un)certainty ofpredictions,
recommendations and
decisions, maynegatively impactprocesses24 w.m.p. van der aalst
table 2. relating the four challenges to process mining speciﬁc tasks (2/2).
creating and managing
event dataprocess discovery conformance checking performance analysis operational support
conﬁdentiality
data science that
ensures conﬁdentiality:how to answerquestions without
revealing secrets?event data (e.g., xesﬁles) may reveal
sensitive information.anonymization andde-identiﬁcation can be
used to avoid disclosure.
note that timestampsand paths may beunique and a source for
re-identiﬁcation (e.g.,
all paths are unique)the discovered model
may reveal sensitive
information, especiallywith respect toinfrequent paths or
small event logs.
drilling-down from themodel may need to beblocked when numbers
get too small (cf.
k-anonymity)conformance checking
shows diagnostics for
deviating cases andresources.access-control is
important and
diagnostics need to beaggregated to avoidrevealing compliance
problems at the level of
individualsperformance analysis
shows bottlenecks and
other problems. linkingthese problems to casesand resources may
disclose sensitive
informationprocess-related
predictions,
recommendations anddecisions may disclosesensitive information,
e.g., based on a
rejection otherproperties can bederived
transparencydata science thatprovides transparency:how to clarify answers
such that they become
indisputable?provenance of eventdata is key. ideally,process mining insightscan be related to the
event data they are
based on. however, thismay conﬂict withconﬁdentiality concernsdiscovered processmodels depend on theevent data used as inputand the parameter
settings and choice of
discovery algorithm.how to ensure that theprocess model is
interpreted correctly?
end-users need tounderstand the relation
between data and model
to trust analysiswhen modeled andobserved behaviordisagree there may bemultiple explanations.
how to ensure that
conformance diagnosticsare interpretedcorrectly?when detectingperformance problems,it should be clear howthese were detected and
what the possible causes
are. animating eventlogs on models helps tomake problems more
transparentpredictions,
recommendations anddecisions are based onprocess models. if
possible, these models
should be transparent.moreover, explanationsshould be added to
predictions,
recommendations anddecisions (“we predict
that this case be late,
because ...”)responsible data science: using event data in a “people friendly” manner 25
...process case
activity activity 
instanceevent
event
attributeposition
transaction 
type*
1*1
*1*1
1
**11
*
event 
levelresource
any dataprocess  
model levelcase/instance 
levelcase
attribute
1*...case id
description
any data
timestamp
each process 
may have an 
arbitrary 
number of 
activities.each case belongs to 
precisely one 
process.
each activity instance refers 
to precisely one activity.each activity instance belongs 
to precisely one case; there 
may be several activity 
instances for each activity/
case combination.
each event refers to 
precisely one case.
each event corresponds to 
one activity instance; for the 
same activity instance there 
may be multiple events.a case can have multiple 
attributes. some of these 
attributes may refer (indirectly)  
to a person, e.g., a customer,
patient, citizen, etc. such 
attributes are important for rpm.
an event has mandatory 
attributes such as a 
timestamp and refers to 
a case and an activity 
(instance).
event attributes may 
refer to a person. for 
example, most events  
have a resource attribute 
referring to the person 
that executed the event.
fig. 10. the typical ingredients of an event log described in terms of a class model
highlighting data elements referring to individuals.
5.3 example: accuracy and process discovery
as mentioned in table 1the accuracy of a discovery process model may depend
on a variety parameter settings. a small change in the input data (log or settings)may completely change the result. one of the main problems of existing tech-
niques is that they do not indicate any form of conﬁdence level. often parts of
the model can be discovered with great certainty whereas other parts are unclearand the discovery technique is basically guessing. nevertheless, this uncertainty
is seldom shown in the model and may lead to incorrect conclusions. to support
rpm, we need to develop process discovery techniques that indicate conﬁdenceinformation in the models returned.
5.4 example: transparency and conformance checking
conformance checking [ 6] can be viewed as a classiﬁcation problem. what kinds
of cases deviate at a particular point? however, if model and log disagree, then
there may be multiple explanations for each deviation. for example, there maybe multiple log-model “alignments” having the same costs. moreover, the costs
assigned to deviations may be arbitrary. as mentioned in table 2it is vital that
conformance diagnostics are interpreted correctly. moreover, the “process miningpipeline” (fig. 8) needs to be managed carefully to avoid misleading conclusions
caused by, for example, data preparation problems.26 w.m.p. van der aalst
5.5 example: fairness and performance analysis
process mining provides the ability to show and analyze bottlenecks in processes
with minimal eﬀort. bottleneck analysis can also be formulated as a classiﬁca-
tion problem. which cases get delayed more than 5 days? who worked on these
delayed cases? performance problems can be related to characteristics of thecase (e.g., a citizen or customer) or the people that worked on it. the process
itself may be “unfair” (discriminate workers or cases) or decision makers can
make “unfair” conclusions based on a superﬁcial analysis of the data. table 1
mentions internal fairness (worker’s perspective) and external fairness (citi-
zen/patient/customer’s perspective) as two concerns. note that the employee
that takes all diﬃcult cases may be slower than others. evaluating employeeswithout taking such context into account will lead to unjustiﬁed conclusions.
the above examples illustrate that our classiﬁcation can be used to identify
a range of novel research challenges in process mining.
6 epilogue
this paper introduced the notion of “responsible data science” (rds) from
four angles: fairness ,accuracy ,conﬁdentiality ,a n d transparency . we advocate
the development and use of positive technological solutions rather than relying
on stricter regulations like the general data protection regulation (gdpr)
approved by the eu parliament in april 2016 [ 10]. gdpr aims to strengthen
and unify data protection for individuals and replaces directive 95/46/ec [ 12].
gdpr is far more restrictive than earlier legislation. sanctions include ﬁnes of
up to 4% of the annual worldwide turnover.
gdpr and other forms of legislation can be seen as environmental laws
protecting society against “pollution” caused by irresponsible data use. how-
ever, legislation may also prevent the use of data (science) in applications whereincredible improvements are possible. simply prohibiting the collection and sys-
tematic use of data would be turning back the clock. next to legislation, positive
technological solutions are needed to ensure rds. green data science needs tech-nological breakthroughs, just like the innovations enabling green energy.
the paper also discussed the four “fact” challenges in the context of process
mining. in today’s society, event data are collected about anything, at any time,
and at any place. today’s process mining tools are able to analyze such data
and can handle event logs with billions of events. these amazing capabilitiesalso imply a great responsibility. fairness, accuracy, conﬁdentiality, and trans-
parency should be key concerns for any process miner. there is a need for a new
generation of process mining techniques and tools that are responsible by design.however, sometimes painful trade-oﬀs are inevitable. figure 5and table 1both
show the need for trade-oﬀs between fairness and accuracy. other trade-oﬀs are
needed between conﬁdentiality and transparency (see fig. 6and table 2).
we invite researchers and practitioners to contribute to rds and rpm.
these topics are urgent: without proper tools and approaches the use of
data may come to a grinding hold. people like michael jordan warned for aresponsible data science: using event data in a “people friendly” manner 27
“big data winter”, due to the simple-minded and statistically unsound
approaches used today. irresponsible uses of data (science) may trigger restrictivelaws and eﬀectuate resistance of customers and citizens.
acknowledgements. this work is partly based by discussions in the context of
the responsible data science (rds) collaboration involving principal scientists from
eindhoven university of technology, leiden university, university of amsterdam, rad-
boud university nijmegen, tilburg university, vu university, amsterdam medical
center, vu medical center, leiden university medical center, delft university oftechnology, and cwi.
references
1. van der aalst, w.m.p.: process mining: discovery, conformance and enhancement
of business processes. springer, berlin (2011)
2. van der aalst, w.m.p., management, b.p.: a comprehensive survey. isrn softw.
eng. 1–37 (2013). doi: 10.1155/2013/507984
3. aalst, w.m.p.: data scientist: the engineer of the future. in: mertins, k., b´ enaben,
f . ,p o l e r ,r . ,b o u r r i ` eres, j.-p. (eds.) enterprise interoperability vi. pic, vol. 7,
pp. 13–26. springer, cham (2014). doi: 10.1007/978-3-319-04948-9 2
4. van der aalst, w.m.p.: green data science: using big data in an “environmentally
friendly” manner. in: camp, o., cordeiro, j. (eds.) proceedings of the 18th inter-national conference on enterprise information systems (iceis 2016), pp. 9–21.
science and technology publications, portugal (2016)
5. van der aalst, w.m.p.: process mining: data science in action. springer, berlin
(2016)
6. van der aalst, w.m.p., adriansyah, a., van dongen, b.: replaying history on
process models for conformance checking and performance analysis. wires datamining knowl. discov. 2(2), 182–192 (2012)
7. burattin, a., sperduti, a., van der aalst, w.m.p.: control-ﬂow discovery from
event streams. in: ieee congress on evolutionary computation (cec 2014), pp.2420–2427. ieee computer society (2014)
8. calders, t., verwer, s.: three naive bayes approaches for discrimination-aware
classiﬁcation. data min. knowl. disc. 21(2), 277–292 (2010)
9. casella, g., berger, r.l.: statistical inference, 2nd edn. duxbury press, delhi
(2002)
10. council of the european union. general data protection regulation (gdpr).
regulation (eu) 2016/679 of the european parliament and of the council of 27
april 2016 on the protection of natural persons with regard to the processing of
personal data and on the free movement of such data, and repealing directive95/46/ec, april 2016
11. donoho, d.: 50 years of data science. technical report, stanford university, sep-
tember 2015. based on a presentation at the tukey centennial workshop, prince-ton, nj, 18 september 2015
12. european commission: directive 95/46/ec of the european parliament and of
the council on the protection of individuals with wegard to the processing ofpersonal data and on the free movement of such data. oﬃcial journal of the
european communities, no l 281/31, october 199528 w.m.p. van der aalst
13. ieee task force on process mining: xes standard deﬁnition (2013).
www.xes-standard.org
14. kamiran, f., calders, t., pechenizkiy, m.: discrimination-aware decision-tree
learning. in: proceedings of the ieee international conference on data mining
(icdm 2010), pp. 869–874 (2010)
15. koops, b.j., oosterlaken, i., romijn, h., swierstra, t., van den hoven, j. (eds.):
responsible innovation 2: concepts, approaches, and applications. springer,
berlin (2015)
16. leemans, s.j.j., fahland, d., aalst, w.m.p.: exploring processes and deviations.
in: fournier, f., mendling, j. (eds.) bpm 2014. lnbip, vol. 202, pp. 304–316.
springer, cham (2015). doi: 10.1007/978-3-319-15895-2 26
17. miller, r.g.: simultaneous statistical inference. springer, berlin (1981)
18. monreale, a., rinzivillo, s., pratesi, f., giannotti, f., pedreschi, d.: privacy-by-
design in big data analytics and social mining. epj data sci. 1(10), 1–26 (2014)
19. naur, p.: concise survey of computer methods. studentlitteratur lund,
akademisk forlag, kobenhaven (1974)
20. nelson, g.s.: practical implications of sharing data: a primer on data privacy,
anonymization, and de-identiﬁcation. paper 1884–2015, thotwave technologies,
chapel hill (2015)
21. owen, r., bessant, j., heintz, m. (eds.): responsible innovation. wiley, hoboken
(2013)
22. pedreshi, d., ruggieri, s., turini, f.: discrimination-aware data mining. in: pro-
ceedings of the 14th acm sigkdd international conference on knowledge dis-covery and data mining, pp. 560–568. acm (2008)
23. president’s council of advisors on science and technology: big data and privacy:
a technological perspective (report to the president). executive oﬃce of thepresident, us-pcast, may 2014
24. press, g.: a very short history of data science. forbes technology (2013). http://
www.forbes.com/sites/gilpress/2013/05/28/a-very-short-history-of-data-science/
25. rajaraman, a., ullman, j.d.: mining of massive datasets. cambridge university
press, cambridge (2011)
26. ruggieri, s., pedreshi, d., turini, f.: dcube: discrimination discovery in data-
bases. in: proceedings of the acm sigmod international conference on man-
agement of data, pp. 1127–1130. acm (2010)
27. tukey, j.w.: the future of data analysis. ann. math. stat. 33(1), 1–67 (1962)
28. vigen, t.: spurious correlations. hachette books, new york (2015)
29. van zelst, s.j., van dongen, b.f., van der aalst, w.m.p.: know what you stream:
generating event streams from cpn models in prom 6. in: proceedings of thebpm2015 demo session. ceurworkshop proceedings, vol. 1418, pp. 85–89 (2015).
http://ceur-ws.org/