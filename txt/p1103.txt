international  journal  of 
environmental research
and public health
article
privacy-preserving process mining in healthcare†
anastasiia pika1,*
, moe t. wynn1, stephanus budiono1, arthur h.m. ter hofstede1,
wil m.p . van der aalst1,2and hajo a. reijers1,3
1school of information systems, queensland university of technology, brisbane 4000, qld, australia;
m.wynn@qut.edu.au (m.t.w.); sn.budiono@qut.edu.au (s.b.); a.terhofstede@qut.edu.au (a.h.m.t.h.);
wvdaalst@pads.rwth-aachen.de (w.m.p .v.d.a.); h.a.reijers@uu.nl (h.a.r.)
2rwth aachen university, process and data science group, 52062 aachen, germany
3utrecht university, department of information and computing sciences, 3508 tc utrecht, the netherlands
*correspondence: a.pika@qut.edu.au
†proceedings of the second international workshop on process-oriented data science for healthcare, vienna,
austria, 1–6 september 2019 “towards privacy-preserving process mining in healthcare”.
received: 17 january 2020; accepted: 26 february 2020; published: 2 march 2020
/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001
/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046
abstract: process mining has been successfully applied in the healthcare domain and has helped to
uncover various insights for improving healthcare processes. while the beneﬁts of process mining
are widely acknowledged, many people rightfully have concerns about irresponsible uses of personal
data. healthcare information systems contain highly sensitive information and healthcare regulations
often require protection of data privacy. the need to comply with strict privacy requirements may
result in a decreased data utility for analysis. until recently, data privacy issues did not get much
attention in the process mining community; however, several privacy-preserving data transformation
techniques have been proposed in the data mining community. many similarities between data
mining and process mining exist, but there are key differences that make privacy-preserving data
mining techniques unsuitable to anonymise process data (without adaptations). in this article, we
analyse data privacy and utility requirements for healthcare process data and assess the suitability
of privacy-preserving data transformation methods to anonymise healthcare data. we demonstrate
how some of these anonymisation methods affect various process mining results using three publicly
available healthcare event logs. we describe a framework for privacy-preserving process mining that
can support healthcare process mining analyses. we also advocate the recording of privacy metadata
to capture information about privacy-preserving transformations performed on an event log.
keywords: process mining; healthcare process data; data privacy; anonymisation; privacy metadata
1. introduction
technological advances in the ﬁelds of business intelligence and data science empower
organisations to become “data-driven” by applying new techniques to analyse large amounts of
data. process mining is a specialised form of data-driven analytics where process data, collated from
different it systems typically available in organisations, are analysed to uncover the real behaviour
and performance of business operations [ 1]. process mining was successfully applied in the healthcare
domain and helped to uncover insights for improving operational efﬁciency of healthcare processes
and evidence-informed decision making [ 2–6]. a recent literature review [ 3] discovered 172 articles
which report applications of various process mining techniques in the healthcare domain.
while the potential beneﬁts of data analytics are widely acknowledged, many people have grave
concerns about irresponsible use of their data. healthcare data can include highly sensitive attributes
(e.g., patient health outcomes/diagnoses, and the type of treatments being undertaken). hence, privacy
of such data needs to be protected. an increased concern of society with protecting the privacy of
int. j. environ. res. public health 2020 ,17, 1612; doi:10.3390/ijerph17051612 www.mdpi.com/journal/ijerphint. j. environ. res. public health 2020 ,17, 1612 2 of 28
personal data is reﬂected in the growing number of privacy regulations that were recently introduced or
updated by governments around the world. these government regulations provide general governance
principles for the collection, storage, and use of personal data. for example, the general data protection
regulation (gdpr) (https://gdpr-info.eu/) requires that organisations fulﬁl “the right to be forgotten”
(i.e., erase all personal data under certain conditions when requested). failure to comply with data
privacy regulations can lead to signiﬁcant penalties (e.g., organisations can be ﬁned up to 20 million
euro or 4% of their annual global turnover, whichever is higher, if they breach the gdpr). data privacy
requirements are also often included in legislation which regulates the healthcare sector (e.g., in the
australian healthcare identiﬁers act 2010 (https://www.legislation.gov.au/details/c2017c00239)).
the need to comply with strict data privacy requirements often results in a decreased data utility,
i.e., the effectiveness of the anonymised data for data analysis. consider the recently introduced my
health records amendment (strengthening privacy) bill 2018 (https://www.myhealthrecord.gov.au/
about/legislation-and-governance/summary-privacy-protections). this bill allows australians to
delete their electronic health records at any time. while this allows protecting privacy, the quality and
value of data analysis may decrease.
the need to consider data privacy in process mining and develop privacy-aware tools was already
raised in the process mining manifesto [ 7]. however, the process mining community, until recently,
largely overlooked the problem. a few recent articles highlight “a clear gap in the research on privacy
in the ﬁeld of process mining” [ 8] and make ﬁrst attempts to address some privacy-related challenges
(e.g., ref. [8–12]) yet, signiﬁcant challenges remain.
privacy considerations are quite well-known in the ﬁeld of data mining and several
privacy-preserving data transformation techniques were proposed [ 13,14] (e.g., data swapping,
generalisation, or noise addition). although there are many similarities between data mining and process
mining, some key differences exist that make some of the well-known privacy-preserving data mining techniques
unsuitable to transform process data. for example, the addition of noise to a data set may have
an unpredictable impact on the accuracy of all kinds of process mining analyses.
in this article, we analyse data privacy and utility requirements for process data typically recorded
in the healthcare domain, assess the suitability of privacy-preserving data transformation methods
proposed in the data mining and process mining ﬁelds to anonymise healthcare process data, and
evaluate the impact of selected privacy-preserving methods on process mining results. the results of
the analyses and the evaluation showed that the problem of privacy protection for healthcare data
while preserving data utility for process mining analyses is challenging. as a possible solution to the
problem, we propose a privacy-preserving process mining framework which is based on the use of
privacy metadata, and we propose a privacy extension for xes logs.
this journal article presents an extended version of the workshop paper presented at pods4h
2019 [ 15] with two new additions (sections 5 and 7). section 5 presents new insights from a detailed
evaluation conducted on three healthcare event logs. section 7 describes the proposed privacy metadata
for xes logs. in addition to new research contributions presented in these two sections, the related
work discussion in section 2 has been extensively revised.
this article is organised as follows. we present related work (section 2), analyse data privacy
and utility requirements for healthcare process data (section 3), and assess the suitability of existing
privacy-preserving methods to anonymise healthcare process data (section 4). we then evaluate the
impact of some generic data transformation approaches on the results of various process mining
methods applied to three publicly available healthcare event logs (section 5), describe the proposed
privacy-preserving process mining framework in section 6, and describe the proposed privacy
extension in section 7. section 8 concludes the paper.
2. related work
in this section, we ﬁrst provide an overview of privacy-preserving data mining (section 2.1)
and describe selected generic data transformation approaches (section 2.1.1) and privacy modelsint. j. environ. res. public health 2020 ,17, 1612 3 of 28
(section 2.1.2). we then discuss existing privacy-preserving approaches proposed by the process
mining community (section 2.2).
2.1. privacy-preserving data mining
privacy and access control considerations are quite well-known in several research communities,
including the statistical community, the database community, the cryptographic community, and the
data mining community. several data transformation techniques, access control mechanisms, and
frameworks to preserve data privacy were proposed by these communities [ 13,14,16,17]. techniques
for protecting respondents’ privacy, which originated in the statistics community, are often referred to
as statistical disclosure control (sdc) [ 17]. the data mining community is concerned with protecting
privacy of personal information that may be recorded about individuals (e.g., medical history); methods
proposed by this community are usually referred to as privacy-preserving data mining (ppdm) [ 13].
distributed ppdm methods, which aim to protect privacy of multiple data owners who wish to conduct
analysis of combined data sets without disclosing their data to other data owners [ 13], originated in the
database and cryptographic communities [ 17]. although privacy-preserving methods “have evolved
in a fairly independent way within research communities”, many methods proposed in one community
are also applied by other research communities [ 17]. for example, data swapping, suppression, noise
addition, and k-anonymity are discussed in both the sdc literature [ 18] and the ppdm literature [ 13].
in this article, we use term ppdm to refer to all privacy-preserving methods (regardless of their origin).
to preserve data privacy, privacy-preserving methods usually reduce the representation accuracy
of the data [ 13]. such data modiﬁcations can affect the quality of analysis results. the effectiveness of
the transformed data for analyses is often quantiﬁed explicitly as its utility [13]. the main challenge of
privacy-preserving methods is to minimise privacy risks while maximising data utility [13,18,19].
most privacy-preserving methods aim to minimise risks of identity disclosure or sensitive attribute
disclosure [ 13,19]. identity disclosure happens when an individual is identiﬁed by an attribute
(e.g., social security number) or by a combination of attributes (e.g., age, gender, postcode, and job
title). sensitive attribute disclosure happens when a value of some sensitive attribute is discovered
by an adversary (e.g., medical diagnosis). many privacy-preserving data transformation methods
that originated in the statistics community (e.g., data swapping or noise addition) do not provide
any formal privacy guarantees and “the level of protection is empirically evaluated a posteriori for
a speciﬁc dataset” [ 19]. for example, distance-based approaches quantify the level of protection by
computing distance between the original data set and the transformed data set [ 18]. other ppdm
methods can “attain a predeﬁned notion of privacy and offer a priori privacy guarantees over the
protected data”, they are usually referred to as privacy models [ 19]. for example, privacy guarantees
can be speciﬁed in terms of k-anonymity . a data set satisﬁes k-anonymity if each record in the data set
is indistinguishable from at least k-1other records. we discuss in detail methods from both categories
in sections 2.1.1 and 2.1.2.
methods for measuring data utility either assess information loss by quantifying differences
between original and anonymised data [ 18] or are designed for speciﬁc applications; for example,
utility can be assessed by comparing classiﬁcation accuracy [ 20] or regression coefﬁcients [ 18] obtained
from original and anonymised data. utility measures designed for speciﬁc applications are more
informative as different data analysis methods have different data requirements [18,20].
privacy-preserving data mining techniques can be generic or speciﬁc [ 14].generic approaches
modify data in such a way that “the transformed data can be used as input to perform any data mining
task” [ 14]. these approaches can provide anonymisation (in this article, anonymisation refers to any
method that can protect data privacy) by modifying records without introducing new values (e.g., data
swapping) or they can modify original values (e.g., by adding noise). in speciﬁc approaches privacy
preservation is tailored for speciﬁc data mining algorithms (e.g., a privacy-preserving approach for
clustering of big data on cloud [ 21] or privacy-preserving decision tree classiﬁcation) [ 14]. furthermore,
outputs of some data mining algorithms can also be sensitive and methods that anonymise such outputsint. j. environ. res. public health 2020 ,17, 1612 4 of 28
were proposed (e.g., association rule hiding) [ 13]. finally, distributed privacy-preserving methods are
proposed for scenarios in which multiple data owners wish to derive insights from combined data
without compromising privacy of their portions of the data [ 13]. such methods often use cryptographic
protocols for secure multi-party computations (smc) [ 13]. in this article, we focus on protecting
privacy of process data within a healthcare organisation, distributed privacy scenarios are considered
outside the scope of this work. furthermore, we do not analyse speciﬁc ppdm methods and methods
for protecting output privacy as they are tailored to speciﬁc data mining algorithms (and are not
applicable to other data or process mining algorithms).
2.1.1. generic privacy-preserving data transformation approaches
in this subsection, we describe generic privacy-preserving data transformation approaches, such
as data swapping, noise addition, suppression, generalisation, and micro-aggregation [ 13,18]. we
evaluate the suitability of these approaches to anonymise process data in section 4.1.
data swapping involves enacting privacy to a dataset by adding a degree of uncertainty.
uncertainty is introduced into individual records by swapping the true values of sensitive attributes
between subsets of records [ 16]. this method allows anonymisation of both numerical and
categorical attributes.
noise addition can be used for both numerical and categorical data [ 14]. numerical values
are often anonymised by factoring randomly and independently generated “white noise” into the
original data [ 13]. white noise is generated using a random distribution, often either uniform or
gaussian. adding noise to categorical values is more complex, and can be achieved, for example, using
clustering-based techniques [ 22]. this method preserves the aggregate distribution of the attribute
values; however, the randomisation leads to the loss of individual records.
suppression anonymises data by omission. values can be removed under three types of data
suppression [ 13]. the most common type is column suppression which targets the presence of highly
sensitive attributes whose values directly identify an individual (e.g., patient names or identiﬁcation
numbers). alternatively, row suppression is used when outlier records are infrequent and difﬁcult to
anonymise. value suppression omits selected sensitive attribute values.
generalisation methods replace data values with approximate values making it difﬁcult for
adversaries to identify records with full conﬁdence [ 13]. the process of generalising usually includes
the construction of a generalisation hierarchy, which is a predeﬁned classiﬁcation of values at
decreasing levels of granularity. for numeric data, values are sorted into numerical ranges. for
categorical data, a domain expert creates semantically meaningful generalisations using a tree structure.
micro-aggregation methods consist of two steps: partition and aggregation [ 23]. partition organises
the original records into clusters whose data is similar to each other. an aggregation operator is then
used to compute a collective value (e.g., mean, median, interval, or mode) for each cluster. original
values in each cluster are then replaced with the computed collective value. micro-aggregation can
be applied to both continuous and categorical data without the need for the data author to create
generalised categories. various approaches to perform micro-aggregation were proposed; for example,
a hybrid micro-aggregation approach which is “based on fuzzy possibilistic clustering” [24].
2.1.2. privacy models
k-anonymity is one of the oldest privacy models whose goal is to prevent identity disclosure [ 13].
k-anonymity methods require that “all combinations of key attributes in a database be repeated at
least for krecords” [ 17]. many approaches for achieving k-anonymity were proposed and often use
suppression, generalisation, or micro-aggregation. k-anonymity helps to prevent identity disclosure;
however, it cannot prevent sensitive attribute disclosure [ 13]. for example, a dataset may satisfy
k-anonymity; however, a group of records with identical key attributes may have the same value of
a sensitive attribute (e.g., diagnosis). although one cannot link an individual to a record, they can still
discover the diagnosis.int. j. environ. res. public health 2020 ,17, 1612 5 of 28
l-diversity and t-closeness privacy models target this shortcoming of k-anonymity [ 13,17].
l-diversity requires that each group of records with identical key attributes (an equivalence class)
contains at least l“well-represented” values for a sensitive attribute [ 13]. there are different deﬁnitions
of “well-represented” values; for example, distinct l-diversity requires that each equivalence class
contains at least ldistinct values [ 17]. the t-closeness model requires that the distance between the
distribution of a sensitive attribute in an equivalence class and the distribution of the attribute in the
data set does not exceed a threshold t[17]. while the t-closeness model provides a better privacy
protection than the l-diversity model, it does so at the expense of data utility by “severely impairing
the correlations between conﬁdential attributes and key attributes” [17].
the differential privacy model “ensures that (almost, and quantiﬁably) no risk is incurred by joining
a statistical database” [ 25]. in this model, sensitive information is collected by a trusted curator who
releases “statistical facts about the underlying population” [26] and original data is not released.
these privacy models were developed for statistical databases in which “records refer to
individuals that are described by a set of usually uni-valued attributes” [ 19]. the assumption that
a record contains all information about an individual is not true for process execution data in which
personal information can be scattered across multiple records and there could be dependencies between
such records (we discuss this in detail in section 3). therefore, existing privacy models that are focused
on statistical databases are not directly applicable to process execution data.
2.2. privacy-preserving process mining
several articles made ﬁrst attempts to address some privacy-related process mining
challenges [8–12,27–30] . mannhardt et al. [ 8] analysed privacy challenges in human-centered industrial
environments and provided some generic guidelines for privacy in process mining. michael et al. [ 29]
proposed a privacy system design for process mining which considers a number of “privacy elements”;
for example, what information should be collected, by whom, for what purposes, how it is stored,
and who can access it. raﬁei and van der aalst [ 31] proposed a log anonymisation method which
protects resource information “against frequency-based attacks” and allows discovering roles from the
anonymised log. liu et al. [ 11] presented a privacy-preserving cross-organisation process discovery
framework based on access control. tillem et al. [ 27,28] presented interactive two-party protocols for
discovery of process models from encrypted data, which are based on multiple communication rounds
(and have high computation costs).
the ﬁrst privacy-preserving data transformation approach presented in the process mining
community [ 9] proposes to use deterministic encryption methods for anonymisation of event log
attribute values. (such methods are also a part of the conﬁdentiality framework proposed by raﬁei
et al. [ 12].) timestamps are treated as numeric values and are encrypted in a way that preserves the
order of events. deterministic encryption methods produce “the same ciphertext for a given plaintext”
and preserve differences between values, which is important for process mining [ 12]. encryption
only provides weak data privacy protection and “could be prone to advanced de-anonymization
techniques” [9].
more advanced privacy-preserving process mining approaches proposed by raﬁei et al. [ 12],
fahrenkrog-peterse et al. [10], and mannhardt et al. [30] will be discussed in detail in section 4.
unlike these privacy-preserving process mining approaches, in this article we focus on privacy of
healthcare process data, evaluate the suitability of existing privacy-preserving data transformation
approaches to anonymise such data, and propose a privacy-preserving process mining framework,
which is based on the use of privacy metadata.
3. data privacy and utility requirements: healthcare
in this section, we ﬁrst describe process execution data typically recorded in the healthcare domain
(section 3.1) and legislative requirements that regulate the use of such data (section 3.2). to realise our
objective of privacy-preserving process mining for the healthcare domain, we then analyse privacyint. j. environ. res. public health 2020 ,17, 1612 6 of 28
requirements for healthcare process data (section 3.3), which is followed by a discussion of data
requirements of process mining approaches to analyse healthcare processes (section 3.4).
3.1. healthcare process data
process mining uses process data in the form of an event log, which represents collated and
aggregated data from it systems available in organisations. an event log contains events where
each event refers to a case, an activity, a point in time, transaction type (e.g., start orcomplete ), and
(optionally) a resource and data attributes. an event log can be seen as a collection of cases and each
case can be seen as a sequence of events.
cases in healthcare processes typically refer to patients receiving treatments (e.g., a patient’s
pathway) and resources refer to medical personnel involved in the process. figure 1 depicts an example
event log which contains six events (represented by rows) related to two cases ( 1and 2). for example,
we can see that case 1refers to a patient whose age is 56, who speaks english and was diagnosed with
pancreatitis; activity register is completed in this case; activity blood test was started on 13/01/2019 at
17:01 byrobert ; and treatment code 3456 is associated with activity triage in case 1. data attributes
can refer to cases (e.g., age, language, and diagnosis) or to events (e.g., treatment codes are recorded
for events associated with activity triage ). in this example, we used some data attributes which
are recorded in two publicly available healthcare logs. the healthcare mimic data set (https://
mimic.physionet.org/mimicdata/) contains information about language and diagnosis (as well as
ethnicity, religion, marital status, and insurance). the dutch academic hospital event log (https:
//data.4tu.nl/repository/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54) contains information about
age, diagnosis, and treatment codes.
figure 1. example of an event log with typical healthcare data attributes.
3.2. legislative requirements
an increased concern of people with protecting the privacy of their data is reﬂected in the growing
number of privacy regulations that were recently introduced (e.g., the eu general data protection
regulation (gdpr) 2018, the california consumer privacy act of 2018) or updated by governments
around the world (e.g., australian privacy regulation 2013 under the privacy act 1988). in addition,
data privacy requirements are often included in legislation governing speciﬁc sectors, e.g., australian
healthcare identiﬁers act 2010.
guidance for de-identiﬁcation of protected health information in the us is provided in the
health insurance portability and accountability act (hipaa) privacy rule. for example, the “safe
harbor” de-identiﬁcation method of the hipaa privacy rule prescribes removal of all elements of
dates (except year) related to an individual (e.g., admission or discharge dates) (https://www.hhs.
gov/hipaa/for-professionals/privacy/special-topics/de-identiﬁcation/index.html#protected). in
australia, the ofﬁce of the australian information commissioner provides guidelines for the use of
health information for research. the guidelines prescribe de-identiﬁcation of personal information
by “removing personal identiﬁers, such as name, address, d.o.b., or other identifying information”
and “removing or altering other information that may allow an individual to be identiﬁed, for
example, because of a rare characteristic of the individual, or a combination of unique or remarkable
characteristics” (https://www.oaic.gov.au/engage-with-us/consultations/health-privacy-guidance/
business-resource-collecting-using-and-disclosing-health-information-for-research). furthermore,
the recently introduced my health records amendment (strengthening privacy) bill 2018 (https:int. j. environ. res. public health 2020 ,17, 1612 7 of 28
//www.myhealthrecord.gov.au/about/legislation-and-governance/summary-privacy-protections)
allows australians to opt out of having an electronic health record and allows the deletion of their
records permanently at any time. whilst providing strong privacy protections for australians, these
measures also introduce data quality issues such as missing and incomplete data. this reduces the
utility of data, decreases the accuracy of results, and inﬂuences analysis.
privacy of public healthcare data is typically protected by replacing sensitive attribute values with
anonymised values (e.g., treatment codes are used in a publicly available dutch academic hospital
event log and subject ids are used in the healthcare mimic data set) or by removing sensitive attributes
from data (e.g., employee information is removed from both the dutch hospital and mimic data sets).
the former method only provides weak privacy protection while the latter method can signiﬁcantly
decrease data utility.
3.3. privacy requirements for healthcare process data
healthcare process data can contain sensitive information such as patient or employee names
or identiﬁers. other attributes in the event log can also reveal patient or employee identities when
combined with background knowledge about the process. for example, accident or admission time,
a rare diagnosis or treatment, or a combination of age and language could potentially identify a patient.
an employee could be identiﬁed by the combination of an activity name and execution time (e.g., when
a blood test is always performed by the same employee during a shift). hence, typical event log
attributes such as case id, activity, time, resource and many data attributes (e.g., a patient’s personal and
treatment information) can contribute to identity disclosure.
furthermore, relations between events in a log can contribute to identity disclosure and this is
especially pertinent for a healthcare event log due to the high variability of process paths typical for
the sector [ 2]. consider, for example, the dutch hospital event log where 82% of cases follow unique
process paths. hence, someone with knowledge of the process could link these cases to individual
patients. moreover, cases which follow the same process path can include other atypical behaviors.
in the dutch hospital log, the ﬁfth most frequent process variant is followed by 8 cases: 7 cases are
related to only one organisational group (“obstetrics and gynecology clinic”) and only one case is
also related to the “radiotherapy” group. although the case does not follow a unique process path,
the relation to the “radiotherapy” group is unique and could be used by someone with knowledge of
the process to identify the patient. other examples of atypical process behaviour which could contribute
to a patient’s identity disclosure include abnormally short or long execution times of activities or
cases, or an abnormally low or high number of resources involved in a case. healthcare processes
may contain many different types of atypical process behaviour as these processes are often “complex
and highly variable” and involve “frequent interactions between clinicians, nursing staff, diagnostic
support specialists and administrative personnel” [2].
if employees can enter or modify some process information (e.g., activity labels), this can introduce
additional privacy threats. for example, an activity label may be modiﬁed to include a doctor’s name,
or some doctor may have a habit to not record some data attribute (hence, missing data can indicate
the doctor’s involvement in the case). in this article we do not further evaluate such scenarios.
3.4. data requirements for process mining approaches
all mainstream process mining algorithms require case ids and activities to be recorded
accurately in the log. also, most algorithms require (accurate) timestamps. a recent literature
review [ 3] discovered that the following types of process mining analyses were frequently used in
healthcare: discovery techniques (which include process discovery as well as organisational mining
approaches such as social network mining), conformance checking, process variant analysis, and
performance analysis.
 process discovery techniques usually take as input a multi-set of traces (i.e., ordered sequences
of activity labels) and do not require timestamps; however, timestamps are typically used toint. j. environ. res. public health 2020 ,17, 1612 8 of 28
order events. most academic process discovery algorithms (implemented in prom) and some
commercial process discovery tools (e.g., celonis) can discover formal models with concurrency
(represented using modeling notations with well-deﬁned semantics, e.g., petri nets). such
algorithms require that all events in the log refer to cases. on the other hand, most commercial
process mining tools (as well as some prom plugins) convert the log to directly follows
graphs (dfg) annotated with frequencies and times, which show how frequently different
activities follow each other and average times between them. such tools then use the annotated
dfg to perform process discovery, conformance, and performance analysis. dfg-based tools do
not require complete traces and only require that “directly-follows” relations between activities
are preserved in the log.
 most academic process conformance and performance analysis techniques (e.g., alignment-based
approaches) use formal models and require that complete traces are recorded in the log; while
most commercial tools work with directly follows graphs.
 organisational mining techniques require resource information to be recorded in the log (in
addition to case ids, activities, and timestamps). moreover, resource and data attributes can also
be required by conformance checking approaches that consider different process perspectives.
 process variant analysis, which is concerned with comparing process behaviour and performance
of different cohorts, often uses case data attributes to distinguish between cohorts.
to comply with strict privacy requirements for healthcare data, one would need to consider
anonymising (1) event log attribute values and (2) atypical process behaviour . however, many process
mining techniques require that healthcare process data is accurate and representative. that is: (1) all
events belong to a particular case ;(2) attributes that represent case identiﬁers and activity labels are accurate; and
(3) timestamps are reliable and accurate . thus, the need to balance the privacy requirements of healthcare
data and the utility requirements of process mining techniques is paramount. in the following section,
we assess whether existing privacy-preserving data transformation approaches can preserve the
attribute values and relations between events as discussed above.
4. anonymising healthcare process data
in this section, we assess the suitability of different data transformation approaches to anonymise
sensitive attribute values (section 4.1) and atypical process behavior (section 4.2) that can be present in
healthcare process data.
4.1. anonymising sensitive attribute values
as discussed in section 3, typical event log attributes such as case, activity, time, resource , and many
data attributes could contribute to identity disclosure. below, we discuss how these attributes could be
anonymised using generic data transformation approaches described in section 2. we evaluate the
suitability of deterministic encryption (referred to here as encryption), which was used to anonymise
event log data [ 9,12], and other traditional data transformation approaches used in the data mining
community such as data swapping, value suppression, generalisation, micro-aggregation, and noise
addition (which, to the best of our knowledge, were not applied to event logs). figure 2 depicts how
some of these techniques can be applied to the event log in figure 1.
case identiﬁers can be encrypted (as well as other event log attributes); however, encryption does
not provide strong data privacy protection (and may not be suitable to protect sensitive healthcare
data). an underlying assumption of all process mining algorithms is that case identiﬁers are unique,
which makes the application of value suppression, generalisation, and micro-aggregation not suitable
(these methods are used to hide infrequent attribute values). adding noise to case identiﬁers can yield
values that are no longer unique, which can decrease the accuracy of all process mining algorithms.
data swapping can be applied to case ids without impact on process mining results.int. j. environ. res. public health 2020 ,17, 1612 9 of 28
figure 2. application of data transformation techniques to the event log in figure 1: case id: swapping;
time: noise addition; resource: generalisation; diagnosis: suppression.
activity labels can be encrypted; however, encrypted labels can be identiﬁed by someone with
knowledge of the process (e.g., most or least frequent activities [ 12]). moreover, encryption makes it
difﬁcult to interpret analysis results. in addition, one must also encrypt process model labels when
applying process mining algorithms that use process models as input (e.g., many process performance
and conformance analysis approaches). the application of value suppression, generalisation, and
micro-aggregation to activity labels may affect the accuracy of process mining results where the
utility loss depends on the process mining algorithm used. for example, removing infrequent activity
labels may not have a signiﬁcant effect on process discovery results (as process models often capture
mainstream process behavior); however, process conformance analysis results may become invalid.
one can use generalisation or micro-aggregation to hide some sensitive activities (e.g., replace activities
“hiv test” and “hepatitis c test” with activity “blood test”). the result of process discovery performed
on such logs will be correct; however, the discovered process model will be on a higher level of
granularity. noise addition and swapping activity labels will invalidate the results of all process
mining algorithms. for example, if activity labels in a log are swapped, the resulting traces will consist
of random activity sequences; hence, discovered process models will be incorrect, as well as other
process mining results.
timestamps can be treated as numerical values and encrypted using methods which preserve the
order of events. such encryption will not affect the results of process mining algorithms that work
with ordered events and do not require timestamps (such as many process discovery algorithms).
on the other hand, an event log with encrypted timestamps will not be suitable for performance
analysis. value suppression, generalisation, and micro-aggregation can be used to anonymise sensitive
timestamps (e.g., as discussed in section 3, according to the hipaa privacy rule admission and
discharge times must be anonymised). this will affect the accuracy of most process mining algorithms.
for example, if value suppression is applied to admission times, the discovered process model will not
include activity “admission”. on the other hand, if generalisation is applied to admission times (by
only leaving the year as prescribed by the hipaa privacy rule), process discovery may not be affected
(provided that admission is the ﬁrst activity in the process and the order of activities is preserved);
however, process performance analysis results may become invalid (as the time between admission
and other activities in the process will no longer be correct). the effect of micro-aggregation depends
on the aggregation operator used to compute a collective value. if the aggregation operator preserves
the order of events, then some process mining algorithms may not be affected (e.g., process discovery);
however, if the operator changes the order of events (e.g., mean or median), then process mining
results may become invalid. adding noise to timestamps or swapping their values will yield incorrect
process mining results (as the order of events in the transformed log is no longer preserved).
resource information can be encrypted without impacting organisational mining results, while
noise addition and swapping will invalidate such results (as resources will no longer be related to
correct events and cases). one can apply generalisation or micro-aggregation to resource information
(e.g., by replacing individual identiﬁers with team identiﬁers), which will yield analysis results
on a team level. value suppression can affect the accuracy of organisational mining techniques
(e.g., a discovered social network may have fewer nodes).int. j. environ. res. public health 2020 ,17, 1612 10 of 28
data attributes can be encrypted, though encryption of numerical values can make it difﬁcult to
conduct some types of analysis. for example, if ageis encrypted, one can no longer compare process
variants for different age cohorts. value suppression can decrease the accuracy of process mining
algorithms that use data (e.g., when infrequent age values are removed, the corresponding cases will
not be included in process variant analysis). using generalisation and micro-aggregation may decrease
the accuracy of conformance analysis methods that consider data; however, it may not have any impact
on variant analysis (e.g., when comparing different age groups). noise addition and data swapping
will yield incorrect results for process mining methods that require data.
application of column suppression to any event log attribute will make it impossible to use
process mining algorithms which use this attribute. for example, if resource information is removed,
one can no longer analyse the organisational perspective. row suppression may affect the results of
all process mining algorithms—the magnitude of the effect will depend on the number and types of
records removed from the data set.
table 1 summarises the suitability of different data transformation approaches to anonymising
event log attribute values. encryption has a minimal effect on data utility for most process mining
algorithms; however, it may not provide a required level of privacy protection. data swapping can
be used to anonymise case ids; however, the application of this method to other event log attributes
will invalidate process mining results. noise addition will nullify all process mining results. value
suppression, generalisation, and micro-aggregation are not suitable for case ids (as they have unique
values), these methods can be applied to other attributes; however, the accuracy of process mining
results may be affected.
table 1. suitability of privacy-preserving data transformation approaches to anonymising event log
attributes: na: not applicable; ‘+’: does not affect process mining results; ‘  ’: can be used to anonymise
an attribute, however invalidates process mining results; ‘+/  ’: can decrease the accuracy of some
process mining methods.
case id activity time resource data
encryption (deterministic) + + +/   + +/  
swapping +        
noise addition          
value suppression na +/   +/  +/  +/ 
generalisation/micro-aggregation na +/   +/  +/  +/ 
4.2. anonymising atypical process behaviour
as discussed in section 3, relations between events in the log (such as event order or grouping of
events by case identiﬁers) can be used to identify atypical process behaviour (which could be linked
to individuals). there could be many different types of atypical process behaviour (e.g., infrequent
activity sequences, an abnormal number of resources, or atypical durations). below, we evaluate three
approaches which target anonymisation of atypical process behaviour: a conﬁdentiality framework [ 12],
pretsa [10], and differential privacy model for event logs [30].
the conﬁdentiality framework for process mining [ 12] combines a few data transformation
techniques. the ﬁrst step of the framework is ﬁltering out all cases “that do not reach the minimal
frequencies” [ 12]. the framework changes the structure of an event log: a new attribute “previous
activity” is added (which speciﬁes for each event the preceding activity in a case) and case ids are
removed. the transformed event log can be used to extract a dfg. since events in the transformed
log are no longer related to cases, it is impossible to identify traces (and atypical process behaviour).
however, the transformed log can no longer be used by process mining algorithms that require
complete traces; it is only suitable for dfg-based tools (e.g., commercial process mining tools).int. j. environ. res. public health 2020 ,17, 1612 11 of 28
pretsa [10] is a log sanitisation algorithm, which represents a log as a preﬁx tree and then
transforms the tree until given privacy guarantees are met while striving to preserve directly follows
relations. the approach allows anonymising two types of atypical process behaviour: infrequent traces
and atypical activity execution times. the article [ 10] evaluates the impact of the log transformation
on the results of process discovery and performance analysis algorithms using three real-life logs
including a hospital log. it also compares the performance of pretsa with a “baseline” approach
which ﬁlters out infrequent traces. the evaluation showed that pretsa outperforms the baseline
approach on all logs and data utility losses are minimal for event logs which do not have many unique
traces. however, for a log in which most traces are unique (a hospital log) the utility of the transformed
log is signiﬁcantly decreased, even more so for stricter privacy requirements (which means that the
algorithm may not be suitable for healthcare process data).
the differential privacy model for event logs [30] targets process discovery algorithms and
supports two types of queries: frequencies of directly follows relations and frequencies of traces
(i.e., full activity execution sequences). noise (laplacian) is added to the output for privacy protection:
higher levels of privacy require more noise. the model was evaluated on two event logs including
a hospital log. the results showed that adding noise to directly follows relations frequencies did not
have a signiﬁcant effect on process discovery results for both event logs. on the other hand, adding
noise to trace frequencies signiﬁcantly affected process discovery results for the hospital log.
5. evaluating the impact of anonymisation on process mining
in this section, we evaluate the impact of some generic data transformation approaches (discussed
in section 4) on the results of process mining methods that are often used in the healthcare domain
(discussed in section 3) for three publicly available healthcare event logs. in section 5.1, we describe
characteristics of the event logs used in the evaluation; in section 5.2, we discuss anonymisation
methods we applied to these logs; in section 5.3, we present the effects of these methods on various
process mining results; and in section 5.4, we summarise the lessons learned from our evaluation.
5.1. event logs
in the evaluation, we used three publicly available event logs originating from three hospitals. the
ﬁrst event log, referred to here as the sepsis log (https://data.4tu.nl/repository/uuid:915d2bfb-7e84-
49ad-a286-dc35f063a460), contains events related to sepsis cases from a hospital (sepsis is a medical
condition caused by an infection), where each case represents a pathway of a patient through the
hospital. the timestamps of events in the log were randomised in a way that preserves the time between
events within a case. the second event log, referred to here as the bpic11 log (https://data.4tu.nl/
repository/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54) (which was used in the first international
business process intelligence challenge (https://www.win.tue.nl/bpi/doku.php?id=2011:challenge)),
originates from a dutch hospital. cases in this log represent pathways of patients of a gynaecology
department. the third event log, referred to here as the billing log (https://data.4tu.nl/repository/
uuid:76c46b83-c930-4798-a1c9-4be94dfeb741), contains events related to the process of billing of
medical services provided by a regional hospital. as with the sepsis log, the timestamps in the
billing log were randomised while preserving the time between events in a case. such timestamp
randomisation may invalidate the results of process mining methods that analyse changes in process
behaviour over time; however, it does not change the results of the process mining methods used in
the evaluation (and most other mainstream process mining techniques) as the time between events in
cases is preserved.
table 2 provides information about the characteristics of these three logs. for the bpic11 log, we
only used events related to cases started during a period of 6 months (1/01/2006–30/06/2006) due to
performance issues of many process mining techniques on the complete log (in table 2 we provide
information about the ﬁltered log). table 2 shows that the event logs have different characteristics,
with the number of activities ranging from 16 to 333 and the number of cases ranging from 220 toint. j. environ. res. public health 2020 ,17, 1612 12 of 28
100,000. in the sepsis log and the bpic11 log most cases follow unique process paths, whereas the
billing log is more structured (on average, 98 cases follow a process path).
table 2. characteristics of the three healthcare event logs used in the evaluation.
event log events activities cases process variants cases per variant log duration
sepsis 15,214 16 1050 846 1.2 1 year & 210 days
bpic11 27,065 333 220 192 1.1 2 years & 88 days
billing 451,359 18 100,000 1020 98 3 years & 37 days
5.2. anonymisation
in section 4.1, we assessed the suitability of different generic data transformation approaches
to anonymise sensitive attribute values. the analysis showed that encryption has a minimal effect
on data utility but provides weak privacy protection; swapping and noise addition invalidate most
process mining results; while the application of suppression and generalisation to some attributes can
affect process mining results for event logs with certain characteristics. in this section, we conduct an
empirical evaluation of the impact of suppression and generalisation on the results of different process
mining algorithms applied in the healthcare domain using hospital logs with different characteristics.
as discussed in section 2, to hide sensitive or infrequent information, one could use suppression,
which removes values, rows or columns, or generalisation, which replaces data values with
approximate values [ 13]. in the experiments, we applied generalisation to the timestamp attribute, and
suppression was applied to the activity, resource and data attributes. these anonymisation operations
target the following privacy threats:
 activity suppression was used to hide infrequent activities: for a given value of k(described
below), we suppressed all activities that were not performed in at least kcases. activity
suppression targets an infrequent activity linkage threat (e.g., when an adversary with background
knowledge of the process can identify a patient by a rare medical test or treatment that was
performed in the case).
 resource suppression was used to hide infrequent resources: for a given value of k, we suppressed
all resources that were not involved in at least kcases. resource suppression targets an infrequent
resource linkage threat (e.g., when an adversary can identify a patient by the involvement in the
patient’s case of a doctor who is only involved in exceptional cases).
 data suppression was used to hide infrequent combinations of case data attributes: for a given
value of k, we suppressed all combinations of case data attributes that were not associated with at
least kcases. this anonymisation method targets infrequent case data linkage threat (e.g., when
a patient can be identiﬁed by a unique combination of case data attributes, e.g., age, language
and diagnosis).
 generalisation was used to replace exact timestamp values with more general values (e.g., by only
keeping the year of an event as we describe below). this anonymisation method targets timestamp
linkage threat (e.g., when a patient can be identiﬁed by their admission time to the hospital).
we created a number of anonymised event logs by applying one anonymisation method to
one attribute at a time (the code used to create the anonymised logs and the resulting anonymised
logs are uploaded as supplementary materials as described in appendix a). please note that each
anonymisation method only targets one speciﬁc privacy threat (as described above) and does not
guarantee privacy protection of personal information in the log (our goal here is to evaluate the impact
of each method on process mining results). below, we provide a detailed description of how different
attributes in the three event logs were anonymised.
suppression was applied to activity labels: events related to a given activity are removed if the
activity was not performed in at least kcases. this was repeated for all activities for three differentint. j. environ. res. public health 2020 ,17, 1612 13 of 28
values of k:k=2,k=10, and k=100. timestamps were generalised with two levels of granularity:
month —month and year is kept for each event (while information about date and time is removed);
and year—only year is kept for each event. in section 3, we described a requirement of the hipaa
privacy rule which prescribes removal of all elements of dates except year related to an individual,
including admission and discharge dates. to show the effect of this anonymisation approach, we
created a version of the sepsis log in which we only generalised the timestamps of admission and
discharge events (the timestamps of other events were not changed). this anonymisation approach
was not applied to the other two logs as the billing log does not include patients’ admission and
discharge times, and it is not clear which events refer to admission and discharge times in the bpic11
log. please note that events in cases are ordered by timestamps in these logs. generalisation of all
timestamps did not change the order of events; however, generalisation of admission and discharge
times resulted in the changed order of events in some cases.
table 3 shows the percentage of cases and events affected by these privacy-preserving
transformations in the three event logs. we can see that few events were affected by activity suppression
in the billing log and the sepsis log, while more events were affected in the bpic11 log (the log has many
activity labels). generalisation of all timestamps affected all events in the three logs, and generalisation
of admission and discharge times in the sepsis log affected 21% of events.
table 3. the percentage of cases and events affected by the application of activity suppression and
time generalisation to the three event logs.
sepsis bpic11 billing
anonymisation methodevents
affectedcases
affectedevents
affectedcases
affectedevents
affectedcases
affected
activity suppression ( k=2) 0% 0% 0.6% 22% 0.0002% 0.001%
activity suppression ( k=10) 0.04% 0.6% 3% 58% 0.0002% 0.001%
activity suppression ( k=100) 0.7% 10.6% 32% 76% 0.1% 0.2%
time generalisation, all (month & year) 100% 100% 100% 100% 100% 100%
time generalisation, adm.&dis. (year) 21% 100% na na na na
the three event logs include information about resources (employees or organisational groups)
who performed process activities. we anonymised resource information in the sepsis log and the
bpic11 log; the billing log was not used as resource information is missing for many events in the
log (44.82%), and we would like to separate data quality issues from the effect of privacy-preserving
transformations. the sepsis log and the bpic11 log include information about organisational groups
(here referred to as resources) responsible for process activities. for each log, we applied value
suppression to the resource attribute: a given value of the resource attribute is removed if the resource
was not involved in at least kcases (for k=2,k=10, and k=100). we also created the second version
of the anonymised logs by applying row suppression: an event is removed if the value of its resource
attribute is not associated with at least kcases (for k=2,k=10, and k=100).
the three event logs have many data attributes; however, most of them have many missing values.
to show the effect of anonymisation of data attributes, we used selected data attributes (which do not
have any missing values) in the sepsis log. the sepsis log includes a number of data attributes with
information related to diagnosis and treatment. we used 22 data attributes recorded for each case that
did not have any missing values (5 data attributes with missing values were removed). these 22 data
attributes have two values ( true orfalse). we applied value suppression to these attributes: if a given
combination of values of the 22 data attributes is associated with fewer than kcases, then the values
are suppressed (for k=2,k=10, and k=100).
table 4 shows the percentage of cases and events in the sepsis log and the bpic11 log that were
affected by the resource and data anonymisation methods (resource suppression refers to both valueint. j. environ. res. public health 2020 ,17, 1612 14 of 28
suppression and row suppression of the resource attribute—the number of the affected events and
cases is the same for both anonymisation methods).
table 4. the percentage of cases and events affected by the application of resource and data
anonymisation to the sepsis log and the bpic11 log.
sepsis bpic11
anonymisation method events affected cases affected events affected cases affected
resource suppression ( k=2) 0.01% 0.2% 0.03% 2.3%
resource suppression ( k=10) 0.01% 0.2% 0.33% 10.5%
resource suppression ( k=100) 5.5% 48.2% 8.3% 65.5%
data suppression ( k=2) 1% 15% na na
data suppression ( k=10) 2.7% 39% na na
data suppression ( k=100) 5.2% 75% na na
5.3. results
in this section, we use the anonymised event logs (described in section 5.2) and show how
the anonymisation methods affect the results of process mining approaches that are frequently
used in the healthcare domain (discussed in section 3): process discovery, process conformance
analysis, process performance analysis, organisational mining, and process variant analysis. for each
process mining category, we selected a well-known process mining method implemented as a plugin
of the open source process mining framework prom (http://www.promtools.org/) (version 6.8).
process discovery, process conformance, and performance analysis were conducted for the logs with
anonymised timestamps and activity labels; organisational mining was performed for the logs with
anonymised resources; and process variant analysis was conducted for the logs with anonymised
data attributes.
5.3.1. process discovery
we used the prom plugin “inductive visual miner” [ 32] (the default miner) and discovered
process models from the original and the anonymised event logs. for each log, two process models
were discovered: (1) a model that captures mainstream process behavior (with paths = 0.8, the default
setting of the plugin); and (2) a model that represents all process paths. we then used the projected
ﬁtness and precision measures [ 33] to evaluate the quality of the discovered process models with
respect to the original logs. fitness “expresses the part of the event log that is represented by the
model”, while precision measures the “behaviour in the model that is present in the event log” [33].
figure 3 shows ﬁtness values and figure 4 shows precision values for the process models
discovered from the original and the anonymised logs. we can see that generalisation of all timestamps
did not have any effect on the quality of the discovered models (ﬁtness and precision values of the
process models discovered from the original and the anonymised logs are the same). the process
discovery plugin we applied uses activity sequences as input and does not require timestamps;
therefore, the process models discovered from the logs in which all timestamps were generalised are
identical to the models discovered from the corresponding original logs (as discussed in section 5.2,
the order of events was not changed in these anonymised logs). on the other hand, generalisation
of admission and discharge timestamps in the sepsis log resulted in the changed order of events in
some cases. this affected both ﬁtness and precision values, especially for the process model that
represents mainstream process behaviour. please note that in the evaluation, we use event logs in
which events are ordered (by time); if one uses data sets in which events are not already ordered, the
effect of anonymisation on process mining results can be different.int. j. environ. res. public health 2020 ,17, 1612 15 of 28
the impact of activity suppression for k = 2 and k = 10 is negligible for the sepsis log and the
billing log, which is expected as few events were affected by these anonymisation techniques in these
logs (see table 3). activity suppression for k = 100 affects the quality of process models discovered
from all logs; however, the effect is more pronounced for the bpic11 log (which has many activity
labels and few cases) and is not very signiﬁcant for the billing log (which has few activity labels and
many cases).
figure 3. the impact of activity suppression and time generalisation on the results of process discovery:
ﬁtness values for the three event logs.
figure 4. the impact of activity suppression and time generalisation on the results of process discovery:
precision values for the three event logs.
5.3.2. process conformance analysis
we applied an alignment-based process conformance analysis plugin [ 34], which takes as input
a normative process model (which speciﬁes the expected process behaviour) and an event log, aligns
the log and the model (by relating events in the log to activities in the model), and provides the average
trace ﬁtness value (along with other detailed conformance analysis measures). for each original event
log, we discovered a process model that represents mainstream process behaviour using the “inductive
visual miner” [ 32] process discovery plugin (with default settings). we then used these process models
as input to the conformance analysis plugin.int. j. environ. res. public health 2020 ,17, 1612 16 of 28
figure 5 shows the average trace ﬁtness values for the original and the anonymised event logs.
we can see that for the sepsis log and the billing log, the average trace ﬁtness was not affected by most
anonymisation methods and was only slightly affected by the application of activity suppression with
k = 100. the negligible impact of activity suppression for these two logs can be explained by the fact
that few events were affected by the transformation (see table 3). the average trace ﬁtness was not
affected by generalisation of all timestamps as the order of events in cases was not changed. on the
other hand, generalisation of admission and discharge timestamps in the sepsis log decreased the
average trace ﬁtness value (due to changes in the order of events).
figure 5 also shows that for the bpic11 log, suppressing more infrequent activities improves the
average trace ﬁtness value. the process model represents mainstream process behaviour and does
not include infrequent activities; therefore, the average trace ﬁtness of the original log is lower than
the average trace ﬁtness of the logs with suppressed infrequent activities. please note that in this
experiment, we showed the effect of anonymisation on the average trace ﬁtness; while the average
trace ﬁtness may not be changed, ﬁtness values for individual traces in the anonymised logs can be
different from ﬁtness values of the corresponding traces in the original logs.
figure 5. the impact of activity suppression and time generalisation on the results of process
conformance analysis: the average trace ﬁtness values for the three event logs.
5.3.3. process performance analysis
we used an alignment-based process performance analysis approach [ 34] implemented as prom
plugin, which takes as input an event log and a process model, aligns the model and the log, and
calculates various process performance metrics including the average case throughput time and the
average times between activities. for each original log, we discovered a process model that represents
all process paths, which was used as input to the plugin.
for each anonymised log, we report the absolute percentage difference between the average case
throughput time in the anonymised log and the corresponding original log. the results are depicted
in figure 6. the ﬁgure shows that the average case throughput times for the logs with suppressed
activities are not very different from the average case throughput times in the corresponding original
logs (the difference is less than 1% for all logs). time generalisation has a more signiﬁcant impact on
the average case throughput time, especially for the logs in which a timestamp only carries information
about the year of an event. for the anonymised sepsis log in which only admission and dischargeint. j. environ. res. public health 2020 ,17, 1612 17 of 28
timestamps were generalised, the absolute percentage difference between the average case throughput
times is 645.73% (the value is not shown in figure 6 due to the large difference with other values).
cases in which the order of events was changed by the transformation may not be aligned with the
process model, which affects the process performance analysis results (the plugin only considers cases
that are aligned with the model).
figure 6. the impact of activity suppression and time generalisation on the results of process
performance analysis: the absolute percentage differences between the average case throughput
times in the anonymised logs and the corresponding original logs.
we also evaluated the effect of anonymisation on the average times between activities
(another process performance metric provided by the plugin). we report the absolute percentage
differences between the average times between activities in the original log and the anonymised
logs. the performance measure is not available for the suppressed activities; and for the remaining
activities, the average times between activities are not changed by activity suppression. the effect of
time generalisation on the average times between activities is shown in figure 7. we can see that the
effect of time generalisation on the average times between activities (figure 7) is more pronounced
than the effect on the average case throughput time (figure 6). figure 7 does not show the value for the
anonymised sepsis log in which only admission and discharge timestamps were generalised, which is
379,921%. the huge difference for this log is expected, as generalisation of admission and discharge
timestamps signiﬁcantly changed the times between these two activities and all other activities in
the process.
figure 7. the impact of time generalisation on the results of process performance analysis: the absolute
percentage differences between the average times between activities in the original log and the
anonymised logs.int. j. environ. res. public health 2020 ,17, 1612 18 of 28
5.3.4. organisational mining
we used prom plugin “mine for a handover-of-work social network” [ 35], which takes as input
an event log and discovers a social network. nodes in the social network represent resources and the
weight of an arc between two nodes is determined by the frequency of activity handovers between
these two resources. we discovered social networks from the original logs and from the anonymised
logs (with suppressed resources) and compared the discovered social networks.
social networks discovered from the anonymised logs do not include suppressed resources,
and hence, one can no longer analyse activity handovers for these resources (the percentages of the
suppressed resources in the logs are shown in figure 8a). for the logs in which value suppression
of the resource attribute was applied, the weights of the arcs between the remaining resources were
not changed. this can be explained by the fact that the plugin treats missing resource values as an
additional resource (not_set) and creates a corresponding node in the social network; as a result, the
weights of the arcs between other resources in the social network are not changed.
for the logs in which event suppression (of the resource attribute) was applied, the weights of the
arcs in the discovered social networks were changed. to evaluate the extent of changes, we compared
the weights with the corresponding weights in the social networks discovered from the original logs,
and we report the pearson correlation coefﬁcient values in figure 8b (pearson coefﬁcient was used to
evaluate social network similarity, e.g., in [ 31]). we can see that the effect of event suppression (of the
resource attribute) on the weights of the arcs between the remaining resources is negligible, even when
most resources are suppressed (for k=100).
figure 8. the impact of resource suppression on the results of social network discovery: ( a) the
percentage of suppressed resources, and ( b) similarity between the arc weights in the social networks
discovered from the logs with suppressed events and the corresponding arc weights in the social
networks discovered from the original logs (pearson correlation coefﬁcient).
5.3.5. process variant analysis
process variant analysis often involves the comparison of process behavior and performance
of different process variants. to compare the performance of different process variants in the sepsis
log, we used the alignment-based process performance analysis approach [ 34] (which was used in
section 5.3.3). to compare process behaviour of different process variants in the sepsis log, we used the
alignment-based conformance analysis plugin [34] (which was used in section 5.3.2).
the sepsis log has a data attribute “infectionsuspected” with value “true” for 848 cases and value
“false” for 202 cases. we split the original log and the anonymised logs (with suppressed data values)
based on the value of this attribute, and we refer to cases with value “true” as process variant 1 and to
cases with value “false” as process variant 2. sepsis is a medical condition caused by an infection, and
processing of sepsis cases in which an infection is suspected may be different from processing of cases
in which an infection is unknown.
figure 9a shows the absolute percentage differences between the average case throughput times
in the anonymised logs and the original log (for both process variants). figure 9b shows the absolute
percentage differences between the average times between activities in the original log and the
anonymised logs. we can see that the impact of data suppression on the average case throughputint. j. environ. res. public health 2020 ,17, 1612 19 of 28
time is minimal, while the impact on the average times between activities is signiﬁcant, especially for
process variant 2. as it is shown in table 4, many cases were affected by data suppression (75% for
k=100), which can explain the signiﬁcant impact on the average times between activities.
figure 9. the impact of data suppression on the performance of different process variants: ( a) the
absolute percentage differences between the average case throughput time in the original log and
the anonymised logs, and ( b) the absolute percentage differences between the average times between
activities in the original log and the anonymised logs (for process variant 1 and process variant 2).
figure 10 shows the average trace ﬁtness values for the original log and for the anonymised logs.
we can see that the impact of data suppression on the average trace ﬁtness values is minimal for both
process variants.
figure 10. the impact of data suppression on the average trace ﬁtness values for process variant 1 and
process variant 2.
5.4. discussion
the evaluation results presented in section 5.3 demonstrated that the impact of an anonymisation
method varies for different process mining algorithms and depends on the required privacy level
(e.g., the value of parameter k) and characteristics of the log. for example, we could see that:
 generalisation of all timestamps (which did not change the order of events) did not have any
effect on the results of process discovery and process conformance analysis plugins that take
as input activity sequences (and do not require timestamps); however, it affected the results of
process performance analysis;
 activity suppression, on the other hand, had a minimal effect on the average case throughput
time (as start and end activities occur very frequently, and hence, are not suppressed); however,
affected process discovery and conformance analysis results in some logs;
 activity suppression affected many events in the bpic11 log (which has many activity labels and
few cases) and few events in the other logs (which have few labels and many cases);int. j. environ. res. public health 2020 ,17, 1612 20 of 28
 smaller values of parameter k(used in suppression) had a minimal effect on process mining
results, while larger values affected the results of some algorithms for some logs (e.g., the results
of process conformance analysis for the bpic11 log, figure 5).
recording the history of privacy-preserving transformations could help to interpret and improve
the accuracy of process mining results. for example, if one knows that timestamps were generalised
without changing the order of events, then one can trust the results of process discovery algorithms
that take as input activity sequences. if a log contains information about events affected by a given
privacy-preserving method, then process mining algorithms could use this information to quantify
the impact of the method (e.g., by highlighting parts of the model that are less trustworthy). we
present a privacy-preserving process mining framework which uses the history of privacy-preserving
transformations recorded in privacy metadata in section 6 and we discuss the proposed privacy
metadata for event logs in section 7.
a limitation of the evaluation presented in this section is the application of versions of some
process mining techniques (the conformance analysis plugin [ 34] and the projected ﬁtness and precision
measures [ 33]) that are optimised for large data sets at the expense of accuracy (due to performance
issues of methods that can guarantee the accuracy of results). in the evaluation, we showed the impact
of generalisation and suppression (which were selected based on the results of the analysis presented
in section 4) on the results of selected process mining algorithms (which are frequently used in the
healthcare domain) for three publicly available hospital logs with different characteristics. further
evaluation could be conducted for other anonymisation techniques and process mining algorithms
using event logs originating from different healthcare processes. in this section, we focused on the
impact of one anonymisation method applied to one event log attribute. a direction for future work
is an evaluation of the impact of anonymisation of multiple attributes and different combinations of
privacy-preserving methods.
6. privacy-preserving process mining framework
on the one hand, the healthcare sector needs to comply with strict data privacy requirements. on
the other hand, healthcare process data often contains many sensitive attributes and highly variable
process behaviour that presents additional threats to privacy. ensuring high levels of privacy protection
for such data while also preserving data utility for process mining purposes remains an open challenge
for the healthcare domain. the analysis of the suitability of existing data transformation approaches
to anonymise healthcare process data (presented in section 4) highlighted the trade-off between
data privacy and utility. the methods that preserve higher data utility for process mining purposes
(e.g., encryption) do not provide strong privacy protection. on the other hand, the methods that
can satisfy stricter privacy requirements (e.g., value suppression and generalisation) can decrease
the accuracy of results. the magnitude of the data utility loss depends on the characteristics of
a particular log and varies for different process mining algorithms as demonstrated in section 5.
furthermore, performing analyses on anonymised process data without understanding how the data
was transformed can yield unpredictable results.
we propose a privacy-preserving process mining framework (figure 11) which uses a history of
privacy-preserving data transformations to quantify their impact and improve the accuracy of process
mining results. the framework can be applied to the healthcare domain as well as other domains with
high privacy needs. the ﬁrst two steps of the framework (i.e., data anonymisation and creation of
privacy metadata) are performed by the data owner or a trusted party. the third step (i.e., conducting
privacy-preserving process mining analysis) can be performed by (not trusted) third parties.
the ﬁrst step of the framework is anonymising sensitive information such as sensitive attribute
values and atypical process behavior. anonymisation of sensitive attribute values could be achieved
using the data transformation approaches discussed in section 4.1. some atypical process behaviours
can be anonymised using approaches discussed in section 4.2; however, methods which couldint. j. environ. res. public health 2020 ,17, 1612 21 of 28
anonymise different types of atypical process behaviour in highly variable processes while preserving
data utility for different algorithms are yet to be developed.
figure 11. privacy-preserving process mining framework.
the second step of the framework is creating privacy metadata , which maintains the history of
privacy-preserving data transformations in a standardised and machine-readable way. the privacy
metadata could be used to record information about (1) types of data transformations used
(e.g., encryption or generalisation); (2) parts of the log affected by these transformations (e.g., certain
attributes or events); (3) the magnitude of the transformations (e.g., by specifying the percentage
of affected events); and (4) the reasoning behind the anonymisation used (e.g., an explicit link
between legislation and anonymisation actions to show compliance). such metadata can be stored
in an extension to the ieee xes log format used for process mining and in section 7 we describe the
proposed privacy extension. please note that one can apply several anonymisation methods, and
hence, the ﬁrst two steps of the framework can be repeated.
the third step of the framework is conducting privacy-preserving process mining analysis of
the anonymised event log with privacy metadata. the privacy metadata can be exploited by new
“privacy-aware” process mining techniques to improve mining results. privacy-aware process mining
methods could also quantify data privacy and utility (e.g., by providing conﬁdence measures).
the results of process mining techniques could also threaten privacy (by identifying patterns which
are linked to individuals). for example, a mined social network could show a much higher number of
handovers between two employees, which could reveal their identities to someone with knowledge of
the process. there may be a need to further protect process mining outputs (e.g., access control) even
after the data itself is anonymised. to the best of our knowledge, anonymisation methods for process
mining outputs are yet to be developed. we invite the process mining community to develop novel
privacy-aware mining and visualisation techniques by leveraging the proposed privacy metadata.
we presented a general framework that uses privacy metadata to support privacy-preserving
process analyses of healthcare process data. the framework supports the selection of anonymisation
methods suitable in a given scenario (and does not prescribe the use of speciﬁc methods), as different
healthcare organisations may have different data privacy and utility requirements which depend
on the applicable legislation and the types of analysis one is interested in. the development of tool
support for the proposed privacy-preserving process mining framework is a direction for future work.
the tool will enable the selection and the application of anonymisation methods and the creation of
corresponding privacy metadata (these steps are performed by a trusted party) and the application
of privacy-preserving process mining algorithms (this step can be performed by third parties). data
governance issues (such as where how and for how long the data is stored, and who can have access
to the data during different stages of its lifecycle) are outside the scope of this work.int. j. environ. res. public health 2020 ,17, 1612 22 of 28
7. privacy metadata
in this section, we describe the proposed privacy extension to the ieee xes log format (the
extension deﬁnition in xml format is provided in appendix b). the privacy extension contains
information about privacy-preserving transformations performed on the log. we propose the
privacy attributes speciﬁed in table 5 and use the preﬁx “privacy” for the attributes. a list of
(privacy-preserving) transformations can be associated with the log, a trace or an event. each
transformation is a container with information stored in the following attributes:
 id: the identiﬁer of the anonymisation operation. for example, if one applies activity suppression
to the log followed by resource generalisation, then all transformations recorded for the
activity suppression will have one id (e.g., “1”), and transformations recorded for the resource
generalisation will have a different id (e.g., “2”).
 level : the attribute is applicable on the log and the trace level and takes one of the two values:
“event” (which indicates that the transformation was applied to event attributes) or “trace” (which
indicates that the transformation was applied to trace attributes).
 method : the applied anonymisation method. possible values include (but are not limited to):
suppression, generalisation, micro-aggregation, swapping, noise addition, encryption.
 type : the attribute takes one of the three values: “update”, “delete”, or “insert”. value
“update” is used if the anonymisation method modiﬁes an attribute value (e.g., by adding noise
or generalising). value “delete” is used if the anonymisation method removes an attribute
value, an event, or a trace. value “insert” is used if the anonymisation method adds a new
event or a trace.
 attributes : a list of attributes affected by the transformation.
 attribute : an attribute affected by the transformation. value “all” indicates that all attributes
were affected by the transformation.
 impact : the attribute is applicable on the log and on the trace level and speciﬁes the number of
traces or events (deﬁned by attribute “level”) affected by the transformation.
 description : a list of properties which contain additional information about the transformation.
 property : a property with additional information about the transformation. for example, it may
be used to specify more details about the anonymisation method (e.g., encryption type), a privacy
risk targeted by the transformation (e.g., attribute disclosure) or privacy legislation.
below, we provide examples of privacy metadata recorded for a log (listing 1), and for a trace
(listing 2) and an event (listing 3) in the log, using the xes log format.
listing 1 shows two transformations applied to the log. in transformation 1(i.e., the
transformation with the value of the “privacy:id” attribute equal to “1”), values of event attribute
“diagnosis” were suppressed in 230 events in the log and additional information provided in the
“privacy:description” list speciﬁes that the transformation targeted the attribute disclosure risk. in the
second transformation, case identiﬁers (trace attribute “concept:name”) were swapped in 1000 traces
in the log.
listing 2 shows two transformations applied to a trace. in transformation 3, event attribute
“org:resource” was generalised in 5 events in the trace. in transformation 4, 2 events in the trace
were suppressed. please note that if an event is removed by an anonymisation approach, then the
corresponding privacy metadata can be recorded on the trace or on the log level.
listing 3 provides information about two transformations applied to an event. in transformation 1,
suppression was applied to attribute “diagnosis”. in transformation 5, noise addition was applied to
attribute “time:timestamp”. please note that the ﬁrst transformation applied to this event has the same
identiﬁer (1) as the ﬁrst transformation recorded for the log (listing 1) which means that information
about diagnosis suppression was recorded on the log and on the event level.int. j. environ. res. public health 2020 ,17, 1612 23 of 28
listing 1. an example of log privacy attributes.
int. j. environ. res. public health 2020 ,17, 1612 23 of 28
second transformation, case identiﬁers (trace attribute “concept:name”) were swapped in 1000 traces
in the log.
1<log >
2 <list key =" privacy:transformations ">
3 <container key =" privacy:transformation ">
4 <int key=" privacy:id " value ="1"/>
5 <string key =" privacy:level " value =" event "/>
6 <string key =" privacy:method " value =" suppression "/>
7 <string key =" privacy:type " value =" delete "/>
8 <list key =" privacy:attributes ">
9 <string key =" privacy:attribute " value =" diagnosis "/>
10 </ list >
11 <int key=" privacy:impact " value =" 230"/>
12 <list key =" privacy:description ">
13 <string key =" privacy:property " value =" attribute disclosure risk "/>
14 </ list >
15 </ container >
16 <container key =" privacy:transformation ">
17 <int key=" privacy:id " value ="2"/>
18 <string key =" privacy:level " value =" trace "/>
19 <string key =" privacy:method " value =" swapping "/>
20 <string key =" privacy:type " value =" update "/>
21 <list key =" privacy:attributes ">
22 <string key =" privacy:attribute " value =" concept:name "/>
23 </ list >
24 <int key=" privacy:impact " value =" 1000 "/>
25 </ container >
26 </ list >
27</log >
listing 1: an example of log privacy attributes.
table 5. proposed privacy attributes.
level key type description
log, trace, event transformations list a list of applied privacy-preserving transformations.
meta transformation containera container attribute which contains information about
each transformation stored in attributes shown below.
meta id int the identiﬁer of the anonymisation operation.
meta level stringpossible values: “trace” or “event” (applicable on the log
and on the trace level).
meta method stringthe applied anonymisation method (e.g., suppression,
generalisation, swapping, noise addition, encryption).
meta type string possible values: “delete”, “update”, or "insert".
meta attributes list a list of affected attributes.
meta attribute stringan affected attribute; value ‘all’ if the transformation
affected all attributes.
meta impact intthe number of affected items (i.e., traces or events);
applicable on the log and on the trace level.
meta description list a list of additional properties of the transformation.
meta property stringan additional property of the transformation; for example,
encryption type or privacy risk (e.g., identity disclosure).
listing 2 shows two transformations applied to a trace. in transformation 3, event attribute
“org:resource” was generalised in 5 events in the trace. in transformation 4, 2 events in the trace
table 5. proposed privacy attributes.
level key type description
log, trace, event transformations list a list of applied privacy-preserving transformations.
meta transformation containera container attribute which contains information about
each transformation stored in attributes shown below.
meta id int the identiﬁer of the anonymisation operation.
meta level stringpossible values: “trace” or “event” (applicable on the log
and on the trace level).
meta method stringthe applied anonymisation method (e.g., suppression,
generalisation, swapping, noise addition, encryption).
meta type string possible values: “delete”, “update”, or "insert".
meta attributes list a list of affected attributes.
meta attribute stringan affected attribute; value ‘all’ if the transformation
affected all attributes.
meta impact intthe number of affected items (i.e., traces or events);
applicable on the log and on the trace level.
meta description list a list of additional properties of the transformation.
meta property stringan additional property of the transformation; for example,
encryption type or privacy risk (e.g., identity disclosure).int. j. environ. res. public health 2020 ,17, 1612 24 of 28
listing 2. an example of trace privacy attributes.
int. j. environ. res. public health 2020 ,17, 1612 24 of 28
were suppressed. please note that if an event is removed by an anonymisation approach, then the
corresponding privacy metadata can be recorded on the trace or on the log level.
1<trace >
2 <list key =" privacy:transformations ">
3 <container key =" privacy:transformation ">
4 <int key=" privacy:id " value ="3"/>
5 <string key =" privacy:level " value =" event "/>
6 <string key =" privacy:method " value =" generalisation "/>
7 <string key =" privacy:type " value =" update "/>
8 <list key =" privacy:attributes ">
9 <string key =" privacy:attribute " value =" org:resource "/>
10 </ list >
11 <int key=" privacy:impact " value ="5"/>
12 </ container >
13 <container key =" privacy:transformation ">
14 <int key=" privacy:id " value ="4"/>
15 <string key =" privacy:level " value =" event "/>
16 <string key =" privacy:method " value =" suppression "/>
17 <string key =" privacy:type " value =" delete "/>
18 <list key =" privacy:attributes ">
19 <string key =" privacy:attribute " value =" all "/>
20 </ list >
21 <int key=" privacy:impact " value ="2"/>
22 </ container >
23 </ list >
24</ trace >
listing 2: an example of trace privacy attributes.
listing 3 provides information about two transformations applied to an event. in transformation 1,
suppression was applied to attribute “diagnosis”. in transformation 5, noise addition was applied to
attribute “time:timestamp”. please note that the ﬁrst transformation applied to this event has the same
identiﬁer (1) as the ﬁrst transformation recorded for the log (listing 1) which means that information
about diagnosis suppression was recorded on the log and on the event level.
1<event >
2 <list key =" privacy:transformations ">
3 <container key =" privacy:transformation ">
4 <int key=" privacy:id " value ="1"/>
5 <string key =" privacy:method " value =" suppression "/>
6 <string key =" privacy:type " value =" delete "/>
7 <list key =" privacy:attributes ">
8 <string key =" privacy:attribute " value =" diagnosis "/>
9 </ list >
10 <list key =" privacy:description ">
11 <string key =" privacy:property " value =" attribute disclosure risk "/>
12 </ list >
13 </ container >
14 <container key =" privacy:transformation ">
15 <int key=" privacy:id " value ="5"/>
16 <string key =" privacy:method " value =" noise addition "/>
17 <string key =" privacy:type " value =" update "/>
18 <list key =" privacy:attributes "/>
19 <string key =" privacy:attribute " value =" time:timestamp "/>
20 </ list >
21 </ container >
22 </ list >
23</ event >
listing 3: an example of event privacy attributes.
a more detailed history of data transformations can help improve the accuracy of process mining
results; however, it may also introduce an avenue for privacy breaches if the metadata can be used to
listing 3. an example of event privacy attributes.
int. j. environ. res. public health 2020 ,17, 1612 24 of 28
were suppressed. please note that if an event is removed by an anonymisation approach, then the
corresponding privacy metadata can be recorded on the trace or on the log level.
1<trace >
2 <list key =" privacy:transformations ">
3 <container key =" privacy:transformation ">
4 <int key=" privacy:id " value ="3"/>
5 <string key =" privacy:level " value =" event "/>
6 <string key =" privacy:method " value =" generalisation "/>
7 <string key =" privacy:type " value =" update "/>
8 <list key =" privacy:attributes ">
9 <string key =" privacy:attribute " value =" org:resource "/>
10 </ list >
11 <int key=" privacy:impact " value ="5"/>
12 </ container >
13 <container key =" privacy:transformation ">
14 <int key=" privacy:id " value ="4"/>
15 <string key =" privacy:level " value =" event "/>
16 <string key =" privacy:method " value =" suppression "/>
17 <string key =" privacy:type " value =" delete "/>
18 <list key =" privacy:attributes ">
19 <string key =" privacy:attribute " value =" all "/>
20 </ list >
21 <int key=" privacy:impact " value ="2"/>
22 </ container >
23 </ list >
24</ trace >
listing 2: an example of trace privacy attributes.
listing 3 provides information about two transformations applied to an event. in transformation 1,
suppression was applied to attribute “diagnosis”. in transformation 5, noise addition was applied to
attribute “time:timestamp”. please note that the ﬁrst transformation applied to this event has the same
identiﬁer (1) as the ﬁrst transformation recorded for the log (listing 1) which means that information
about diagnosis suppression was recorded on the log and on the event level.
1<event >
2 <list key =" privacy:transformations ">
3 <container key =" privacy:transformation ">
4 <int key=" privacy:id " value ="1"/>
5 <string key =" privacy:method " value =" suppression "/>
6 <string key =" privacy:type " value =" delete "/>
7 <list key =" privacy:attributes ">
8 <string key =" privacy:attribute " value =" diagnosis "/>
9 </ list >
10 <list key =" privacy:description ">
11 <string key =" privacy:property " value =" attribute disclosure risk "/>
12 </ list >
13 </ container >
14 <container key =" privacy:transformation ">
15 <int key=" privacy:id " value ="5"/>
16 <string key =" privacy:method " value =" noise addition "/>
17 <string key =" privacy:type " value =" update "/>
18 <list key =" privacy:attributes "/>
19 <string key =" privacy:attribute " value =" time:timestamp "/>
20 </ list >
21 </ container >
22 </ list >
23</ event >
listing 3: an example of event privacy attributes.
a more detailed history of data transformations can help improve the accuracy of process mining
results; however, it may also introduce an avenue for privacy breaches if the metadata can be used to
a more detailed history of data transformations can help improve the accuracy of process mining
results; however, it may also introduce an avenue for privacy breaches if the metadata can be used to
rediscover the original data. data privacy and utility requirements may vary for different healthcare
organisations as they may be subjects to different data privacy regulations. moreover, data analysis
can be performed inside of an organisation, it can be outsourced to a third party, or healthcare data
sets can be released for public use – data privacy requirements will be different in these scenarios. to
cater for different data privacy and utility needs, the proposed privacy extension allows recording
information about privacy-preserving transformations with different levels of detail. for example,int. j. environ. res. public health 2020 ,17, 1612 25 of 28
one can specify all events that were affected by an anonymisation method or one can specify that the
anonymisation affected a given number of events in the log. such ﬂexibility allows organisations to
select a required level of balance of data privacy and utility.
in this section, we presented the ﬁrst proposal of the privacy extension; we will invite the
process mining community to provide feedback and discuss possible enhancements of the proposed
extension. a direction for future work is the development of a tool which could support different
(existing) log anonymisation methods and annotate the anonymised log with the privacy metadata.
such privacy metadata could be used by process mining algorithms to reason about the extent of
changes in the anonymised log, to quantify the impact of privacy-preserving transformations and
improve the accuracy of process mining results. for example, a process discovery algorithm could
provide a conﬁdence measure for the discovered process model by using information about the
number of events in the log which were affected by activity suppression; or a process conformance
analysis algorithm could highlight traces affected by anonymisation (and hence, less trustworthy).
the development of such privacy-aware process mining algorithms is another direction for future work.
in the presented privacy extension, we focused on recording the history of changes in a log caused
by privacy-preserving methods (e.g., generalisation of certain attribute values or suppression of certain
events) while preserving data privacy. it is also possible to capture in metadata log characteristics that
were not changed by anonymisation. for example, one could record attribute values that were not
modiﬁed (e.g., certain activity labels) or a list of traces in which the order of events was not altered.
recording such information in a naive way could lead to privacy breaches; for example, if one records
a list of attribute values that were not changed by a given privacy-preserving transformation and
keeps the history of all privacy-preserving transformations, it may be possible to identify some private
attribute values from such metadata. an investigation of privacy-preserving ways to capture unaltered
log characteristics is another direction for future work.
8. conclusions
keeping healthcare process data private while preserving data utility for process mining presents
a challenge for the healthcare domain. until recently, the process mining community did not pay much
attention to data privacy issues, while several privacy-preserving data transformation techniques were
proposed in the data mining community. however, some of these techniques are not suitable for process
data. in this article, we analysed data privacy and utility requirements for healthcare process data and
assessed the suitability of existing privacy-preserving data transformation approaches to anonymise
such data. we evaluated the effect of some of these anonymisation methods on various process mining
results using three publicly available healthcare event logs. the experiments demonstrated that the
impact of anonymisation methods varies for different process mining algorithms and depends on the
characteristics of a particular log. we proposed a privacy-preserving process mining framework which
uses privacy metadata and can support process mining analyses of healthcare processes. finally, we
proposed privacy metadata which records the history of privacy-preserving transformations performed
on a log. a direction for future work is the development of tool support for the proposed privacy
metadata and the development of privacy-aware process mining algorithms that could leverage the
privacy metadata.
supplementary materials: the following are available online at http://www.mdpi.com/1660-4601/17/5/1612/
s1.
author contributions: conceptualization: a.p ., m.t.w., a.h.m.t.h., w.m.p .v.d.a., and h.a.r.; methodology:
a.p ., m.t.w., a.h.m.t.h., w.m.p .v.d.a., and h.a.r.; validation: a.p ., m.t.w., and s.b.; formal analysis: a.p . and
m.t.w.; investigation: a.p ., m.t.w., and s.b.; data curation: a.p . and s.b.; writing—original draft preparation:
a.p ., m.t.w., and s.b.; writing—review & editing: a.p ., m.t.w., a.h.m.t.h., w.m.p .v.d.a., and h.a.r. all authors
have read and agreed to the published version of the manuscript.
funding: this research received no external funding.
conﬂicts of interest: the authors declare no conﬂict of interest.int. j. environ. res. public health 2020 ,17, 1612 26 of 28
appendix a. anonymisation code and anonymised logs
the anonymised event logs described in section 5.2 and the code used to create the anonymised
logs are uploaded as supplementary materials to the article:
 the anonymisation code (which is also available online (https://bitbucket.org/sbudiono/
transformation-tools)) can generalise timestamps with a given level of granularity (e.g., a year
or a month) and suppress infrequent values of a given attribute for a given value of k(code
parameters and examples of usage are described in the ‘readme’ ﬁle (https://bitbucket.org/
sbudiono/transformation-tools/src/master/readme.md)). values of the parameters used in
the experiments are described in section 5.2.
 all anonymised logs described in tables 3 and 4 are uploaded as supplementary materials.
appendix b. privacy extension deﬁnition
listing a1 provides a deﬁnition of the proposed privacy attributes in xml format, the attributes
are discussed in detail in section 7. listing a1 speciﬁes the following types of the privacy attributes
(standard xes attribute types): listattributes ( transformations, attributes, description ), acontainer attribute
(transformation ),intattributes ( id, impact ), and string attributes ( level, method, type, attribute, property ).
listing a1. the proposed privacy extension in xml format.
int. j. environ. res. public health 2020 ,17, 1612 26 of 28
a.p ., m.t.w., and s.b.; writing—review & editing: a.p ., m.t.w., a.h.m.t.h., w.m.p .v.d.a., and h.a.r. all authors
have read and agreed to the published version of the manuscript.
funding: this research received no external funding.
conﬂicts of interest: the authors declare no conﬂict of interest.
appendix a. supplementary materials: anonymisation code and anonymised logs
the anonymised event logs described in section 5.2 and the code used to create the anonymised
logs are uploaded as supplementary materials to the article:
• the anonymisation code (which is also available online (https://bitbucket.org/sbudiono/
transformation-tools)) can generalise timestamps with a given level of granularity (e.g., a year
or a month) and suppress infrequent values of a given attribute for a given value of k(code
parameters and examples of usage are described in the ‘readme’ ﬁle (https://bitbucket.org/
sbudiono/transformation-tools/src/master/readme.md)). values of the parameters used in
the experiments are described in section 5.2.
• all anonymised logs described in tables 3 and 4 are uploaded as supplementary materials.
appendix b. privacy extension deﬁnition
listing 4 provides a deﬁnition of the proposed privacy attributes in xml format, the attributes
are discussed in detail in section 7. listing 4 speciﬁes the following types of the privacy attributes
(standard xes attribute types): listattributes ( transformations, attributes, description ), acontainer attribute
(transformation ),intattributes ( id, impact ), and string attributes ( level, method, type, attribute, property ).
1< xesextension name =" privacy " prefix =" privacy ">
2 <log >
3 <list key =" transformations ">
4 <alias mapping ="en" name =" privacy - preserving transformations "/>
5 </ list >
6 </log >
7 <trace >
8 <list key =" transformations ">
9 <alias mapping ="en" name =" privacy - preserving transformations "/>
10 </ list >
11 </ trace >
12 <event >
13 <list key =" transformations ">
14 <alias mapping ="en" name =" privacy - preserving transformations "/>
15 </ list >
16 </ event >
17 <meta >
18 <container key =" transformation ">
19 <alias mapping ="en" name =" privacy - preserving transformation "/>
20 </ container >
21 <int key="id">
22 <alias mapping ="en" name =" anonymisation operation identifier "/>
23 </int >
24 <string key =" level ">
25 <alias mapping ="en" name =" level "/>
26 </ string >
27 <string key =" method ">
28 <alias mapping ="en" name =" anonymisation method "/>
29 </ string >
30 <string key =" type ">
31 <alias mapping ="en" name =" transformation type "/>
32 </ string >
33 <list key =" attributes ">
34 <alias mapping ="en" name =" affected attributes "/>
35 </ list >int. j. environ. res. public health 2020 ,17, 1612 27 of 28
int. j. environ. res. public health 2020 ,17, 1612 27 of 28
36 <string key =" attribute ">
37 <alias mapping ="en" name =" affected attribute "/>
38 </ string >
39 <int key=" impact ">
40 <alias mapping ="en" name =" transformation impact "/>
41 </int >
42 <list key =" description ">
43 <alias mapping ="en" name =" description "/>
44 </ list >
45 <string key =" property ">
46 <alias mapping ="en" name =" additional property of the transformation "/>
47 </ string >
48 </ meta >
49</ xesextension >
listing 4: the proposed privacy extension in xml format.
references
1. van der aalst, w.m.p . process mining: data science in action ; springer: berlin, germany, 2016.
2. andrews, r.; suriadi, s.; wynn, m.t.; ter hofstede, a.h.m. healthcare process analysis. in process modelling
and management for healthcare ; crc press: boca raton, fl, usa, 2017.
3. erdogan, t.g.; tarhan, a. systematic mapping of process mining studies in healthcare. ieee access 2018 ,
6, 24543–24567.
4. mans, r.s.; van der aalst, w.m.p .; vanwersch, r.j. process mining in healthcare: evaluating and exploiting
operational healthcare processes ; springer: cham, switzerland, 2015.
5. partington, a.; wynn, m.t.; suriadi, s.; ouyang, c.; karnon, j. process mining for clinical processes:
a comparative analysis of four australian hospitals. acm (tmis) 2015 ,5, 19.
6. rojas, e.; sepúlveda, m.; munoz-gama, j.; capurro, d.; traver, v .; fernandez-llatas, c. question-driven
methodology for analyzing emergency room processes using process mining. appl. sci. 2017 ,7, 302.
7. van der aalst, w.m.p .; adriansyah, a.; de medeiros, a.k.a.; arcieri, f.; baier, t.; blickle, t.; bose, j.c.; van
den brand, p .; brandtjen, r.; buijs, j.; et al. process mining manifesto. in bpm 2011 workshops proceedings ;
springer: berlin, germany, 2011.
8. mannhardt, f.; petersen, s.a.; oliveira, m.f. privacy challenges for process mining in human-centered
industrial environments. in proceedings of the 14th international conference on intelligent environments
(ie), rome, italy, 25–28 june 2018; pp. 64–71.
9. burattin, a.; conti, m.; turato, d. toward an anonymous process mining. in proceedings of the ficloud
2015, rome, italy, 24–26 august 2015; pp. 58–63.
10. fahrenkrog-petersen, s.a.; van der aa, h.; weidlich, m. pretsa: event log sanitization for privacy-aware
process discovery. in proceedings of the 2019 international conference on process mining (icpm),
aachen, germany, 24–26 june 2019. doi:10.1109/icpm.2019.00012.
11. liu, c.; duan, h.; qingtian, z.; zhou, m.; lu, f.; cheng, j. towards comprehensive support for privacy
preservation cross-organization business process mining. ieee trans. serv. comput. 2019 ,12, 639–653.
12. raﬁei, m.; von waldthausen, l.; van der aalst, w. ensuring conﬁdentiality in process mining. in
proceedings of the simpda 2018, seville, spain, 13–14 december 2018.
13. aggarwal, c.c. data mining: the textbook ; springer: cham, switzerland, 2015.
14. toshniwal, d. privacy preserving data mining techniques for hiding sensitive data: a step towards open
data. in data science landscape ; springer: singapore, 2018; pp. 205–212.
15. pika, a.; wynn, m.t.; budiono, s.; ter hofstede, a.h.m.; van der aalst, w.m.p .; reijers, h.a. towards
privacy-preserving process mining in healthcare. in business process management workshops, proceedings of
the international workshop on process-oriented data science for healthcare vienna, austria, 1–6 september 2019 ;
springer: cham, switzerland, 2019; lnbip 362, pp. 483–495.
16. fienberg, s.e.; mcintyre, j. data swapping: variations on a theme by dalenius and reiss. in international
workshop on psd ; springer: berlin/heidelberg, germany, 2004; pp. 14–29.
references
1. van der aalst, w.m.p . process mining: data science in action ; springer: berlin, germany, 2016.
2. andrews, r.; suriadi, s.; wynn, m.t.; ter hofstede, a.h.m. healthcare process analysis. in process modelling
and management for healthcare ; crc press: boca raton, fl, usa, 2017.
3. erdogan, t.g.; tarhan, a. systematic mapping of process mining studies in healthcare. ieee access 2018 ,
6, 24543–24567. [crossref]
4. mans, r.s.; van der aalst, w.m.p .; vanwersch, r.j. process mining in healthcare: evaluating and exploiting
operational healthcare processes ; springer: cham, switzerland, 2015.
5. partington, a.; wynn, m.t.; suriadi, s.; ouyang, c.; karnon, j. process mining for clinical processes:
a comparative analysis of four australian hospitals. acm (tmis) 2015 ,5, 19. [crossref]
6. rojas, e.; sepúlveda, m.; munoz-gama, j.; capurro, d.; traver, v .; fernandez-llatas, c. question-driven
methodology for analyzing emergency room processes using process mining. appl. sci. 2017 ,7, 302.
[crossref]
7. van der aalst, w.m.p .; adriansyah, a.; de medeiros, a.k.a.; arcieri, f.; baier, t.; blickle, t.; bose, j.c.; van
den brand, p .; brandtjen, r.; buijs, j.; et al. process mining manifesto. in bpm 2011 workshops proceedings ;
springer: berlin, germany, 2011.
8. mannhardt, f.; petersen, s.a.; oliveira, m.f. privacy challenges for process mining in human-centered
industrial environments. in proceedings of the 14th international conference on intelligent environments
(ie), rome, italy, 25–28 june 2018; pp. 64–71.
9. burattin, a.; conti, m.; turato, d. toward an anonymous process mining. in proceedings of the ficloud
2015, rome, italy, 24–26 august 2015; pp. 58–63.
10. fahrenkrog-petersen, s.a.; van der aa, h.; weidlich, m. pretsa: event log sanitization for privacy-aware
process discovery. in proceedings of the 2019 international conference on process mining (icpm),
aachen, germany, 24–26 june 2019. doi:10.1109/icpm.2019.00012. [crossref]
11. liu, c.; duan, h.; qingtian, z.; zhou, m.; lu, f.; cheng, j. towards comprehensive support for privacy
preservation cross-organization business process mining. ieee trans. serv. comput. 2019 ,12, 639–653.
[crossref]
12. raﬁei, m.; von waldthausen, l.; van der aalst, w. ensuring conﬁdentiality in process mining. in
proceedings of the simpda 2018, seville, spain, 13–14 december 2018.
13. aggarwal, c.c. data mining: the textbook ; springer: cham, switzerland, 2015.
14. toshniwal, d. privacy preserving data mining techniques for hiding sensitive data: a step towards open
data. in data science landscape ; springer: singapore, 2018; pp. 205–212.
15. pika, a.; wynn, m.t.; budiono, s.; ter hofstede, a.h.m.; van der aalst, w.m.p .; reijers, h.a. towards
privacy-preserving process mining in healthcare. in business process management workshops, proceedings of
the international workshop on process-oriented data science for healthcare vienna, austria, 1–6 september 2019 ;
springer: cham, switzerland, 2019; lnbip 362, pp. 483–495.
16. fienberg, s.e.; mcintyre, j. data swapping: variations on a theme by dalenius and reiss. in international
workshop on psd ; springer: berlin/heidelberg, germany, 2004; pp. 14–29.int. j. environ. res. public health 2020 ,17, 1612 28 of 28
17. domingo-ferrer, j.; torra, v . a critique of k-anonymity and some of its enhancements. in proceedings
of the 2008 third international conference on availability, reliability and security, barcelona, spain,
4–7 march 2008; pp. 990–993.
18. templ, m. statistical disclosure control for microdata ; springer: cham, switzerland, 2017.
19. sánchez, d.; batet, m. toward sensitive document release with privacy guarantees. eng. appl. artif. intell.
2017 ,59, 23–34. [crossref]
20. aggarwal, c.c.; philip, s.y. privacy-preserving data mining: models and algorithms ; springer science &
business media: new york, ny, usa, 2008.
21. zhang, q.; yang, l.t.; castiglione, a.; chen, z.; li, p . secure weighted possibilistic c-means algorithm on
cloud for clustering big data. inf. sci. 2019 ,479, 515–525. [crossref]
22. giggins, h.; brankovic, l. vicus: a noise addition technique for categorical data. in proceedings of the
tenth australasian data mining conference, sydney, australia, 5–7 december 2012; volume 134, pp. 139–148.
23. domingo-ferrer, j.; mateo-sanz, j.m. practical data-oriented microaggregation for statistical disclosure
control. ieee trans. knowl. data eng. 2002 ,14, 189–201. [crossref]
24. abidi, b.; yahia, s.b.; perera, c. hybrid microaggregation for privacy preserving data mining. j. ambient
intell. hum. comput. 2020 ,11, 23–38. [crossref]
25. dwork, c. differential privacy: a survey of results. in theory and applications of models of computation ;
springer: berlin/heidelberg, germany, 2008; pp. 1–19.
26. dwork, c.; smith, a. differential privacy for statistics: what we know and what we want to learn. j. privacy
conﬁd. 2010 ,1. [crossref]
27. tillem, g.; erkin, z.; lagendijk, r.l. privacy-preserving alpha algorithm for software analysis. in
proceedings of the 37th wic symposium on information theory in the benelux/6th wic/ieee sp
symposium on information theory and signal processing in the benelux, benelux, 19–20 may 2016.
28. tillem, g.; erkin, z.; lagendijk, r.l. mining sequential patterns from outsourced data via encryption
switching. in proceedings of the 16th annual conference on privacy, security and trust (pst), belfast, uk ,
28–30 august 2018; pp. 1–10.
29. michael, j.; koschmider, a.; mannhardt, f.; baracaldo, n.; rumpe, b. user-centered and privacy-driven
process mining system design for iot. in proceedings of the international conference on advanced
information systems engineering, rome, italy, 3–7 june 2019; pp. 194–206.
30. mannhardt, f.; koschmider, a.; baracaldo, n.; weidlich, m.; michael, j. privacy-preserving process mining:
differential privacy for event logs. informatik spektrum 2019 ,42, 349–351. [crossref]
31. raﬁei, m.; van der aalst, w.m. mining roles from event logs while preserving privacy. in proceedings
of the international conference on business process management, vienna, austria, 1–6 september 2019;
pp. 676–689.
32. leemans, s.j.; fahland, d.; van der aalst, w.m.p . process and deviation exploration with inductive visual
miner. bpm (demos) 2014 ,1295 , 8.
33. leemans, s.j.; fahland, d.; van der aalst, w.m.p . scalable process discovery and conformance checking.
softw. syst. model. 2018 ,17, 599–631. [crossref] [pubmed]
34. van der aalst, w.m.p .; adriansyah, a.; van dongen, b. replaying history on process models for conformance
checking and performance analysis. wiley interdiscip. rev. data min. knowl. discov. 2012 ,2, 182–192.
[crossref]
35. song, m.; van der aalst, w.m.p . towards comprehensive support for organizational mining. decis. support
syst. 2008 ,46, 300–317. [crossref]
c2020 by the authors. licensee mdpi, basel, switzerland. this article is an open access
article distributed under the terms and conditions of the creative commons attribution
(cc by) license (http://creativecommons.org/licenses/by/4.0/).