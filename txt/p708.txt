chapter 7
model-driven design-space exploration for
software-intensive embedded systems
twan basten, martijn hendriks, nikola tr ˇcka, lou somers, marc geilen, yang
yang, georgeta igna, sebastian de smet, marc v oorhoeve†, wil van der aalst,
henk corporaal, and frits vaandrager
twan basten
embedded system institute, p.o. box 513, 5600 mb eindhoven, the netherlands &
eindhoven university of technology, faculty of electrical engineering, electronic systems group,
p.o. box 513, 5600 mb eindhoven, the netherlands
e-mail: a.a.basten@tue.nl
martijn hendriks
embedded systems institute, p.o. box 513, 5600 mb eindhoven, the netherlands
e-mail: martijn.hendriks@esi.nl
nikola tr ˇcka
united technologies research center, 411 silver lane, east hartford, ct 06108, united states
nikola tr ˇcka was employed at eindhoven university of technology when this work was done.
lou somers
oc´e-technologies b.v ., p.o. box 101, 5900 ma venlo, the netherlands &
eindhoven university of technology, faculty of mathematics and computer science, software
engineering and technology group, p.o. box 513, 5600 mb eindhoven, the netherlands
e-mail: lou.somers@oce.com
marc geilenyang yanghenk corporaal
eindhoven university of technology, faculty of electrical engineering, electronic systems group,
p.o. box 513, 5600 mb eindhoven, the netherlands
e-mail: {m.c.w.geilen,y.yang,h.corporaal}@tue.nl
georgeta ignafrits vaandrager
radboud university nijmegen, institute for computing and information sciences, department of
model-based system development, p.o. box 9010, 6500 gl nijmegen, the netherlands
e-mail: {g.igna,f.vaandrager}@cs.run.nl
sebastian de smet
oc´e-technologies b.v ., p.o. box 101, 5900 ma venlo, the netherlands
e-mail: sebastian.desmet@oce.com
†marc v oorhoeve
eindhoven university of technology, faculty of mathematics and computer science, architecture
of information systems group, p.o. box 513, 5600 mb eindhoven, the netherlands
5 april 1950 - 7 october 2011
wil van der aalst
eindhoven university of technology, faculty of mathematics and computer science, architecture
of information systems group, p.o. box 513, 5600 mb eindhoven, the netherlands
e-mail: w.m.p.v.d.aalst@tue.nl
12 twan basten et al.
abstract the complexity of today’s embedded systems is increasing rapidly. ever
more functionality is realised in software, for reasons of cost and ﬂexibility. this
leads to many implementation alternatives that vary in functionality, performance,
hardware, etc. to cope with this complexity, systematic development support during
the early phases of design is needed. model-driven development provides this sup-
port. it bridges the gap between ad-hoc back-of-the-envelope or spreadsheet calcula-
tions and physical prototypes. models provide insight in system-level performance
characteristics of potential implementation options and are a good means of doc-
umentation and communication. they ultimately lead to shorter, more predictable
development times and better controlled product quality. this chapter presents the
octopus tool set for model-driven design-space exploration. it supports designers in
modelling and analysing design alternatives for embedded software and hardware.
it follows the y-chart paradigm, which advocates a separation between application
software functionality, platform implementation choices, and the mapping of soft-
ware functionality onto the platform. the tool set enables fast and accurate explo-
ration of design alternatives for software-intensive embedded systems.
7.1 motivation
industries in the high-tech embedded systems domain (including for example pro-
fessional printing, lithographic systems, medical imaging, and automotive) are fac-
ing the challenge of rapidly increasing complexity of next generations of their sys-
tems: ever more functionality is being added; user expectations regarding quality
and reliability increase; an ever tighter integration between the physical processes
being controlled and the embedded hardware and software is needed; and techno-
logical developments push towards networked, multi-processor and multi-core plat-
forms. the added complexity materialises in the software and hardware embedded
at the core of the systems. important decisions need to be made early in the devel-
opment trajectory: which functionality should be realised in software and which
in hardware? what is the number and type of processors to be integrated? how
should storage (both working memory and disk storage) and transfer of data be
organised? is dedicated hardware development beneﬁcial? how to distribute func-
tionality? how to parallelise software? how can we meet timing, reliability, and
robustness requirements? the decisions should take into account the application re-
quirements, cost and time-to-market constraints, as well as aspects like the need to
reuse earlier designs or to integrate third-party components.
industries often adopt some form of model-based design for the software and
hardware embedded in their systems. figure 7.1 illustrates a typical process. white-
board and spreadsheet analysis play an important role in early decision making
about design alternatives. system decompositions are explored behind a whiteboard.
spreadsheets are then used to capture application workloads and platform character-
istics, targeting analysis of average- or worst-case utilisation of platform resources.
they provide a quick and easy method to quantitatively explore alternatives from7 model-driven design-space exploration for software-intensive embedded systems 3
iterative design and 
prototype development
tuning and performance   
optimisationspreadsheets:
- quick- easy
- generic
- flexible- widely used
- dynamics cannot be captured easily- costly, time-consuming iterations
fig. 7.1 typical industrial design practice for embedded hardware and software: iterative design
and development, intensively using spreadsheets, tuning functionality, and optimising performance
in prototypes.
performance and cost perspectives, at a high abstraction level. promising alterna-
tives are then realised (using various design and coding tools), to validate and ﬁne-
tune functionality and performance at the level of an implementation model. imple-
mentation models typically realise important parts of the functionality, they integrate
real code, and may run on prototype hardware. the entire process may be iterated
several times before arriving at the ﬁnal result.
design iterations through prototypes are time-consuming and costly. only a few
design alternatives can be explored in detail. the number of design alternatives is
however extremely large. the challenge is therefore to effectively handle these many
possibilities, without loosing interesting options, and avoiding design iterations and
extensive tuning and re-engineering at the implementation level. spreadsheet anal-
ysis is suitable for a coarse pruning of options. however, it is not well suited to
capture system dynamics due to for example pipelined, parallel processing, data-
dependent workload variations, scheduling and arbitration on shared resources, vari-
ations in data granularity, etc. (see fig. 7.2).
understanding and analysing the pipelined, parallel processing of dynamic
streams of data is challenging. the relation between design parameters (such as the
number and type of processing units, memory size and organisation, interconnect,
scheduling and arbitration policies) and metrics of interest (timing, resource utilisa-
tion, energy usage, cost, etc.) is often difﬁcult to establish. an important challenge
in embedded-system design is therefore to ﬁnd the right abstractions to support ac-
curate and extensive design-space exploration (dse).
this chapter presents an approach to model-driven dse and a supporting tool set,
the octopus tool set. the approach targets an abstraction level that captures the im-
portant dynamics while omitting the detailed functional and operational behaviour.
the abstractions bridge the gap between spreadsheet analysis and implementation4 twan basten et al.
workload variationsdifferent workload 
granularitiespipelined, parallel processing 
(of a single scanned page)
suboptimal performance
fig. 7.2 a gantt chart showing the execution of a print pipeline. dynamics in the processing
pipeline cause hick-ups in print performance due to under-dimensioning of the embedded execu-
tion platform. (figure from [7])
models and prototypes. the approach is designed to speciﬁcally cope with the chal-
lenges of dse.
an important characteristic of dse is that many different questions may need to
be answered, related to system architecture and dimensioning, resource cost and per-
formance of various design alternatives, identiﬁcation of performance bottlenecks,
sensitivity to workload variations or spec changes, energy efﬁciency, etc. different
models may be required to address these questions. models should be intuitive to de-
velop for engineers, potentially from different disciplines (hardware, software, con-
trol), and they should be consistent with each other. multiple tools may be needed
to support the modelling and analysis.
given these characteristics, our approach to address the challenges of model-
driven dse is based on two important principles: (1) separation of concerns and (2)
reuse and integration of existing techniques and tools. the modelling follows the
y-chart paradigm of [6, 39] (see fig. 7.3) that separates the concerns of modelling
the application functionality, the embedded platform, and the mapping of applica-
tion functionality onto the platform. this separation allows to explore variations in
some of these aspects, for example the platform conﬁguration or the resource ar-
bitration, while ﬁxing other aspects, such as the parallelised task structure of the
application. it also facilitates reuse of aspect models over different designs. the tool
set architecture separates the modelling of design alternatives, their analysis, the in-
terpretation and diagnostics of analysis results, and the exploration of the space of
alternatives (see fig. 7.4). this separation is obtained by introducing an interme-
diate representation, the dse intermediate representation (dseir), and automatic
model transformations to and from this representation. this setup allows the use of
a ﬂexible combination of models and tools. it supports domain-speciﬁc modelling
in combination with generic analysis tools. multiple analyses can be applied on the
same model, guaranteeing model consistency among these analyses; different analy-7 model-driven design-space exploration for software-intensive embedded systems 5
sis types and analyses based on multiple models can be integrated in a single search
of the design space. results can be interpreted in a uniﬁed diagnostics framework.
platform application
mapping
analysis
diagnostics
fig. 7.3 the y-chart paradigm for design-space exploration separates the modelling of application
functionality, platform functionality, and mapping of application functionality onto the platform;
after analysis and diagnostics, any of these aspects may be changed to explore alternatives (either
automatically or interactively by a designer). (y-chart: [6, 39]; ﬁgure from [8])
fig. 7.4 top view of the octopus tool set. (figure from [7])
chapter overview. section 7.2 provides an overview of the challenges we have
encountered in modelling and analysis support for taking design decisions during
early development. the experience draws upon our work in the professional print-
ing domain, but the challenges are valid for a wide range of high-tech embedded
systems. section 7.3 explains the model-driven dse approach we propose to han-
dle these challenges. this section also surveys related work. to illustrate the pos-
sibilities for domain-speciﬁc modelling, section 7.4 presents dpml, the data path
modelling language, which is a domain-speciﬁc modelling language for the print-6 twan basten et al.
ing domain. a dse case study from the professional printing domain is introduced
to illustrate dpml. section 7.5 introduces the intermediate representation dseir,
which is at the core of the octopus tool set. section 7.6 presents model transfor-
mations to a number of analysis tools. section 7.7 illustrates the support integrated
in octopus for interpretation and diagnostics of analysis results. section 7.8 brieﬂy
discusses some implementation choices underlying the tool set. section 7.9 presents
the results we obtained in several industrial case studies. section 7.10 concludes.
bibliographical notes. an extended abstract of this chapter appeared as [7]. the
octopus tool set was ﬁrst described in [8]. our philosophy behind model-driven
dse was originally presented in [69]. sections 7.2 and 7.3 are based on [69]. sec-
tion 7.4 describing dpml is based on [65], which provides a more elaborate de-
scription of dpml. dseir, summarised in sect. 7.5, is described in more detail
in [69].
7.2 challenges in early design
making the right decisions early in the design process of a complex software-
intensive embedded system is a difﬁcult task. in this section, we discuss the chal-
lenges we faced while conducting several case studies at oc ´e-technologies, involv-
ing the design of digital data paths in professional printers. these challenges are
characteristic for other application domains as well.
multi-functional printing systems perform a variety of image processing func-
tions on digital documents that support the standard scanning, copying, and printing
use cases. the digital data path encompasses the complete trajectory of the image
data from source (for example the scanner or the network) to target (the imaging
unit or the network).
memory interconnect
memory fpga memory cpu / gpuscanboard printboardmemory memory disk
fig. 7.5 a template for a typical printer data path architecture. (figure from [69])
data path platform template. figure 7.5 shows a template of a typical embed-
ded platform architecture for the digital data path of a professional printer. several
special-purpose boards are used to perform dedicated tasks, typically directly re-
lated to the actual scanning and printing. for computation, the data path platform7 model-driven design-space exploration for software-intensive embedded systems 7
may provide both general-purpose processors (cpus, gpus) and special-purpose
fpga-based boards. ram memory and hard disks are used for temporary and per-
sistent storage. the components are connected by interconnect infrastructure (e.g.
pci, usb). the architecture template shows the components needed for the digi-
tal image processing, leaving out user controls, network interfaces, etc. note that
the template is in fact generic for almost any modern software-intensive embedded
system.
printer use cases. each printer needs to support dozens of use cases. the stan-
dard ones are scanning, printing, copying, scan-to-email, and print-from-disk. each
use case typically involves several image processing steps such as rendering, zoom-
ing, rotating, compressing, half-toning, etc.; these steps may need several compo-
nents in the platform, and different choices for implementing these steps may be
possible. moreover, print and scan use cases can be mixed. they can also be instan-
tiated for documents with different paper sizes, numbers and types of pages, etc. it
is clear that this results in an explosion of possibilities.
ip1 scan ip2 ipn printupload
fig. 7.6 an example printer use case: copying.
as an illustration, we sketch the copying use case in some more detail; see
fig. 7.6. after scanning, each page is ﬁrst processed by the scanboard (that imple-
ments the scan task) and then further processed in several image processing steps
(ip1 : : :ipn); the resulting image is then printed by the printboard (executing the
print task). the image processing steps need to be done on some computation re-
source. intermediate results are stored in memory and/or on disk. the latter is done
for example to allow more than one copy of the document to be printed or to cope
with errors. uploading a processed image to disk can be done in parallel with any
further processing.
questions in data path design. the scanboard and printboard are typically the
ﬁrst components to be selected for a speciﬁc printer. they determine the maximum
possible scan and print speeds in pages per minute. the rest of the data path should
be designed in such a way that these scan and print speeds are realised at minimum
cost.
typically, a number of questions need to be answered early in the development
trajectory. which types of processing units should be used? how many? what clock
speeds are needed? what amount of memory is needed? which, and how many,
buses are required? how should image processing steps be mapped onto the re-
sources? other questions relate to scheduling, resource allocation, and arbitration.
what should be the scheduling and arbitration policies on shared resources? what8 twan basten et al.
are the appropriate task priorities? can we apply page caching to improve the per-
formance? how to allocate memory in ram? how to share memory? how to min-
imise buffering between tasks? how to mask workload variations? is the memory
allocation policy free of deadlocks?
the data path design should take into account all basic use cases, as well as
combinations such as simultaneous printing and scanning. it should also take into
account different job types (text, images), paper sizes, etc. the design should be
such that no bottlenecks are created for the most important use cases (normal print-
ing, scanning, and copying, on the default paper size for normal jobs). performance
can be traded off for costs for use cases that occur less frequently though. dse
should also provide insight in these trade-offs. furthermore, printing products typ-
ically evolve over time. this raises questions such as what is the impact of a new
scan- or print board with higher specs on an existing data path design. it is clear that
dse complexity is large and that quantifying all the mentioned aspects early in the
design process is challenging.
modelling dynamics. modelling the above use cases for dse is possible with
a spreadsheet at a high level of abstraction as a ﬁrst-order approximation. spread-
sheet estimates, however, may lead to over- or under-dimensioning of the ultimate
design, which is costly to repair in later design stages. the cause for over- or
under-dimensioning is the difﬁculty to capture various dynamic aspects of software-
intensive embedded systems in a spreadsheet. first, there are various sources of
variability. the complexity of a page to be printed, the size of a compressed page,
and the execution time on a general-purpose processor are all stochastic and rarely
exactly predictable. second, the ﬂow of information is often iterative or conditional.
an example of a conditional ﬂow is a smart-storage heuristic that takes a page from
disk only if it is not still in memory. third, pipelined and parallel steps in a job
and simultaneously active print and scan jobs may dynamically interact on shared
resources such as memory, buses, and shared processors. finally, scheduling and ar-
bitration policies are often crucial for performance, but result in dynamic behaviour
that is hard if not impossible to model in a spreadsheet-type model.
mixed abstraction levels. although many image processing steps work on pix-
els or lines, parts of most use cases can be accurately modelled at the page level. the
throughput of the data path in images per minute is also the most important metric of
interest. a mixture of abstraction levels may be needed to achieve the required accu-
racy while maintaining analysis efﬁciency. fpgas for example come with limited
memory sizes. only a limited number of lines of a page ﬁt in fpga memory. the
page level thus becomes too coarse and a ﬁner granularity of modelling is needed.
modelling complete use cases at the line or pixel level would make most analyses
intractable though; appropriate transitions between abstraction levels are needed.
variety in analysis questions. the typical analysis questions in the list of dse
questions above may, in theory, all potentially be answered by a generic modelling
and analysis tool; it is clear, however, that the various types of dse questions may
be of a very different nature. deadlock and schedulability checks, for example, are
best done using a model checker. performance analysis would typically be done with7 model-driven design-space exploration for software-intensive embedded systems 9
analytic models, like spreadsheets, for a coarse evaluation and simulation for a more
reﬁned analysis that takes into account the dynamics in the system. low-level fpga
buffer optimisation can be done with fast, yet restrictive, dataﬂow analysis. this
variety in analysis questions suggests the use of different tools. this does require
the development of multiple models though, leading to extra modelling effort and
a risk of model inconsistencies and interpretation difﬁculties. ideally, one master
model would form a basis for analyses performed with different techniques and
tools.
model parametrisation. there is a need to support a high degree of parametri-
sation of models: multiple use cases need to be captured, each of them with many
variations; models are needed for various dse questions; design decisions may need
to be reconsidered to cope with late design changes; and to speed up development,
there is a clear wish to reuse models across variations of the same product, both
to allow product customisation and to support the development of product families.
the desired parametrisation goes beyond simple parameters capturing for example
workloads, task execution times, and memory requirements; they should also cover
for example the ﬂow of use case processing, communication mechanisms, platform
resources, and scheduling and arbitration policies.
customisation for the printer domain. the basic principles of printer plat-
forms and printer use cases are not rapidly changing. having the models written
in a printer-speciﬁc language, and maintaining a library of those models, would
drastically decrease the modelling effort for new printers, reduce modelling errors,
and improve communication and documentation of design choices. the design of a
domain-speciﬁc language for the printer domain is challenging. on the one hand, we
want a simple language, which only contains constructs that are needed to describe
the current designs. on the other hand, it should also be possible to use (simple
extensions of) the language to describe the designs of tomorrow.
7.3 model-driven design-space exploration
the previous section clariﬁed the challenges in early dse. in this section, we ﬁrst
identify the main beneﬁts of a model-driven approach to tackling these challenges.
we then set out the key ingredients of our approach to model-driven dse. along
the way, we survey methods, languages, and tools that ﬁt in such an approach. the
following sections then elaborate on the octopus tool set that is being developed to
support the model-driven dse approach and that integrates several of the surveyed
methods, languages, and tools.
the beneﬁts of model-driven dse. the ultimate objective of model-driven
dse is to reduce development time (and thereby time-to-market), while maintain-
ing or improving product quality. this is achieved by appropriate modelling and
analysis during early development. models should capture the essential system dy-
namics without including unnecessary details. only then, effective exploration of10 twan basten et al.
fast exploration
- improved insight
- model reuse- flexible analysis options- improved model consistency
- improved documentation
- reduced development time
fig. 7.7 model-driven design-space exploration.
design alternatives is feasible. figure 7.7 visualises the model-driven dse approach
and summarises the targeted beneﬁts. with appropriate modelling and tool support,
(1) insight in system dynamics and design alternatives improves, (2) models are
re-usable within the product development trajectory and across developments, (3) it
becomes feasible to apply different analyses, (4) different models may be used while
safeguarding their consistency, and (5) documentation improves by using the mod-
els themselves as the design documentation. in combination, these beneﬁts lead to
(6) the intended reduction in development time.
separation of concerns. to address the challenges outlined in the previous sec-
tion and to realise the above-mentioned beneﬁts, we propose a rigourous separation
of concerns.
first, the tool set organisation should separate the modelling, analysis, diagnos-
tics, and search activities, as already illustrated in fig. 7.4. modules for each of these
activities are decoupled through an intermediate representation, dseir (dse inter-
mediate representation; see sect. 7.5). such an organisation realises the required
ﬂexibility in modelling and analysis needs. the use of an intermediate representa-
tion allows reuse of analysis techniques and tools across different models and in
combination with different modelling environments. model consistency is ensured
by appropriate model transformations to and from the intermediate representation.
the challenge is to develop an intermediate representation that is sufﬁciently rich
to support dse but not so complex that it prohibits model transformations to var-
ious analysis techniques and tools. these model transformations should preserve
precisely deﬁned properties, so that analysis results from different tools can be com-
bined and results can be interpreted in the original model.
second, modelling should follow the y-chart philosophy [6, 39]. this philoso-
phy is based on the observation that dse typically involves the co-development of
an application, a platform, and the mapping of the application onto the platform (as7 model-driven design-space exploration for software-intensive embedded systems 11
already illustrated in fig. 7.3). diagnostic information is then used to, automatically
or manually, improve application, platform, and/or mapping. this separation of ap-
plication, platform, and mapping is important to allow independent evaluation of
various alternatives of one of these system aspects while keeping the others ﬁxed
the others. often, for example, various platform and mapping options are inves-
tigated for a ﬁxed set of applications. dseir separates the application, platform,
and mapping modelling. in combination with the tool set organisation illustrated in
fig. 7.4, dseir thus supports the y-chart philosophy.
application-centric domain-speciﬁc modelling. a key challenge in modelling
for dse is to fully take into account the relevant system dynamics such as realis-
able concurrency, variations in application behaviour, and resource behaviour and
sharing. given the complexity of the dse process, a modelling abstraction level
is needed that abstracts from implementation details but is more reﬁned than typi-
cal spreadsheet-type analysis. modelling should be simple and allow efﬁcient and
accurate analysis.
another important aspect in modelling for dse is that the abstractions need to
appeal to the designer and adhere to his or her intuition. domain-speciﬁc abstrac-
tions and customisation should therefore be supported. modelling should further-
more be application-centric. the application functionality and the quality (perfor-
mance, energy efﬁciency, reliability) with which it is provided is what is visible
to users and customers. application functionality should therefore be leading, and
the models should capture all behaviour variation and all concurrency explicitly.
we propose to model platforms as sets of resources that have no behaviour of their
own; their purpose is only to (further) restrict application behaviour and to introduce
proper timing. this leads to simple, predictable, and tractable models. scheduling
and mapping of applications onto resources can then be uniﬁed into the concept of
prioritised dynamic binding. if needed, complex resource behaviour (work division,
run-time reconﬁguration, etc.) can be modelled through (automatic) translations into
application behaviour.
a variety of modelling environments and approaches in use today, either in in-
dustry or in academia, can support the envisioned modelling style. we mention
some of them, without claiming to be complete: aadl [1], dpml [65], mod-
elica [47], ptides [19], ptolemy [21], m atlab /simulink [45], sysml [64], sys-
temc [51], uml [72], and uml-marte [73]. the mentioned environments often
target different application domains and/or different system aspects. figure 7.8 po-
sitions these modelling approaches in the architectural framework of the octopus
tool set. dpml, the data path modelling language, is discussed in more detail in
sect. 7.4.
analysis, search, diagnostics. the previous section illustrated the variety of de-
sign questions and challenges that are typically encountered early during develop-
ment. no single tool or analysis method is ﬁt to address all these questions. we
foresee the combined use of different analysis tools in one dse process. a wide
variety of, mostly academic, but also some commercial, tools is available that can
be used to support dse.12 twan basten et al.
cpn tools
poosl
simulink/simevents
nusmv
spin
uppaal
rtc toolbox
sdf3
symta/s
mrmc
prism
alloy analyzer
cplex
yicesaadl, dpml, modelica, 
ptides, ptolemy, 
simulink, systemc, 
sysml, uml, 
uml-marte
bip, cal, cif, dif, 
dseir
excel, timedoctor, 
resvis, improvise, promglobal 
optimization 
toolbox,
jgap, opt4j, 
pisacif tooling, cofluent
design, daedalus, ese, 
formula, forsyde, metro ii, 
mldesigner, octopus,
scade, systemcodesigneranalysismodelling
intermediatesframeworks
search
diagnostics
fig. 7.8 methods, languages, and tools that ﬁt in the top-level architectural view of fig. 7.4. every
entry is only mentioned where it ﬁts best in the architectural view (even though it may ﬁt in other
places as well). (figure adapted from [69])
for quick exploration and performance optimisation, discrete-event simulators
such as cpn tools [34], poosl [66], and simulink/simevents [45, 44] are suit-
able. model checkers such as nusmv [50], spin [25], and uppaal [10] can be used
for functional veriﬁcation, protocol checking, and schedule and timing optimisation.
model checkers may not be able to cope with the full complexity of modern embed-
ded systems, but they may play an important role in verifying and optimising critical
parts of the system. yet other tools, such as the rtc toolbox [57], sdf3 [62], and
symta/s [63], are suited for timing analysis of data-intensive system parts, such as
image processing chains.
questions regarding performance, reliability, and schedulability under soft dead-
lines can be answered by increasingly popular probabilistic model checking tech-
niques, using tools like prism [54] and mrmc [37]. these techniques enhance
the expressivity of regular model checking, allowing for more realistic modelling
of aspects such as arrival rates and failure times. scalability of these techniques to
realistic analysis problems remains a challenge though.
in recent years, also constraint programming and sat/smt solvers have gained
popularity. with the rise of more powerful computers and improvements in the tech-
niques themselves, tools like cplex [28], alloy analyzer [32], and yices [80] are
increasingly often used to ﬁnd optimal or feasible solutions for system aspects such
as resource bindings or schedules.
the octopus tool set has links to three analysis tools, namely cpn tools, uppaal,
and sdf3. the model transformations that realise these links and the intended use
of these tools in the octopus context are discussed in sect. 7.6.
besides support for evaluation of metrics for design alternatives or for the op-
timisation of parts of the system, we also need support to explore the large space
of design alternatives. the mathworks global optimization toolbox [43] supports
a wide variety of customisable search algorithms. the jgap library [36] is a java7 model-driven design-space exploration for software-intensive embedded systems 13
library for developing genetic search algorithms. opt4j [42] and pisa [11] are
customisable genetic search frameworks that support dse.
most of the tools mentioned above already give good diagnostic reports, which
in many cases can be successfully converted and interpreted in the original domain.
microsoft excel is also a useful tool in this context. visualisation of gantt charts
(using tools such as timedoctor [68] or resvis [59], from which the screenshot of
fig. 7.2 is taken) helps understanding the dynamic behaviour of design alternatives.
sophisticated mining and visualisation is possible with improvise [31] or prom [55].
section 7.7 presents the diagnostic support as it is developed in the octopus context,
which includes gantt chart visualisation through resvis.
intermediate representation: ﬂexibility, consistency, customisation. it can-
not be expected that designers master the wide variety of modelling languages and
tools mentioned so far, and apply them in combination in dse. to successfully deal
with integration, customisation, and adaptation of models, as well as to facilitate
the reuse of models across tools and to ensure consistency between models, we
foresee the need for an intermediate representation to connect different languages
and tools in a dse process. such an intermediate representation must in the ﬁrst
place be able to model the three main ingredients of the y-chart (application, plat-
form, mapping) in an explicit form. it should not have too many speciﬁc constructs
to facilitate translation from different domain-speciﬁc modelling languages and to
different target analysis tools, yet it must be powerful and expressive enough to
accommodate developers. a good balance between modelling expressiveness and
language complexity is needed. besides the y-chart parts, the intermediate repre-
sentation must provide generic means to specify sets of design alternatives, quanti-
tative and qualitative properties, experimental setups, diagnostic information, etc.,
i.e. all ingredients of a dse process. the intermediate representation should have
a formal semantic basis, to avoid interpretation problems and ambiguity between
different models and analysis results. the intermediate representation does not nec-
essarily need execution support, because execution can be done through back-end
analysis tools. intermediate representations and languages like bip [9], cal [20],
cif [74], dif [27] and the intermediate representation dseir (see sect. 7.5) un-
derlying the octopus tool set are examples of languages that can be adapted to fully
support model-driven dse as sketched in this section.
a dse tool set. to realise the goals set out, it is important to provide a ﬂexible
tool set implementation. we propose a service-based implementation of the tool set
architecture of fig. 7.4. modules should communicate with other modules through
clean service interfaces. domain-speciﬁc modelling tools with import/export facili-
ties to dseir are in the modelling module. the analysis module provides analysis
services such as performance evaluation, formal veriﬁcation, mapping optimisation,
schedulability analysis, etc. the diagnostics module provides ways to visualise anal-
ysis results and gives high-level interpretations of system dynamics in a way intu-
itive to system designers. the search module contains support for search techniques
to be used during dse. information ﬂows between the modules go through the
dseir kernel module that implements the intermediate representation.14 twan basten et al.
there are several frameworks and tool sets that support dse, or aspects of it, for
various application domains. examples are cofluent design [15], daedalus [49],
ese [75], formula [33], forsyde [58], metro ii [17], mldesigner [46],
scade [22], and systemcodesigner [38]. the large number of available lan-
guages, tools, and frameworks are a clear indication of the potential of high-level
modelling, analysis, and dse. the octopus tool set, described in more detail in the
remainder of this chapter, is closest to the views outlined in this section. octopus
explicitly aims to leverage the combined strengths of existing tools and methods in
dse. its service-based implementation is discussed in sect. 7.8.
7.4 dpml: data path modelling language
the entry point of our tool chain is the modelling module (see fig. 7.4). models
can be developed in a domain-speciﬁc language (dsl), that functions as a front end
for the tool chain. for modelling printer data paths, we have designed dpml (data
path modelling language). this section ﬁrst introduces a typical dse case study
as a running example. it then presents the design goals for dpml, followed by an
overview of dpml and a presentation of the main dpml concepts along the lines
of the y-chart separation of concerns (see fig. 7.3).
7.4.1 running example: high-end colour copier
the scan path of a copier is the part of the data path that receives data from the
scanner hardware, processes them, and stores them for later use (e.g. sending them
to the printer hardware or to an e-mail account). figure 7.9 shows the high-level ar-
chitecture of a part of the scan path in a high-end colour copier. its structure follows
the y-chart approach. the application consists of six tasks: a step that downloads
image data from the scanner hardware, four image processing steps ip1, : : :, ip4,
and a step that writes the processed image data to disk. the tasks pass image data
through various statically allocated buffers (i.e. the buffer slot size is constant and
both the size and the number of slots are determined at design time). note that
task ip4 reads from and writes to the same buffer slot. the platform consists of a
general-purpose board with cpus/gpus and potentially several dedicated boards,
for example for the interface with the print engine (not shown in the ﬁgure). the
mapping of the tasks and the buffers to the platform is depicted by the colour-star
combination. steps ip1, ip2, and ip4 all use the cpu, whereas the other steps each
have their own computational resource.
there are several important aspects that complicate the modelling and analysis:
various steps use different data granularities (indicated in the ﬁgure by the dif-
ferent widths of the arrows to and from the buffers). the download step writes
the data of a complete image in a buffer slot of buffer 1. step ip1 processes these7 model-driven design-space exploration for software-intensive embedded systems 15
downloadip1ip3ip4writeip2
pcie x1
sata
bus
memorybus
pcie x16
cpu
gpu
pch
gigabit ethernetcontroller
main ramgpu rambuffer 1buffer 2buffer 3buffer 4
fig. 7.9 high-level architecture of a part of the scan path in a high-end colour copier.
data and outputs the result in ten so-called bands . step ip2 processes each band
and appends it to the single slot of buffer 3. as soon as the data of a complete
image are present in buffer 3, step ip3 processes them and writes the result in
ﬁner grained bands to buffer 4. steps ip4 and write to disk process these bands
individually.
buffers 1, 2, and 4 can have multiple slots which allows pipelining of the pro-
cessing steps of consecutive images.
the scheduling on the cpu is priority-based preemptive.
the execution times of the steps are not known exactly, and often heavily depend
on the input image and on the result of the algorithms in the steps that change the
data size (e.g. compression).
the main performance indicator of the data path is throughput . it is important
that the data path is not the limiting factor of the machine. for cost reasons, the
scan hardware should be fully utilised. another, albeit less important, performance
indicator is the total amount of main memory that buffers 1, 2, and 4 consume. since
memory is a scarce resource, the buffers should not be over-dimensioned. the dse
question is therefore as follows:
minimise the amount of memory allocated to buffers 1, 2, and 4 while retain-
ing a minimum given throughput.16 twan basten et al.
7.4.2 the dpml design goals
dpml is intended to support modelling for dse of printer data paths. when de-
signing dpml, four goals were kept in mind:
first, dpml must be particularly suited to analyse the speed (or throughput) of
data path designs. this means that all information necessary for obtaining the
speed of a data path must be present, but behavioural issues that do not inﬂuence
speed may be abstracted away. the data sizes of images and image parts being
manipulated and transferred play a dominant role.
furthermore, dpml has to be expressive andﬂexible . this means that it must be
possible to express a wide variety of models, with different behavioural and struc-
tural properties. this is important, because we cannot always foresee what kinds
of designs engineers may want to analyse in the future, or what other purposes
(than analysing speed) may be found for dpml models. therefore, it must be
easy to model many things in dpml, and it must also be easy to change dpml
to add more features. this requirement is essential if the tool chain is to be a
sustainable solution.
dpml has to closely match the problem domain . this means that all elements
commonly found in printer data paths must be well supported and easy to model.
the most visible example of this is the concept of pages ﬂowing through the steps
of the application. without this, even simple designs would require considerable
modelling effort and thus a steeper learning curve for people using the tools for
the ﬁrst time. this may in turn impact the adoption of the tool set as a means to
improve the data path design process.
important are also the features that dpml does nothave; features that would
make it a more generic speciﬁcation language, such as support for caches, the
ability to fully specify the behaviour of steps, the possibility to specify real-time
deadlines, or the ability to ﬁne-tune a task scheduler. leaving out such features
makes dpml a simple language, in which models of commonly occurring data
path designs are not signiﬁcantly more complex than what an engineer would
draw in an informal sketch of the same design.
finally, dpml has to support modular designs. this way, parts, or elements that
are used in multiple designs can be reused, thus saving modelling time. addi-
tionally, a modular setup allows engineers to share knowledge obtained during
design or engineering (such as the actual speed of a hardware component, or the
rationale behind a design decision) with engineers in other projects who may use
some of the same parts.
7.4.3 dpml overview
dpml is a combined visual and textual language. the visual parts outline the coarse
structure of a data path design, such as the series of processing steps that a data path7 model-driven design-space exploration for software-intensive embedded systems 17
may be required to perform, and the component layout of a hardware platform.
these are represented visually so that, even in large models, it is easy to get a good
overview. the textual parts are used to express all details of every element in a data
path design, such as the behaviour of a step or the capacity of a memory. these
details are expressed with a custom, text-based language so that it is easy to add
new constructs and features.
structurally, a complete dpml model consists of the three distinct components
of the y-chart paradigm:
anapplication , which functionally describes a series of steps that a data path
may be required to perform.
aplatform , which describes the various hardware components that a data path
consists of and how they are laid out.
amapping between these two, which describes which hardware components are
used by which steps, and how.
dpml models can be edited in a custom editor that is based on the open source
qt library [56]. a single dpml model is contained in multiple small ﬁles, each of
which describes a reusable element. a textual dpml element is stored as a plain text
ﬁle and visual dpml elements are stored in a custom xml format. to facilitate the
analysis and further conversion of dpml models, e.g. to the octopus intermediate
representation dseir, these ﬁles are converted to a simpler data format, dpml
compact. dpml compact models can be simulated using a native simulator. the
advantage of having this compact format is that changes in and additions to the
language do not affect the simulator and the model transformations as long as dpml
compact remains unchanged.
7.4.4 dpml: the application view
figure 7.10 displays the application component of our running example described
in dpml. the model captures the pipelined processing of scan jobs consisting of
any number of scanned pages. every rounded rectangle in fig. 7.10 is a step, which
is a single image processing operation that the data path must perform on a single
image or part of an image. each step has two or more pins, the squares or circles on
the sides.
fig. 7.10 a dpml application.
pins can have three possible types: simple ,data , andpixeldata . visually,
asimple pin is a semicircle, a data pin is a square, and a pixeldata pin is a18 twan basten et al.
square with a small square in the middle. every pixeldata pin also is a data
pin, and every data pin is also a simple pin. when a step has an output data
pin, this means that this step produces data at that pin. if it is a pixeldata pin,
this additionally implies that the data produced represent a bitmap image. similarly,
when a step has an input data pin, it means that this consumes data at that pin.
arcs between steps indicate data dependencies. they further determine the exe-
cution order of the application. by default, a step can only start when for all its input
pins, the preceding step has completed. this explains the need for simple input
pins; no data are consumed or produced on those pins, but they can be used to ﬁx
the execution order between steps.
because a printer data path is all about image processing, we assume that we
can model the duration of a single image processing step as a function of the size
of the image data, the speed and availability of all resources used, and predeﬁned
information about the pages that are to be processed. most notably, we assume that
it does not depend on the precise content of the image. this assumption is important,
because it means that instead of formally specifying all aspects of a step behaviour,
it is sufﬁcient to just specify the data sizes that it produces.
in dpml, a task is used to describe a single operation that we may want a data
path to perform. each step in an application is in fact an instantiation of such a
task: a step behaves exactly as dictated by the task, and each step corresponds to
exactly one task. it is, however, possible for multiple steps to belong to the same
task. multiple compression step instances of the same task may for example occur
in a single image processing pipeline. because a step cannot exist without a task,
the set of available tasks deﬁnes the set of operations we can use in an application.
the relationship between tasks and steps is therefore somewhat comparable to the
relationship between classes and objects in object-oriented programming languages.
tasks are stored as small text ﬁles with content as shown in fig. 7.11. a task
deﬁnition has three parts: a header, a set of pin declarations, and a set of properties.
the header deﬁnes the name of the task, in this case “resample”. the pin declara-
tions deﬁne the number of input and output pins, their names, and their data types.
the resample task takes a raster image and produces a raster image, so there is one
input pin and one output pin, both of type pixeldata . the properties constitute
the functional description of the behaviour of a task. because we assume that the
actual content of the image produced does not matter, we only need to determine
the size of the output image (so in fact we are only describing a very small part of
the required behaviour, focusing on resource usage and performance aspects).
in the example of the resample task, the width and length of the output image
depend on the width and length of the input image as well as a user setting, the
zoom factor. because it is possible for a single scan job to require some pages to
be zoomed and some pages not, the zoom setting is looked up as a property of the
current page.
tasks can specify more properties than just the sizes of its output images, such
as a boolean condition that must be true for a task to be able to start. all of these
other properties are optional.7 model-driven design-space exploration for software-intensive embedded systems 19
fig. 7.11 an example of a task in dpml. (figure from [65])
7.4.5 dpml: the platform view
a platform deﬁnes the hardware necessary to perform the steps in a data path. such
hardware typically includes processors, memories, hard drives, caches, cables and
buses, and of course the actual printing and scanning components. in dpml, these
components belong to a set of three resource types:
amemory is something that can store and retrieve data, so it includes hardware
such as ram chips and hard drives. a memory has a particular capacity, which
is its only limiting factor.
abusis something that can transfer data. a bus typically has a maximum band-
width (the maximum number of bytes it can transfer per second).
anexecutor is something that can execute a step and has some processing speed.
executors are subdivided into processors, scanners, and printers for clarity, but
from a semantics point of view there is no difference between a processor, a
scanner, and a printer in dpml.
with just these blocks, we can create sufﬁciently realistic platform models for
analysing the performance of a data path design. the platform of our running exam-
ple looks like in fig. 7.12.
memory blocks are shown as rounded rectangles, bus and executor blocks as
rectangles. buses are the only components that can limit data transfer speed. thus,
thedisk memory, which models a hard drive, can in fact read and write data in-
ﬁnitely fast. the sata bus models both the real sata bus by means of which
the hard drive is connected to the pc motherboard and the hard drive’s inherent
maximum read/write speed. note that the model represents the main ram and
gpu ram memories in the form of the (statically allocated) buffers main ram1,
main ram2,main ram3, andgpuram; the latter are the actual resources that
tasks need to compete for.
a line between blocks in a platform model is called a connection , meaning that
the two (or more) blocks are directly connected to one another. connections limit
the possible routes by which data can ﬂow through the platform. an additional re-
quirement is that, on a route between an executor block and a memory block, there
may only be (one or multiple) bus blocks.20 twan basten et al.
fig. 7.12 a platform model in dpml.
resource blocks have properties, much in the same way as the steps of an ap-
plication have behaviour. for example, properties of resource blocks include the
bandwidth of a bus and the capacity of a memory. analogously to steps and tasks or
to objects and classes, the properties of resource blocks are speciﬁed in small chunks
of code called resources . a resource describes that a particular piece of hardware
exists and has some particular properties. for example, a resource may describe
a particular intel processor. a resource block based on that resource, describes that
such an intel processor is used in an actual hardware platform. a resource block can-
not exist without an associated resource, and there can be multiple resource blocks
based on a single resource. resources are typically simpler in structure than tasks.
many resources only have one or two properties.
fig. 7.13 some example resources.7 model-driven design-space exploration for software-intensive embedded systems 21
as shown in fig. 7.13, each resource type has its own (small) set of properties.
thetransferspeed property for buses and the capacity property for memo-
ries always have the same unit: they are expressed in bytes per second and in bytes,
respectively.
dpml platform models only have buses, memories, and executors. this means
there are no more specialised versions of such resources, such as caches or hard
drives. it turns out that, currently, such elements are not needed.
recall that a data path is a component that performs image processing and trans-
fer operations. this allows us to make the following assumptions:
1. data transferred between steps are new, i.e. a step has not recently read or written
exactly the same image.
2. data are read and written linearly, i.e. in a single stream of ordered bytes.
3. the amount of working memory needed for processing an image is small.
assumptions 1 and 2 imply that caches do not inﬂuence processing speed when
an image is read from memory, because every chunk of image data read constitutes
a cache miss. additionally, because of assumption 3, we can assume that reads and
writes to the internal memory used by the image processing steps (such as local
variables) always constitute a cache hit, i.e. that they seldom go all the way to the
actual memory block.
because data are written and read linearly (assumption 2), we can ignore the
fact that physical hard drives are relatively slow when reading randomly offset data.
compared to the time spent reading relatively large chunks of sequentially stored
bytes (image data), the seek time needed to move the read head to the right position
is negligible.
note that it is not impossible to design a data path in which one or more of the
above assumptions do not hold. the analysis of such a dpml model may yield sig-
niﬁcantly different results than the real data path would. therefore, it is important
that dpml users are aware of these assumptions. if it turns out that some assump-
tions are invalid more often than not, additional features may be added to dpml to
overcome this issue. note that due to the modular and extensible structure of dpml,
it is relatively easy to do so if the need arises. new properties for memory resources
that describe, for instance, a hard drive’s random seek penalty and its fragmentation
state may be used to estimate seek times penalties if deemed signiﬁcant. similarly,
there is no structural reason why a fourth resource type, such as a cache, could not
be added to the language if necessary.
7.4.6 dpml: the mapping view
a mapping deﬁnes how an application relates to a platform. in a mapping, we spec-
ify which steps run on which executor blocks, which data are stored where, and
which memory claims and releases are performed. like applications and platforms,
mappings are partly textual and partly graphical.22 twan basten et al.
dpml mappings have three different kinds of links by which elements are
mapped onto one another: storage links, allocation links, and execution links. visu-
ally, a mapping is simply displayed by an application and a platform shown along-
side one another, and links are represented as arrows between the application and
the platform.
storage links specify where the output data on each data output pin of a step
have to be stored for a subsequent step to be able to read and process them. a storage
link is thus a link between output data pins and memory blocks. figure 7.14 shows
how we can model this in dpml for our running example.
ip1 ip2 ip3 ip4 download write
fig. 7.14 storage links in the mapping assign memory blocks to data output pins.
allocation links are used to keep track of memory claims and releases. before
data can be written to memory, it must ﬁrst be allocated. this is important, because
if a step cannot start because memory is full, the step must be blocked until sufﬁcient
memory is available for its output data pins.
scan resample
scan resample
fig. 7.15 some allocation and release links and their short-hand notation.
there are two kinds of allocation links, see fig. 7.15: claim links (blue) and re-
lease links (green). they are responsible for claiming and releasing the memory
block, respectively. even though links are part of the mapping, they are drawn en-
tirely in the application; this is because instead of saying “step a claims memory7 model-driven design-space exploration for software-intensive embedded systems 23
b”, we say “step a claims the memory needed for output pin c”. using the storage
links, analysis tools can then determine which memory block that pin is mapped
to, and using the properties of the task associated to the output pin’s step, it can be
determined how much memory should be claimed.
the allocation links may create a busy picture that is difﬁcult to oversee. there-
fore, dpml provides two shorthand rules: if a data output pin has no claim link,
then the step that the pin belongs to is assumed to perform the claim. similarly, if it
has no release link, then the following step is assumed to perform the release. the
second rule can only be applied if there is only a single step that consumes the data
produced by the data output pin. if there are more than one (or zero) following
steps, a release link should always be drawn.
ip1 ip2 ip3 ip4 download write
fig. 7.16 execution links.
execution links , ﬁnally, describe which steps run on which executor blocks.
like storage links, they are drawn as simple arrows from steps to executor blocks.
every step can have at most one execution link, but an executor can be associated to
any number of execution links.
note that it is allowed for a step to not have an execution link, but only if the step
has no data pins. if a step consumes or produces data, then this means that data
are being transferred from or to a memory, via some buses, to or from an executor
block. only by means of an execution link, this route can be computed.
unlike storage links, each execution link has two additional properties: an asso-
ciated implementation and a priority . each (red) arrow between steps and executor
blocks in fig. 7.16 is an execution link.
animplementation describes how a step can be mapped onto an executor block.
because we are interested in the speed of the data path, an important property of a24 twan basten et al.
single step is its duration. a step’s speed is typically limited either by the available
processing power, or by the available bus capacity for reading and writing its input
and output data. in order to know which of the two limits a step’s speed, we need to
compute both. a step’s bus usage can be derived from the size in bytes of its inputs
and outputs and from the platform layout. computing a step’s processing speed,
however, requires some more information from the user.
the duration of a single step depends on properties of the processor, on prop-
erties of the data (such as the image width and length) and on the particular im-
plementation of the step. a dpml implementation captures this dependency in the
property processingduration . this property speciﬁes the amount of time that
a step is expected to take to process a single block of image data, given the cur-
rent circumstances. as this time usually depends on some property of the input
and/or output data as well as the amount of processor speed that is assigned to it,
processingduration is usually a function of these properties.
figure 7.17 shows how a typical implementation for a resample task may look.
this implementation models a situation in which the speed of resampling de-
pends on the number of pixels of the largest image, which is the input image
when scaling down, or the output image when scaling up. moreover, on average,
20 clock cycles are used per pixel in the largest image. with this information, the
processingduration of the step can be computed. the implementation can
directly refer to all properties of the associated task (such as the input pin and out-
put pin properties).
fig. 7.17 an implementation.
apriority , formulated as an integer number, speciﬁes which step gets to “go
ﬁrst” if multiple steps want to use a resource at the same time. we enforce one
important convention with respect to priorities: the higher the number, the lower
the priority. so a step with priority 3 is considered more important than a step with
priority 7. it is possible for multiple execution links to have the same priority, and
this should imply that resources are fairly shared between the competing steps.7 model-driven design-space exploration for software-intensive embedded systems 25
7.5 dseir: dse intermediate representation
section 7.3 motivated the importance of an intermediate representation to support
dse. we are developing the intermediate representation dseir speciﬁcally for the
purpose of model-driven dse. in modelling design alternatives, dseir follows the
y-chart paradigm. it further has support for deﬁning experiments. it has been re-
alised as a java library, ﬁlling in the central module of the architecture of fig. 7.4
on page 5. the current implementation supports four views: application, platform,
mapping, and experiment. dseir can be used through a java interface, an xml in-
terface, and an eclipse-based prototype gui. this section introduces and illustrates
the four views of dseir. we use the dse case study of the previous section as a
running example.
7.5.1 dseir: the application view
figure 7.18 shows a fragment of the dseir representation of the application part
of our running printer example, in both the xml and the graphical format. the
dseir application language is inspired by dataﬂow languages, in which data trans-
formations play the most prominent role, but it intends to also support petri-net and
automata-based modelling concepts. an application consists of a number of tasks,
and models the functional behaviour of the system. each task has one or more ports,
a number of load declarations and a number of edges. ports are collections of values
of some type (integer, integer array) and are either ordered in a ﬁfo (ﬁrst-in ﬁrst-out)
way or unordered. ports provide inputs to tasks. the load declarations specify the
load of the task for the services that it uses. these services are expected to be pro-
vided by the platform (such as a computation service that is provided by a cpu).
the task loads are used in combination with platform information to determine the
execution time of the task. the use of service types avoids direct references to plat-
form resources which avoids coupling of the application description with a speciﬁc
platform description. an edge leads from the current task to either a port of another
task or to a port of the same task. the purpose of an edge is to add a new value to
the target port. an edge has an expression in the dseir expression language that
gives the new value in the target port. furthermore, both a task and an edge can have
acondition , which is a boolean expression that determines whether the task or edge
is enabled and can execute. finally, an edge can have zero or more handover speci-
ﬁcations (the ‘ ho’ entries in the xml representation in fig. 7.18) which contain an
amount of allocated services that should be passed on to the next task. this allows
modelling of resource reservations, for instance, memory pointers that are passed on
from one task to the other without releasing the memory. an application can have
a number of global variables of type integer, and a number of parameters, which
also are of type integer. both can be used in expressions in the application part. the
dseir expression language is sufﬁciently powerful to capture applications at the26 twan basten et al.
(a)
 <param name="numpages" />      <global name="time_scale" value="1000" />     <global name="scan_cf" value="10" />     <global name="ip1_load" value="300" />     <global name="a4_pixels_600_dpi" value="34802530" />     <global name="a4_rgb_bytes_600_dpi" value="3*a4_pixels_600_dpi" />     <global name="numbands" value="(a4_rgb_bytes_600_dpi / (1024 * 1024 * 10)) + 1" />      <task id="download">         <port id="p" type="int" init="1" />         <load service="transfer" value="a4_rgb_bytes_600_dpi/(scan_cf*1024)" />         <load service="result_storage" value="1" />         <edge port="p" value="p+1" cnd="p &lt; numpages" />         <edge port="p_b" task="ip1" value="[p,1]">             <ho service="result_storage" value="1" />         </edge>     </task>      <task id="ip1" cnd="p_b[0] == next_p">         <port id="p_b" type="int[2]" order="unordered" />         <port id="next_p" type="int" init="1" />         <load service="computation" value="(ip1_load*time_scale)/numbands)" />         <load service="internal_storage" value="1" />         <load service="result_storage" value="1" />         <edge port="p_b" value="[p_b[0], p_b[1]+1]" cnd="p_b[1] &lt; numbands">             <ho service="internal_storage" value="1" />         </edge>         <edge port="next_p" value="if (p_b[1] &lt; numbands) then next_p else next_p+1" />         <edge task="ip2" port="p_b" value="p_b">             <ho service="result_storage" value="1" />         </edge>     </task>  
(b)
fig. 7.18 an application in dseir: (a) xml; (b) graphical representation.
intended abstraction level; it is kept as simple as possible though to facilitate model
transformations to analysis tools.
figure 7.18 shows the speciﬁcations for the download and ip1 tasks of the run-
ning example. the xml representation includes the load and handover speciﬁca-
tions, which in the ﬁgure are omitted from the graphical representation. compar-
ing the dpml model of the previous section with the dseir model, we see some
differences. first of all, all concepts in dseir are independent of any speciﬁc ap-
plication domain, whereas dpml intentionally contains elements from the domain
of professional printing (with built-in concepts like pages, pixeldata pins, and
printer and scanner resources). furthermore, the conversion from pages to bands is7 model-driven design-space exploration for software-intensive embedded systems 27
explicitly visible in the dseir task graph, whereas it is part of the implementation
speciﬁcations in dpml. the most important difference, however, is the fact that
dseir allows to specify task workloads and high-level resource management as-
pects in an abstract way in the application view, via services, loads, and handovers.
this allows a strict decoupling between application and platform aspects, as further
illustrated below. this ﬁts with the goal of dseir as a domain-independent inter-
mediate representation, which should allow to capture a wide diversity of mapping,
scheduling, and resource allocation strategies. for a domain-speciﬁc language like
dpml, predeﬁned solutions for some of these aspects may be acceptable, which
keeps the language simpler and more intuitive for domain engineers.
the semantics of a dseir application model is petri-net like. a task can execute
if all its ports have at least one value and if the condition of the task evaluates to true
for the chosen port values. a task can choose any value from an unordered port or
the ﬁrst value from a ﬁfo port. upon execution start it consumes the chosen value
from each port. when the task is ﬁnished, it executes all its enabled actions in an
atomic fashion, which produces new values in a (sub)set of the target ports.
    <resource id="cpu" capacity="4">         <time service="computation" value="1" />     </resource>      <resource id="ethernet" capacity="1">         <time service="transfer" value="(1000 * time_scale) / 70 * 1024)" />     </resource>      <resource id="b1" capacity="b1_size">         <time service="internal_storage" value="0" />         <time service="result_storage" value="0" />     </resource>      <resource id="b2" capacity="b2_size">         <time service="internal_storage" value="0" />         <time service="result_storage" value="0" />     </resource>  
fig. 7.19 dseir resource deﬁnitions in xml and graphical format.
7.5.2 dseir: the platform view
a platform in dseir consists of a number of resource declarations. figure 7.19
shows a fragment of the dseir platform representation for our running printer ex-
ample. each resource has a capacity which can be read as the number of available
units. furthermore, a resource provides a number of services that tasks can use. a
resource has a certain service time for each service it provides. this service time
equals the number of time units that is needed to process one unit of load. this is
used in combination with the load of a task to compute the task’s execution time.
the platform in the example has a quad-core cpu resource that provides a compu -
tation service; the buffers provide internal storage and result storage28 twan basten et al.
services, which allows to distinguish buffers for input and output data. the service
time of the buffers is set to 0, which corresponds to an inﬁnite processing speed. a
platform can also have a number of parameters, which, like the application parame-
ters, are of type integer. these can be used in expressions in the platform part, such
as the capacity and speed expressions of resources. in the example, the buffer sizes
are parameters (to be optimised in the dse).
in line with the motivation for application-centric modelling laid out in sect. 7.3
and in contrast to dpml models, a dseir platform model is simply a collection of
resources without explicit structure. the structured platform models of dpml con-
form to the typical views of designers, whereas the unstructured models of dseir
ﬁt well with an intermediate representation that should be conceptually as simple as
possible.
7.5.3 dseir: the mapping view
the mapping ties an application to a platform, and consists of an allocator entry for
each task, and priority anddeadline speciﬁcations. an allocator element speciﬁes to
which resources the services required by the application are mapped. furthermore, it
speciﬁes the amount of the resource that is allocated to the task. this amount should
be less than or equal to the resource capacity. an allocator can be preemptive; by
default it is non-preemptive. if it is preemptive then a running task can be preempted
and the preemptive resource can be allocated to another task. if an allocator is non-
preemptive and the available resource capacity is not enough for the task, then the
task cannot run. such resource-arbitration choices are made at time 0 (the start of
system execution) and each time a task ﬁnishes. the priority elements specify the
priority of tasks. by default, tasks have priority 0 (the lowest priority in dseir).
the priority must be an integer expression and can depend on the run-time state,
e.g. the port values for the current execution of the task. this allows full dynamic
priority scheduling. deadline speciﬁcations can be used for schedulability analysis
(see [40]).
figure 7.20 shows fragments of a dseir mapping representation. the visual
representation allows to reuse allocators for multiple tasks. in the running example,
however, each task has its own allocator, with the same name. this is because in
this example each task asks for a unique combination of resources. task ip1, for
instance, is bound to allocator ip1, which provides internal storage through
buffer b1, result storage through buffer b2, and computation through the
cpu resource. allocator ip2 (not shown in fig. 7.20) binds buffer b2 for inter -
nal storage to task ip2, thus, accurately capturing the sharing of b2. priorities
are left unspeciﬁed, resulting in the default (lowest) priority of 0 for all tasks. dead-
lines are also left unspeciﬁed, because they are not used in this example.
the examples given throughout this section show how the intended y-chart sepa-
ration of concerns between application and platform is achieved through a mapping
view. the only interaction is through service deﬁnitions and allocators. application7 model-driven design-space exploration for software-intensive embedded systems 29
    <allocator task="ip1">         <entry service="computation" resource="cpu" amount="1" />         <entry service="internal_storage" resource="b1" amount="1" />         <entry service="result_storage" resource="b2" amount="1" />     </allocator>  
fig. 7.20 mapping in dseir.
and platform deﬁnition can be adapted fully transparently, as long as the service
names do not change. the execution model of dseir is based on dynamic priority-
based preemptive scheduling, which is a generic mechanism that allows designers to
specify their own resource allocation and scheduling strategies through the allocator
deﬁnitions. this ﬁts with the needs of a generic intermediate representation. consid-
ering the domain-speciﬁc language dpml, the resource allocation and scheduling
mechanism is ﬁxed in the native simulator, which on the one hand limits ﬂexibility
but on the other hand relieves designers from the task of specifying these aspects.
7.5.4 dseir: the experiment view
dse involves more than specifying design alternatives. it also requires the deﬁni-
tion of experiments, among others. experiments can also be described in dseir.
figure 7.21 shows an experiment deﬁnition. an experiment deﬁnition contains one
or more experiment entries. if there is more than one, then these experiments are
executed sequentially. an experiment entry has a name, that identiﬁes the type of
experiment, and can contain a model entry. the model entry can specify one or more
models. multiple models are speciﬁed using model parameters that may take differ-
ent values. finally, an experiment contains a number of properties. every experi-
ment type has its own set of properties with their own meaning. these experiment
entries can be seen as invocations of predeﬁned analysis recipes.
the example in fig. 7.21 takes the models for our running example. the ﬁrst
experiment entry analyses all possible buffer size combinations for the buffers
allocated in the main ram memory, for a range of compression factors (pa-
rameters mincf andmaxcf ) and for 100 pages. it performs simulations (us-
ing cpn tools [34], see next section) to explore the throughput (deﬁned by the
‘observers ’ entry) that can be achieved for each combination of buffer sizes.
per combination, 10 simulations are performed (deﬁned by the ‘ number ’ entry).
the other parameters set some values to format the output of the simulations. the30 twan basten et al.
    <experiment name="generate_traces">         <model mapping="mapping.xml">             <pvalue name="numpages" values="100" />             <pvalue name="mincf" values="10" />             <pvalue name="maxcf" values="25" />             <pvalue name="b1_size" values="1,2,3,4" />             <pvalue name="b2_size" values="1,2,3,4" />             <pvalue name="b4_size" values="1,2,3,4" />         </model>         <property name="observers" value="rt(sink)" />         <property name="outputdir" value="traces" />         <property name="number" value="10" />         <property name="timescaledivision" value="1000000.0" />         <property name="objectname" value="p" />         <property name="launchresvis" value="false" />     </experiment>      <experiment name="extract_paralyzer_view">         <property name="sourcedir" value="traces" />         <property name="quantities" value="timeunits per image = rt,                     ram = b1_size * 25 + b2_size * 10 + b4_size * 6.2" />         <property name="launchparalyzer" value="true" />     </experiment>  
fig. 7.21 an experiment in dseir.
second experiment entry takes the output and extracts a pareto space that illustrates
the trade-offs in the space, taking into account the variations that occur due to vari-
ation in compression factors. more details about the latter are given in the section
presenting the diagnostic support in the octopus tool set, sect. 7.7.
the experiment view is an important part of dseir that allows designers to
specify and maintain experiments. it is, for example, straightforward to re-run the
same experiment on variants of a model. the tool set implementation, explained
in some detail in sect. 7.8, is such that it is easy to add new analysis plugins. an
analysis plugin predeﬁnes an analysis recipe, as mentioned above, deﬁning which
tools are called, in which order, and with which parameter settings. this allows for
example to easily add domain-speciﬁc analyses.
7.6 analysis and model transformations
the previous two sections have introduced dpml, which served as an illustration of
domain support that can be provided, and the intermediate representation, dseir.
together, dpml and dseir ﬁll in the modelling perspective in our model-driven
dse philosophy and in the octopus tool set. design alternatives can be captured and
it is possible to deﬁne experiments to explore the space of alternatives. the exper-
iments may perform various types of analysis on the speciﬁed design alternatives.
this section presents three types of analysis supported in octopus.
from an industrial perspective, simulation is the most important analysis tech-
nique. simulation technology is mature and it may serve many different purposes,
ranging from building a basic understanding of system behaviour, to detailed tim-
ing analysis and functional validation. the current tool set uses cpn tools [34] for
simulation of dseir models.7 model-driven design-space exploration for software-intensive embedded systems 31
another class of widely used analysis techniques are model checking techniques
that are based on the underlying principle that a model is exhaustively analysed to
conclude whether or not properties of interest hold for the model at hand. octopus
supports translation to the uppaal [10] model checker. uppaal offers (timed and un-
timed) model checking, which may be used to perform deadlock analysis, property
checking, and timing and schedule optimisation.
finally, for data-intensive applications, like print pipelines, performance and re-
source usage are often dominated by the ﬂow of data through the system and the
operations performed on these data; this is in contrast to control-intensive oper-
ations where communication and synchronisation typically determine the perfor-
mance. specialised dataﬂow analysis techniques allow fast exploration of design
alternatives at a high level of abstraction. octopus supports an experimental inter-
face to the sdf3 [62] analysis tool for dataﬂow analysis.
7.6.1 simulation with cpn tools
coloured petri nets (cpns) [35] are an expressive, precisely deﬁned, and well-es-
tablished formalism, extending classical petri nets with data, time, and hierarchy.
cpns have been used in many domains (e.g. manufacturing, workﬂow management,
distributed computing, and embedded systems). cpn tools provides a powerful
framework for modelling cpns and for performance analysis (stochastic discrete-
event simulation) on cpn models.
dseir as outlined in the previous section deﬁnes a syntax. an earlier version
of dseir has a precisely deﬁned semantics [70], deﬁned by means of a structural
operational semantics. this semantics elegantly separates the y-chart aspects (ap-
plication, platform, and mapping). it would be possible to provide also the current
version with a semantics along these lines. however, since dseir also needs ex-
ecution support, we have chosen to provide both the semantics and the execution
support via a transformation to cpns. the goal of the transformation from dseir
to cpn tools in the octopus tool set is therefore twofold, namely (1) to precisely
deﬁne the semantics of dseir, and (2) to provide execution support for the full
dseir language.
file readercpn 
template 
filecpn file file writercpn 
java 
objectsadd dseir 
model 
informationcpn java 
objects
fig. 7.22 translating dseir speciﬁcations to cpn models.
figure 7.22 illustrates the setup of the transformation from dseir models to
cpns. the basis of the transformation is a cpn template that contains the basic
structure of the cpn model to be generated, the high-level dynamics of the resource
handling, and monitors for producing simulation output. the template is ﬁlled with32 twan basten et al.
the information from a concrete dseir model. the resulting cpn model can then
be simulated by cpn tools. currently, there is an analysis recipe (see sect. 7.8) that
allows to simulate a speciﬁed number of runs of a given model. the execution traces
resulting from these runs can then be further analysed, extracting properties such as
the average throughput or resource utilisation, or observed bounds on performance
properties such as latency or resource usage.
fig. 7.23 top-level view of the cpn model generated for the running example.
figure 7.23 shows the top-level view of the cpn model generated for our running
example. it illustrates the main structure of the cpn template used in the translation
from dseir to cpn tools. any generated model consists of (1) the application
view (block app; a hierarchical transition in cpn terminology), (2) the speciﬁ-
cation of the resource handler (hierarchical transition rh), and (3) the interface
between the two (components app torh,app torhho, and rhtoapp,
called places in cpn terms). the information going from the application model to
the resource handler contains the deﬁnition of a task to be started, the initial load
of this task, and the handovers the task expects to receive. information of the actual
resource amounts (in terms of services) that a task occupies and notiﬁcations of a
task being ﬁnished are communicated from resource handler to application.
figure 7.24 shows the translation of the download task, which is part of the ap-
plication model generated for our running example. the task is split into a start
event (transition download s) and an end event (transition download e). the ﬁrst
event sends task information to the resource handler (with the initial load speci-
ﬁed in fig. 7.18(a)) and puts a token into the waiting place psedownload ; the
second event occurs when the resource handler informs the application layer that
the task has been ﬁnished. when this happens, the complete result storage is sent
as a handover to the next task (see the annotation of the arc to place torhho)
and an incremented page number is sent to place pdownload inport p; the latter
represents the self-loop of the download task in fig. 7.18(b)).
figure 7.25 shows the internals of the resource handler generated for the running
example; we brieﬂy explain the logic of the resource-handling mechanism. transi-
tionupdatehandover ensures that arriving handovers are properly processed; this
transition has the highest priority (500). transition rcvtran accepts new tasks and7 model-driven design-space exploration for software-intensive embedded systems 33
fig. 7.24 a fragment of the generated cpn application model for the running example.
fig. 7.25 the resource handler for the running example.
adds them into the queue of running tasks (place schedtasklist ). transition dis-
patch removes ﬁnished tasks from the queue and informs the application layer. the
actual scheduling is done by the schedule transition which has the lowest priority
(2000) and executes only if the queue is not empty. the function sch(shown on the
left) modiﬁes the task queue, according to the rules deﬁned by the mapping speci-
ﬁcation. place nextinvocation ensures that time progresses only to the ﬁrst moment
when some running task gets done. transition updateremainingtime updates the
load of running tasks to reﬂect progress of time. places newevt andlastupdtime
are auxiliary places to ensure a correct ordering of transitions. note that the ma-
jor part of the dynamics of the resource handler is premodelled and stored in the
template. only bodies of already speciﬁed functions are ﬁlled in when generating a
cpn model for a concrete dseir speciﬁcation.34 twan basten et al.
the translation from dseir to cpn tools is fast; also the simulations themselves
are fast and scalable. however, cpn tools compiles a model into an executable for
performing the simulations. this compilation step is the slowest part in the transfor-
mation and analysis trajectory. for the exploration of large design spaces, in which
many alternative models are simulated, this compilation step may become very time
consuming.
7.6.2 analysis with uppaal
the timed automata formalism extends traditional ﬁnite-state automata with real-
valued clocks [4]. this results in a concise formalism that is well suited to model
state-based systems in which time plays a role. properties of interest for such models
can be phrased in temporal logic. timed computation tree logic (tctl) is a logic
that allows to specify properties with respect to the reachability of states within
speciﬁed time bounds. this allows to specify, for example, that a page should be
processed within a given latency, or that a certain error state should not be reachable.
the fact that tctl is decidable for timed automata [2] has led to the development
of a number of analysis tools, model checkers , that compute whether a given timed
automaton model satisﬁes a given tctl speciﬁcation. this section discusses the
link from dseir to the uppaal [10] model checker. the uppaal input language
extends the lean timed automata formalism with data (integer variables, language
constructs to create c-like structures, etc.) and a c-like language to manipulate data.
these features ease the creation and maintenance of models.
the translation from dseir to uppaal is based on the following principles. first,
every task in the application is translated to a separate uppaal timed automaton,
which has a clock to track the progress of the task. tasks communicate through
global variables that model the ports of the tasks. second, resources are also mod-
elled by global variables. third, tasks read and write these in order to implement the
allocation strategies as deﬁned in dseir.
figure 7.26 shows the uppaal timed automaton for the download task of the
running example. the transition from initialize toidle initialises the port of the
download task with its initial value. the transition from idletoactive models the
start of the task. it picks the ﬁrst port value (index i0) because the port is ﬁfo. fur-
thermore, functions such as claim are called in order to do the bookkeeping with
respect to resources, and the clock xis set to 0. the transition from active toidle
models the completion of the task. the release function releases the resources and
produce generates the values for the tasks’ outgoing edges.
the main strength of the uppaal model checker is that it enables exhaustive
analysis of a dseir model. currently, there are analysis recipes (see sect. 7.8)
to (1) check for deadlock situations, and (2) ﬁnd precise bounds on resource usage
(used memory and queue sizes, for instance) and latency. in addition, an experi-
mental version of uppaal-based schedulability analysis has been implemented (see
[40]). these analysis recipes are only applicable to small to medium-sized models7 model-driven design-space exploration for software-intensive embedded systems 35
x <= maxduration()download_port_p[0] = 1claim(i0), consume(i0), active_download=true, x=0release(), produce(), active_download=false, progresscount++idleinitialize
activehurry!i0 == 0 && i0 < num_download_port_p && enabled_download(i0) && priook()x >= minduration()i0 : int[0, buf_size-1]
fig. 7.26 the uppaal timed automaton of the download task from the running example.
with limited non-determinism, because of the state-space explosion that is inherent
in model checking. state-space explosion refers to the exponential growth of the
state space with increasing model size. in this respect, model checking techniques
contrast with simulation-based techniques which scale much better (but are not ex-
haustive).
not all aspects of the dseir language are translatable to uppaal. the main con-
cept that cannot be translated directly is the concept of preemption. the dseir lan-
guage is targeted at the system level of software-intensive embedded systems. this
motivated the choice to allow dseir to approximate the progress of task execution
by piece-wise linear behaviour. consider, for instance, the situation that two tasks
share the same processor core. the ﬁne-grained division of time that may occur
in reality is approximated by slowing both tasks down by some factor. this can-
not be modelled by timed automata, although an approximation has been presented
in [29, 26]. this approximation, however, fragments the symbolic state space built
by uppaal, which has a strong negative effect on the scalability of uppaal analyses.
two extensions of the uppaal tool provide means to deal with preemptive be-
haviour more elegantly. first, uppaal supports analysis of stopwatch automata
(timed automata in which clocks can be stopped [14]) based on an over-approx-
imation. this is useful to model situations in which a task is completely preempted.
this type of preemption is actually what is covered by the current translation from
dseir to uppaal; the aforementioned approximation is not supported. second, the
statistical model checking (smc) extension of uppaal [18, 13] features networks
of priced timed automata, where clocks may have different rates in different loca-
tions. these networks of priced timed automata are as expressive as general linear
hybrid automata [3]. smc essentially provides stochastic discrete-event simulation
for the combination of the timed automata and tctl formalisms. the linear hybrid
automata formalism is ideal for expressing the piece-wise linear progress of tasks as
described above. the translation to this uppaal variant, however, has not yet been
realised because uppaal-smc has only been developed recently.36 twan basten et al.
besides the fundamental limitation with respect to preemptive behaviour, there
are some practical limitations that have to do with the present uppaal implementa-
tion: (1) the limited range of variables (32 bits for integers and 16 bits for clocks)
sometimes leads to inaccuracies in approximating real values and to scaling prob-
lems in terms of the length and duration of executions being analysed; (2) the model
state in uppaal needs to be statically deﬁned, which does not match well with the
fact that dseir models do not have a priori bounds on the state (the port contents);
this may lead to run-time errors that are typically hard to diagnose.
supporting model transformations from dseir to multiple analysis tools raises
the interesting question of consistency between these transformations. one may pick
up the challenge to formally prove the correctness of a transformation with respect
to the dseir semantics. an illustration of the type of proofs needed to show cor-
rectness of transformations can be found in [26, 71]. those proofs were done in the
context of an earlier version of dseir. we did not provide a formal correctness
proof of the translation to uppaal with respect to the cpn semantics for the cur-
rent version of dseir. pragmatically, when applying the cpn tools and uppaal
analyses on models speciﬁed in the common subset of dseir supported by both
translations, we get the same outcome.
7.6.3 dataﬂow analysis with sdf3
the model transformations discussed in the previous two subsections provide sim-
ulation support for the full dseir language and specialised analysis support for a
subset of the language. the transformation from dseir to dataﬂow is of a different
nature. the target of the transformation is resource-aware synchronous dataflow
(rasdf) [77, 76], which extends the classical synchronous dataflow (sdf) [41]
model of computation with a notion of resources in the style of the y-chart. sdf
has limited expressiveness, but this restriction allows more powerful analysis. it is
for example possible to minimise memory requirements for a given throughput re-
quirement [60, 78].
sdf models tasks by means of actors with a ﬁxed execution time and tasks are
assumed to communicate ﬁxed amounts of data in all their executions. sdf there-
fore does not allow to capture dynamics such as variable execution times and com-
munication rates explicitly. with conservative (worst-case) task execution times and
communication rates, however, it is possible to determine bounds on performance
and resource usage, such as the minimal throughput that can be guaranteed and the
smallest amount of memory that sufﬁces to guarantee that throughput. the model
transformation from dseir to rasdf therefore aims to translate a subset of the
dseir language to rasdf models that are conservative in terms of performance
and resource usage. figure 7.27 illustrates the transformation.
the ﬁgure shows that it is possible to directly transform a restricted subset of
dseir, dseir-rasdf, to rasdf. dseir-rasdf is the subset that simply lim-
its dseir to rasdf models. using static analysis of rasdf models, it is possible7 model-driven design-space exploration for software-intensive embedded systems 37
dseir
rasdf
xml filesdf3
analysisdseir-rasdf file writerconservative 
approximation
fig. 7.27 the transformation from dseir to rasdf/sdf3.
to enlarge the subset of dseir models that can be conservatively captured as a
dseir-rasdf model. dseir-rasdf models can then be exported to the sdf3
tool [62] for analysis, allowing the already mentioned throughput and resource us-
age analysis. more details about this transformation can be found in [48].
the described approach can only be applied to models with limited variation in
task execution times and communication rates. if the variation in execution times
and communication rates is too large, such as for example the variations because
of compression rates in the running example (see sect. 7.7), then the conservative
bounds on the throughput and memory requirements that can be guaranteed are
too loose to be practically useful. as a future extension, it will be interesting to
investigate a link to scenario-aware dataflow (sadf) [67, 79]. sadf allows to
capture a ﬁnite, discrete number of workload scenarios, each characterised by an
sdf model. a workload scenario may correspond to for example different types
of pages (text, image) to be printed. scenario transitions can then be captured in a
state-based model such as a ﬁnite state machine or a markov chain. many analysis
techniques for sdf can be generalised to sadf [61], which therefore provides an
interesting compromise between expressiveness and analysis opportunities.
7.7 diagnostics
the octopus tool set has two tools for visualisation and diagnostics. the resvis
tool, short for resource visualisation, see [59], can be used for detailed analysis of
the behaviour of individual design alternatives. the paralyzer tool, short for pareto
analyzer , see [23], on the other hand, provides pareto analysis and supports trade-off
analysis between different design alternatives. the tools thus are complementary to
each other. they ﬁll in the diagnostics module of fig. 7.4 on page 5.
7.7.1 visualisation and analysis of execution traces
the resvis tool can be used to visualise individual executions of the modelled sys-
tem by means of a gantt chart. a gantt chart shows the task activity and/or resource38 twan basten et al.
usage over time. figure 7.2 on page 4 shows a resvis gantt chart for part of an
event trace from one of the case studies performed with oc ´e-technologies. fig-
ure 7.28 shows a part of an event trace (top) for the running colour copier example
and the accompanying resource plots of buffers b2 and b4 (middle and bottom, re-
spectively). the model for the running example reserves one slot for b1 and four
slots for b2 and b4. the event traces show the execution of the individual tasks. the
resource plots show the resource usage of individual resources. the tasks and re-
source usage blocks can be coloured according to one of some predeﬁned schemes
such as by page number, use case (e.g. printing, scanning), or job (e.g. printing a
speciﬁc number of pages of a given type). this is speciﬁc for the printer domain.
the colouring in figs. 7.2 and 7.28 is by page number.
fig. 7.28 gantt chart and resource plots of buffers b2 and b4 of the running example.
event traces and resource plots are very suitable to study the detailed dynamic
behaviour of a design. they show for example implicit dependencies between tasks
such as unexpected blocking and utilisation of resources. a typical use is to ﬁnd
bottleneck tasks and bottleneck resources. these are tasks and resources that deter-
mine the performance of the system. the octopus tool set contains algorithms to
compute an over-approximation of the set of critical tasks to support this type of
analysis [24].
for instance, fig. 7.29 shows the critical tasks of the trace shown in fig. 7.28.
visualisation of critical tasks can ease the identiﬁcation of bottleneck tasks and re-7 model-driven design-space exploration for software-intensive embedded systems 39
fig. 7.29 the critical tasks in the execution trace of fig. 7.28 are highlighted.
sources. figure 7.29 shows that ip1, ip2, ip3, and ip4 form the critical path. (the
colouring is not visible for the very fast task ip2.) note that the application as shown
in fig. 7.9 has no data dependencies from ip3 to ip2 and also not from ip2 to ip1.
yet, the critical tasks show that ip1 must sometimes wait for ip2, and that ip2 must
sometimes wait for ip3. this is caused by full buffers between these tasks, which
limits parallelism.
fig. 7.30 the critical tasks after increasing the number of slots of b2 to 10 (above), and after an
additional increase of the number of slots of b1 to 2 (below).
figure 7.30 shows critical tasks after an increase of b2 to 10 (top graph) and after
an additional increase of b1 to 2 (bottom graph). both changes positively affect
the throughput of the system. after the last increase, there is little or no room left40 twan basten et al.
for further improvement. the critical path visualisation is a great help for solving
problems with respect to time-related performance issues.
7.7.2 trade-off analysis with uncertain information
design-space exploration is a multi-objective optimisation problem. on the one
hand, there is the design space consisting of all possible design alternatives. on
the other hand, there is the cost space with its multiple cost dimensions, such as
throughput, total memory usage, etc. every design alternative is linked to the cost
space, and the question is to ﬁnd the best design solutions with respect to the consid-
ered cost dimensions. pareto analysis is a well-known way to deal with this [53]. the
octopus tool set uses the paralyzer library to perform pareto analysis, including the
application of constraints and cost functions, and a means to visualise the trade-offs
in two or more dimensions [23]. furthermore, it can cope with uncertainty, which
is typically present in the early phases of system design, by associating a design
alternative with setsof points in the cost space, not with just a single point.
consider the running example introduced in sect. 7.4.1. the dse question asked
is: minimise the amount of memory allocated to the buffers while retaining a min-
imum given throughput. in order to answer this question, a dseir model was cre-
ated, which is parameterised with the three buffer sizes; fragments of this model are
shown in sect. 7.5. the possible values for the buffer sizes are taken from the set
f1;2; : : : ; 10g, which gives a design space of 10 1010=1000 design alterna-
tives. the octopus tool set has been used to compute the throughput and the memory
consumption of this set of design alternatives. the exact analysis through the up-
paal model checker and the analysis recipe for analysing bounds as mentioned in
the previous section turns out to be too slow and the state space becomes too large
because of the workload variations (different compression factors) and the number
of pages in a job. therefore, 30 simulation runs per conﬁguration were made. the
results are shown in fig. 7.31.
the graph shows the cost space of the dse question. the y-axis shows mem-
ory used for buffering; the x-axis shows throughput. the inverse of throughput is
used so less is better. each coloured rectangle represents a pareto-optimal design
alternative. this is a design alternative which is not dominated by any other design
alternative, where a design alternative dominates another one if it is not worse in
any dimension of the cost space and better in at least one dimension. for instance,
the blue rectangle at the right-hand side represents the design alternative in which
all buffers have size 1. this is the cheapest design alternative in terms of memory
usage, but its throughput is low. note that design alternatives are associated with
a subset of the cost space and not with a single point. each design alternative has
variation in its throughput caused by variations (stochastic behaviour) in the input
(not every image is the same, leading to different compression rates). this varia-
tion is visualised by using rectangles of various sizes instead of points in the graph.
because of the use of simulation, the throughput bounds for each of the rectangles7 model-driven design-space exploration for software-intensive embedded systems 41
300350400450500550600650700750800850900
1 / throughput0255075100125150175200225250275300325350375400425memory
fig. 7.31 trade-off view of the total buffer size versus the throughput for the running example.
are only approximations of the true bounds. the graph shows that the throughput
increases with larger buffers. however, using more than 175 units of memory does
not result in a signiﬁcant further increase in performance. the many conﬁgurations
with more than 175 units of memory are on the pareto front because they overlap
in the throughput dimension. they are therefore not dominated by other conﬁgura-
tions. this is a consequence of the fact that design alternatives are associated with
sets of points in the cost space instead of with a single point.
7.8 implementation aspects
the octopus tool set currently consists of various separate applications: the domain-
speciﬁc data path tooling introduced in sect. 7.4, the resvis and paralyzer diag-
nostics tools, which have been discussed in the previous section, and the generic
octopus application which is discussed in this section. the main aim of the oc-
topus application is to provide a formal, ﬂexible, and extensible infrastructure to
efﬁciently solve dse problems that have been modelled in the dseir modelling
language described in sect. 7.5.
a dse problem, by deﬁnition, has a number of design alternatives that need to
be analysed. the analysis runs for different design alternatives are typically inde-42 twan basten et al.
pendent (although it is an interesting topic for further research to investigate in-
cremental dse techniques where analysis results for one design alternative can be
reused for other alternatives). evaluation of design alternatives can be distributed
with little effort over a possibly heterogeneous set of computational nodes. the oc-
topus implementation has built-in support to automatically distribute analyses over
computational nodes and collect results from these analyses. almost any modern
computer has multiple processing cores, so this support is very useful in practice.
the simulations performed for the running example of the high-end colour copier
reported on in the previous section show that distribution indeed can be very effec-
tive. the computation time decreases almost linearly with the number of available
computational nodes.
the extensibility requirement for the octopus tool set, together with the wish for
platform independence and distribution support have led to the decision to build oc-
topus upon the osgi runtime [52] environment. this java-based module framework
facilitates extensible service-oriented architectures. an important feature is the ser-
vice registry which enables publication and lookup of services (implementations
of java interfaces). the modules are so-called osgi bundles, which are plain jar
ﬁles. the osgi framework enables dynamic addition, update, and removal of bun-
dles and of services. typically, bundles use other services to implement their own
service interface. this service orientation often results in loosely coupled and eas-
ily testable components. the osgi implementation that is currently used is apache
felix [5]. the octopus workﬂow treats both models and analysis results as data
that can be transformed and visualised. the octopus implementation architecture
and data-centric approach is similar to that of the macroscopic tools [12] and, more
speciﬁc, the cishell [16] as described by b ¨orner.
analysis plugin
tool plugin
analysis plugin
tool plugin
loadbalancer
network manager
experimentrunner
octopus api + spioctopus platformtool apiplugins
fig. 7.32 high-level architectural overview of the octopus implementation.
figure 7.32 shows the high-level architecture of the octopus implementation. it
consists of the following components:
theoctopus api (application programming interface) consists of several parts.
it contains types to create dseir models, either programmatically in java or7 model-driven design-space exploration for software-intensive embedded systems 43
from an xml description. furthermore, it contains types for analysis results,
such as a generic execution trace format. it also contains the service interfaces of
the octopus platform services that allow developers to program tool and analysis
plugins, as discussed below.
theoctopus spi (service provider interface) contains the service interfaces that
plugin developers should implement and register in the osgi framework to ex-
tend the functionality of the tool set with a new analysis tool.
the octopus platform currently contains components that are necessary to re-
alise the api and spi functionality. most notably, the loadbalancer andnet-
workmanager realise the distribution capabilities of the tool set explained above,
and the experimentrunner provides a user interface to run experiments.
tool plugins are osgi bundles that register implementations for certain spi
types. they facilitate the use of dedicated analysis tools and typically imple-
ment a model transformation from dseir to the input language of the supported
tool, and a transformation from the tool output to a general format speciﬁed in
the octopus api such as the trace format for execution traces. tool plugins are
free to specify their own api, as different tools can have very different func-
tionality. currently, as discussed in sect. 7.6, there are mature plugins for cpn
tools [34] and uppaal [10], and an experimental plugin for the dataﬂow analysis
tool sdf3 [62].
analysis plugins provide a means for the user to use the tool plugins, and they
thus typically use the apis of the tool plugins and the octopus api. these plu-
gins form the analysis recipes that can be used in the dseir experiment view.
they implement part of the spi, in order to register themselves in the octopus
framework. the current version of octopus has the following analysis plugins:
–generatetraces uses the cpn tools plugin to generate a speciﬁed number of
execution traces for a given non-empty set of models, and collects the output
in the form of execution traces. this plugin was used to generate the traces
underlying the pareto analysis shown in fig. 7.31. individual traces can be
visualised by the resvis diagnostics tool.
–verifysystem uses the uppaal plugin to verify that the system satisﬁes some
sanity properties such as, for example, that there is no deadlock.
–randomuppaaltrace uses the uppaal plugin to generate a random trace. it
uses a built-in uppaal facility for generating execution traces, and makes this
available to the octopus user. in general, the generatetraces plugin is faster
though.
–generatebounds uses the uppaal plugin to compute lower and upper bounds
on application latency, resource usage, and port usage (the number of items
present in any of the ports in a dseir application model).
–generateparalyzerview collects performance data created by any of the afore-
mentioned analysis plugins in order to generate a pareto trade-off view as
shown in fig. 7.31.
–criticalpathanalyzer applies critical-path analysis to a speciﬁc trace ﬁle. the
results can then be visualised in resvis, as illustrated in figs. 7.29 and 7.30.44 twan basten et al.
7.9 industrial experiences
we have used octopus in four case studies at oc ´e-technologies. these case studies
all involve design-space exploration of printer data paths of professional printers.
7.9.1 high-performance production black-and-white printing
figure 7.33 shows an abstracted view of an fpga-based data path platform of a
high-performance production black-and-white printer, that supports use cases such
as printing, copying, scanning, scan-to-email, and print-from-store. all required im-
age processing algorithms are realised in the main fpga (the ip blocks in the ﬁg-
ure); the fpga is connected to two memories via a memory bus. the machine can
be accessed locally through the scanner and both locally and remotely through a
print controller. print jobs enter the system through the data store shown in the ﬁg-
ure. the use cases all use different combinations of components in the platform.
a print job arriving from the data store undergoes several image processing steps,
with intermediate results being stored in one of the memories, after which the pro-
cessed result is both sent to the printer block and stored in the data store. the latter
is useful for error recovery and for printing multiple versions of the same document.
a scan job uses the scanner board and several ip blocks, with intermediate results
stored in one of the memories and the ﬁnal result stored in the data store. scan-
ning and printing can execute in parallel, and also within the scan and print image
processing pipelines, tasks may be executed in parallel. resources like the mem-
ory and the associated memory bus, as well as the usb are shared between tasks
and between print and scan jobs running in parallel. moreover, the available usb
bandwidth dynamically ﬂuctuates depending on whether it is used in one or in two
directions simultaneously.
given the characteristics of the use cases and the fpga-based platform, the
data path in this case study can be modelled quite accurately with ﬁxed work-
loads (for the various processing tasks, the memory bus, and the memories) at the
abstraction level of pages. only the usb client shows variation due to the above-
mentioned variation in available bandwidth between unidirectional and bidirectional
use. schedule optimisation using uppaal analysis is therefore feasible. usb be-
haviour can either be approximated with a ﬁxed bandwidth or with a discrete ap-
proximation of the ﬂuctuating behaviour as explained in sect. 7.6.2. the models
we developed were used for determining performance bounds and resource bottle-
necks, for analysing interaction between scanning and printing, and for exploration
of scheduling priorities and resource allocation (memory allocation, page caching)
alternatives. one of the concrete results was an improved task prioritisation for the
static priority scheduling employed in the platform. further, fig. 7.34 shows the
gantt chart of simultaneously running scan (red) and print (blue) jobs. the scan job
is disrupted, because printing has priority on the usb. it only continues after the
print job has ﬁnished. this behaviour materialises because the model lacks a crucial7 model-driven design-space exploration for software-intensive embedded systems 45
fig. 7.33 an abstracted view of the platform of a high-performance production black-and-white
print and scan data path. (figure from [30])
scheduling rule, stating that uploads over the usb are handled in the order of ar-
rival, irrespective of the origin (the scan or print job) of the upload. this scheduling
rule is actually enforced in the print controller, which is a component external to the
data path. the analysis shows the importance of this external scheduling rule.
fig. 7.34 the gantt chart of simultaneously running scan jobs (red) and print jobs (blue).
an interesting conclusion with respect to the octopus tool set is that automat-
ically generated uppaal models are in comparison better tractable than the hand-
crafted models reported on in [29]. longer print jobs can be analysed, due to a
reduction in memory needed by uppaal. analysis times increase though. memory
usage and analysis times strongly depend on the size of the print jobs being anal-
ysed. for latency optimisation of two simultaneously running jobs, memory usage
and analysis time range from kilobytes and seconds for small jobs of a few pages to
gigabytes and hours for large jobs with hundreds of pages. details can be found in
[26].46 twan basten et al.
fig. 7.35 a data path platform template used in several high-end colour copiers. (figure from [8])
7.9.2 high-end colour printing
the other three case studies involved variants of a family of high-end colour copiers.
figure 7.35 shows a template of the data path platform used for these printers. it is a
heterogeneous multi-processor platform that combines one or more cpus (running
microsoft windows) with a gpu, one or more harddisks (hdds), and an fpga.
because of heterogeneity and the use of general cpus, capturing the platform in a
high-level abstraction is more challenging than modelling the platform of the ﬁrst
case study. the variation in workloads due to compression and decompression steps
in the print and scan pipelines adds complexity, as well as the fact that pages are
broken into bands and sometimes even lines to increase pipelining opportunities in
the image processing pipeline. the latter is needed because high-resolution colour
printing and scanning involves much larger volumes of data than black-and-white
printing and scanning. the gantt chart shown in fig. 7.2 is in fact taken from one
of these three case studies, and illustrative for the mentioned challenges. also the
running example is taken from one of these case studies.
the complexity of the models for the colour printer data path case studies is such
that only simulation is sufﬁciently fast to do any practically meaningful analysis. for
the ﬁrst one of these cases, we started out with handcrafted models. later we made
dseir models, and for the last case study also dpml models. automatically gen-
erated cpn models turned out to yield simulation times similar to the handcrafted
cpn models. simulation times range from seconds to minutes, depending on the
size of the jobs being simulated. the translation of a dseir model to a cpn takes
typically less than a second. the time that cpn tools needs to compile a (hand-
crafted or generated) cpn model into a simulation executable is in the order of tens
of seconds. simulation times for dpml models with the native dpml simulator
are of the same order of magnitude as cpn tools simulation times.
our analyses identiﬁed performance bounds for the print and scan pipelines
and resource bottlenecks, and they were used to explore the interaction between7 model-driven design-space exploration for software-intensive embedded systems 47
scanning and printing. buffer requirements were analysed and potential savings in
buffering were identiﬁed. the impact of several changes in the image processing
pipelines were analysed before the changes were realised, and a sensitivity analysis
was performed for task workloads that were not precisely known.
the three colour copier case studies showed that the octopus tools can success-
fully deal with several modelling challenges, like heterogeneous processing plat-
forms with cpus, gpus, fpgas, and various buses, varying and mixed abstrac-
tion levels (pages, bands, lines), preemptive and non-preemptive scheduling, and
stochastic workload variations (due to input variation and caching). the mixing of
abstraction levels in a single model was crucial to obtain the appropriate combina-
tion of accuracy and acceptable simulation speed.
7.9.3 general lessons learned
we can draw some general conclusions from the performed case studies. dpml
and dseir allow to capture industrially relevant dse problems. both dpml and
dseir models can be made with little effort, similar to the time investment needed
for a spreadsheet model. because of the provided modelling templates, creating
a dpml or dseir model takes much less effort than creating a cpn tools or
uppaal model. an important advantage of the use of an intermediate representation
is that one model sufﬁces to use different analysis tools, which means a further,
substantial reduction in modelling effort when compared to handcrafting models for
multiple tools. model consistency is moreover automatically guaranteed. the aspect
that is in practice the most tedious and time-consuming part of the modelling are the
task workload models (processing, bandwidth, and storage requirements). these
workload models are typically estimates based on experience of engineers or based
on proﬁling measurements on partial prototypes or on earlier, similar machines.
note that these workload models are typically independent of the chosen modelling
approach. spreadsheet models, dpml, dseir, cpn tools, and uppaal alike need
the same workload models as input. an important positive observation from the
case studies is that the involved designers all reported a better understanding of the
systems. ultimately, the dse models are envisioned to play an important role in
documenting a design.
7.10 discussion and conclusions
this chapter has presented the octopus view on model-driven design-space ex-
ploration (dse) for software-intensive systems, elaborating on the dse process
and envisioned tool support for this process. model-driven dse supports the sys-
tematic evaluation of design choices early in the development. it has the potential
to replace or complement the spreadsheet-type analysis typically done nowadays.48 twan basten et al.
model-driven dse can thus reduce the number of design iterations, improve prod-
uct quality, and reduce cost.
to facilitate the practical use of model-driven dse, we believe it is important
to leverage the possibilities and combined strengths of the many existing languages
and tools developed for modelling and analysis, and to present them to designers
through domain-speciﬁc abstractions. we therefore set out to develop the octopus
dse framework that intends to integrate languages and tools in a unifying frame-
work. dseir, an intermediate representation for dse, plays a central role in con-
necting tools and techniques in the dse process in a ﬂexible, extensible way, en-
couraging reuse of tools and of models. dseir allows to integrate domain-speciﬁc
modelling, different analysis and exploration techniques, and diagnostics tools in
customisable tool chains. through dseir, model consistency and a consistent in-
terpretation and representation of analysis results can be safeguarded. the current
prototype tools combine simulation and model checking in the octopus framework.
dpml, a domain-speciﬁc modelling language for printer data paths, has been de-
veloped to provide support for the professional printing domain. the ﬁrst industrial
experiences with the octopus approach in the printing domain have been successful.
several challenges and directions for future work remain, both scientiﬁc chal-
lenges and challenges related to industrial adoption of model-driven dse.
first of all, dseir needs further validation, also in other domains. it is already
clear that extensions are needed, so that it covers all aspects of the dse process.
in particular, besides design alternatives and experiments, it would be beneﬁcial to
standardise the language for phrasing dse questions and the format capturing dse
results. this would further facilitate the exchange of information between tools and
the consistent interpretation of results.
another direction for future research are the model transformations to and from
dseir. the three transformations to analysis tools presented in this chapter are all
of a different nature. it is important to precisely deﬁne the types of analysis sup-
ported by a transformation, the properties preserved by the transformation, and its
limitations. also techniques to facilitate (semi-)automatic translations and mainte-
nance of transformations are important, to cope with changes in the octopus frame-
work or the targeted analysis tools.
given precisely deﬁned modelling languages and model transformations, it be-
comes interesting to explore integration of analysis techniques. can we effectively
combine the strengths of different types of analysis, involving for example model
checking, simulation, and dataﬂow analysis? what about adding optimisation tech-
niques such as constraint programming and sat/smt solving? no single analysis
technique is suitable for all purposes. integration needs to be achieved without re-
sorting to one big uniﬁed model, because such a uniﬁed model will not be practically
manageable. but how do we provide model consistency when combining multiple
models and analysis techniques?
on a more fundamental level, integration of techniques leads to the question how
to properly handle combinations of discrete, continuous, and probabilistic aspects.
such combinations materialise from combinations of timing aspects, user interac-7 model-driven design-space exploration for software-intensive embedded systems 49
tions, discrete objects being manipulated, physical processes being controlled, fail-
ures, wireless communication, etc.
scalability of analysis is another important aspect. many of the analysis tech-
niques do not scale to industrial problems. is it possible to improve scalability of
individual techniques? can we support modular analysis and compositional reason-
ing across analysis techniques, across abstraction levels, and for combinations of
discrete, continuous, and probabilistic aspects?
early in the design process, precise information on the workloads of tasks to be
performed and on the platform components to be used is often unavailable. envi-
ronment parameters and user interactions may further be uncontrollable and unpre-
dictable. how can we cope with uncertain and incomplete information? how do we
guarantee robustness of the end result of dse against (small) variations in parameter
values? can we develop appropriate sensitivity analysis techniques?
the increasingly dynamic nature of modern embedded systems also needs to be
taken into account. today’s systems are open, connected, and adaptive in order to
enrich their functionality, enlarge their working range and extend their life time,
to reduce cost, and to improve quality under uncertain and changing circumstances.
system-level control loops play an increasingly important role. what is the best way
to co-design control and embedded hardware and software?
to achieve industrial acceptance, we need systematic, semi-automatic dse meth-
ods that can cope with the complexity of next generations of high-tech systems.
these methods should be able to cope with the many different use cases that a typ-
ical embedded platform needs to support, and the trade-offs that need to be made
between the many objectives that play a role in dse. model versioning and tracking
of decision making need to be supported. model calibration and model validation
are other important aspects to take into account.
model-driven dse as presented in this chapter aims to support decision making
early in the development process. it needs to be connected to other phases in de-
velopment such as coding and code generation, hardware synthesis, and possibly
model-based testing. industrially mature dse tools are a prerequisite. dsl support,
tool chain customisation, integration with other development tools, and training all
need to be taken care of.
in conclusion, the views and results presented in this chapter provide a solid basis
for model-driven dse. the motivation for the work comes from important indus-
trial challenges, which in turn generate interesting scientiﬁc challenges. it is this
combination of industrial and scientiﬁc challenges that makes the work particularly
interesting. the scientiﬁc challenges point out the need for integration of modelling
and analysis techniques, and not necessarily the further development of specialised
techniques. a stronger focus on integration would beneﬁt the transfer of academic
results to industrial practice.
acknowledgements this work has been carried out as part of the octopus project with oc ´e-
technologies b.v . under the responsibility of the embedded systems institute. this project is
partially supported by the netherlands ministry of economic affairs, agriculture, and innovation
under the bsik program.50 twan basten et al.
references
1. aadl (2012). url http://www.aadl.info/ . accessed october 2012
2. alur, r., courcoubetis, c., dill, d.l.: model-checking in dense real-time. inf. comput. 104,
2–34 (1993)
3. alur, r., courcoubetis, c., halbwachs, n., henzinger, t.a., ho, p.h., nicollin, x., olivero,
a., sifakis, j., yovine, s.: the algorithmic analysis of hybrid systems. theor. comput. sci.
138, 3–34 (1995)
4. alur, r., dill, d.l.: a theory of timed automata. theor. comput. sci. 126, 183–235 (1994)
5. apache felix (2012). url http://felix.apache.org/ . accessed october 2012
6. balarin, f., giusto, p., jurecska, a., passerone, c., sentovich, e., tabbara, b., chiodo, m.,
hsieh, h., lavagno, l., sangiovanni-vincentelli, a., suzuki, k.: hardware-software co-
design of embedded systems: the polis approach. kluwer academic publishers, norwell,
ma (1997)
7. basten, t., hendriks, m., somers, l., tr ˇcka, n.: model-driven design-space exploration for
software-intensive embedded systems (extended abstract). in: m. jurdzinski, d. nickovic
(eds.) formal modeling and analysis of timed systems, lecture notes in computer science ,
vol. 7595, pp. 1–6. springer, berlin (2012)
8. basten, t., van benthum, e., geilen, m., hendriks, m., houben, f., igna, g., reckers, f.,
de smet, s., somers, l., teeselink, e., tr ˇcka, n., vaandrager, f., verriet, j., v oorhoeve, m.,
yang, y .: model-driven design-space exploration for embedded systems: the octopus toolset.
in: t. margaria, b. steffen (eds.) leveraging applications of formal methods, veriﬁcation,
and validation, lecture notes in computer science , vol. 6415, pp. 90–105. springer, heidel-
berg (2010). url http://dse.esi.nl/ . accessed october 2012
9. basu, a., bozga, m., sifakis, j.: modeling heterogeneous real-time components in bip. in:
proceedings of the fourth ieee international conference on software engineering and for-
mal methods (sefm 2006), pp. 3–12 (2006)
10. behrmann, g., david, a., larsen, k.g., hakansson, j., pettersson, p., yi, w., hendriks, m.:
uppaal 4.0. in: proceedings of the third international conference on the quantitative eval-
uation of systems (qest06), pp. 125–126 (2006). url http://www.uppaal.com/ .
accessed october 2012
11. bleuler, s., laumanns, m., thiele, l., zitzler, e.: pisa – a platform and programming lan-
guage independent interface for search algorithms. in: c.m. fonseca, p.j. fleming, e. zitzler,
k. deb, l. thiele (eds.) evolutionary multi-criterion optimization, lecture notes in com-
puter science , vol. 2632, pp. 494–508. springer, berlin (2003). url http://www.tik.
ee.ethz.ch/pisa/ . accessed october 2012
12. b ¨orner, k.: plug-and-play macroscopes. commun. acm 54, 60–69 (2011)
13. bulychev, p.e., david, a., larsen, k.g., miku ˇcionis, m., poulsen, d.b., legay, a., wang,
z.: uppaal-smc: statistical model checking for priced timed automata. in: proceedings of
the 10th workshop on quantitative aspects of programming languages and systems (qapl
2012), pp. 1–16 (2012)
14. cassez, f., larsen, k.: the impressive power of stopwatches. in: c. palamidessi (ed.) con-
cur 2000 concurrency theory, lecture notes in computer science , vol. 1877, pp. 138–152.
springer, berlin (2000)
15. cofluent design, cofluent studio (2012). url http://www.cofluentdesign.
com/ . accessed october 2012
16. cyberinfrastructure shell (2012). url http://cishell.org/home.html . accessed
october 2012
17. davare, a., densmore, d., meyerowitz, t., pinto, a., sangiovanni-vincentelli, a., yang, g.,
zeng, h., zhu, q.: a next-generation design framework for platform-based design. in: pro-
ceedings of the 2007 design and veriﬁcation conference (dvcon 2007) (2007)
18. david, a., larsen, k.g., legay, a., miku ˇcionis, m., poulsen, d.b., van vliet, j., wang, z.:
statistical model checking for networks of priced timed automata. in: formal modeling
and analysis of timed systems, lecture notes in computer science , vol. 6919, pp. 80–96.
springer, berlin (2011)7 model-driven design-space exploration for software-intensive embedded systems 51
19. derler, p., lee, e.a., matic, s.: simulation and implementation of the ptides programming
model. in: proceedings of the 12th ieee/acm international symposium on distributed sim-
ulation and real-time applications (ds-rt ’08), pp. 330–333 (2008)
20. eker, j., janneck, j.w.: cal language report speciﬁcation of the cal actor language. erl
technical memo ucb/erl m03/48, university of california, berkeley, ca (2003)
21. eker, j., janneck, j.w., lee, e.a., liu, j., liu, x., ludvig, j., neuendorffer, s., sachs, s.,
xiong, y .: taming heterogeneity - the ptolemy approach. proc. ieee 91, 127–144 (2003)
22. esterel technologies, scade suite (2012). url http://www.
esterel-technologies.com/products/scade-suite . accessed october
2012
23. hendriks, m., geilen, m., basten, t.: pareto analysis with uncertainty. in: proceedings of
the 9th ieee/ifip international conference on embedded and ubiquitous computing (euc
2011), pp. 189–196 (2011)
24. hendriks, m., vaandrager, f.w.: reconstructing critical paths from execution traces. in: pro-
ceedings of the 10th ieee/ifip international conference on embedded and ubiquitous com-
puting (euc 2012) (2012)
25. holzmann, g.j.: the spin model checker: primer and reference manual. addison-wesley,
boston, ma (2004). url http://spinroot.com/ . accessed october 2012
26. houben, f., igna, g., vaandrager, f.: modeling task systems using parameterized partial or-
ders. int. j. softw. tools technol. transf. (2012). accepted for publication
27. hsu, c.j., keceli, f., ko, m.y ., shahparnia, s., bhattacharyya, s.s.: dif: an interchange
format for dataﬂow-based design tools. in: a.d. pimentel, s. vassiliadis (eds.) computer
systems: architectures, modeling, and simulation, lecture notes in computer science , vol.
3133, pp. 3–32. springer, berlin (2004)
28. ibm ilog cplex optimizer (2012). url http://www.ibm.com/cplex/ . accessed
october 2012
29. igna, g., kannan, v ., yang, y ., basten, t., geilen, m., vaandrager, f., v oorhoeve, m., de smet,
s., somers, l.: formal modeling and scheduling of data paths of digital document printers.
in: f. cassez, c. jard (eds.) formal modeling and analysis of timed systems, lecture notes
in computer science , vol. 5215, pp. 170–187. springer, heidelberg (2008)
30. igna, g., vaandrager, f.: veriﬁcation of printer datapaths using timed automata. in: t. mar-
garia, b. steffen (eds.) leveraging applications of formal methods, veriﬁcation, and valida-
tion, lecture notes in computer science , vol. 6416, pp. 412–423. springer, heidelberg (2010)
31. improvise (2012). url http://www.cs.ou.edu/ ˜weaver/improvise/index.
html . accessed october 2012
32. jackson, d.: software abstractions: logic, language, and analysis. mit press, cambridge,
ma (2006)
33. jackson, e.k., kang, e., dahlweid, m., seifert, d., santen, t.: components, platforms and
possibilities: towards generic automation for mda. in: proceedings of the tenth acm inter-
national conference on embedded software (emsoft 2010), pp. 39–48 (2010)
34. jensen, k., kristensen, l., wells, l.: coloured petri nets and cpn tools for modelling and
validation of concurrent systems. int. j. softw. tools technol. transf. 9, 213–254 (2007)
35. jensen, k., kristensen, l.m.: coloured petri nets: modelling and validation of concurrent
systems. springer, berlin (2009)
36. ja v a genetic algoritms package (2012). url http://jgap.sourceforge.net/ .
accessed october 2012
37. katoen, j.p., zapreev, i.s., hahn, e.m., hermanns, h., jansen, d.n.: the ins and outs of the
probabilistic model checker mrmc. in: proceedings of the sixth international conference
on the quantitative evaluation of systems (qest 2009), pp. 167–176 (2009)
38. keinert, j., streub ¨uhr, m., schlichter, t., falk, j., gladigau, j., haubelt, c., teich, j., meredith,
m.: systemcodesigner–an automatic esl synthesis approach by design space exploration
and behavioral synthesis for streaming applications. acm trans. des. autom. electron. syst.
14, article no. 1 (2009). url http://www12.informatik.uni-erlangen.de/
research/scd/ . accessed october 201252 twan basten et al.
39. kienhuis, b., deprettere, e., vissers, k., van der wolf, p.: an approach for quantitative anal-
ysis of application-speciﬁc dataﬂow architectures. in: proceedings of the 1997 ieee inter-
national conference on application-speciﬁc systems, architectures and processors (asap
’97), pp. 338–349 (1997)
40. kumar, a.: adding schedulability analysis to the octopus toolset. master’s thesis, eindhoven
university of technology, faculty of mathematics and computer science, design and anal-
ysis of systems group, eindhoven (2011)
41. lee, e.a., messerschmitt, d.g.: static scheduling of synchronous data ﬂow programs for
digital signal processing. ieee trans. comput. 36, 24–35 (1987)
42. lukasiewycz, m., glaß, m., reimann, f., teich, j.: opt4j: meta-heuristic optimization frame-
work for java. in: proceedings of the 13th annual conference genetic and evolutionary
computing conference (gecco 2011), pp. 1723–1730 (2011). url http://opt4j.
sourceforge.net/ . accessed october 2012
43. mathworks - global optimization toolbox - solve multiple maxima, multiple minima,
and nonsmooth optimization problems (2012). url http://www.mathworks.com/
products/global-optimization . accessed october 2012
44. mathworks - simevents - discrete-event simulation software (2012). url http://www.
mathworks.com/products/simevents/ . accessed october 2012
45. mathworks - simulink - simulation and model-based design (2012). url http://www.
mathworks.com/products/simulink/ . accessed october 2012
46. mldesign technologies, mldesigner (2012). url http://www.mldesigner.com/ .
accessed october 2012
47. modelica and the modelica association (2012). url http://www.modelica.org/ .
accessed october 2012
48. moily, a.: supporting design-space exploration with synchronous data ﬂow graphs in the oc-
topus toolset. master’s thesis, eindhoven university of technology, faculty of mathematics
and computer science, software engineering and technology group, eindhoven (2011)
49. nikolov, h., thompson, m., stefanov, t., pimentel, a., polstra, s., bose, r., zissulescu, c.,
deprettere, e.: daedalus: toward composable multimedia mp-soc design. in: proceedings
of the 45th annual design automation conference (dac 2008), pp. 574–579 (2008). url
http://daedalus.liacs.nl/ . accessed october 2012
50. nusmv (2012). url http://nusmv.fbk.eu/ . accessed october 2012
51. open systemc initiative (osci) (2012). url http://www.systemc.org/
52. osgi alliance: osgi service platform release 4 (2012). url http://www.osgi.org/
specifications/homepage . accessed october 2012
53. pareto, v .: manual of political economy (manuale di economia politica). kelley, new york
(1971 (1906)). translated by a. s. schwier and a. n. page
54. prism (2012). url http://www.prismmodelchecker.org/ . accessed october
2012
55. prom - process mining workbench (2012). url http://www.promtools.org/
prom6/ . accessed october 2012
56. qt - a cross-platform application and ui framework (2012). url http://qt.nokia.
com/products/ . accessed october 2012
57. rtctoolbox: modular performance analysis with real-time calculus (2012). url http:
//www.mpa.ethz.ch/rtctoolbox/ . accessed october 2012
58. sander, i., jantsch, a.: system modeling and transformational design reﬁnement in forsyde.
ieee trans. comput.-aided des. integr. circuits syst. 23, 17–32 (2004)
59. schindler, k.: measurement data visualization and performance visualization. internship re-
port, eindhoven university of technology, department of mathematics and computer science
(2008)
60. stuijk, s., geilen, m., basten, t.: throughput-buffering trade-off exploration for cyclo-static
and synchronous dataﬂow graphs. ieee trans. comput. 57, 1331–1345 (2008)
61. stuijk, s., geilen, m., theelen, b., basten, t.: scenario-aware dataﬂow: modeling, analysis
and implementation of dynamic applications. in: proceedings of the 2011 international con-
ference on embedded computer systems: architectures, modeling, and simulation (samos
xi), pp. 404–411 (2011)7 model-driven design-space exploration for software-intensive embedded systems 53
62. stuijk, s., geilen, m.c.w., basten, t.: sdf3: sdf for free. in: proceedings of the 6th in-
ternational conference on application of concurrency to system design (acsd 2006), pp.
276–278 (2006)
63. symtavision symta/s (2012). url http://www.symtavision.com/symtas.
html/ . accessed october 2012
64. sysml (2012). url http://www.sysml.org/ . accessed october 2012
65. teeselink, e., somers, l., basten, t., tr ˇcka, n., hendriks, m.: a visual language for mod-
eling and analyzing printer data path architectures. in: proceedings of the industry track of
software language engineering 2011 (itsle 2011), pp. 1–20 (2011)
66. theelen, b.d., florescu, o., geilen, m.c.w., huang, j., van der putten, p.h.a., v oeten, j.p.m.:
software/hardware engineering with the parallel object-oriented speciﬁcation language. in:
proceedings of the 5th ieee/acm international conference on formal methods and models
for codesign (memocode ’07), pp. 139–148 (2007)
67. theelen, b.d., geilen, m.c.w., basten, t., v oeten, j.p.m., gheorghita, s.v ., stuijk, s.: a
scenario-aware data ﬂow model for combined long-run average and worst-case performance
analysis. in: proceedings of the fourth acm and ieee international conference on formal
methods and models for codesign (memocode 2006), pp. 185–194 (2006)
68. timedoctor (2012). url http://sourceforge.net/projects/timedoctor/ .
accessed october 2012
69. tr ˇcka, n., hendriks, m., basten, t., geilen, m., somers, l.: integrated model-driven design-
space exploration for embedded systems. in: proceedings of the international conference on
embedded computer systems: architectures, modeling, and simulation (samos xi), pp.
339–346 (2011). url http://dse.esi.nl/ . accessed october 2012
70. tr ˇcka, n., v oorhoeve, m., basten, t.: parameterized timed partial orders with resources: for-
mal deﬁnition and semantics. es report esr-2010-01, eindhoven university of technology,
department of electrical engineering, electronic systems group, eindhoven (2010)
71. tr ˇcka, n., v oorhoeve, m., basten, t.: parameterized partial orders for modeling embedded
system use cases: formal deﬁnition and translation to coloured petri nets. in: proceedings of
the 11th international conference on application of concurrency to system design (acsd
2011), pp. 13–18 (2011)
72. uml - object management group (2012). url http://www.uml.org . accessed octo-
ber 2012
73. uml proﬁle for marte: modeling and analysis of real-time and embedded systems
(2012). url http://www.omgmarte.org/ . accessed october 2012
74. van beek, d.a., collins, p., nadales, d.e., rooda, j.e., schiffelers, r.r.h.: new concepts in
the abstract format of the compositional interchange format. in: proceedings of the 3rd ifac
conference on analysis and design of hybrid systems (adhs 2009), pp. 250–255 (2009)
75. viskic, i., yu, l., gajski, d.: design exploration and automatic generation of mpsoc plat-
form tlms from kahn process network applications. in: proceedings of the acm sig-
plan/sigbed 2010 conference on languages, compilers, and tools for embedded sys-
tems (lctes ’10), pp. 77–84 (2010)
76. yang, y .: exploring resource/performance trade-offs for streaming applications on embedded
multiprocessors. ph.d. thesis, eindhoven university of technology, eindhoven (2012)
77. yang, y ., geilen, m., basten, t., stuijk, s., corporaal, h.: exploring trade-offs between per-
formance and resource requirements for synchronous dataﬂow graphs. in: proceedings of the
7th ieee/acm/ifip workshop on embedded systems for real-time multimedia (estime-
dia 2009), pp. 96–105 (2009)
78. yang, y ., geilen, m., basten, t., stuijk, s., corporaal, h.: automated bottleneck-driven
design-space exploration of media processing systems. in: proceedings of the conference
on design, automation and test in europe (date 2010), pp. 1041–1046 (2010)
79. yang, y ., geilen, m., basten, t., stuijk, s., corporaal, h.: playing games with scenario- and
resource-aware sdf graphs through policy iteration. in: proceedings of the conference on
design, automation and test in europe (date 2012), pp. 194–199 (2012)
80. yices: an smt solver (2012). url http://yices.csl.sri.com/ . accessed october
2012