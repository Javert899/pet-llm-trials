process mining: discovering direct successors in process logs 
laura maruster1, a.j.m.m.(ton) weijters1, w.m.p.(wil) van der aalst1,  
 and antal van den bosch2  
1 eindhoven university of technology, 5600 mb eindhoven, the netherlands 
{l.maruster, a.j.m.m.weijters, w.m.p.aalst}@tm.tue.nl   
 2 tilburg university, 5000 le tilburg, the netherlands 
antal.vdnbosch@kub.nl  
abstract. workflow management technology requires the existence of explicit process 
models, i.e. a completely specified workflow design needs to be developed in order to 
enact a given workflow process. such a workflow design is time consuming and often 
subjective and incomplete. we propose a learning method that uses the workflow log, 
which contains information about the process as it is actually being executed. in our 
method we will use a logistic regression model to discover the direct connections be-
tween events of a realistic not complete workflow log with noise. experimental results 
are used to show the usefulness and limitations of the presented method. 
1   introduction 
the managing of complex business processes calls for the development of powerful informa-
tion systems, able to control and support the flow of work. these systems are called workflow 
management systems (wfms), where a wfms is generally thought of as “a generic software 
tool, which allows for definition, execution, registration and control of workflows” [1]. de-
spite the workflow technology promise, many problems are encountered when applying it. 
one of the problems is that these systems require a workflow design, i.e. a designer has to 
construct a detailed model accurately describing the routing of work. the drawback of such 
an approach is that the workflow design requires a lot of effort from the workflow designers, 
workers and management, is time consuming and often subjective and incomplete.  
instead of hand-designing the workflow, we propose to collect the information related to 
that process and discover the underlying workflow from this information history. we assume 
that it is possible to record events such that (i) each event refers to a task, (ii) each event re-
fers to a case and (iii) events are totally ordered. we call this information history the work-
flow log. we use the term process mining for the method of distilling a structured process 
description from a set of real executions. 
to illustrate the idea of process mining, consider the workflow log from table 1. in this 
example, there are seven cases that have been processed and twelve executed tasks. we can 
notice the following: for each case, the execution starts with task a and ends with task l, if c 
is executed, then e is executed. also, sometimes we see task h and i after g and h before g. 
using the information shown in table 1, we can discover the process model shown in figure 
1. we represent the model using petri nets [1]. the petri net model starts with task a and 
finishes with task l. after executing a, either task b or task f can be executed. if task f is 
executed, tasks h and g can be executed in parallel. table 1. an example of a workflow log 
af
bd
c eh
g ik
jl
 
fig. 1. a process model for the log shown in table 1 
a parallel execution of tasks h and g means that they can appear in any order.  
the idea of discovering models from process logs was previously investigated in contexts 
such as software engineering processes and workflow management [2-9]. cook and wolf 
propose three methods for process discovery in case of software engineer processes: a finite 
state-machine method, a neural network and a markov approach [3]. their methods focus on 
sequential processes. also, they have provided some specific metrics for detection of concur-
rent processes, like entropy, event type counts, periodicity and causality [4]. herbst and 
karagiannis used a hidden markov model in the context of workflow management, in the case 
of sequential processes [6,8,9] and concurrent processes [7]. in the works mentioned, the 
focus was on identifying the dependency relations between events. in [10], a technique for 
discovering the underlying process from hospital data is presented, under the assumption that 
the workflow log does not contain any noisy data. a heuristic method that can handle noise is 
presented in [11]; however, in some situations, the used metric is not robust enough for dis-
covering the complete process.  
in this paper, the problem of process discovery from process logs is defined as: (i) for each 
task, find its direct successor task(s), (ii) in the presence of noise and (iii) when the log is 
incomplete. knowing the direct successors, a petri net model can be constructed, but we do 
not address this subject in the present paper, this issue is presented elsewhere [10, 11]. 
it is realistic to assume that workflow logs contain noise. different situations can lead to 
noisy logs, like input errors or missing information (for example, in a hospital environment, a 
patient started a treatment into hospital x and continues it in the hospital y; in the workflow 
log of hospital y we cannot see the treatment activities that occurred in hospital x).  
the novelty of the present approach resides in the fact that we use a global learning ap-
proach, namely we develop a logistic regression model and we find a threshold value that can 
be used to detect direct successors. as basic material, we use the “dependency/frequency 
tables”, as in [11]. in addition to the “causality metric” that indicates the strength of the causal 
relation between two events used in [11], we introduce two other metrics. 
the content of this paper is organized as follows: in section 2 we introduce the two new 
metrics that we use to determine the “direct successor” relationship and we recall the “causal-
ity metric” introduced in [11]. the data we use to develop the logistic regression model is 
presented in section 3. section 4 presents the description of the logistic regression model and 
two different performance test experiments are presented. the paper concludes with a discus-
sion of limitations of the current method and addresses future research issues.  case number executed tasks 
case 1 a f g h i k l 
case 2 a b c e j l 
case 3 a f h g i k l 
case 4 a f g i h k l 
case 5 a b c e j l 
case 6 a b d j l 
case 7 a b c e j l 2   succession and direct succession  
in this section we discuss some issues relating the notion of succession and we define the 
concept of direct succession. furthermore, we describe three succession metrics that we used 
to determine the direct succession relationship. at the end of this section we give an example 
of dependency/frequency table, with the corresponding values of the three metrics. 
2.1   the succession and direct succession relations 
before introducing the definitions of succession and direct succession relations, we have to 
define formally the notion of workflow log and workflow trace.   
definition 1: (workflow trace, workflow log) let t be a set of tasks. θ∈t* is a workflow 
trace and w⊆t* is a workflow log. we denote with #l the count of all traces θ . 
an example of a workflow log is given in table 1. a workflow trace for case 1 is a f g h i 
k l. for the same workflow log from table 1, #l = 7.   
definition 2: (succession relation) let w be a workflow log over t, i.e. w⊆t*. let a, b∈t*. 
then: 
• b succeeds a (notation a>wb) if and only if there is a trace θ = t1t2…tn-1 and i ∈ {1,…, 
n-2} such that θ∈w and ti=a and ti+1=b. 
in the log from table 1, a >w f, f >w g, b >w c, h >w g, etc.  
• we denote (a>b) = m, m≥0, where m is the number of cases for which the relation a>wb 
holds. for example, if for the log w, the relation a>wb holds 100 times and the relation 
b>wa holds only 10 times, then (a>b) = 100 and (b>a) = 10.  
definition 3: (direct succession relation) let w be a workflow log over t, i.e. w⊆t* and a, 
b∈t . then b directly succeeds a (notation a→wb) if either: 
1. (a>b) > 0 and (b>a) = 0  
or 
2. (a>b) > 0 and (b>a) > 0 and ((a>b) – (b>a) ≥ σ), σ > 0. 
 
let us consider again the petri net from figure 1. a pair of two events can be in three pos-
sible situations and subsequently the relations between the events are: 
a) if events c and e are in sequence, i.e. (c>e) > 0 and (e>c) = 0, then c>we and c→we. 
b) if there is a choice between events b and f, i.e. (b>f) = 0 and (f>b) = 0, then b>/w f, 
f>/w b, b→/w f, f→/w b. 
c) if events g and h are in parallel, i.e. (g>h) > 0 and (h>g) > 0, then g>wh, h>wg, 
g→/w h, h→/w g. 
the first condition from definition 3 says that if for a given workflow log w, only b suc-
ceeds a and a never succeeds b, then there is a direct succession between a and b. this will 
holds if there is no noise in w. however, if there is noise, we have to consider the second 
condition for direct succession, because both (a>b) > 0 and (b>a) > 0. the problem is to 
distinguish between a situation when (i) a and b are occurring in parallel and (ii) when a and 
b are really in a direct succession relation, but there is noise. in the rest of the paper we de-
scribe the methodology of finding the threshold value σ. 
in order to find the threshold value σ, we use three metrics of the succession relation, 
which are described in the next subsection. 2.2   the local metric (lm), global metric (gm) and causality metric (cm) 
the local metric lm. considering tasks a and b, the local metric lm is expressing the 
tendency of succession relation by comparing the magnitude of (a>b) versus (b>a). 
the idea of lm measure presented below is borrowed from statistics and it is used to calcu-
late the confidence intervals for errors.  
,1)1(96.1+−−=nppplm ,1)(
+>=nbap )()( abban >+>= .  
we are interested to know with a probability of 95% the likelihood of succession, by com-
paring the magnitude of (a>b) versus (b>a). for example, if (a>b)=30, (b>a)=1 and 
(a>c)=60, (c>a)=2, which is the most likely successor of a: b or c? although both ratios 
(a>b)/(b>a) and (a>c)/(c>a) equal 30, c is more likely than b to be the successor of a. 
our measure gives in case of a and b a value of lm=0.85 and in case of a and c a value of 
lm=0.90, which is in line with our intuition.  
let us now consider again the examples from figure 1. if we suppose that the number of 
lines in the log #l=1000, we can have three situations: (i) (c>e)=1000, (e>c)=0, lm= 
0.997; (ii) (h>g)=600, (g>h)=400, lm=0.569; (iii) (f>b)=0, (b>f)=0, lm=0. in the se-
quential case (i), because always e succeeds c, lm≈1. when h and g are in parallel, in case 
(ii), lm=0.569, thus a value much smaller than 1. in the case of choice between f and b, in 
case (iii), lm=0. in general, we can conclude that lm can have a value (i) close to 1 when 
there is a clear tendency of succession between x and y, (ii) in the neighborhood of 0.5 when 
there is both a succession between x and y and between y and x, but a clear tendency can-
not be identified and (iii) zero when there is no succession relation between x and y. 
the global metric gm. the previous measure lm was expressing the tendency of succes-
sion by comparing the magnitude of (a>b) versus (b>a) at a local level. let us consider that 
the number of traces in our log #l=1000, the frequency of events #a=1000, #b=1000 and 
#c=1000. we also know that (a>b)=900, (b>a)=0 and (a>c)=50 and (c>a)=0. the ques-
tion is who is the most likely successor of a: b or c? for b, lm=0.996 and for c, 
lm=0.942, so we can conclude that they can be both considered as successors. however, one 
can argue that c succeeds a not as frequently, thus b should be considered a more likely 
successor. therefore, we build the gm measure presented below. 
balabbagm*###))()(( >−>= .  
applying the formula above, we obtain for b as direct successor a value of gm=0.90, while 
for c, gm=0.05, thus b is more likely to directly succeeds a. in conclusion, for determining 
the likelihood of succession between two events a and b, the gm metric is indeed a global 
metric because it takes into account the overall frequency of events a and b, while the lm 
metric is a local metric because it compares the magnitude of (b>a) with (a>b). 
the causality metric cm. the causality metric cm was first introduced in [11]. if for a 
given workflow log when task a occurs, shortly later task b also occurs, it is possible that 
task a causes the occurrence of task b. the cm metric is computed as following: if task b 
occurs after task a and n is the number of events between a and b, then cm is incremented 
with a factor (δ)n, where δ is a causality factor, δ∈[0.0,1.0]. we set δ=0.8. the contribution to 
cm is maximal 1, if task b appears right after task a and then n=0. conversely, if task a 
occurs after task b, cm is decreased with (δ)n. after processing the whole log, cm is divided 
by the minimum between the overall frequency of a and the overall frequency of b. 2.3   the dependency/frequency table 
the starting point of our method is the construction of a so-called dependency/frequency 
(d/f) table from the workflow log information. an excerpt from the d/f table for the petri 
net presented in figure 1 is shown in table 2. the information contained in the d/f table are: 
(i) the identifier for task a and b, (ii) the overall frequency of task a (#a), (iii) the overall 
frequency of task b (#b), (iv) the frequency of task b directly succeeded by another task a 
(b>a), (v) the frequency of task a directly succeeded by another task b (a>b), (vi) the fre-
quency of b directly or indirectly succeeded by another task a, but before the next appear-
ance of b (b>>>a), (vii) the frequency of a directly or indirectly succeeded by another task 
b, but before the next appearance of a (a>>>b), (viii) the local metric lm, (ix) the global 
metric gm and (x) the causality metric cm. the last column (ds) from table 2 is discussed 
in the next section. 
table 2. example of d/f table with direct succession (ds column) information. “t” means that task b 
is a direct successor of task a, while “f” means that b is not a direct successor of a  
ab #a #b (b>a) (a>b) (b>>>a) (a>>>b) lm gm cm ds 
b a 536 1000 536 0 536 0 0.00 -1.0 -1.0 f 
b b 536 536 0 0 0 0 0.00 0.00 0.00 f 
b d 536 279 0 279 0 279 0.99 1.86 1.00 t 
b j 536 536 0 0 0 536 0.00 0.00 0.72 f 
b l 536 1000 0 0 0 536 0.00 0.00 0.57 f 
b c 536 257 0 257 0 257 0.99 1.86 1.00 t 
b e 536 257 0 0 0 257 0.00 0.00 0.80 f 
3   data generation 
for developing a model that will be used to decide when two events are in direct succes-
sion relation, we need to generate training data that resemble real workflow logs. our data 
generation procedure consists on combinations of the following four possible elements that 
can vary from workflow to workflow and subsequently affect the workflow log: 
• number of event types: we generate petri nets with 12, 22, 32 and 42 event types. 
• amount of information in the workflow log: the amount of information is expressed by 
varying the number of traces (one trace or line actually represents the processing of one case) 
starting with 1000, 2000, 3000, etc. and end with 10000 traces. 
• amount of noise: we generate noise performing four different operations, (a) delete the 
head of a event sequence, (b) delete the tail of a sequence, (c) delete a part of the body and 
(d) interchange two random chosen events. during the noise generation process, minimal one 
event and maximal one third of the sequence is deleted. we generate three levels of noise: 0% 
noise (the common workflow log), 5% noise and 10% (we select 5% and respectively 10% of 
the original event sequences and we apply one of the four above described noise generation 
operations). 
• unbalance in and/or splits: in figure 1, after executing the event a, which is an or-
split, it is possible to exist an unbalance between executing tasks b and f. for example, 80% of cases will execute task b and only 20% will execute task f. we progressively produced 
unbalance at different levels.  
for each log that resulted from all possible combinations of the four elements mentioned 
before we produce a d/f table. in the d/f table a new field is added (the ds column) which 
records if there is a direct succession relationship between events a and b or not 
(true/false). an example of the d/f table with direct succession information is shown in 
table 2. all d/f tables are concatenated into one unique and large final d/f/ds table that 
will be used to build the logistic regression model. 
4   the logistic regression model 
we have to develop a model that can be used to determine when two events a and b are in a 
direct succession relationship. the idea is to combine the three metrics described earlier and 
to find a threshold value σ over which two events a and b can be considered to be in the 
direct succession relationship. in this section we develop a logistic regression model and we 
perform some validation tests. 
the logistic regression estimates the probability of a certain dichotomic characteristic to 
occur. we want to predict whether “events a and b are in a direct succession relationship”, 
that can be true/false. therefore, we set as dependent variable the ds field from the d/f/ds 
file. the independent variables are the three metrics that we built earlier, i.e. the global metric 
gm, the local metric lm and the causality metric cm. in conclusion, we want to obtain a 
model that, given a certain combination of lm, gm and cm values for two events a and b, 
to predict the probability π of a and b being in the direct succession relationship.  
the form of the logistic regression is log( π/(1-π) ) = b0 + b1*lm + b2 *gm +  b3*cm, 
where the ratio π/(1-π) represents the odds. for example, if the proportion of direct succes-
sors is 0.2, the odds equal 0.25 (0.2/0.8=0.25). the significance of individual logistic regres-
sion coefficients bi is given by the wald statistics which indicates significance in our model; 
that means that all independent variables have a significant effect on direct succession pre-
dictability (wald tests the null hypothesis that a particular coefficient bi is zero). the model 
goodness of fit test is a chi-square function that tests the null hypothesis that none of the 
independents are linearly related to the log odds of the dependent. it has a value of 
108262.186, at probability p<.000, inferring that at least one of the population coefficients 
differs from zero. the coefficients of the logistic regression model are shown in table 3.  
table 3. logistic analysis summary of three succession predictors of direct succession relation. the 
discrete dependent variable ds measures the question “are events a and b in a direct succession rela-
tionship?”; ** means significant at p<0.01 
variables in the equa-
tion* b wald df sig** exp(b) 
lm 6.376 2422.070 1 .000 587.389 
gm 4.324 920.638 1 .000 75.507 
cm 8.654 4490.230 1 .000 5735.643 
constant -8.280 4561.956 1 .000 .000 
 
using the bi coefficients from table 5, we can write the following expression lr from eq. 1:  lr = -8.280 + 6.376*lm+ 4.324*gm+ 8.654*cm  (1) 
then the estimated probabilityπˆcan be calculated with the following formula (eq.2): 
)1/(ˆlr lree+=π . (2) 
the influence of lm, gm and cm can be detected by looking at column exp(b) in table 
3. for example, when cm increases one unit, the odds that the dependent =1 increase by a 
factor of ~5736, when the others variables are controlled. comparing between gm, lm and 
cm, we can notice that cm is the most important variable in the model. 
inspecting the correct and incorrect estimates we can assess the model performance. our 
model predicts the t value of ds in 95,1% of cases and the f value of ds in 99,2% cases. 
these values for correct/incorrect estimates are obtained at a cut value of 0.8, i.e. are counted 
as correct estimates those values that exceed 0.8. we set the cut value at 0.8, because we are 
interested in knowing the classification score when the estimated probability is high. because 
95% of the events which are in direct succession relationship are correctly predicted by the 
model, we conclude that we can set the threshold σ = 0.8. that means that we will accept that 
there is a direct successor relationship between events a and b, if the estimated probability 
would exceed 0.8. the following step is to test the model performance on test material.  
model testing. we describe two different type of tests: (i) k-fold cross-validation on test 
material extracted from the learning material and (ii) model check on a completely new test 
material.  
k-fold cross-validation (k-fold cv) is a model evaluation method that can be used to see 
how well a model will generalizes to new data of the same type as the training data. the data 
set is divided into k subsets. each time, one of the k subsets is used as the test set and the 
other k-1 subsets are put together to form a training set. then the average error across all k 
trials is computed. every data point gets to be in a test set exactly once, and gets to be in a 
training set k-1 times. the variance of the resulting estimate is reduced as k is increased. we 
take k=10. the results of our 10-fold cv gives for the 10 training sets an average performance 
of 95.1 and for the 10 testing sets an average performance of 94.9, so we can conclude that 
our model will perform good in case of new data. 
in order to test the model performance on completely new data, we build a new more com-
plex petri net with 33 event types. this new pn has 6 or-splits, 3 and-splits and three 
loops (our training material contains petri nets with at most one loop).  we consider three 
petri nets with three different levels of unbalance and using the formula from eq. 2, we pre-
dict the probability of direct succession for the petri net. for these three petri nets, we 
counted the number of direct successors correctly found with our method. the average of 
direct successors that were correctly found is 94.3. therefore we can conclude that even in 
case of completely new data, i.e. a workflow log generated by a more complex petri net, the 
method has a good performance of determining the direct successors.  
5   conclusions and future directions 
using the presented method, we developed a model that estimates the probability that two 
events a and b are in the direct successor relation. the model performance is good, i.e. 95% 
of the original direct succession relations were found. however, it is interesting to investigate 
what is the reason that the rest of 5% direct connections were not discovered. inspecting these cases, we notice that although between event a and b there is a direct succession rela-
tion, the value of (a>b) is too small, and subsequently, the values for all three metrics are 
also small. to illustrate such a situation, inspect figure 1. if we suppose that event h is al-
ways processed in 1 time unit, event g in 3 time units and i in 2 time units and h always 
finishes its execution before i starts, then we will always see the sequence “afhgikl” and 
never the sequence “afgihkl”. although k is the direct successor of h, the method will 
not find the connection between h and k.  
in conclusion, we presented a global learning method that uses information contained in 
workflow logs to discover the direct successor relations between events. the method is able 
to find almost all direct connections in the presence of parallelism, noise and an incomplete 
log. also, we tested our model on a workflow log generated by a more complex petri net than 
the learning material, resulting in a performance close to that of the first experiment.  
we plan to do future research in several directions. first, because in many applications, the 
workflow log contains a timestamp for each event, we want to use this additional information 
to improve our model. second, we want to provide a method to determine the relations be-
tween the direct successors and finally to construct the petri net. 
references 
 
[1] w.m.p. van der aalst. the application of petri nets to workflow management. j. of circuits, 
systems, and computers, 8(1): 21-66, 1998. 
[2] r. agrawal, d. gunopulos, and f. leymann. mining process models from workflow logs. in 
sixth international conference on extended database technology, pg. 469-483, 1998.  
[3] j.e. cook and a.l. wolf. discovering models of software processes from event-based data, 
acm transactions on software engineering and methodology, 7(3):215-249, 1998. 
[4] j.e. cook and a.l. wolf. event-based detection of concurrency. in proceedings of the sixth 
international symposium on the foundations of software engineering (fse-6), orlando, fl, no-
vember 1998, pp. 35-45.  
[5] j.e. cook and a.l. wolf. software process validation: quantitatively measuring the correspon-
dence of a process to a model. acm transactions on software engineering and methodology, 8(2): 
147-176, 1999. 
[6] j. herbst. a machine learning approach to workflow management.  in 11th european conference 
on machine learning, volume 1810 of lecture notes in computer science, pages 183-194, 
springer, berlin, germany, 2000. 
[7] j. herbst. dealing with concurrency in workflow induction in u. baake, r. zobel and m. al-
akaidi, european concurrent engineering conf., scs europe, gent, belgium, 2000. 
[8] j. herbst and d. karagiannis. an inductive approach to the acquisition and adaptation of work-
flow models. in m. ibrahim and b. drabble, editors, proceedings of the ijcai’99 workshop on in-
telligent workflow and process management: the new frontier for ai in business, pg. 52-57, 
stockholm, sweden, august 1999. 
[9] j. herbst and d. karagiannis. integrating machine learning and workflow management to support 
acquisition and adaptation of workflow models. international journal of intelligent systems in ac-
counting, finance and management, 9:67-92, 2000. 
[10] l. maruster, w.m.p. van der aalst, t. weijters, a. van den bosch, w. daelemans. automated 
discovery of workflow models from hospital data. in kröse, b. et al. (eds.): proceedings 13th bel-
gium-netherlands conference on artificial intelligence (bnaic’01), 25-26 october 2001, amster-
dam, the netherlands, pp. 183-190. 
[11] t. weijters, w.m.p. van der aalst.  process mining: discovering workflow models from event-
based data. in kröse, b. et. al, (eds.): proceedings 13th belgium-netherlands conference on artifi-
cial intelligence (bnaic’01), 25-26 october 2001, amsterdam, the netherlands, pp. 283-290. 