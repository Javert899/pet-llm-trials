mediating between modeled and observed
behavior: the quest for the “right” process
wil m.p. van der aalst
department of mathematics and computer science
eindhoven university of technology
po box 513, nl-5600 mb eindhoven, the netherlands
www: vdaalst.com
abstract —operational processes leave trails in the information
systems supporting them. such event data are the starting point
for process mining – an emerging scientiﬁc discipline relating
modeled and observed behavior. whereas an event log describes
example behavior of the underlying process, a process model
aims to describe an abstraction of the same process. models may
be descriptive or normative. descriptive models aim to describe
the underlying process and are used for discussion, performance
analysis, obtaining insights, and prediction. normative models
describe the desired behavior and are used for workﬂow manage-
ment, system conﬁguration, auditing, compliance management,
and conformance checking. differences between modeled and
observed behavior may point to undesirable deviations or in-
adequate models. in this paper, we discuss challenges related to
ﬁnding the “right” process, i.e., the process model that describes
the real underlying process or a process that behaves as desired.
i. i ntroduction
recently, process mining emerged as a new scientiﬁc
discipline on the interface between process models and event
data [1]. conventional business process management (bpm)
[2] and workﬂow management (wfm) [3] approaches and tools
are mostly model-driven with little consideration for event
data. data mining (dm) [4], business intelligence (bi), and
machine learning (ml) [5] focus on data without considering
end-to-end process models. process mining aims to bridge the
gap between bpm and wfm on the one hand and dm, bi,
and ml on the other hand (cf. figure 1).
the practical relevance of process mining is increasing as
more and more event data become available (cf. the recent
attention for “big data”). process mining techniques aim to
discover, monitor and improve real processes by extracting
knowledge from event logs . the two most prominent process
mining tasks are: (i) process discovery : learning a process
model from example behavior recorded in an event log, and (ii)
conformance checking : diagnosing and quantifying discrepan-
cies between observed behavior and modeled behavior.
starting point for any process mining task is an event log .
each event in such a log refers to an activity (i.e., a well-
deﬁned step in some process) and is related to a particular
case (i.e., a process instance ). the events belonging to a case
are ordered, and can be seen as one “run” of the process. such
a run is often referred to as a trace . it is important to note that
an event log contains only example behavior, i.e., we cannot
assume that all possible runs have been observed.
given a process model (discovered or made by hand) and
an event log one can try to align modeled and observed
process 
mining
data-oriented analysis  
(data mining, machine learning, business intelligence)process model analysis  
(simulation, verification, optimization, gaming, etc.)
performance-
oriented 
questions, 
problems and 
solutionscompliance-
oriented 
questions, 
problems and 
solutionsfig. 1. process mining is on the interface between process model analysis
and data-oriented analysis and can be used to answer a variety of performance
and compliance-related questions.
behavior . an alignment relates a trace in a event log to its
corresponding path in the model. if there is not a direct match,
the trace is aligned with the closest or most likely path. such
alignments can be used to answer performance-oriented and
compliance-oriented questions (cf. figure 1). alignments can
be used to show how often paths are taken and activities are
being executed. moreover, events often bear a timestamp which
can be used to compute ﬂow times, waiting times, service
times, etc. for example, alignments can be used to highlight
bottlenecks in the process model. similarly, alignments can
be used to show where model and event log disagree. this is
commonly referred to as conformance checking .
this paper focuses on the relation between modeled and
observed behavior and considers two use cases: (i) evaluating
the results of process discovery (“how good is the model i
discovered?”) and (ii) comparing discrepancies between some
normative or descriptive model and an event log (“where do
model and log disagree?”). the latter question is relevant
for compliance-related questions (e.g., auditing). also note
that discrepancies between model and event log may point to
problems in the model (e.g., the model is a poor representation
of reality and as such has no predictive power) or problems in
the process described by the model (e.g., people deviate from
the desired process or cases are handled too late). the secondpart of the title of this paper – the quest for the “right”
process – refers to both types of problems.
real 
processevent 
dataprocess 
modelrecordprocess 
discovery
conformance 
checkingprocess 
discovery
conformance 
checking“unknown”“only examples”
fig. 2. process discovery and conformance checking relate a descriptive or
normative process model to an unknown process using event data.
unlike existing approaches, we initially assume that the
“real process” is known to better understand the quality of pro-
cess discovery and conformance checking. figure 2 provides
an overview of the problem investigated in this paper. the
real process is generally unknown. we can only see the event
data generated by this process (arrow record ). these event
data can be used to discover a process model (arrow process
discovery ). moreover, the process model (discovered or man-
made) and the event log can be compared as illustrated by the
solid double-headed arrow conformance checking . however,
both process discovery and conformance checking aim to tell
something about the unknown real process rather than the
example traces in the event log (see the two dashed arrows
in figure 2).
in the remainder we investigate the relationship between
the real process, the event log, and a given process model .
section ii reviews existing approaches and literature related to
conformance checking. section iii provides formal deﬁnitions
for processes, process models, and event logs. using these
deﬁnitions and assuming that the real process is known, we
discuss various precision andrecall notions in section iv. in
section v the assumption that the real process is known is
dropped and event data are used to “guess” the real process.
section vi reﬂects on the ﬁndings in the previous sections and
discusses additional challenges. related work is considered in
section vii. section viii concludes the paper.
ii. e xisting conformance checking approaches
to introduce conformance checking and to describe some
of the existing approaches, we use the process model shown
in figure 3. a labeled petri net is used to represent the
process. a petri net is a bipartite graph consisting of places
and transitions. transitions are the active components and
places are the passive components. places may contain tokens.
the network structure does not change, but transitions may
consume and produce tokens. a transition may have a label
describing the corresponding activity. for example, transition
t1models activity “register request” having short name aandt2models the skipping of activity “examine ﬁle” modeled by
transitiont3. transitions without a label, e.g., t2, are invisible
and do not correspond to actual activities.
the process starts with a token in place start and ends
with a token in place end. figure 3 allows for traces such as
ha;c;b;d;f;gi,ha;c;d;hi, andha;b;c;d;e;c;d;g;f i(because
of the loop there are inﬁnitely many possible traces). the
notation itself is not relevant: any other notation could have
been used (e.g., bpmn, uml activity diagrams, epcs, bpel,
etc.).
anevent log is a multiset of traces. each trace is a
sequence of activities. multiple cases may follow the same
trace. an example log is l1= [ha;c;d;f;gi10;ha;c;d;hi5;
ha;b;c;d;e;c;d;g;f i5].l1contains information about 20
cases, e.g., 10 cases followed trace ha;c;d;f;gi. figure 3
shows only three of these 20 cases. there are 105+54+
59 = 115 events in total. l2= [ha;c;d;fi10;ha;c;d;c;hi5;
ha;b;d;e;c;d;g;f;h i5]is another event log also containing 20
cases.
conformance checking techniques investigate how well an
event logland a process model mﬁt together. let mbe the
labeled petri net in figure 3. clearly, l1is perfectly ﬁtting m
whereasl2is not.
a. four quality dimensions for comparing model and log
there are four quality dimensions for comparing model
and log: (1) ﬁtness , (2) simplicity , (3) precision , and (4)
generalization [1]. a model with good ﬁtness allows for most
of the behavior seen in the event log. a model has a perfect
ﬁtness if all traces in the log can be replayed by the model
from beginning to end. the simplest model that can explain
the behavior seen in the log is the best model. this principle is
known as occam’s razor . fitness and simplicity alone are not
sufﬁcient to judge the quality of a discovered process model.
for example, it is very easy to construct an extremely simple
petri net (“ﬂower model”) that is able to replay all traces in an
event log (but also any other event log referring to the same
set of activities). similarly, it is undesirable to have a model
that only allows for the exact behavior seen in the event log.
remember that the log contains only example behavior and
that many traces that are possible may not have been seen yet.
a model is precise if it does not allow for “too much” behavior.
clearly, the “ﬂower model” lacks precision. a model that is not
precise is “underﬁtting”. underﬁtting is the problem that the
model over-generalizes the example behavior in the log (i.e.,
the model allows for behaviors very different from what was
seen in the log). at the same time, the model should generalize
and not restrict behavior to just the examples seen in the log.
a model that does not generalize is “overﬁtting”. overﬁtting
is the problem that a very speciﬁc model is generated whereas
it is obvious that the log only holds example behavior (i.e., the
model explains the particular sample log, but there is a high
probability that the model is unable to explain the next batch
of cases).
figure 4 shows that the four quality dimensions may be
competing. just like the four forces enabling a airplane to ﬂy
(lift, gravity, drag, and thrust), one needs to balance ﬁtness,
simplicity, precision, and generalization when discovering pro-
cess models from event logs.a
starta = register request
b = examine file
c = check ticket
d = decide
e = reinitiate request 
f = send acceptance letter
g = pay compensation
h = send rejection letterb
cdg
h
eendc1
c2c3
c4c5t1f
t2
t3
t4t5
t6t7t8
t9
t10t11
c6c7 c8
c9
a,c,d,f,g
a,b,c,d,e,c,d,g,f
a,c,d,hprocess 
model
event 
logfig. 3. a process model represented using a labeled petri net; the event log shows three example traces that could have been generated by this process.
fitness
simplicitygeneralizationprecision
process
mining
fig. 4. the four forces of process mining: lift = ﬁtness (ability to explain
observed behavior), gravity = simplicity (occam’s razor), drag = precision
(avoiding underﬁtting), and thrust = generalization (avoiding overﬁtting).
many ﬁtness notions have been deﬁned in literature [1],
[6], [7], [8], [9], [10], [11], [12]. independent of the notion
used, all approaches will indicate that l1is perfectly ﬁtting
mwhereasl2is clearly not. precision can be quantiﬁed by
counting “escaping edges” [13], [10], [11]. also notions for
simplicity and generalization have been deﬁned [1], [6], [14].
the problem is that many different conformance metrics can
be deﬁned. these metrics all aim to capture an intuitive notion
and struggle with the problem that the real process is unknown.
b. token based replay
a na ¨ıve approach towards conformance checking would
be to simply count the fraction of cases that can be “parsed
completely”. in terms of figure 3, this would be the fraction of
cases corresponding to ﬁring sequences leading from the state
with a token in start to the state with a token in end. letmbe
the process model in figure 3 and l1andl2as deﬁned before.
100% of the cases in l1are ﬁttingmwhereas 0%of the cases
inl2are ﬁttingm. however, the latter example shows that
such a na ¨ıve approach is unable to distinguish between an
almost ﬁtting trace and a trace that has little in common with
any path in the model.
therefore, more sophisticated approaches have been devel-oped. a good example is the token-based replay approach that
counts produced p, consumed c, missingm, and remaining
rtokens [1], [12]. consider the perfectly ﬁtting trace 1=
ha;c;d;f;giand the labeled petri net in figure 3. initially,
all four counters are set to zero: p=c=m=r= 0. then
the environment produces token for place start to initialize the
process:p= 1. when ﬁring transition t1for the ﬁrst activity
in(a), one token is consumed and two tokens are produced:
p= 1 + 2 = 3 andc= 0 + 1 = 1 . when executing t4for
the second activity in (c), one token is consumed and one
token is produced: p= 3 + 1 = 4 andc= 1 + 1 = 2 . to
execute the third activity in (d), we ﬁrst need to execute t2:
p= 4+1 = 5 andc= 2+1 = 3 . then we can execute t5which
consumes two tokens and produces one token: p= 5 + 1 = 6
andc= 3 + 2 = 5 . etc. after replaying the entire trace,
the environment consumes the token from place endto close
the process. after this the total number of produced tokens is
p= 11 and the total number of consumed tokens is c= 11 .
clearly, there are no problems when replaying the 1, i.e.,
there are no missing or remaining tokens ( m=r= 0).
the ﬁtness of a case with trace is deﬁned as follows
[12]:
tness (;m) =1
2
1 m
c
+1
2
1 r
p
the ﬁrst parts computes the fraction of missing tokens relative
to the number of consumed tokens. 1 m
c= 1 if there are
no missing tokens ( m= 0) and 1 m
c= 0 if all tokens to
be consumed were missing ( m=c). similarly, 1 r
p= 1 if
there are no remaining tokens and 1 r
p= 0 if none of the
produced tokens was actually consumed. tness (1;m) = 1
because there are no missing or remaining tokens.
let us now consider a trace that cannot be replayed prop-
erly:2=ha;b;d;hi. the ﬁrst two activities can be executed
without any problems; ﬁre t1andt2. when trying to execute t5
for the third activity in 2(d), we encounter a problem because
placec4is empty. we record the missing token m= 1 and
continue. at the end a token remains in place c2. hence,r= 1.
after replaying 2completely: p= 6,c= 6,m= 1, and
r= 1. hence, tness (2;m) =1
2(1 1
6) +1
2(1 1
6)0:83.the same approach can be used to compute the ﬁtness
of an event log consisting of many cases: tness (l;m ).
simply take the sums of all produced, consumed, missing,
and remaining tokens, and apply the same formula. if multiple
cases follow the same trace, the produced, consumed, missing,
and remaining tokens should be multiplied accordingly.
c. aligning event log and process model
token-based replay has problems when dealing with more
complex process models having duplicate and silent activities
and event logs with long traces. the results may be misleading
and there is no explicit relation between model and log.
alignments were introduced to tackle these problems [6], [7],
[13], [15], [16]. consider the following three alignments for
the traces in l1and modelmin figure 3:
1=acdfg
acdfg
t1t4t2t5t7t8t9t11
2=acdh
acdh
t1t4t2t5t10
3=abcdecdgf
abcdecdgf
t1t3t4t5t6t4t2t5t7t9t8t11
the top row of each alignment corresponds to “moves in the
log” and the bottom two rows correspond to “moves in the
model”. moves in the model are represented by the transition
and its label. this is needed because there could be multiple
transitions having the same label. if a move in the model
cannot be mimicked by a move in the log, then a “ ” (“no
move”) appears in the top row. for example, in the third
position of 1the log cannot mimic the invisible transition
t2. theabovet2indicates that t2does not correspond to
a visible activity. note that all “no moves” (i.e., the seven 
symbols) in 1 3are “caused” by invisible transitions.
two example alignments for l2and process model min
figure 3:
4=acdf
acdfg
t1t4t2t5t7t8t9t11
5=acdch
acdh
t1t4t2t5t10
alignment4shows a “” (“no move”) in the top row
that does not correspond to an invisible transition. the model
makes agmove (occurrence of transition t9) that is not in
the log. if a move in the log cannot be mimicked by a move
in the model, then a “ ” (“no move”) appears in the bottom
row. for example, in 5the secondcmove in the log is not
mimicked by a move in the model. note that the “no moves”
not corresponding to invisible transitions point to deviations
between model and log.
amove is a pair (x;(y;t))where the ﬁrst element refers
to the log and the second element refers to the model. for
example, (a;(a;t1))means that both log and model makean “amove” and the move in the model is caused by
the occurrence of transition t1.(;(g;t9))means that the
occurrence of transition t9with labelgis not mimicked by
a corresponding move of the log. (c;)means that the log
makes an “cmove” not followed by the model.
analignment is a sequence of legal moves such that after
removing allsymbols, the top row corresponds to a trace
in the log and the bottom row corresponds to a ﬁring sequence
starting in the initial state (token in start) and ending in the
ﬁnal state (token in end).1–3are examples of alignments
for the traces in l1and their corresponding ﬁring sequences
in figure 3. 4and5are examples of alignments for the ﬁrst
two traces in l2and complete ﬁring sequences of the same
process model.
given a log trace and a process model there may be many
(if not inﬁnitely many) alignments. consider the following two
alignments forha;c;d;fi2l2:
4=acdf
acdfg
t1t4t2t5t7t8t9t11
0
4=acdf
acbdh
t1t4t3t5t7t10
4seems to be better alignment than 0
4because it has only
one deviation (move in model only; (;(g;t9))) whereas0
4
has three deviations: (;(b;t3)),(f;), and (;(h;t10)).
to select the most appropriate one, we associate costs to
undesirable moves and select an alignment with the lowest total
costs. to quantify the costs of misalignments we introduce a
cost function . moves where log and model agree have no
costs, i.e.,(x;(x;t)) = 0 for allx2a. moves in model only
have no costs if the transition is invisible, i.e., (;(;t)) = 0 .
(;(x;t))>0is the cost when the model makes an “ x
move” without a corresponding move of the log. (x;)>0
is the cost for an “ xmove” in just the log. these costs may
depend on the nature of the activity, e.g., skipping a payment
may be more severe than sending too many letters. the cost
function can also be used to assign a penalty to events that are
executed too late or by the wrong person [17].
anoptimal alignment has the lowest possible costs, i.e., the
observed trace is related to a path in the model that is “closest”
in terms of the cost function . if the optimal alignment has
non-zero costs, there was a deviation. the total cost of an
optimal alignment quantiﬁes non-conformance. it is possible to
convert such misalignment costs into a ﬁtness values between
0 (poor ﬁtness, i.e., maximal costs) and 1 (perfect ﬁtness, zero
costs). this can be done at the level of a single trace or at the
level of an entire event log. we refer to [6], [7] for details.
the notion of an optimal alignment can be used to create
a so-called oracle . given a full or partial trace, the oracle
returns the most likely path in the model (or a set of paths).
the resulting aligned path(s) can be used to evaluate the full
or partial trace, e.g., to compute costs, time, risks, energy
consumption, etc. hence, there may be different cost functions
to evaluate paths returned by the oracle. in fact, the cost
functions and oracle can also be used to make predictions about
the completion of cases.once the oracle established an optimal alignment for every
trace in the event log, these alignments can also be used
as a basis to quantify other conformance notations such as
precision and generalization [6]. for example, precision can
be computed by counting “escaping edges” as shown in [13],
[10], [11]. alignments can also be used to project additional
information extracted from event logs onto the model. for
example, timestamps in the event log can be used to identify
bottlenecks in the process and to predict delays in running
processes [1], [18], [19].
d. problem
the approaches based on replay and alignments illustrate
that there are various ways to quantify ﬁtness. literature
identiﬁes four quality dimensions for comparing model and log
[1], [6], [14], [20]. unfortunately, these do not seem orthogonal
and for each of the four dimensions different metrics can
be deﬁned [21]. even for an intuitively clear notion such as
replay ﬁtness various metrics are possible [1], [6], [7], [8],
[9], [10], [11], [12], [20], [22]. therefore, we take a step back
and reconsider the relation between modeled and observed
behavior. in order to do this we ﬁrst assume that we know
the real process and use the concepts precision (the fraction of
retrieved items that are indeed relevant) and recall (the fraction
of relevant items that are indeed retrieved) from information
retrieval.
iii. p rocesses , event logs and models
in this section, we formalize the basic concepts used
in the remainder. process models, processes, and event logs
share common notions such as activity andtrace (sequence of
activities).
deﬁnition 1 (universe of activities, universe of traces):
ais the universe of activities , i.e., the set of all possible and
relevant activities. other activities cannot be observed (or are
abstracted from). elements of amay have attributes , e.g.,
costs, resource information, duration information, etc. a trace
2ais a sequence of activities found in an event log or
corresponding to a run of some process model. u=ais
the universe of traces.
we assume that an activity is identiﬁed by attributes
relevant for learning, i.e., irrelevant attributes are removed
and attribute values may be coarsened. jajis the number of
unique activities. process models with hundreds of activities
(or more) tend to be unreadable. in the remainder we will refer
to activities using a single letter (e.g. a), however, an activity
could also be decide (gold;manager;reject )to represent a
decision to reject a gold customer’s request by a manager.
in a process a trace 2 u has a likelihood (). we
assume the process is in steady state and that cases do not
inﬂuence each other.
deﬁnition 2 (process): aprocess is a discrete probability
function2u! [0;1]which assigns a probability ()to
any trace2u.
note thatp
2u() = 1 . an event log is similar to a
process, but each trace has a frequency l()rather than a
likelihood(). formally, an event log is a multiset of sample
traces from a known or unknown process.deﬁnition 3 (event log): anevent logl2i b(u)is a
multiset of observed traces.
the same trace may appear multiple times. for ex-
ample in event log l1= [ha;c;d;f;gi10;ha;c;d;hi5;
ha;b;c;d;e;c;d;g;f i5], traceha;c;d;f;giappears 10 times.
(x) =p
2x()for a set or multiset of traces x
(counting elements multiple times if needed). for example,
(l1) = 10(ha;c;d;f;gi) + 5(ha;c;d;hi) + 5
(ha;b;c;d;e;c;d;g;f i).
like any process model, the labeled petri net in figure 3
deﬁnes a set of possible traces. in this paper, a process model
is deﬁned as a set of traces, i.e., we abstract from the concrete
notation.
deﬁnition 4 (process model): aprocess model is a set of
tracesmu.
a process model splits the universe of traces uinto two
classes:m(all traces possible according to the model) and m
(all traces impossible according to the model).
deﬁnition 5 (complement): m=unmis the comple-
ment of some model m.
in figure 2 we used the terms real process, event data,
and process model. the real process is represented by process
02u ! [0;1]. the event data are represented by event
logl2i b(u). the process model obtained through process
discovery or used for conformance checking is represented by
m1u.
let us assume that we also have m0u as the “ideal” or
“desired” model derived from the real process 0. for example,
m0could be based on 0and some predeﬁned threshold.
consider the following two exemplary model derivations:
m0=f2uj0()gfor some trace probability
threshold2[0;1].
m0u such thatm0is the smallest set that satisﬁes:
0(m0)and82m0802m00()>  0(0)for
some threshold 2[0;1].
the latter way of computing m0can be explained intuitively,
e.g., if= 0:8, thenm0is the so-called “ 80% model”
covering at least 80% of the process behavior with a preference
for more likely traces. the 80 20rule (also known as the
pareto principle) informally states that often 80% of the
behavior can be explained by 20% of the most frequent traces.
from a more philosophical point of view one could argue
that anything is possible (just wait long enough and it will
happen): murphy’s law for process mining. taking this view-
point, it is not interesting to look for an m0with0(m0) = 1 ,
becausem0=uas a consequence (i.e., the model does not
contain any information).
in the remainder, 12u! [0;1]deﬁnes the probability of
a trace according to the model. in a stochastic process model
(e.g., a generalized stochastic petri net or markov chain [23])
such probabilities are deﬁned explicitly. if the model does not
contain such information, 1is estimated (e.g., assume equal
probabilities for all choices or use domain knowledge and/or
historic information).the following table summarizes the conventions used in
the remainder:
process model
(probability) (selection)
real process (“ideal”
or “desired”)02u! [0;1]m0u
descriptive or norma-
tive process12u! [0;1]m1u
iv. p recision and recall of models assuming an
a p riori distribution
in reality we do not know the real process 0and only
see event log las a reﬂection of the actual process. however,
assuming that 0,m0,1, andm1are known, we can use
standard information retrieval notions such as precision and
recall .1to illustrate this consider figure 5. the area tp
(true positives) corresponds to all traces that are possible
according to both the ideal/desired model m0and the de-
scriptive/normative model m1. the areafn (false negatives)
corresponds to all traces that are not possible according to
the descriptive/normative model m1, but that are possible
according to the ideal/desired model m0.tn (true negatives)
andfp(false positives) are deﬁned in a similar fashion. tp,
fn,tn, andfp can be used to deﬁne precision and recall.
however, since there are inﬁnitely many possible traces in u
and also models may describe inﬁnitely many traces, we cannot
simply count the number of traces in the four classes. there-
fore, we assign weights to traces based on their likelihood.
however, there are two ways to determine the likelihood: based
on0(real likelihood) or 1(modeled/estimated likelihood).
therefore, there are multiple precision and recall notions.
m0
m0m1ideal or desired model based on 
perfect knowledge of real process
fntp
fp
tndescriptive or normative model 
(man-made or discovered)
fig. 5. comparing m0andm1:tp =m0\m1,tn =m0\m1,
fp =m0\m1, andfn =m0\m1.
deﬁnition 6 (precision and recall): let02u! [0;1]
be the real process having m0u as its “ideal” or “desired”
model. letm1u be the “descriptive” or “normative” model,
and (optionally) 12u! [0;1]a function determining the
1note that the notion of precision used here (i.e., the fraction of retrieved
items that are indeed relevant) is different from the notion of precision used
in figure 4 and section ii-a.likelihood of traces according to the model. tp=m0\m1,
tn=m0\m1,fp=m0\m1, andfn=m0\m1.
precision and recall are deﬁned as follows:
precision0;m0;m1=0(tp)
0(m1)=0(tp)
0(tp) +0(fp)
recall0;m0;m1=0(tp)
0(m0)=0(tp)
0(tp) +0(fn)
precision1;m0;m1=1(tp)
1(m1)=1(tp)
1(tp) +1(fp)
recall1;m0;m1=1(tp)
1(m0)=1(tp)
1(tp) +1(fn)
a
startbend c1t1
t2d
ct3
t4c2e
0.10.9
0.90.1
t5
fig. 6.m0=fha;c;ei;hb;c;ei;ha;c;d;c;ei;:::gand the probabilities are
as shown, e.g., 0(ha;c;ei) = 0:90:9 = 0:81,0(hb;c;ei) = 0:10:9 =
0:09,0(ha;c;d;c;ei) = 0:90:10:9 = 0:081, etc.
a
start end c1t1c
bt2
t3c2e0.9
0.1t4
fig. 7.m1=fha;c;ei;ha;b;eig,1(ha;c;ei) = 0:9, and1(ha;b;ei) =
0:1.
to illustrate these notions consider 0andm0shown in
figure 6 and 1andm1shown in figure 7. tp=fha;c;eig,
0(tp) = 0:81, and1(tp) = 0:9.
precision0;m0;m1=0(tp)
0(m1)=0:81
0:81= 1
recall0;m0;m1=0(tp)
0(m0)=0:81
1= 0:81
precision1;m0;m1=1(tp)
1(m1)=0:9
1= 0:9
recall1;m0;m1=1(tp)
1(m0)=0:9
0:9= 1
next to precision and recall, related notions such as er-
ror((fp[fn)
(u)=(fp[fn)),accuracy ((tp[tn)
(u)=
(tp[tn)), and f1-measure can be deﬁned. the f1-
measure is the harmonic mean of precision and recall:
f;m 0;m1= 2precision;m 0;m1recall;m 0;m1
precision;m0;m1+recall;m 0;m1. note that 
may refer to 0or1.at ﬁrst it may seem odd to consider both 0(real but
generally unknown probabilities) and 1(modeled and possi-
bly inaccurate probabilities). however, both views are needed.
this can be illustrated as follows.
ifm1allows for many traces that rarely or never hap-
pen in reality, then precision0;m0;m1is hardly affected by
this because the contribution of these traces to 0(fp)is
marginal. extending m1with behavior that is impossible
according to 0, does not increase precision0;m0;m1. in fact,
if0(m0) = 0 , then precision0;m0;m1= 1 by deﬁnition.
hence, precision0;m0;m1is unable to detect the loss of
precision. however, adding behavior to m1that is impossi-
ble according to 0will impact precision1;m0;m1as1is
likely to reﬂect the extra behavior allowed by m1. hence,
precision1;m0;m1will be able to signal the loss of precision.
similarly, it seems reasonable to assume that 1(m1) =
0(i.e., traces that are impossible according to the model
have a model-based probability of 0). hence, 1(fn) = 0
and recall1;m0;m1= 1 . this illustrates that recall based
on1tends to give optimistic values. it is better to use
recall0;m0;m1instead.
we cannot avoid using a weight function because there
may be inﬁnitely many traces (due to loops) and we need to
quantify the surfaces in figure 5 in some way. it is essential to
distinguish between highways (frequent paths) and dirt roads
(infrequent paths). from a conformance point of view, it is
more relevant that both models agree on the highways than
their consensus on insigniﬁcant paths. however, using a weight
function complicates matters as 0is unknown and 1may not
be given.
v. p recision and recall based onevent data
in the previous section we did not consider the event log
and assumed that 0,m0,1, andm1are known. typically,
0andm0are unknown. the only things that are known
are event log l, modelm1, and (optionally) 1. therefore,
we use event log lto approximate 0,m0, and possibly 1.
given approximate values landml, we can apply the earlier
deﬁnitions of precision and recall.
figure 8 relates lto the unknown m0and the known
m1. the larger event log lis, the more it will cover m0.
moreover, more likely traces in the real process tend to appear
more frequently in event log l. this observation can be used
to approximate the real process.
deﬁnition 7 (estimator based on log): letl2i b(u)be
an event log. l2u! [0;1]is an estimator of0based on
l:l() =l()
jljfor2u.
just likem0can be derived from 0,mlcan be derived
froml. for example, mlu such thatmlis the smallest
set that satisﬁes: l(ml)and82ml802mll()>
l(0)for some threshold 2[0;1].mlis the estimator
model form0based onl.
if there is no information about 1in the “descriptive” or
“normative” model, we can use replay algorithms to estimate
probabilities for choices [1], [24]. for example, for a choice
betweenaandbin the model we can measure the fraction oftimesa(andb) occurs in the event log and use this to extend
the model with stochastic information.
in the remainder we assume that (1) lis an estimator for
0, (2)mlis an estimator for m0, (3)m1is given, and (4)
1is known (either it is given or estimated based on replaying
loglonm1).
now we can apply all of the earlier computations using
these estimators: tpl=ml\m1,tnl=ml\m1,fpl=
ml\m1,fnl=ml\m1,
precisionl;ml;m1=l(tpl)
l(m1)
recalll;ml;m1=l(tpl)
l(ml)
precision1;ml;m1=1(tpl)
1(m1)
recall1;ml;m1=1(tpl)
1(ml)
consider again the process shown in figure 6 and 1
andm1shown in figure 7. however, now assume that
0andm0are unknown. instead, we have an event log
l= [ha;c;ei16;hb;c;ei2;ha;c;d;c;ei1;ha;c;d;c;d;c;ei1].
l(ha;c;ei) =16
20= 0:8,l(hb;c;ei) =2
20= 0:1,
l(ha;c;d;c;ei) =1
20= 0:05, andl(ha;c;d;c;d;c;ei) =
1
20= 0:05. assumeml=f2lg, i.e., the process model
able to reproduce all observed traces. tpl=fha;c;eig,
l(tpl) = 0:8, and1(tpl) = 0:9.
precisionl;ml;m1=l(tpl)
l(m1)=0:8
0:8= 1
recalll;ml;m1=l(tpl)
l(ml)=0:8
1= 0:8
precision1;ml;m1=1(tpl)
1(m1)=0:9
1= 0:9
recall1;ml;m1=1(tpl)
1(ml)=0:9
0:9= 1
letl=f2u j62lg. by deﬁnition l(l) = 0 ,
so things outside the event log are assumed to be impossible.
ifml=f2 u jl()>0g, thenl(ml) = 1 and
l(ml) = 0 . as a result l(fpl) =l(tnl) = 0 and
precisionl;ml;m1= 1. this is a concern as precision is not
affected by adding non-observed behavior to m1.
ifm1=f2u j1()>0g, then1(m1) = 1 and
1(m1) = 0 . as a result 1(tnl) =1(fnl) = 0 and
recall1;ml;m1= 1.
hence, in practice the only two meaningful metrics are
recalll;ml;m1andprecision1;ml;m1.
if we assume l(ml) = 1 (all observed observed behavior
is possible in ml) and1(m1) = 1 (behavior that is not
modeled is considered to be impossible), these two metrics
can be simpliﬁed as follows:
precision1;ml;m1=1(tpl)
1(m1)=1(tpl) =1(ml)m0
m0m1ideal or desired model based on 
perfect knowledge of real process
fntp
fp tndescriptive or normative model 
(man-made or discovered)
levent log
regular behavior in log 
covered by model
exceptional behavior in log 
covered by model
exceptional behavior in log 
not covered by the modelregular behavior in log not 
covered by model
problem ii: in practice it is 
unclear where this line isproblem i: event log does 
not provide information 
about the whole universe of 
traces only a selected partfig. 8. event log lis ﬁnite and contains only samples of the real process. a trace 2ltypically ﬁts into the unknown ideal or desired model m0. however,
may be infrequent in reality and not part of m0. tracemay ﬁt the descriptive or normative model m1(or not).
recalll;ml;m1=l(tpl)
l(ml)=l(tpl) =l(m1)
these correspond to the traditional precision and ﬁtness
notions [1], [6] discussed in section ii-a. the notion of
ﬁtness (ability to replay event log) in figure 4 corresponds
torecall1;ml;m1and the notion of precision (not allowing
for too much behavior unrelated to event log) in figure 4
corresponds to precision1;ml;m1.
interestingly, in section iv we already concluded that
precision based on 1and recall based on 0seem to be
the only two notions that make sense. hence, whether we
approximate 0bylor not, we come to the same conclusion.
vi. b eyond precision and recall
in the previous section we concluded that recalll;ml;m1
andprecision1;ml;m1are the only two meaningful metrics.
as indicated, recall1;ml;m1refers to the classical notion of
ﬁtness and precision1;ml;m1refers to the classical notion of
precision described in section ii-a. however, figure 4 shows
two additional quality dimensions: simplicity andgeneraliza-
tion.
simplicity (often referred to as occam’s razor) is a concern
not addressed by precision and recall. simplicity seems only
indirectly related to the notions used before (i.e., 0,m0,l,
ml,1, andm1). for example, the complexity of process
modelm1 u should not be deﬁned as the number of
possible traces. a simple process model having a loop has
inﬁnitely many possible traces. concurrency may introduce a
factorial number of possible interleavings, but does not need
to be more complex than a model with choices instead of
concurrency. hence, jm1jis a bad indicator for a model’s
perceived simplicity. consider for example the “ﬂower model”
that is able to generate any trace over some alphabet. themodel is easy to understand and can be represented compactly,
but is the “largest model” in terms of corresponding traces.
there are various techniques to quantify model complexity.
the complexity of the model could be deﬁned by the num-
ber of nodes and arcs in the underlying graph. also more
sophisticated metrics can be used, e.g., metrics that take the
“structuredness” or “entropy” of the model into account. a
detailed discussion of these complexity metrics is outside the
scope of this paper. we refer to [25] for pointers to over twenty
metrics described in literature.
the notion of generalization described in section ii-a
seems to be related to the quality of las an estimator for 0.
a model that does not generalize is “overﬁtting”. overﬁtting
is the problem that a very speciﬁc model is generated whereas
it is obvious that the log only holds example behavior. gen-
eralization can be deﬁned as the probability that the next, not
yet observed, case can be replayed by the process model. in
other words, the probability that for the next observed trace
n:n2m1. it is difﬁcult to reason about generalization
because the notion refers to unseen examples. in [6], [14]
possible metrics are discussed.
if we have an event log lwith just a few traces and most
traces are unique, then lis a poor estimator for 0. in this
case, the next trace nis likely to be different from all traces
seen before. hence, a model allowing for just the traces in l
will not allow for n. if we have an event log with many traces
and most traces appear many times, then lis a much better
estimator for 0due to the strong law of large numbers that
states that the sample average converges to the expected value
(assuming independence between the different cases). hence,
a model allowing for the traces in lwill, most likely, also
allow forn. these extreme examples illustrate the connection
between generalization in figure 4 and the quality of las
an estimator for the unknown 0.in the context of discovery algorithms, often a notion of
completeness is deﬁned. an event log is complete if it contains
enough example behavior to discover the underlying process.
for example, in the context of the algorithm [26] a log is
complete if all direct successions have been observed, i.e., if
acan be followed by bin the process it should be observed
at least once in the event log. clearly, completeness is related
to generalization and the quality of las an estimator for 0.
the above reasoning reconﬁrms the necessity of the four
conventional quality dimensions described in section ii-a.
this is quite surprising since a completely different starting
point was used. unlike existing process mining literature, our
reasoning is based on the assumption that we know the real
process0and we consider probabilities explicitly (rather
than considering only the possibility of a trace). yet, we
come to the same conclusion. nonetheless, the new insights
may help to provide more objective metrics for precision and
generalization.
moreover, we advocate the use of a so-called pareto front
for process discovery. rather than aggregating the four quality
dimensions into a single value and selecting the best model
according to an aggregate value, we suggest building a set of
process models that are pareto-efﬁcient . a process model is
pareto-efﬁcient if there is no other model that scores better
with respect to one dimension (e.g., ﬁtness) and not worse
with respect to the other three dimensions (e.g., simplicity,
generalization, and precision). the pareto front is very natural
in the context of genetic process mining [9], [14] where many
different possible models are explored.
the achilles’ heel of the precision and recall metrics
deﬁned in section v is the deﬁnition of :0is unknown,
lis just an approximation, and 1may be very different
from reality. moreover, depending on the choice of the
interpretation of recall;m;m 1andprecision;m;m 1changes.
it would be interesting to investigate hybrid approaches using
an estimator 02u! [0;1]that uses both model and log,
e.g., the likelihood of a trace is based on the frequency in the
log and the likelihood of the corresponding path in the model.
the weight of the log-based part could be based on the log’s
size or completeness.
functioncan be used to select the desired perspective
(model or log). moreover, it is interesting to think of a
parameterized function cwherecis a timestamp or some
context attribute. for example, in december, when it is raining,
the probability of trace is higher than in june when the sun
shines. the probability of may depend on the month, the
day of the week, the weather, the workload, etc. this relates
to notions such as concept drift [27] and contextual process
mining [28].
vii. r elated work
see [1] for an introduction to process mining and the
process mining manifesto [29] for the main challenges in
process mining.
cook et al. [30], [31] were among the ﬁrst to quantify the
relationship between event logs and process model. they com-
pare event streams of the model with event steams generated
from the event log.several authors proposing process discovery algorithms
also provide a quality metric (often related to ﬁtness). for
example, in [9] the authors deﬁne a ﬁtness function for search-
ing for the optimal model using a genetic approach. in [32]
a “process mining evaluation framework” for benchmarking
process discovery algorithms is proposed.
the ﬁrst comprehensive approach to conformance analysis
was proposed in [12] by rozinat and van der aalst. two
different types of metrics are proposed: (a) ﬁtness metrics ,
i.e., the extent to which the log traces can be associated
with valid execution paths speciﬁed by the process model,
and (b) appropriateness metrics , i.e., the degree of accuracy
in which the process model describes the observed behavior,
combined with the degree of clarity in which it is represented.
fitness in [12] is measured by “replaying the event log”
and counting the number of missing and remaining tokens.
this typically results in rather high ﬁtness values as also
pointed out in [16], [21]. in [12] four appropriateness metrics
are deﬁned. simple behavioral appropriateness looks at the
average number of enabled transitions. if most transitions are
continuously enabled, the model is likely to lack precision (i.e.,
underﬁtting). advanced behavioral appropriateness compares
the “footprint” of the log (follows and precedes relationships)
to the “footprint” of the model. simple structural appropri-
ateness andadvanced structural appropriateness quantify the
complexity of the model.
one of the drawbacks of the approach in [12] and most
other approaches that “play the token game”, is that ﬁtness
is typically overestimated. when a model and log do not ﬁt
well together, replay will overload the process model with
superﬂuous tokens. as a result, the model will allow for too
much behavior. approaches such as the one in [12] also have
problems when the model has “invisible activities” (silent steps
that are not recorded in the event log) or “duplicate activities”
(multiple transitions bearing the same label). to deal with such
phenomena state-space exploration and heuristics are needed
to replay the event log. in fact, most conformance techniques
give up after the ﬁrst non-ﬁtting event or simply “guess” the
corresponding path in the model. therefore, adriansyah et al.
formulated conformance checking problems as an optimization
problem [16], [7].
lion’s share of attention in conformance checking has
been devoted to checking ﬁtness. however, in recent papers
researchers started to explore the other quality dimensions [6],
[14]. for example, munoz-gama et al. quantiﬁed additional
precision notions [13], [10], [11].
as shown in this paper, it is difﬁcult to use classical
quality notions such as precision andrecall for process mining.
the main reason is that event logs only contain positive
examples, i.e., one can see what “did happen” but not what
“could not happen”. therefore, some authors suggest inserting
artiﬁcially generated “negative events” [8], [33]. goedertier
et al. proposed such events for both process discovery and
conformance checking [8]. de weerdt et al. deﬁned a so-called
f-measure based on artiﬁcially generated negative events [33].
the authors of the latter paper also conducted a comparative
analysis of several conformance metrics [21], [20].
in [34] a so-called completeness metric and soundness
metric are deﬁned. these metrics compare the traces of themodel with the traces in the log. this approach suffers from
several drawbacks. first of all, only complete traces are
compared. second, it is assumed that the model’s behavior
can be enumerated. finally, it is assumed that the log contains
all possible traces.
in [35], the techniques presented in [16], [7] are general-
ized to artifact-centric processes (the so-called proclets). the
conformance notions in [35] also take interactions between
process instances into account.
several approaches create so-called behavioral footprints
to compare event log and model [1]. the key idea is that a
footprint can be based on observed behavior and modeled be-
havior as described in [1]. another example of such a footprint
is the so-called “behavioral proﬁle” [36]. the problem of this
approach is that it cannot handle loops properly (unlike [1],
[12]).
all techniques discussed thus far, compare model and log.
there are also many compliance approaches that compare a
model and another model or a model and a set of rules [37],
[38], [39], [40]. these approaches are very different from the
techniques discussed in this paper as they do not take the actual
observed behavior into account. in fact, these approaches
do not even take the likelihood of traces into account. this
problem is discussed in [41], [42] where process equivalence
is quantiﬁed based on observed behavior.
although not shown in this paper, alignments can also
be used for performance analysis as most event logs contain
timestamps [1]. replaying event logs with timestamps allows
for bottleneck analysis and prediction as demonstrated in [18],
[19].
viii. c onclusion
in this paper, we investigated the relation between observed
and modeled behavior. this topic is highly relevant as torrents
of event data have become available. as a result, models can be
discovered using process mining techniques and it is possible
to reﬂect on existing process models using real event data.
in literature typically four quality dimensions for compar-
ing model and log are considered: ﬁtness ,simplicity ,precision ,
andgeneralization [1]. at a ﬁrst glance these dimensions seem
arbitrary and not well deﬁned. therefore, this paper used well-
known notions from information retrieval (precision and recall)
to compare observed and modeled behavior. unlike existing
papers we started from the assumption that the real process
is known. moreover, we also take the likelihood of traces into
account.
as shown, it is crucial to consider the likelihood of traces
when comparing the ideal/desired model and the descrip-
tive/normative model. there are two main reasons. there may
be inﬁnitely many traces (e.g., in case of loops). therefore,
it does not make sense to count traces without considering
their likelihood. moreover, some traces may be very frequent
(highways) whereas other traces occur rarely (dirt roads). if
observed and modeled behavior disagree on a “highway”, this
is more severe than disagreement on some infrequent trace.
in case the real process 0and the desired process model
m0are known, precision1;m0;m1andrecall0;m0;m1makemost sense. if only an event log lis available and 0and
m0are unknown, then precision1;ml;m1andrecalll;ml;m1
are the two obvious metrics to consider. surprisingly these
correspond to the traditional precision and ﬁtness notions.
lis a log-based estimate of 0. the quality of such
an approximation is an important factor when comparing
observed and modeled behavior using precision and recall.
the approximation’s quality is closely related to the notion
of generalization. simplicity is another obvious concern that
is not covered by precision and recall. hence, despite taking
a completely different starting point, we end up with the
conventional four quality dimensions for analyzing the relation
between observed and modeled behavior [1].
the second part of the title of this paper – the quest
for the “right” process – can be viewed from two angles.
when the process model and the real process (reﬂected in the
event log) disagree, one can “blame” (1) the model or (2) the
actual process. when evaluating process discovery techniques,
the real process is always “right” and the discovered process
model is “wrong” in case of deviations. for other applications,
the process model and/or the real process may be “wrong”. for
example, if people often deviate from the normative process
model, then conformance analysis may trigger measures to
inﬂuence the behavior of these people. however, deviations
may also point to inefﬁciencies in the normative process. for
example, people may bypass activities for good reasons. in this
case the model need to be repaired based on observed behavior
[43]
acknowledgment
the author would like to thank all process mining re-
searchers and prom developers that contributed to the insights
reported in this paper. unfortunately, just a few can be men-
tioned here. anne rozinat laid the foundations for confor-
mance checking as we know it today [12], [32], [44]. she
was the ﬁrst to deﬁne clear conformance notions outside the
scope of a speciﬁc discovery algorithm (including token-based
replay). before the work of anne, ﬁtness was investigated by
ana karla alves de medeiros in the context of genetic mining
[9]. arya adriansyah (with great support from boudewijn van
dongen) developed the notion of alignments and various a-
based algorithms [6], [15], [16], [7], [13]. jorge munoz gama
and josep carmona made the precision notion more concrete
by quantifying “escaping edges” [13], [10], [11]. joos buijs
and boudewijn van dongen contributed the idea to consider the
“real process” in the context of evaluating discovered process
trees [14].
references
[1] w. van der aalst, process mining: discovery, conformance and en-
hancement of business processes . springer-verlag, berlin, 2011.
[2] ——, “business process management: a comprehensive survey,” isrn
software engineering , pp. 1–37, 2013, doi:10.1155/2013/507984.
[3] w. van der aalst and k. van hee, workﬂow management: models,
methods, and systems . mit press, cambridge, ma, 2004.
[4] d. hand, h. mannila, and p. smyth, principles of data mining . mit
press, cambridge, ma, 2001.
[5] t. mitchell, machine learning . mcgraw-hill, new york, 1997.[6] w. van der aalst, a. adriansyah, and b. van dongen, “replaying
history on process models for conformance checking and performance
analysis,” wires data mining and knowledge discovery , vol. 2, no. 2,
pp. 182–192, 2012.
[7] a. adriansyah, b. van dongen, and w. van der aalst, “conformance
checking using cost-based fitness analysis,” in ieee international
enterprise computing conference (edoc 2011) , c. chi and p. john-
son, eds. ieee computer society, 2011, pp. 55–64.
[8] s. goedertier, d. martens, j. vanthienen, and b. baesens, “robust
process discovery with artiﬁcial negative events,” journal of machine
learning research , vol. 10, pp. 1305–1340, 2009.
[9] a.k. alves de medeiros, a. weijters, and w. van der aalst, “ge-
netic process mining: an experimental evaluation,” data mining and
knowledge discovery , vol. 14, no. 2, pp. 245–304, 2007.
[10] j. munoz-gama and j. carmona, “a fresh look at precision in process
conformance,” in business process management (bpm 2010) , ser.
lecture notes in computer science, r. hull, j. mendling, and s. tai,
eds., vol. 6336. springer-verlag, berlin, 2010, pp. 211–226.
[11] ——, “enhancing precision in process conformance: stability, conﬁ-
dence and severity,” in ieee symposium on computational intelligence
and data mining (cidm 2011) , n. chawla, i. king, and a. sperduti,
eds. paris, france: ieee, april 2011, pp. 184–191.
[12] a. rozinat and w. van der aalst, “conformance checking of processes
based on monitoring real behavior,” information systems , vol. 33,
no. 1, pp. 64–95, 2008.
[13] a. adriansyah, j. munoz-gama, j. carmona, b. van dongen, and
w. van der aalst, “alignment based precision checking,” in business
process management workshops, international workshop on business
process intelligence (bpi 2012) , ser. lecture notes in business infor-
mation processing, m. rosa and p. soffer, eds., vol. 132. springer-
verlag, berlin, 2013, pp. 137–149.
[14] j. buijs, b. van dongen, and w. van der aalst, “on the role of fitness,
precision, generalization and simplicity in process discovery,” in otm
federated conferences, 20th international conference on cooperative
information systems (coopis 2012) , ser. lecture notes in computer
science, r. meersman, s. rinderle, p. dadam, and x. zhou, eds., vol.
7565. springer-verlag, berlin, 2012, pp. 305–322.
[15] a. adriansyah, n. sidorova, and b. van dongen, “cost-based fitness in
conformance checking,” in international conference on application of
concurrency to system design (acsd 2011) . ieee computer society,
2011, pp. 57–66.
[16] a. adriansyah, b. van dongen, and w. van der aalst, “towards robust
conformance checking,” in bpm 2010 workshops, proceedings of
the sixth workshop on business process intelligence (bpi2010) , ser.
lecture notes in business information processing, m. muehlen and
j. su, eds., vol. 66. springer-verlag, berlin, 2011, pp. 122–133.
[17] m. de leoni, w. van der aalst, and b. van dongen, “data- and
resource-aware conformance checking of business processes,” in
business information systems (bis 2012) , ser. lecture notes in busi-
ness information processing, w. abramowicz, d. kriksciuniene, and
v . sakalauskas, eds., vol. 117. springer-verlag, berlin, 2012, pp.
48–59.
[18] w. van der aalst, m. pesic, and m. song, “beyond process mining:
from the past to present and future,” in advanced information systems
engineering, proceedings of the 22nd international conference on
advanced information systems engineering (caise’10) , ser. lecture
notes in computer science, b. pernici, ed., vol. 6051. springer-
verlag, berlin, 2010, pp. 38–52.
[19] w. van der aalst, m. schonenberg, and m. song, “time prediction
based on process mining,” information systems , vol. 36, no. 2, pp.
450–475, 2011.
[20] j. de weerdt, m. backer, j. vanthienen, and b. baesens, “a multi-
dimensional quality assessment of state-of-the-art process discovery
algorithms using real-life event logs,” information systems , vol. 37,
no. 7, pp. 654–676, 2012.
[21] j. de weerdt, m. de backer, j. vanthienen, and b. baesens, “a critical
evaluation of model-log metrics in process discovery,” in bpm 2010
workshops, proceedings of the sixth workshop on business process
intelligence (bpi2010) , ser. lecture notes in business information
processing, m. muehlen and j. su, eds., vol. 66. springer-verlag,
berlin, 2011, pp. 158–169.[22] j. buijs, b. van dongen, and w. van der aalst, “a genetic algorithm
for discovering process trees,” in ieee congress on evolutionary
computation (cec 2012) . ieee computer society, 2012, pp. 1–8.
[23] m. a. marsan, g. balbo, g. conte, s. donatelli, and g. franceschinis,
modelling with generalized stochastic petri nets , ser. wiley series in
parallel computing. wiley, new york, 1995.
[24] a. rozinat, r. mans, m. song, and w. van der aalst, “discovering
simulation models,” information systems , vol. 34, no. 3, pp. 305–327,
2009.
[25] j. mendling, g. neumann, and w. van der aalst, “understanding the
occurrence of errors in process models based on metrics,” in pro-
ceedings of the otm conference on cooperative information systems
(coopis 2007) , ser. lecture notes in computer science, f. curbera,
f. leymann, and m. weske, eds., vol. 4803. springer-verlag, berlin,
2007, pp. 113–130.
[26] w. van der aalst, a. weijters, and l. maruster, “workﬂow mining:
discovering process models from event logs,” ieee transactions on
knowledge and data engineering , vol. 16, no. 9, pp. 1128–1142, 2004.
[27] r.p. jagadeesh chandra bose, w. van der aalst, i. zliobaite, and
m. pechenizkiy, “handling concept drift in process mining,” in in-
ternational conference on advanced information systems engineering
(caise 2011) , ser. lecture notes in computer science, h. mouratidis
and c. rolland, eds., vol. 6741. springer-verlag, berlin, 2011, pp.
391–405.
[28] w. van der aalst and s. dustdar, “process mining put into context,”
ieee internet computing , vol. 16, no. 1, pp. 82–86, 2012.
[29] ieee task force on process mining, “process mining manifesto,” in
bpm workshops , ser. lecture notes in business information processing,
vol. 99. springer-verlag, berlin, 2011.
[30] j. cook and a. wolf, “software process validation: quantitatively mea-
suring the correspondence of a process to a model,” acm transactions
on software engineering and methodology , vol. 8, no. 2, pp. 147–176,
1999.
[31] j. cook, c. he, and c. ma, “measuring behavioral correspondence to
a timed concurrent model,” in proceedings of the 2001 international
conference on software mainenance , 2001, pp. 332–341.
[32] a. rozinat, a.k. alves de medeiros, c. g ¨unther, a. weijters, and
w. van der aalst, “the need for a process mining evaluation frame-
work in research and practice,” in bpm 2007 international workshops
(bpi, bpd, cbp , prohealth, refmod, semantics4ws) , ser. lecture notes
in computer science, a. hofstede, b. benatallah, and h. paik, eds.,
vol. 4928. springer-verlag, berlin, 2008, pp. 84–89.
[33] j. de weerdt, m. de backer, j. vanthienen, and b. baesens, “a
robust f-measure for evaluating discovered process models,” in ieee
symposium on computational intelligence and data mining (cidm
2011) , n. chawla, i. king, and a. sperduti, eds. paris, france: ieee,
april 2011, pp. 148–155.
[34] g. greco, a. guzzo, l. pontieri, and d. sacc `a, “discovering expressive
process models by clustering log traces,” ieee transaction on
knowledge and data engineering , vol. 18, no. 8, pp. 1010–1027, 2006.
[35] d. fahland, m. de leoni, b. van dongen, and w. van der aalst,
“conformance checking of interacting processes with overlapping
instances,” in business process management (bpm 2011) , ser. lecture
notes in computer science, s. rinderle, f. toumani, and k. wolf, eds.,
vol. 6896. springer-verlag, berlin, 2011, pp. 345–361.
[36] m. weidlich, a. polyvyanyy, n. desai, j. mendling, and m. weske,
“process compliance measurement based on behavioral proﬁles,”
information systems , vol. 36, no. 7, pp. 1009–1025, 2011.
[37] a. elgammal, o. turetken, w. heuvel, and m. papazoglou, “root-
cause analysis of design-time compliance violations on the basis
of property patterns,” in proceedings of service-oriented computing
(icsoc 2010) , ser. lecture notes in computer science, p. maglio,
m. weske, j. yang, and m. fantinato, eds., vol. 6470. springer-verlag,
berlin, 2010, pp. 17–31.
[38] g. governatori, j. hoffmann, s. sadiq, and i. weber, “detecting
regulatory compliance for business process models through seman-
tic annotations,” in 4th international workshop on business process
design , 2008.
[39] g. governatori, z. milosevic, and s. sadiq, “compliance checking
between business processes and business contracts,” in 10th inter-national enterprise distributed object computing conference (edoc
2006) . ieee computing society, 2006, pp. 221–232.
[40] n. lohmann, “compliance by design for artifact-centric business
processes,” in business process management (bpm 2011) , ser. lecture
notes in computer science, s. rinderle, f. toumani, and k. wolf, eds.,
vol. 6896. springer-verlag, berlin, 2011, pp. 99–115.
[41] w. van der aalst, a.k. alves de medeiros, and a. weijters, “process
equivalence: comparing two process models based on observed be-
havior,” in international conference on business process management
(bpm 2006) , ser. lecture notes in computer science, s. dustdar,
j. fiadeiro, and a. sheth, eds., vol. 4102. springer-verlag, berlin,
2006, pp. 129–144.
[42] a.k. alves de medeiros, w. van der aalst, and a. weijters, “quan-
tifying process equivalence based on observed behavior,” data and
knowledge engineering , vol. 64, no. 1, pp. 55–74, 2008.
[43] d. fahland and w. van der aalst, “repairing process models to reﬂect
reality,” in international conference on business process management
(bpm 2012) , ser. lecture notes in computer science, a. barros, a. gal,
and e. kindler, eds., vol. 7481. springer-verlag, berlin, 2012, pp.
229–245.
[44] a. rozinat, i. de jong, c. g ¨unther, and w. van der aalst, “process
mining applied to the test process of wafer scanners in asml,” ieee
transactions on systems, man and cybernetics, part c , vol. 39, no. 4,
pp. 474–479, 2009.