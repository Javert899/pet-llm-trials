1 
  liquid business process mod el coll ections  
  
wil m.p. van der aalst1,2, marcello la rosa2,3, arthur h.m. ter hofstede2,1, moe t. wynn2 
 
1 eindhoven university of technology, eindhoven, the netherlands  
w.m.p.v.d.aalst@tue.nl  
2 queensland university of techno logy, brisbane, australia  
{m.larosa, a.terhofstede, m.wynn}@qut.edu.au  
3 nicta queensland research lab, brisbane, australia  
abstract.  many organizations realize that increasing amounts of data (“big data”) 
need to be dealt with intelligently in order to c ompete with other organizations in 
terms of efficiency, speed and services. the goal is not to collect as much data as 
possible, but to turn event data into valuable insights that can be used to improve 
business processes. however, data -oriented analysis a pproaches fail to relate event 
data to process models. at the same time, large organizations are generating piles of 
process models that are disconnected from the real processes and information 
systems. these models are often just used for documentation an d discussion. even in 
more mature organizations using analysis techniques like simulation, models are 
often not connected to event data. in this chapter we propose to manage large 
collections of process models and event data in an integrated manner. observ ed and 
modeled behavior need to be continuously compared and aligned. this results in a 
“liquid” business process model collection, i.e. a collection of process models that is 
in sync with the actual organizational behavior. the collection should self -adap t to 
evolving organizational behavior and incorporate relevant execution data (e.g. 2 
  process performance and resource utilization) extracted from the logs, thereby 
allowing insightful reports to be produced from factual organizational data.  
keywords: busine ss process management, process mining, process model collection, 
process model.  
 
introduction  
business process management (bpm) has become an established discipline (dumas et al. 2013, 
van der aalst 2013 a) dedicated to the way an organization identifies, c aptures, analyses, 
improves, implements and monitors its business processes. through the management of the 
process lifecycle , bpm influences the effectiveness and efficiency of a corporation and is a 
significant contributor to its overall performance and c ompetitiveness. business processes are 
thus seen as strategic corporate assets and, in the case of comprehensive supply chains, complex 
call center operations or advanced distribution networks, can represent multi -million dollar 
assets  (gotts 2010). as pro cesses determine how an organization operates, what activities need 
to be fulfilled and what data and resources are required for their successful execution, they are 
crucial to a plethora of key performance indicators. this importance of processes has moti vated 
organizations to significantly invest  in methods, tools and techniques facilitating process 
lifecycle management. for example, wolf and harmon (2012) report that 37% of organizations 
surveyed spend more than usd 500,000, and 4% more than usd 10 milli on, on investments in 
business process analysis, management, monitoring, redesign and improvement, and similar 
amounts for related software acquisitions.  
at the core of the process lifecycle are conceptual models of processes that help involved 3 
  stakeholde rs gain a shared understanding of their processes. such visual depictions are called 
process models . a process model is a directed graph describing the triggers, activities, data and 
resources of a business process. such models can be used for documentatio n, discussion, 
enactment (e.g., to configure a bpm system), and various types of analysis. simulation  is a 
classical model -based analysis technique used for "what -if" analysis. given an "as -is" model 
describing the current situation, multiple "to -be" model s can be constructed to explore different 
alternatives while measuring key performance indicators (kpis) like flow times, utilization, 
response times, faults, risks, and costs. the value of such analyses stands or falls on the quality 
of the "as -is" model:  is the simulation model able to capture reality?  
as the ultimate source of process information, a process model informs critical decisions  such as 
investments related to process -aware information systems, complex organizational re -designs or 
crucial comp liance assessments (davies et al. 2006). these requirements demand that process 
models accurately reflect the corresponding real -world processes, i.e. the actual organizational 
behavior . 
as a consequence, a substantial research branch of bpm, namely proces s mining  (van der aalst 
2011, ieee task force on process mining 2012), has been dedicated to techniques for 
extracting organizational behavior from event logs  and using this information to enhance existing 
process models (fahland and van der aalst 2012) or  discover new ones (van dongen et al. 2009). 
event logs are recorded by a variety of it systems commonly available within organizations, 
such as enterprise resource planning (erp) systems, content management systems (cmss), 
customer relationship management  (crm) systems, database management systems (dbmss) 
or e-mail servers. process mining is fueled by the incredible growth of event data (hilbert and 
lopez 2011).  4 
  in the practical deployment of process modeling, however, a new challenge emerges, i.e. scaling  
up or process modeling in the large  (raduescu et al. 2006, rosemann 2006, van der aalst 
2013 a). for large organizations it is common to maintain collections of thousands of complex 
process models which all need to be managed appropriately to cater for the  various demands of 
their stakeholders. for instance, suncorp, the largest australian insurer, manages a collection of 
3,000+ process models with models ranging from 25 to 500 activities (la rosa et al. 2013).  
these large organizations employ some form of process model repository management, e.g. 
suncorp uses aris. however, such forms of repository management are often not adequate as 
the models in these repositories tend:  
issue 1:  to be based on subjective  perceptions about the real processes rather than actual 
objective  process data,  
issue 2:  to be out of sync  with organizational behavior, as the frequency of real world 
process changes is such that cost -effective process model updates are not possible,  
issue 3:  to be designed for the most common purposes  of the stakeholders instead of 
catering for specific demands of individual stakeholders.  
these three issues lead to severe limitations  of process model collections, dramatically 
compromising the quality of business decisions that are made based on these artifacts. current 
process min ing techniques are not adequate to address these issues, since they focus on single  
process models as opposed to collections thereof. concomitantly, recently -emerged research 
techniques for managing large process model collections (dijkman et al. 2012), ar e concerned 
with challenges such as how to identify similar models in a repository (dijkman et al. 2011), 
merge these models (la rosa et al. 2013) and modularize them (reijers et al. 2011), but have 
never considered  organizational behavior despite the wide  availability of event logs in today’s 5 
  organizations.  
this chapter proposes the new notion of a “liquid” business process model/log collection , i.e. a 
collection of process models that:  
i) is aligned  with the organizational behavior, as recorded in event log s, 
ii) can self-adapt  to evolving organizational behavior, thereby consistently remaining 
current and relevant,  
iii) incorporates relevant execution data  (e.g. process performance and resource 
allocations) extracted from the logs, thereby allowing insightful report s to be 
produced from factual organizational data.  
such collections provide a rich source of input for various types of analysis, including process 
mining and simulation.  
the remainder of the chapter is organized as follows. section 2 provides an overview of bpm by 
discussing selected bpm use cases. the process mining spectrum and the importance of aligning 
observed and modeled behavior are discussed in section 3 whereas the management techniques 
for process model collections are discussed in section 4. sec tion 5 elaborates on the main 
innovations needed to make business process model collections “liquid” using event data. 
section 6 discusses the different challenges in terms of five research streams. initial tool support 
realized through prom and apromore i s described in section 7. section 8 concludes the chapter.  
bpm use cases  
in this chapter we propose a “liquid” business process model collection where modeled and 
observed behaviors are aligned and multiple evolving processes are considered. to position s uch 
liquid business process model and event data collections, we first provide an overview of 6 
  classical bpm approaches using typical use cases.  
in (van der aalst 2013 a) twenty use cases are used to structure the bpm discipline and to show 
"how, where, and when" bpm techniques can be used. these are summarized in figure 1. 
models are depicted as pentagons marked with the letter m. a model may be descriptive ( d), 
normative ( n), and/or executable ( e). a " d|n|e " tag inside a pentagon means that the 
correspondin g model is descriptive, normative, or executable. configurable models are depicted 
as pentagons marked with cm. event data (e.g., an event log) are denoted by a disk symbol 
(cylinder shape) marked with the letter e. information systems used to support proc esses at 
runtime are depicted as squares with rounded corners and marked with the letter s. diagnostic 
information is denoted by a star shape marked with the letter d. we distinguish between 
conformance -related diagnostics (star shape marked with cd) and p erformance -related 
diagnostics (star shape marked with pd). the twenty atomic use cases can be chained together in 
so-called composite use cases. these composite cases can be used to describe realistic bpm 
scenarios.  
 7 
  
m
d|n|edesign model
em
d|ediscover model from 
event data
m
d|n|eselect model from 
collectionmmm
d|n|e(desm)
(discm)
(selm)
m
d|n|emerge modelsmmm
d|n|e
m
d|n|ecompose model m
d|n|em
d|n|e
m
d|n|e(merm)
(compm)
cm
d|n|emerge models into 
configurable modelmmm
d|n|ecm
d|n|edesign configurable 
model
m
d|n|econfigure configurable 
model
cm
d|n|e(descm)
(mercm)
(concm)analyze performance 
based on model
m
epd
verify model
m
ecd(perfm)
(verm)
check conformance 
using event data
m
ecde
analyze performance 
using event data
m
eepd(confed)
(perfed)
repair model
m
d|n|ecd m
d|n|e
extend model
m
eem
e
improve model
m
d|n|em
d|n|epd(repm)
(extm)
(impm)slog event data
es
monitor
s d(loged)
(mon)
adapt while running
m
e(adawr) s m
e
enact model
m
esrefine model
m
d|nm
e(refm)
(enm) 
 
figure 1 : twenty bpm use cases (van der aalst, 2013 a). use cases log event data (loged), discover model 
from event data (discm), check conformance using event data (confed), analyze performance using event 
data (perfed), repair model (repm), extend model (extm), i mprove model (impm) are most related to process 
mining. use case analyze performance based on model (perfm) includes traditional forms of simulation.  
in (van der aalst, 2013 a), the bpm literature is analyzed to see trends in terms of the twenty use 
cases, e.g., topics that are getting more and more attention. here we only mention the use cases 8 
  most related to process mining.  
 use case log event data  (loged) refers to the recording of event data, often referred to 
as event logs. such event logs are used as i nput for various process mining techniques. 
xes (extensible event stream), the successor of mxml (mining xml format), is a 
standard format for storing event logs (www.xes -standard.org).  
 use case discover model from event data (discm) refers to the automate d generation of 
a process model using process mining techniques. examples of discovery techniques are 
the alpha algorithm, language -based regions, and state -based regions. note that classical 
synthesis approaches need to be adapted since the event log only  contains examples.  
 use case check conformance using event data  (confed) refers to all kinds of analysis 
aiming at uncovering discrepancies between modeled and observed behavior. 
conformance checking may be done for auditing purposes, e.g., to uncover frau d or 
malpractices. token -based (rozinat and van der aalst, 2008) and alignment -based (van 
der aalst, adriansyah, and van dongen 2012) techniques replay the event log to identify 
non-conformance (weerdt, de backer, vanthienen, and baesens, 2011).  
 use case analyze performance using event data  (perfed) refers to the combined use of 
models and timed event data. by replaying an event log with timestamps on a model, one 
can measure delays, e.g., the time in -between two subsequent activities. the results of 
timed replay can be used to highlight bottlenecks. moreover, the gathered timing 
information can be used for simulation or prediction techniques (de weerdt et al. 2012).  
 use case repair model  (repm) uses the diagnostics provided by use case confed to 
adapt the m odel such that it better matches reality. on the one hand, a process model 
should correspond to the observed behavior. on the other hand, there may be other forces 9 
  influencing the desired target model, e.g., a reference model, desired normative behavior, 
and domain knowledge.  
 event logs refer to activities being executed and events may be annotated with additional 
information such as the person/resource executing or initiating the activity, the timestamp 
of the event, or data elements recorded with the even t. use case extend model  (extm) 
refers to the use of such additional information to enrich the process model. for example, 
timestamps of events may be used to add delay distributions to the model. data elements 
may be used to infer decision rules that can be added to the model. resource information 
can be used to attach roles to activities in the model (rozinat, wynn, van der aalst, ter 
hofstede, fidge, 2009).  
 use case improve model  (impm) uses the performance related diagnostics obtained 
through use case p erfed. impm is used to generate alternative process models aiming at 
process improvements, e.g., to reduce costs or response times. these models can be used 
to do ``what -if'' analysis. note that unlike repm the focus impm is on improving the 
process itself . 
these use cases illustrate the different ways in which models and event data can be used. use 
case analyze performance based on model (perfm) includes traditional forms of simulation not 
directly driven by event data. see (van der aalst 2010) for a discu ssion on the relation between 
simulation, bpm and process mining.  
process mining  
over the last decade, process mining emerged as a new scientific discipline on the interface 
between process models and event data (van der aalst, 2011). on the one hand, conv entional 10 
  business process management (bpm) and workflow management (wfm) approaches and tools 
are mostly model -driven with little consideration for event data. on the other hand, data mining 
(dm), business intelligence (bi), and machine learning (ml) focus  on data without considering 
end-to-end process models. process mining aims to bridge the gap between bpm and wfm on 
the one hand and dm, bi, and ml on the other hand.  
the starting point for process mining is not just any data, but event data  (ieee task fo rce on 
process mining 2012). data should refer to discrete events that happened in reality. a collection 
of related events is referred to as an event log. each event in such a log refers to an activity (i.e., 
a well -defined step in some process) and is rel ated to a particular case (i.e., a process instance). 
the events belonging to a case are ordered and can be seen as one “run” of the process. it is 
important to note that an event log contains only example behavior, i.e., we cannot assume that 
all possible  runs have been observed. in fact, an event log often contains only a fraction of the 
possible behavior (van der aalst, 2011). frequently, event logs store additional information 
about events and these additional data attributes may be used during analysis . for example, many 
process mining techniques use extra information such as the resource (i.e., person or device) 
executing or initiating the activity, the timestamp of the event, or data elements recorded with the 
event (e.g., the size of an order).  
 11 
  
register 
requestexamine 
casuallyexamine 
thoroughly
check ticketdecidepay 
compensation
reject 
request
reinitiate 
requeststartend
performance information (e.g., the average time 
between two subsequent activities) can be extracted 
from the event log and visualized on top of the model.a
aaa
am
mpete
mike
ellenrole a:
assistant
sue
seanrole e:
expert
sararole m:
managerdecision rules (e.g., a decision tree based 
on data known at the time a particular 
choice was made) can be learned from 
the event log and used to annotate 
decisions.the event log can be 
used to discover roles 
in the organization 
(e.g., groups of 
people with similar 
work patterns). these 
roles can be used to 
relate individuals and 
activities.
e
discovery techniques can be used to find a control-
flow model (in this case in terms of a bpmn model) 
that describes the observed behavior best.starting point is an event log. 
each event refers to a process 
instance (case) and an 
activity. events are ordered 
and additional properties (e.g. 
timestamp or resource data) 
may be present.
 
 
figure 2: process mining techniques extract knowledge from event logs in order to discover, monitor and improve 
processes.  
event logs can be used to conduct various types of process mining, as is illustrated in figure 2. 
here we o nly mention the three main forms of process mining. the first type of process mining 
is discovery . a discovery technique takes an event log and produces a model without using any 
a-priori information. process discovery is the most prominent process mining technique. for 
many organizations it is surprising to see that existing techniques are indeed able to discover real 12 
  processes merely based on example executions in event logs. the second type of process mining 
is conformance . here, an existing process mode l is compared with an event log of the same 
process. conformance checking can be used to check if reality, as recorded in the log, conforms 
to the model and vice versa. the third type of process mining is enhancement . here, the idea is 
to extend or improve  an existing process model using information about the actual process 
recorded in some event log. whereas conformance checking measures the alignment between 
model and reality, this third type of process mining aims at changing or extending the a -priori 
model. for instance, by using timestamps in the event log one can extend the model to show 
bottlenecks, service levels, throughput times, and frequencies. note that the three main forms of 
process mining correspond to some of the uses cases mentioned before.  
modeled (normative or 
descriptive) behaviordeviating behavior may be squeezed into model for 
analysis (e.g., performance analysis, prediction, and 
decision mining) 
deviating behavior can be 
identified and subsequently 
used for conformance 
checking 
 
figure 3: process mining aligns observed and modeled behavior: "moves" seen in reality are related to "moves" in 
the model (if possible).  
one of the key contributions of process mining is its ability to relate observed and mod eled 
behavior at the event level, i.e., traces observed in reality (process instances in event log) are 
aligned with traces allowed by the model (complete runs of the model). as shown in figure 3 it 13 
  is useful to align both even when model and reality disag ree (van der aalst, adriansyah, and van 
dongen 2012). first of all, it is useful to highlight where and why there are discrepancies 
between observed and modeled behavior. second, deviating traces need to be "squeezed" into the 
model for subsequent analysis , e.g., performance analysis or predicting remaining flow times. 
the latter is essential in case of non -conformance (van der aalst, adriansyah, and van dongen 
2012). without aligning model and event log, subsequent analysis is impossible or biased 
towards conforming cases.  
management of large process model collections  
as organizations start to develop and maintain large collections of process models, there is an 
increasing need for continuous and efficient management of these process repositories. in order 
to reduce redundancy and improve maintainability of process model collections, efficient 
techniques to manage multiple process models in relation to each other as well as management of 
different versions of a single model are required. in (dijkman, la rosa , reijers 2012), an 
overview of state -of-the-art management techniques for process model collections is provided. a 
pictorial representation of such techniques is provided in figure 4.  14 
  
variants management
merging
refactoring
process model 
repositorysimilarity 
search
querying0.8 
 
figure 4: overview of techniques for the management of process model collections (adapted from dijkman, la 
rosa, reijers 2012)  
as a collection of process models evolves over time, it may start to display unnecessary internal 
complexity. a common example is redundancy in the form of exact or appro ximate clones.  such 
clones are typically the result of copy/paste activities and they adversely affect the 
maintainability of process model collections, besides leading to unwanted inconsistencies in the 
repository, if they are modified independently of e ach other. clones can manifest themselves 
both at the level of entire process models as well as fragments thereof. researchers have 
proposed various techniques for detecting such clones within process model repositories (see e.g. 
guo, zou 2008; dumas, garc ía-bañuelos, la rosa, uba 2013, ekanayake, dumas, garcía -
bañuelos, la rosa, ter hofstede 2012). refactoring  techniques, inspired from software 
engineering, have been explored to improve the maintainability and readability of process model 
collections. exam ples of such refactoring techniques are extracting the identified clones and 
storing them as reusable sub -processes (dumas, garcía -bañuelos, la rosa, uba 2013), 15 
  standardizing approximate clones (ekanayake, dumas, garcía -bañuelosla rosa, ter hofstede 
2012),  and modularizing process models into different levels of abstraction (weber, reichert, 
mendling, reijers 2011, dijkman, gfeller, kuster, volzer 2011).  
another set of management techniques relate to the notion of similarity search  (becker and laue 
2012). g iven a collection of process models and a search process model, similarity search 
techniques identify and return those models from the collection that are deemed similar (e.g., 
potentially inexact matches) to the search model. a potential use case of simil arity search is for 
an organization to identify which of its own processes are similar to a standardized (reference) 
process model. research in this area approaches this challenge from two angles: i) the definition 
and implementation of similarity measures  that return a similarity rating (e.g ., between 0 and 1) 
for two process models (dijkman, dumas, van dongen, kaarik, mendling 2011); and ii) the 
implementation of indexing techniques for improving the retrieval of similar process models 
(yan, dijkman, gref en 2010, kunze, weske, 2011) or of models containing the query model as a 
sub-graph (e.g. jin, wang, la rosa, ter hofstede, wen, 2012). more in general, such indexing 
techniques can be used as the backbone for efficient querying  of process model repositori es. the 
query could be expressed as a model (e.g. jin, wang, la rosa, ter hofstede, wen, 2012, awad, 
sakr 2012) or in textual form (e.g. jin, wang, wen 2011).   
process model merging  is concerned with merging a collection of process variants into one 
conso lidated process model which can be very useful in the context of organizational mergers, 
restructurings and rationalizations. this can lead to a collection of reduced size that has been 
standardized and optimized for the current business context, which in turn can significantly 
improve the maintainability of the collection as a whole. some merging techniques enforce 
behavior -preservation such that the merged model maintains the behavior of all individual 16 
  models allowing one to replay the behavior of each in put variant on the merged model (la rosa  
et al.,  2013).  some of the merging techniques take into account notions of label similarities 
when merging so that it is possible to merge activities from different models that have similar but 
not identical labels  (gottschalk, van der aalst, jansen -vullers, 2008; la rosa  et al.,  2010) 
whereas others only merge activities with identical labels (reijers, mans, van der toorn, 2009; 
sun, kumar, yen, 2006; mendling, simon, 2006).   
given a collection of process models, variants management  mechanisms (e.g., pascalau, awad, 
sakr, weske, 2010, ekanayake, la rosa, ter hofstede, fauvet, 2011, weidlich, mendling, 
weske, 2011) are required to keep track of the organization of the collection such that users can 
browse it and vie w its evolution as changes are made. these mechanisms are based on various 
types of relations between the process models of the repository. for example, ekanayake, la 
rosa, ter hofstede and fauvet (2011) exploit information on shared clones across process 
models and versions thereof, in order to provide change propagation and access control features. 
other common relations that can be used to manage variants are aggregation and generalization 
relations (kurniawan, ghose, le, dam 2012). an aggregation relati on exists between a business 
process model and it parts (usually sub -processes) while a generalization relation exists between 
a more general process model and a more specific one. aggregation and generalization are 
typically used to develop a hierarchical  classification of processes models, enabling users to 
navigate a collection of process models by traversing the hierarchy.  
while there is a plethora of techniques for managing large process model collections, a 
collection remains “static” in most cases un til a process improvement initiative is carried out. 
this is an area where considerable progress can still be made towards “self -healing” or “self -
adapting” process model collections.   17 
  research innovations  
we propose to integrate process mining and the ma nagement of process model collections, 
leading to a solution for the management of liquid process model collections. this results in the 
following innovations:  
 innovation 1: confidence → evidence . to address issue 1 mentioned in the 
introduction (process model collections based on subjective perceptions), we propose to 
create a new entry point to the process lifecycle. traditionally, business processes are 
first designed and t hen executed on the basis of these designs, which are informed by 
domain expertise ( confidence -based process design ). this has proven to be time -
consuming and error -prone since organizational behavior is inherently difficult to capture 
formally. this probl em is exacerbated in the context of large firms executing many 
complex and disparate business processes. we propose to leverage off the organizational 
knowledge stored in event logs to provide an alternative starting point to process model 
design ( evidence -based process design ). by developing semi -automated techniques, this 
design activity will be less expensive  and result in more current  process models.  
 innovation 2: static → dynamic . in response to issue 2 (process model collections out 
of-synch with org anizational behavior), we challenge the static  nature of process model 
collections, manually built once, and updated from time to time. rather, we propose 
liquid  process model collections as a dynamic  artifact that can self -adapt to evolving 
business opera tions, as recorded in logs, thereby consistently remaining current and 
relevant. this continuous realignment  of process model collections with organizational 
behavior can virtually eliminate the risk of obsolete process models and concomitantly 
increase th e value of bpm initiatives. as opposed to traditional process monitoring and 18 
  controlling, it will be possible to adapt process model collections based on event logs, 
even if their constituent process models are not automatically enacted by a business 
proce ss management system.  
 innovation 3: generic → demand -driven . today’s  process model collections are 
“common ground”: they are inspired by generic  business needs and designed before 
specific stakeholder demands are articulated. as such, they are problem -independent and 
user-independent. with t he availability of liquid process model collections, which  
incorporate execution data  inferred from event logs, such as actual process performance 
metrics or resource allocations, it will be possible to generate insightful reports, the result 
of which will  be new, demand -driven process models tailored to the needs of specific 
stakeholders. this will address issue 3 (process model collections designed for the most 
common purposes).  
these innovations result in the so -called “liquid” business process model/log  collection 
mentioned in section 1.  
realization  
in order to realize the above innovations, we propose a research agenda consisting of five 
interrelated research streams (rs1 -rs5), which are illustrated in figure 5. each stream aims to 
realize one or more o f the innova tions identified in section 5. details for each stream are 
provided below.  
 19 
  
innovation 1: evidence-based process design
innovation 2: continuous process realignment innovation 3: demand-driven process modellingrs2: enhancementevent log
rs1: liquid
process model
collection
domain
expert
business
analyst
rs4: consolidation
 avg time: 3 hours
 avg freq.: 15/week
 ...rs3: discovery
 
rs5: reportingfraudulent 
processes 
involving frank
in the last 
week?
products, 
brands, 
customers...
processes
by productprocesses
by brandout-dated
process model
collection??
?
 user: frank
 completed: last week
process
stakeholderah ok, the new 
system release 
has led to less 
approval taskserp
e-mail
servercrm
dbms cms 
figure 5: the proposed research agenda consists of five research streams (rs1 -rs5), each mapped to one or 
more innovations.  
 
research stre am 1 (rs1): fundamentals of liquid process model collections. this research 
stream focusses on the foundational notions behind liquid process model collections  and 
techniques for operationalizing them. as such, rs1 is the enabler for the realization of 
innovations 1 -3 through the other research streams. the first priority in this stream should be the 
extension of the concept of alignment  between a log and a single  process model (van der aalst, 
adriansyah, and van dongen 2012, adriansyah et al. 2011) to the realm of process model 
collections, in order to determine an overall alignment score  between logs and process model 
collections. this notion, illustrated in figure 6, will need to consider different ways of 
partitioning the log, e.g. along entire traces or  portions thereof (i.e. sub -traces), in order to 20 
  identify the sequence of events that best matches an individual process model. further, for each 
model in the collection its execution occurrence frequency (i.e. number of cases) needs to be 
considered, as i nferred from the logs, in order to weigh individual alignments.  
existing process discovery (van der aalst 2011) and graph matching (bunke 1997) algorithms 
may serve as the basis for a novel algorithm to compute the overall alignment score. the process 
mining environment prom can be extended to incorporate these algorithms.  
rs1 should also identify suitable execution properties  inferred from the logs (e.g. performance 
indicators, bottlenecks and resource utilization) that can be linked to different elements of the 
process model collection to enrich it (e.g. the average duration of single tasks or the frequency of 
a whole process). the types of relations should be captured in the form of a conceptual model . 
we need to formalize a data structure  to persist both  the alignment and these properties for 
subsequent business analysis (proposed to be realized as part of rs5). finally, a customizable 
dashboard  needs to be designed and implemented, which uses this data structure to visualize 
relevant execution properties  at various levels of abstraction and allows one to navigate across 
these levels. such a dashboard can be implemented on top of the apromore repository.   21 
  
log
p
q
r
s
e
f
ga
b
da
b
c
d
ep
q
f
x
e
f
gprocess model collection
a a
b b
c c
d d
e ea
db cp
q
f
x
p p
q q
r >>
s >>
e >>
f f
- x
g -a
db c
e
consolidated
for reuse best-matching
alignment?full alignment
partial alignmentlack of accuracy
superfluous activity
skipped
eventmissing activityoverall alignment 
score = ?consolidated
for reuse best-matching
alignment?trace sub-
traceactivity
event 
figure 6: the backbone of liquid process model collections is a notion o f overall alignment score between logs and 
process model collections.  
 
research stream 2 (rs2): enhancement of static process model collections. this stream 
aims to make existing, static process models liquid, thus contributing to the realization of 
innova tion 1 (evidence -based process design). this involves  devising and implementing 
techniques for applying appropriate changes to an existing process model collection in order to 
improve its alignment (as defined in rs1) with the organizational behavior recor ded in the log. 
genetic algorithms such as simulated annealing (suman 2004) can be leveraged for this purpose, 22 
  to apply perturbations to the models in the collection that keep change to a minimum.  
in this stream we will also develop a method for enriching  an aligned process model collection 
with log -inferred execution properties, as identified in rs1. once again, the techniques 
envisaged in this research stream can be realized on top of prom.  
 
research stream 3 (rs3): domain -driven discovery of liquid pro cess model collections. 
this stream, complementary to rs2, aims to develop a set of parametric algorithms for the semi -
automatic discovery of liquid process model collections from scratch. therefore, it contributes to 
the realization of innovation 1 (evide nce-based process design).  
the novelty of these algorithms is that they will operate over process model collections, rather 
than single models, and be driven by domain knowledge , which can be provided by subject -
matter experts via parameters. in fact, whi le a plethora of process discovery algorithms is 
available (van der aalst 2011), none of them allows domain knowledge to influence the 
discovery. different algorithms should be developed based on the possible types of input. 
examples of different types of input are company -specific dimensions (e.g. products, brands, 
customer types) as well as quality metrics mandated by the company (e.g. a threshold for process 
model size). findings from multi -database mining (wu et al. 2005) can potentially inform the 
envisaged algorithms in order to discover process model collections from heterogeneous logs, 
i.e. logs generated from various systems. finally, mechanisms should be developed so that the 
discovered collections can be enriched with execution properties relevant  to the domain 
knowledge used as input. these mechanisms should be based on the techniques defined in rs2.  
this stream also aims to develop a technique for inferring relationships between the discovered 23 
  process models (e.g. abstraction and order relations hips) and use these to compose a process 
architecture (eid -sabbagh et al. 2012) to accompany the collection. similar to rs2, the 
discovery algorithms and the technique for inferring process architectures can be implemented in 
prom.  
 
research stream 4 (rs4 ): consolidation of liquid process model collections. this stream 
aims to design and implement an approach for retaining model alignment with organizational 
behavior, as the latter evolves over time. as such, this stream contributes to the realization of 
innovation 2 (continuous process realignment).  
a challenge to achieve this continuous realignment  will be how to distinguish transient changes 
in organizational behavior from those of a more permanent nature, aka concept drifts (bose et al. 
2011), that nee d to be reflected onto the process model collection. techniques for process model 
change required to realign a collection should be informed by rs2, and extended to deal with 
updates in the associated execution properties. conversely, a mechanism should be  developed to 
recognize changes intentionally made to the process model collection that may have not yet been 
observed in the log. this situation is illustrated in figure 7.  
another component of this research stream involves the design of a console that no tifies process 
stakeholders about potential changes to the process models of the collection that concern them, 
and which can visualize these changes as delta -differences on top of the existing models. such a 
console will assist the stakeholders with change  assessment, i.e. deciding whether and if so, to 
what degree to commit changes. for example, one may decide to only incorporate those changes 
that have been observed relatively frequently as part of new organizational behavior. this 
console should also all ow stakeholders to provide feedback on those changes that are considered 24 
  undesirable. this may provide input to targeted analyses, which is the scope of rs5.  
further, in this stream process model version control mechanisms (ekanayake et al. 2011) should 
be extended so that one can keep track of all versions of process model collections and their 
execution properties by storing delta -differences. this may provide input to targeted analyses in 
rs5, e.g. by comparing changes that have occurred over given timef rames.  
the consolidation approaches and console can be implemented in apromore.  
 
research stream 5 (rs5): reporting for liquid process model collections. this stream aims 
to develop techniques to generate sophisticated reports on a liquid process model co llection. 
such reports should cover historical organizational behavior (descriptive nature) or forecasts of 
future organizational behavior (predictive nature).  
concept drift
liquid process
model collection
(currently in use)liquid process
model collection
(discovered from
new behaviour)log at time1 log at time2 > time1a
b
c
x
e
f
ya
b
c
x
e
yb
c
c
x
eb
c
c
x
eb
c
c
x
e
ea
b
c
x
d
f
ya
b
c
d
e
f
ga
b
c
d
e
gb
c
c
d
fb
c
c
d
eb
c
c
d
e
ea
b
c
d
d
f
g
process
stakeholderintentional changes
since last versionnon-transient changes
since last log
liquid process
model collection
(consolidated)x
yx
yy
 
figure 7: consolidating a process model collection in order to cope  with evolutions of organizational behavior (e.g. 
due to new laws).  
these reports are likely to provide deep insights into different organizational aspects, ranging 
from performance issues to compliance violations and fraudulent activities. for example, it  will 
be possible to generate all process models in which the employee frank was involved, in the last 
month, that lasted 10 days; or all process models affected by a fraud through a change of account 25 
  in the last year; or all process models that are likely  to be impacted by the bankruptcy of an 
important client in the next month.  
the results of these reports are  presented in the form of new process models, driven by specific 
stakeholders’ demands, and serve as the basis for more informed organizational deci sions. as 
such, this stream aims to realize innovation 3 (demand -driven process modeling).  
process mining techniques are able to reveal the reasons for a good or bad performance and may 
even provide predictions and recommendations. however, for "what -if" analysis we will often 
need to resort to simulation. see v an der aalst (2010) for a discussion on the interplay between 
simulation and process mining.  
this stream also involves the design and development of a query language exploiting the data 
structure def ined in rs1. this language is to be supported by a dashboard that business analysts 
can use to submit their queries.  
indexing techniques from graph databases (jin et al. 2013) will need to be extended to index 
specific elements of a liquid process model co llection (e.g. execution properties) in order to 
efficiently execute queries over large, property -rich collections, while statistical techniques (e.g. 
regression analysis – freedman 2005) and data mining algorithms (e.g. decision tree building 
algorithms – quinlan 1993) can be employed as a basis for generating process model forecasts. 
the liquidity property of the collection will guarantee that the results of the queries will always 
be relevant.  
the envisioned dashboard and the underlying reporting techni ques can be implemented in 
apromore.  26 
  supporting software  
to realize the intended innovations we will build on prom and apromore and tightly integrate 
both. prom is a generic open -source framework for implementing process mining tools in a 
standard environm ent. it can be downloaded from www.processmining.org . the prom 
framework can load event logs in standard formats such as xes and mxml. the prom toolkit 
has been around for about a decade. during this period, the  prom framework has matured to a 
professional level. dozens of developers in different countries contributed to prom in the form 
of plug -ins. in the current version more than 600 plug -ins are available distributed over 100 
packages that can be loaded separ ately. through these plug -ins prom supports the entire process 
mining spectrum:  
 online and off -line process mining . event data can be partitioned into "pre mortem" 
and "post mortem" event logs. the term "post mortem" event data refers to information 
about cases that have completed, i.e., these data can be used for process improvement and 
auditing, but not for influencing the cases they refer to. the term "pre mortem" event data 
refers to cases that have not yet completed. if a case is still running, i.e., t he case is still 
"alive" (pre mortem), then it may be possible that information in the event log about this 
case (i.e., current data) can be exploited to ensure the correct or efficient handling of this 
case. "post mortem" event data is most relevant for o ff-line process mining, e.g., 
discovering the control -flow of a process based on one year of event data. for online 
process mining, mixtures of "pre mortem" (current) and "post mortem" (historic) data are 
needed. for example, historic information can be us ed to learn a predictive model. 
subsequently, information about a running case is combined with the predictive model to 
provide an estimate for the remaining flow time of the case.  27 
   different model types.  two types of models can be identified: "de jure mode ls" and "de 
facto models". a de jure model is normative, i.e., it specifies how things should be done 
or handled. for example, a process model used to configure a bpm system is normative 
and forces people to work in a particular way. a de facto model is de scriptive and its goal 
is not to steer or control reality. instead, de facto models aim to capture reality. both de 
jure and de facto models may cover multiple perspectives including the control -flow 
perspective ("how?"), the organizational perspective ("w ho?"), and the case perspective 
("what?"). these are supported by prom. the control -flow perspective describes the 
ordering of activities. the organizational perspective describes resources (worker, 
machines, customers, services, etc.) and organizational e ntities (roles, departments, 
positions, etc.). the case perspective describes data and rules. process mining can be 
used to determine the degree to which organizational aspects, as modeled in the above 
process perspectives, conform to observed behavior.  
 different types of process mining . as discussed before, process mining includes 
discovery, conformance checking, and enhancement. however, also more advanced 
forms of process mining like prediction, recommendation, and concept -drift analysis are 
supported by  prom.  
currently, prom does not support the management of collections of models and logs.  
apromore is an open and extensible repository to store and disclose business process models of a 
variety of languages, such as bpmn, eepcs, yawl and workflow nets. ap romore provides 
state-of-the-art features to facilitate the management of large process model collections.  
the backbone of apromore is a fragment -based version control mechanism (ekanayake et al. 
2011). accordingly, each single -entry single -exit (sese) fr agment of each process model is 28 
  independently versioned, and can be associated with an owner. this mechanism allows 
automatic detection of cloned fragments both between different versions of the same process 
model as well as between different process model s. cloned fragments are only stored once (thus 
allowing “vertical sharing” between different versions of the same process model, and 
“horizontal sharing” between different process models).  
sharing fragments vertically allows apromore to easily track differ ences between versions of the 
same model. sharing fragments horizontally enables change propagation features. when one 
makes a change to a process model fragment, the owners of all process models in the repository 
that will be impacted by this change (beca use these models contain clones to the fragment being 
changed) will be notified. depending on the change propagation policy, one can decide to 
commit the change, or not (in the latter case, a separate version of that fragment will be created).  
this fragmen t-based version control mechanism also allows access control at the level of single 
fragments, and concurrency control: multiple users can simultaneously work even on the same 
process model, provided they edit unrelated fragments.  
moreover, like in softwa re code, process model versions are organized in branches, where one 
branch may be created by “branching out” from a version in an existing branch.  
besides this fragment -based version control mechanism, apromore provides a wealth of features 
to manage larg e process model models, such as, for example, searching for similar models, 
merging similar models into a consolidated process model, identifying clones within and across 
process models, and cluster them.  
apromore relies on an internal format called canoni cal process format  to support the above 
features. whether one is after finding the similarity between your process models, or detecting 29 
  clones, apromore performs all these operations on the canonical format of the process models 
stored in its repository. t his way one can, for example, compare process models defined in 
different languages such as epcs and bpmn, or merge them into a process model and then 
decide the target language for this new model.  
the canonical process format provides a common, unambiguou s representation of business 
processes captured in different languages and/or at different abstraction levels, such that all 
process models can be treated alike.  
apromore is available as a sofware as a service (saas) at www.apromore.org . it relies on a 
plugin framework based on osgi. this way, new features can be added in the form of osgi 
plugins on -the-fly, and similarly, existing features can be uninstalled without the need to restart 
apromore.  
presently, apromore is tailored towards the management of process models rather than event 
logs. thus, the tool does not provide any feature to align modeled and observed behavior. in 
order to address the challenges mentioned above, a tight integration between apromore and 
prom is needed. moreover, we also envision the integration of other tools. for example, from 
prom we can call the simulation cpn tools for large -scale simulation experiments.  
conclusion  
this chapter discussed the need to relate modeled and observed behavior for large collections of 
processes . after introducing the main bpm use cases, we discussed the state -of-the-art in 
process mining and managing large process model collections. the “disconnect” between 
process mining research and the management of large mod el collections is severely limiting the 
application of bpm technologies. therefore, we suggest three major research innovations and 30 
  five different research streams to realize these innovations. the aim is to create “liquid” business 
process model collectio ns, i.e., collections of process models that are synchronized with the 
organizational reality and continuously adapt to evolving circumstances. this is the only way to 
breathe life into business process model collections. without it, model collections will  be static, 
and thus of limited value; they will soon become outdated unless they are manually updated, 
which is often an expensive operation.  
in this chapter we invite the research community to contribute to the research streams identified 
in this chapter , and hope that we made a good case that apromore and prom provide a good 
starting point for realizing the ideas presented. as described in the five research streams, the 
challenges that need to be dealt with are manifold, ranging from the continuous align ment of 
models and event logs to refactoring of process model collections with event data and advanced 
reporting on such collections.  
in ekanayake et al. (2013) we did some initial work in the direction of bridging this gap between 
process mining and manag ement of large process model collections. specifically, we developed 
a technique on top of apromore and prom, called slice, mine & dice (smd) , that can mine a 
hierarchical collection of process models from an event log, where each model in the collection 
has a bounded complexity (e.g. on the model size) that can b e set by the user. further, in v an der 
aalst (2013 c), we introduced the process cube  notion. the process cube structures event data 
using  different dimensions (type of process instance, type of eve nt and time window) in order to 
discover multiple inter -related processes or check the conformance thereof. moreover, by 
precisely aligning event data and process models we enable new types of simulation (van der 
aalst 2010, 2013b) where real and simulated  behaviors are combined. these ideas illustrate that 
“liquid” business process model collection s are likely to trigger new forms of process 31 
  management.  
references  
–, cxo “ the importance of bpm in a fast changing business environment”. issue 9, april 2008. www.cxo.eu.com  
(accessed: feb 2013).  
–, ebizq, “for many businesses, bpm ranks as a top priority for 2012”. 02/01/2012. www.ebizq.net  (accessed: feb 
2013).  
–, wintergreen research,  “business process management (bpm) market shares, strategies, and forecasts 
worldwide 2012 to 2018”, 2012.  
adriansyah, a., van dongen, b., and van der aalst, w.m.p. “conformance checking using cost -based fitness 
analysis”, proceedings of edoc , ieee compute r society, 2011  
awad, a., sakr, s: on efficient processing of bpmn -q queries. computers in industry  63(9): 867 -88, 2012.  
becker, m., laue, r.: a comparative survey of business process similarity measures. computers in industry  63(2): 
148-167, 2012.  
bose, r .p.j.c., van der aalst, w.m.p., zliobaite, i., and pechenizkiy, m. “handling concept drift in process 
mining”, proceedings of caise , lncs 6741, springer, 2011  
bunke, h. “on a relation between graph edit distance and maximum common subgraph”, pattern recogn ition 
letters  (18:8), 1997.  
davies, i., green, p., rosemann, m., indulska, m., and gallo, s. "how do practitioners use conceptual modeling in 
practice?", data & knowledge engineering  (58:3), 2006.  
dijkman, r.m., dumas, m., van dongen, b.f., käärik, r., and  mendling, j., “similarity of business process 
models: metrics and evaluation”, information systems  (36:2), 2011  
dijkman, r.m., la rosa, m., and reijers, h.a. “managing large collections of business process models – current 
techniques and challenges”, comp uters in industry , (63:2), 2012.  
dumas, m., garcía -bañuelos, l., la rosa, m., uba, r.: fast detection of exact clones in business process model 
repositories.  inf. syst.  38(4): 619 -633, 2013.  
dumas, m., la rosa, m., mendling, j. and reijers, h.a. fundament als of business process management , springer, 
2013.  32 
  eid-sabbagh, r. -h., dijkman, r.m., and weske, m. “business process architecture: use and correctness”, 
proceedings of bpm , lncs 7481, springer, 2012.  
ekanayake, c.c., dumas, d., garcía -bañuelos, l., la ro sa, m: slice, mine and dice: complexity -aware 
automated discovery of business process models. proceedings of bpm , lncs 8094: 49 -64, 2013.  
ekanayake, c.c., dumas, d., garcía -bañuelos, l., la rosa, m., ter hofstede, a.h.m.: approximate clone 
detection in rep ositories of business process models. proceedings of bpm : 302 -318, 2012.  
ekanayake, c.c., la rosa, m., ter hofstede, a.h.m., and fauvet, m.c., “fragment -based version management for 
repositories of business process models”, proceedings of coopis , lncs 7044 , springer, 2011.  
fahland, d., and van der aalst, w.m.p, “repairing process models to reflect reality”, proceedings of bpm , lncs 
7481, springer, 2012.  
freedman, d.a., statistical models: theory and practice , cambridge university press, 2005.  
gotts, i., “pu tting the m back in bpm”, bptrends , 2010. www.bptrends.com  (accessed: feb 2013).  
gottschalk, f. van der aalst, w.m.p. jansen -vullers,m.h. merging event -driven process chains, in: proc. of 
coopis, vol. 5331 of lncs, s pringer, 2008, pp. 418 –426. 
hilbert, m. and lopez, p., the world's technological capacity to store, communicate, and compute information. 
science, 332(6025), 60 -65, 2011.  
ieee task force on process mining. process mining manifesto. in f. daniel, k. barkaou i, and s. dustdar, editors, 
business process management workshops, volume 99 of lecture notes in business information 
processing, pages 169 -194. springer -verlag, berlin, 2012.  
jin, t., wang, j., wu, n., la rosa, m., and ter hofstede, a.h.m. “efficient quer ying of large process model 
repositories”, computers in industry , (64:1), 2013.  
kunze, m., weske, m.: metric trees for efficient similarity search in large process model repositories. business 
process management workshops 2010: 535 -546, 2011.  
la rosa, m.,  dumas, m., uba, r. and dijkman, r.m.,  business process model merging: an approach to business 
process  consolidation,  acm transactions on software engineering and methodology  (22:2), 2013.  
mendling, j. simon, c. business process design by view integration,  in: proc. of bpm workshops, vol. 4103 of 
lncs, springer, 2006: 55 –64. 
pascalau, e., awad, a., sakr, s., weske, m.: on maintaining consistency of process model variants. business 33 
  process management workshops 2010: 289 -300, 2011.  
quinlan, j.r., c4.5: progra ms for machine learning . morgan kaufmann publishers inc., 1993.  
raduescu, c., tan, h. m., jayaganesh, m., bandara, w., zur muehlen, m., and lippe, s. "a framework of issues in 
large process modeling projects," proceedings of ecis , association for informati on systems, 2006.  
reijers, h.a. mans, r.s. van der toorn, r.a. improved model management with aggregated business process 
models, data and knowledge engineering  68 (2), 2009: 221 –243. 
reijers, h.a., mendling, j., and dijkman, r.m., “human and automatic mod ularizations of process models to 
enhance their comprehension”, information systems  (36:5): 2011.  
rozinat, a. and van der aalst, w.m.p . conformance checking of processes based on monitoring real behavior. 
information systems, 33(1), 64 -95, 2008 . 
rozinat, a ., wynn, m., van der aalst, w.m.p., ter hofstede, a.h.m., fidge, c., workflow simulation for 
operational decision support. data and knowledge engineering. 68(9): 834 -850, 2009.  
rosemann, m. "potential pitfalls of process modeling: part b," business process  management journal  (12:3), 
2006.  
suman, b. “study of simulated annealing based algorithms for multi -objective optimization of a constrained 
problem”, computers & chemical engineering , (28:9), 2004.  
sun, s. kumar, a. yen,j. merging workflows: a new perspec tive on connecting business processes, decision 
support systems 42 (2) (2006) 844 –85 
jin, t., wang, j., la rosa, m., ter hofstede, a.h.m., wen, l.: efficient querying of large process model 
repositories. computers in industry  64(1): 41 -49, 2013.  
jin, t., w ang, j., wen, l: querying business process models based on semantics. dasfaa (2) 2011: 164 -178. 
van der aalst, w.m.p., business process management: a comprehensive survey. isrn software engineering, 
pages 1 -37, 2013 a. doi:10.1155/2013/507984.  
van der aalst , w.m.p. business process simulation survival guide. bpm center report bpm -13-11, 
bpmcenter.org, 2013 b. 
van der aalst, w.m.p.: process cubes: slicing, dicing, rolling up and drilling down event data for process 
mining, proceedings of ap -bpm , lnbip 159, 1 -22, springer, 2013 c. 
van der aalst, w.m.p., process mining: discovery, conformance and enhancement of business processes , 34 
  springer, 2011.  
van der aalst, w.m.p., business process simulation revisited. in j. barjis, editor, enterprise and organizational 
model ing and simulation, volume 63 of lecture notes in business information processing, pages 1 -14. 
springer -verlag, berlin, 2010.  
van der aalst w.m.p., adriansyah, a., and van dongen b., replaying history on process models for conformance 
checking and performa nce analysis. wires data mining and knowledge discovery, 2(2):182 -192, 2012.  
van dongen, b.f., alves de medeiros, a.k., and wen, l., “process mining: overview and outlook of petri net 
discovery algorithms”, transactions on petri nets and other models of co ncurrency 2 , springer, 2009.  
de weerdt, j., de backer, m., vanthienen, j., baesens, b., a multi -dimensional quality assessment of state -of-the-
art process discovery algorithms using real -life event logs. information systems 37(7):654 -676, 2012.  
weber, b. r eichert, m. mendling, j. reijers, h.a. refactoring large process model repositories, computers in 
industry  62 (5), 2011. 467 –486. 
weidlich, w., mendling, j., weske, m.: a foundational approach for managing process variability. caise 2011: 
267-282, 2011.  
wolf, c. and harmon, p., “the state of business process management 2012”, bptrends , 2012. www.bptrends.com  
(accessed: feb 2013).  
wu, x., zhang, c., zhang, s., “database classification for multi -database mining”, inform ation systems (30:1), 
2005.  
yan, z. dijkman, r. grefen, p. fast business process similarity search with feature based similarity estimation, in: 
on the move to meaningful internet systems: otm 2010, vol. 6426 of lncs, springer, 2010, pp. 60 –77. 
 
 
prof.dr.i r. wil van der aalst  is a full professor of information systems at the technische 
universiteit eindhoven (tu/e). he is also the academic supervisor of the international 
laboratory of process -aware information systems of the national research university, hi gher 
school of economics in moscow. moreover, since 2003 he has a part -time appointment at 
queensland university of technology (qut). at tu/e he is the scientific director of the data 35 
  science center eindhoven (dsc/e). his personal research interests includ e process mining, petri 
nets, and business process management. many of his papers are highly cited (he  has an h -index 
of more than 107  according to google scholar). in 2012, he received the degree of doctor 
honoris causa from hasselt university. in 2013, h e was appointed as distinguished university 
professor of tu/e and was awarded an honorary guest professorship at tsinghua university. he 
is also a member of the royal holland society of sciences and humanities and the academy of 
europe . 
 
marcello la rosa  is associate professor and the information systems school academic 
director for corporate engagements at queensland university of technology in brisbane, 
australia. he is also a researcher at the national ict australia. his research interests focus on 
proce ss consolidation, configuration, mining and automation. marcello has published over 60 
refereed papers on these topics including papers in top journals like acm tosem, formal 
aspects of computing and information systems. he leads the apromore initiative 
(www.apromore.org ) – a strategic collaboration between various universities for the 
development of an advanced process model repository. marcello has taught business process 
management (bpm) to students and practitione rs in australia for over eight years. he is co -
author of “fundamentals of business process management” (springer, 2013), the first 
comprehensive textbook on bpm. he was awarded with the best paper award at the 11th 
international conference on bpm. more inf ormation on marcello can be found at 
www.marcellolarosa.com . 
 
arthur ter hofstede  is a professor in the information systems school in the science and 36 
  engineering faculty, queensland university of technology, br isbane, australia, and is head of 
the business process management discipline. he is also a professor in the information systems 
group of the school of industrial engineering of eindhoven university of technology,  
eindhoven, the netherlands. his research in terests are in the areas of business process 
automation and process mining.  
 
moe wynn  is a researcher in the field of business process management (bpm) within the 
information systems school at queensland university of technology, australia. she received 
her phd in the area of business process automation/workflow management in 2007. her current 
research interests include cost -aware bpm, ri sk-aware bpm, process automation, and process 
mining. she works on inter -disciplinary research projects (e.g., in the healthcare domain) and 
carries out collaborative research with industry partners.  she has published over 45 refereed 
papers in internatio nal journals and conferences in the field of bpm in the past ten years. her 
research appeared in the following journals:  information sciences, data and knowledge 
engineering, information and software technology, formal aspects of computing, journal of 
com puter and system sciences, international journal of cooperative information systems, 
transactions on petri nets and other models of concurrency, computers in industry, and 
journal of information technology theory and applications.  
 