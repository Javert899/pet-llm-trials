green data science 
using big data in an “environmentally friendly” manner 
wil m.p. van der aalst 
eindhoven university of technology, department of mathematics and computer science, 
po box 513, nl-5600 mb eindhoven, the netherlands. 
w.m.p.v.d.aalst@tue.nl 
keywords: data science, big data, fairness, conﬁdentiality, accuracy, transparency, process mining. 
abstract: the widespread use of “big data” is heavily impacting organizations and individuals for which these data are 
collected. sophisticated data science techniques aim to extract as much value from data as possible. powerful 
mixtures of big data and analytics are rapidly changing the way we do business, socialize, conduct research, 
and govern society. big data is considered as the “new oil” and data science aims to transform this into new 
forms of “energy”: insights, diagnostics, predictions, and automated decisions. however, the process of trans- 
forming “new oil” (data) into “new energy” (analytics) may negatively impact citizens, patients, customers, 
and employees. systematic discrimination based on data, invasions of privacy, non-transparent life-changing 
decisions, and inaccurate conclusions illustrate that data science techniques may lead to new forms of “pollu- 
tion”. we use the term “green data science” for technological solutions that enable individuals, organizations 
and society to reap the beneﬁts from the widespread availability of data while ensuring fairness, conﬁden- 
tiality, accuracy, and transparency. to illustrate the scientiﬁc challenges related to “green data science”, we 
focus on process mining as a concrete example. recent breakthroughs in process mining resulted in powerful 
techniques to discover the real processes, to detect deviations from normative process models, and to analyze 
bottlenecks and waste. therefore, this paper poses the question: how to beneﬁt from process mining while 
avoiding “pollutions” related to unfairness, undesired disclosures, inaccuracies, and non-transparency? 
1 introduction 
in recent years, data science emerged as a new and 
important discipline. it can be viewed as an amal- 
gamation of classical disciplines like statistics, data 
mining, databases, and distributed systems. we use 
the following deﬁnition: “data science is an inter- 
disciplinary ﬁeld aiming to turn data into real value. 
data may be structured or unstructured, big or small, 
static or streaming. value may be provided in the form 
of predictions, models learned from data, or any type 
of data visualization delivering insights. data science 
includes data extraction, data preparation, data ex- 
ploration, data transformation, storage and retrieval, 
computing infrastructures, various types of mining 
and learning, presentation of explanations and pre- 
dictions, and the exploitation of results taking into 
account ethical, social, legal, and business aspects. ” 
(aalst, 2016).
related to data science is the overhyped term “big 
data” that is used to refer to the massive amounts 
of data collected. organizations are heavily invest- 
ing in big data technologies, but at the same time citizens, patients, customers, and employees are con-
cerned about the use of their data. we live in an 
era characterized by unprecedented opportunities to 
sense, store, and analyze data related to human ac- 
tivities in great detail and resolution. this introduces 
new risks and intended or unintended abuse enabled 
by powerful analysis techniques. data may be sensi- 
tive and personal, and should not be revealed or used 
for proposes different from what was agreed upon. 
moreover, analysis techniques may discriminate mi- 
norities even when attributes like gender and race are 
removed. using data science technology as a “black 
box” making life-changing decisions (e.g., medical
prioritization or mortgage approvals) triggers a vari- 
ety of ethical dilemmas. 
sustainable data science is only possible when 
citizens, patients, customers, and employees are pro- 
tected against irresponsible uses of data (big or 
small). therefore, we need to separate the “good” 
and “bad” of data science. compare this with envi- 
ronmentally friendly forms of green energy (e.g. so- 
lar power) that overcome problems related to tradi- 
tional forms of energy. data science may result in 
w.m.p.vanderaalst.greendatascience:usingbigdatainan 
environmentallyfriendlymanner.proceedingsofthe18thinternational
conferenceonenterpriseinformationsystems(iceis2016),insticc,
rome,april2016.unfair decision making, undesired disclosures, inac-
curacies, and non-transparency. these irresponsible 
uses of data can be viewed as “pollution”. abandon- 
ing the systematic use of data may help to overcome 
these problems. however, this would be comparable 
to abandoning the use of energy altogether. data sci- 
ence is used to make products and services more reli- 
able, convenient, efﬁcient, and cost effective. more- 
over, most new products and services depend on the 
collection and use of data. therefore, we argue that 
the “prohibition of data (science)” is not a viable so- 
lution.
in this paper, we coin the term “green data sci-
ence” (gds) to refer to the collection of techniques 
and approaches trying to reap the beneﬁts of data sci- 
ence and big data while ensuring fairness, conﬁden- 
tiality, accuracy, and transparency. we believe that 
technological solutions can be used to avoid pollution 
and protect the environment in which data is collected 
and used.
section 2 elaborates on the following four chal- 
lenges:
•fairness – data science without prejudice: how 
to avoid unfair conclusions even if they are true? 
•conﬁdentiality – data science that ensures con- 
ﬁdentiality: how to answer questions without re- 
vealing secrets? 
•accuracy – data science without guesswork: 
how to answer questions with a guaranteed level 
of accuracy? 
•transparency – data science that provides trans- 
parency: how to clarify answers such that they 
become indisputable?
concerns related to privacy and personal data pro- 
tection triggered legislation like the eu’s data protec- 
tion directive. directive 95/46/ec (“on the protection
of individuals with regard to the processing of per- 
sonal data and on the free movement of such data”) of 
the european parliament and the council was adopted 
on 24 october 1995 (european commission, 1995). 
the general data protection regulation (gdpr) is 
currently under development and aims to strengthen 
and unify data protection for individuals within the
eu (european commission, 2015). gdpr will re- 
place directive 95/46/ec and is expected to be ﬁnal- 
ized in spring 2016 and will be much more restrictive 
than earlier legislation. sanctions include ﬁnes of up 
to 4% of the annual worldwide turnover. gdpr and 
other forms of legislation limiting the use of data, may 
prevent the use of data science also in situations where 
data is used in a positive manner. prohibiting the col- 
lection and systematic use of data is like turning back 
the clock. next to legislation, positive technological solutions are needed to ensure fairness, conﬁdential- 
ity, accuracy, and transparency. by just imposing re- 
strictions, individuals, organizations and society can- 
not exploit data (science) in a positive way. 
the four challenges discussed in section 2 are 
quite general. therefore, we focus on a concrete 
subdiscipline in data science in section 3: process 
mining (aalst, 2011). process mining seeks the con-
frontation between event data (i.e., observed behav- 
ior) and process models (hand-made or discovered au- 
tomatically). event data are related to explicit process 
models, e.g., petri nets or bpmn models. for exam- 
ple, process models are discovered from event data or 
event data are replayed on models to analyze com- 
pliance and performance. process mining provides 
a bridge between data-driven approaches (data min- 
ing, machine learning and business intelligence) and 
process-centric approaches (business process model- 
ing, model-based analysis, and business process man- 
agement/reengineering). process mining results may
drive redesigns, show the need for new controls, trig- 
ger interventions, and enable automated decision sup-
port. individuals inside (e.g., end-users and workers) 
andoutside (e.g., customers, citizens, or patients) the 
organization may be impacted by process mining re- 
sults. therefore, section 3 lists process mining chal- 
lenges related to fairness, conﬁdentiality, accuracy, 
and transparency. 
in the long run, data science is only sustainable 
if we are willing to address the problems discussed 
in this paper. rather than abandoning the use of data 
altogether, we should ﬁnd positive technological ways 
to protect individuals. 
2 four challenges 
figure 1 sketches the “data science pipeline”. individ- 
uals interact with a range of hardware/software sys- 
tems (information systems, smartphones, websites,
wearables, etc.) ➊. data related to machine and in- 
teraction events are collected ➋and preprocessed
for analysis ➌. during preprocessing data may be 
transformed, cleaned, anonymized, de-identiﬁed, etc.
models may be learned from data or made/modiﬁed 
by hand ➍. for compliance checking, models are of- 
ten normative and made by hand rather than discov- 
ered from data. analysis results based on data (and 
possibly also models) are presented to analysts, man- 
agers, etc. ➎or used to inﬂuence the behavior of in- 
formation systems and devices ➏. based on the data, 
decisions are made or recommendations are provided. 
analysis results may also be used to change systems, 
laws, procedures, guidelines, responsibilities, etc. ➐.data in a 
variety of 
systems data used as 
input for 
analytics information 
systems, 
devices, etc.
   
  
 
results models 
extract, load, 
transform, clean, 
anonymize, de-
identify, etc.report, discover, 
mine, learn, check, 
predict, recommend, etc.interaction with individuals 
interpretation by analysts, managers, etc.1
2 34
56
7
figure 1: the “data science pipeline” facing four challenges. 
figure 1 also lists the four challenges discussed 
in the remainder of this section. each of the chal- 
lenges requires an understanding of the whole data 
pipeline. flawed analysis results or bad decisions 
may be caused by different factors such as a sampling 
bias, careless preprocessing, inadequate analysis, or 
an opinionated presentation. 
2.1 fairness - data science without 
prejudice: how to avoid unfair 
conclusions even if they are true? 
data science techniques need to ensure fairness : au- 
tomated decisions and insights should not be used to 
discriminate in ways that are unacceptable from a le- 
gal or ethical point of view. discrimination can be de- 
ﬁned as “the harmful treatment of an individual based 
on their membership of a speciﬁc group or category 
(race, gender, nationality, disability, marital status, or 
age)”. however, most analysis techniques aim to dis- 
criminate among groups. banks handing out loans
and credit cards try to discriminate between groups 
that will pay their debts and groups that will run into
ﬁnancial problems. insurance companies try to dis- 
criminate between groups that are likely to claim and 
groups that are less likely to claim insurance. hos- 
pitals try to discriminate between groups for which 
a particular treatment is likely to be effective and 
groups for which this is less likely. hiring employ- 
ees, providing scholarships, screening suspects, etc.can all be seen as classiﬁcation problems: the goal 
is to explain a response variable (e.g., person will pay 
back the loan) in terms of predictor variables (e.g., 
credit history, employment status, age, etc.). ideally, 
the learned model explains the response variable as 
good as possible without discriminating on the basis 
of sensitive attributes (race, gender, etc.). 
to explain discrimination discovery anddiscrimi-
nation prevention , let us consider the set of all (poten- 
tial) customers of some insurance company specializ- 
ing in car insurance. for each customer we have the 
following variables: 
•name,
•birthdate,
•gender (male or female), 
•nationality,
•car brand (alfa, bmw, etc.), 
•years of driving experience, 
•number of claims in the last year, 
•number of claims in the last ﬁve years, and 
•status (insured, refused, or left). 
the status ﬁeld is used to distinguish current cus- 
tomers (status=insured) from customers that were re-
fused (status=refused) or that left the insurance com- 
pany during the last year (status=left). customers that 
were refused or that left more than a year ago are re- 
moved from the data set. techniques for discrimination discovery aim to 
identify groups that are discriminated based on sen-
sitive variables, i.e., variables that should not matter. 
for example, we may ﬁnd that “males have a higher 
likelihood to be rejected than females” or that “for- 
eigners driving a bmw have a higher likelihood to be 
rejected than dutch bmw drivers”. discrimination 
may be caused by human judgment or by automated 
decision algorithms using a predictive model. the 
decision algorithms may discriminate due to a sam- 
pling bias, incomplete data, or incorrect labels. if ear- 
lier rejections are used to learn new rejections, then 
prejudices may be reinforced. similar “self-fulﬁlling 
prophecies” can be caused by sampling or missing 
values. 
even when there is no intent to discriminate, dis- 
crimination may still occur. even when the auto- 
mated decision algorithm does not use gender and
uses only non-sensitive variables, the actual decisions 
may still be such that (fe)males or foreigners have a 
much higher probability to be rejected. the decision 
algorithm may also favor more frequent values for a 
variable. as a result, minority groups may be treated 
unfairly.
fairness 
accuracy low accuracy highest accuracy 
possible using all data 
without constraints analysis results 
and model are 
non-discriminating 
analysis results and 
model are created 
without considering 
discrimination possible compromise 
between fairness and 
accuracy ideal 
situation 
(impossible)
figure 2: tradeoff between fairness and accuracy. 
discrimination prevention aims to create auto- 
mated decision algorithms that do not discriminate us- 
ing sensitive variables. it is not sufﬁcient to remove 
these sensitive variables: due to correlations and the 
handling of outliers, unintentional discrimination may 
still take place. one can add constraints to the deci- 
sion algorithm to ensure fairness using a predeﬁned 
criterion. for example, the constraint “males and fe- 
males should have approximately the same probabil- 
ity to be rejected” can be added to a decision-tree 
learning algorithm. next to adding algorithm-speciﬁc 
constraints used during analysis one can also use pre- 
processing (modify the input data by resampling or 
relabeling) or postprocessing (modify models, e.g., 
relabel mixed leaf nodes in a decision tree). in gen- 
eral there is often a trade-off between maximizing ac- 
curacy and minimizing discrimination (see figure 2).by rejecting fewer males (better fairness), the insur- 
ance company may need to pay more claims. 
discrimination prevention often needs to use sen- 
sitive variables (gender, age, nationality, etc.) to en- 
sure fairness. this creates a paradox , e.g., informa- 
tion on gender needs to be used to avoid discrimina- 
tion based on gender. 
the ﬁrst paper on discrimination-aware data min- 
ing appeared in 2008 (pedreshi et al., 2008). since 
then, several papers mostly focusing on fair classiﬁca- 
tion appeared: (calders and verwer, 2010; kamiran 
et al., 2010; ruggieri et al., 2010). these examples 
show that unfairness during analysis can be actively 
prevented. however, unfairness is not limited to clas- 
siﬁcation and more advanced forms of analytics also 
need to ensure fairness. 
2.2 conﬁdentiality - data science that 
ensures conﬁdentiality: how to 
answer questions without 
revealing secrets? 
the application of data science techniques should not 
reveal certain types of personal or otherwise sensi- 
tive information. often personal data need to be kept 
conﬁdential . the general data protection regula- 
tion (gdpr) currently under development (european 
commission, 2015) focuses on personal information: 
“the principles of data protection should apply to any in- 
formation concerning an identiﬁed or identiﬁable natural 
person. data including pseudonymized data, which could 
be attributed to a natural person by the use of additional in- 
formation, should be considered as information on an iden- 
tiﬁable natural person. to determine whether a person is 
identiﬁable, account should be taken of all the means rea- 
sonably likely to be used either by the controller or by any 
other person to identify the individual directly or indirectly. 
to ascertain whether means are reasonably likely to be used 
to identify the individual, account should be taken of all ob- 
jective factors, such as the costs of and the amount of time 
required for identiﬁcation, taking into consideration both 
available technology at the time of the processing and tech- 
nological development. the principles of data protection 
should therefore not apply to anonymous information, that 
is information which does not relate to an identiﬁed or iden- 
tiﬁable natural person or to data rendered anonymous in 
such a way that the data subject is not or no longer identiﬁ- 
able. ” 
conﬁdentiality is not limited to personal data. compa- 
nies may want to hide sales volumes or production times 
when presenting results to certain stakeholders. one also 
needs to bear in mind that few information systems hold 
information that can be shared or analyzed without limits 
(e.g., the existence of personal data cannot be avoided). the 
“data science pipeline” depicted in figure 1 shows that there 
are different types of data having different audiences. here 
we focus on: (1) the “raw data” stored in the information 
system ➋, (2) the data used as input for analysis ➌, and (3) the analysis results interpreted by analysts and managers 
➎. whereas the raw data may refer to individuals, the data 
used for analysis is often (partly) de-identiﬁed, and analysis 
results may refer to aggregate data only. it is important to 
note that conﬁdentiality may be endangered along the whole 
pipeline and includes analysis results.
consider a data set that contains sensitive information. 
records in such a data set may have three types of variables: 
•direct identiﬁers : variables that uniquely identify a 
person, house, car, company, or other entity. for ex- 
ample, a social security number identiﬁes a person. 
•key variables : subsets of variables that together can be 
used to identify some entity. for example, it may be 
possible to identify a person based on gender, age, and 
employer. a car may be uniquely identiﬁed based on 
registration date, model, and color. key variables are 
also referred to as implicit identiﬁers or quasi identi-
ﬁers.
•non-identifying variables : variables that cannot be 
used to identify some entity (direct or indirect). 
conﬁdentiality is impaired by unintended or malicious 
disclosures. we consider three types of such disclosures: 
•identity disclosure : information about an entity (per- 
son, house, etc.) is revealed. this can be done through 
direct or implicit identiﬁers. for example, the salaries 
of employees are disclosed unintentionally or an in- 
truder is able to retrieve patient data. 
•attribute disclosure : information about an entity can be 
derived indirectly. if there is only one male surgeon in 
the age group 40-45, then aggregate data for this cate- 
gory reveals information about this person. 
•partial disclosure : information about a group of entities 
can be inferred. aggregate information on male sur- 
geons in the age group 40-45 may disclose an unusual 
number of medical errors. these cannot be linked to 
a particular surgeon. nevertheless, one may conclude 
that surgeons in this group are more likely to make er- 
rors.
de-identiﬁcation of data refers to the process of remov- 
ing or obscuring variables with the goal to minimize unin- 
tended disclosures. in many cases re-identiﬁcation is pos- 
sible by linking different data sources. for example, the 
combination of wedding date and birth date may allow for 
the re-identiﬁcation of a particular person. anonymization
of data refers to de-identiﬁcation that is irreversible: re- 
identiﬁcation is impossible. a range of de-identiﬁcation 
methods is available: removing variables, randomization, 
hashing, shufﬂing, sub-sampling, aggregation, truncation, 
generalization, adding noise, etc. adding some noise to a 
continuous variable or the coarsening of values may have a 
limited impact on the quality of analysis results while en- 
suring conﬁdentiality.
there is a trade-off between minimizing the disclosure 
of sensitive information and the usefulness of analysis re- 
sults (see figure 3). removing variables, aggregation, and 
adding noise can make it hard to produce any meaningful 
analysis results. emphasis on conﬁdentiality (like security) 
may also reduce convenience. note that personalization of-
ten conﬂicts with fairness and conﬁdentiality . disclosing all 
data, supports analysis, but jeopardizes conﬁdentiality. 
confidentiality 
data utility no meaningful 
analysis possible full use of data 
potential possible full disclosure 
of sensitive 
data no sensitive 
data disclosed 
possible compromise 
between confidentiality 
and utility ideal 
situation 
(impossible)
figure 3: tradeoff between conﬁdentiality and utility. 
access rights to the different types of data and analy- 
sis results in the “data science pipeline” (figure 1) vary per 
group. for example, very few people will have access to 
the “raw data” stored in the information system ➋. more 
people will have access to the data used for analysis and 
the actual analysis results. poor cybersecurity may endan- 
ger conﬁdentiality. good policies ensuring proper authen- 
tication (are you who you say you are?) and authorization
(what are you allowed to do?) are needed to protect access 
to the pipeline in figure 1. cybersecurity measures should 
not complicate access, data preparation, and analysis; oth-
erwise people may start using illegal copies and replicate 
data.
see (monreale et al., 2014; nelson, 2015; president’s 
council, 2014) for approaches to ensure conﬁdentiality. 
2.3 accuracy - data science without 
guesswork: how to answer 
questions with a guaranteed level 
of accuracy? 
increasingly decisions are made using a combination of al- 
gorithms and data rather than human judgement. hence,
analysis results need to be accurate and should not deceive 
end-users and decision makers. yet, there are several fac- 
tors endangering accuracy. 
first of all, there is the problem of overﬁtting the data 
leading to “bogus conclusions”. there are numerous exam- 
ples of so-called spurious correlations illustrating the prob-
lem. some examples (taken from (vigen, 2015)): 
•the per capita cheese consumption strongly correlates
with the number of people who died by becoming tan- 
gled in their bedsheets. 
•the number of japanese passenger cars sold in the 
us strongly correlates with the number of suicides by 
crashing of motor vehicle. 
•us spending on science, space and technology strongly 
correlates with suicides by hanging, strangulation and 
suffocation.
•the total revenue generated by arcades strongly corre- 
lates with the number of computer science doctorates 
awarded in the us. according to bonferroni’s principle we need to avoid treating random observations as if they are real and sig- 
niﬁcant (rajaraman and ullman, 2011). the following example, inspired by a similar example in (rajaraman 
and ullman, 2011), illustrates the risk of treating completely random events as patterns. 
adutch government agency is searching for terrorists by examining hotel visits of all of its 18 million citizens 
(18×10 6). the hypothesis is that terrorists meet multiple times at some hotel to plan an attack. hence, the 
agency looks for suspicious “events” {p1,p2}†{d1,d2}where persons p1andp2meet on days d1andd2.
how many of such suspicious events will the agency ﬁnd if the behavior of people is completely random? to 
estimate this number we need to make some additional assumptions. on average, dutch people go to a hotel 
every 100 days and a hotel can accommodate 100 people at the same time. we further assume that there are 
18 ×10 6
100×100=1800 dutch hotels where potential terrorists can meet.
the probability that two persons ( p1andp2) visit a hotel on a given day dis 1
100×1
100=10 −4. the probability 
thatp1andp2visit the same hotel on day dis 10 −4×1
1800=5.55 ×10 −8. the probability that p1andp2visit
the same hotel on two different days d1andd2is (5.55 ×10 −8)2=3.086×10 −15 . note that different hotels 
may be used on both days. hence, the probability of suspicious event {p1,p2}†{d1,d2}is 3 .086×10 −15 .
how many candidate events are there? assume an observation period of 1000 days. hence, there are 1000 ×
(1000−1)/2=499,500 combinations of days d1andd2. note that the order of days does not matter, but the 
days need to be different. there are (18 ×10 6)×(18 ×10 6−1)/2=1.62 ×10 14 combinations of persons p1
andp2. again the ordering of p1andp2does not matter, but p1/ne}ationslash=p2. hence, there are 499 ,500×1.62 ×10 14 =
8.09 ×10 19 candidate events {p1,p2}†{d1,d2}.
the expected number of suspicious events is equal to the product of the number of candidate events {p1,p2}†
{d1,d2}and the probability of such events (assuming independence): 8 .09 ×10 19 ×3.086×10 −15 =249,749.
hence, there will be around a quarter million observed suspicious events {p1,p2}†{d1,d2}in a 1000 day 
period!
suppose that there are only a handful of terrorists and related meetings in hotels. the dutch government agency 
will need to investigate around a quarter million suspicious events involving hundreds of thousands innocent 
citizens. using bonferroni’s principle, we know beforehand that this is not wise: there will be too many false 
positives. 
example: bonferroni’s principle explained using an example taken from (aalst, 2016). to apply the principle, compute the 
number of observations of some phenomena one is interested in under the assumption that things occur at random. if this 
number is signiﬁcantly larger than the real number of instances one expects, then most of the ﬁndings will be false positives. 
when using many variables relative to the number of in- 
stances, classiﬁcation may result in complex rules overﬁt- 
ting the data. this is often referred to as the curse of di- 
mensionality : as dimensionality increases, the number of 
combinations grows so fast that the available data become 
sparse. with a ﬁxed number of instances, the predictive 
power reduces as the dimensionality increases. using cross- 
validation most ﬁndings (e.g., classiﬁcation rules) will get 
rejected. however, if there are many ﬁndings, some may 
survive cross-validation by sheer luck. 
in statistics, bonferroni’s correction is a method (named 
after the italian mathematician carlo emilio bonferroni) to 
compensate for the problem of multiple comparisons. nor- 
mally, one rejects the null hypothesis if the likelihood of 
the observed data under the null hypothesis is low (casella 
and berger, 2002). if we test many hypotheses, we also in- 
crease the likelihood of a rare event. hence, the likelihood 
of incorrectly rejecting a null hypothesis increases (miller, 
1981). if the desired signiﬁcance level for the whole col- 
lection of null hypotheses is α, then the bonferroni correc- 
tion suggests that one should test each individual hypoth- 
esis at a signiﬁcance level of α
kwhere kis the number of 
null hypotheses. for example, if α=0.05 and k=20, then
α
k=0.0025 is the required signiﬁcance level for testing the 
individual hypotheses. next to overﬁtting the data and testing multiple hy- 
potheses, there is the problem of uncertainty in the input 
data and the problem of not showing uncertainty in the re- 
sults.
uncertainty in the input data is related to the fourth “v” 
in the four “v’s of big data” (v olume, velocity, variety, 
and veracity). veracity refers to the trustworthiness of the 
input data. sensor data may be uncertain, multiple users 
may use the same account, tweets may be generated by soft- 
ware rather than people, etc. these uncertainties are often 
not taken into account during analysis assuming that things 
“even out” in larger data sets. this does not need to be the 
case and the reliability of analysis results is affected by un- 
reliable or probabilistic input data. 
when we say, “we are 95% conﬁdent that the true value 
of parameter xis in our conﬁdence interval [a,b]”, we mean 
that 95% of the hypothetically observed conﬁdence inter- 
vals will hold the true value of parameter x. averages, sums, 
standard deviations, etc. are often based on sample data. 
therefore, it is important to provide a conﬁdence interval. 
for example, given a mean of 35 .4 the 95% conﬁdence in- 
terval may be [35 .3,35 .6], but the 95% conﬁdence interval 
may also be [15 .3,55 .6]. in the latter case, we will inter- 
pret the mean of 35 .4 as a “wild guess” rather than a rep- 
resentative value for true average value. although we are used to conﬁdence intervals for numerical values, decision 
makers have problems interpreting the expected accuracy 
of more complex analysis results like decision trees, asso- 
ciation rules, process models, etc. cross-validation tech-
niques like k-fold checking and confusion matrices give 
some insights. however, models and decisions tend to be 
too “crisp” (hiding uncertainties). explicit vagueness or 
more explicit conﬁdence diagnostics may help to better in- 
terpret analysis results. parts of models should be kept de- 
liberately “vague” if analysis is not conclusive. 
2.4 transparency - data science that 
provides transparency: how to 
clarify answers such that they
become indisputable?
data science techniques are used to make a variety of de- 
cisions. some of these decisions are made automatically 
based on rules learned from historic data. for example, a 
mortgage application may be rejected automatically based 
on a decision tree. other decisions are based on analysis re- 
sults (e.g., process models or frequent patterns). for exam- 
ple, when analysis reveals previously unknown bottlenecks, 
then this may have consequences for the organization of 
work and changes in stafﬁng (or even layoffs). automated 
decision rules ( ➏in figure 1) need to be as accurate as pos- 
sible (e.g., to reduce costs and delays). analysis results ( ➎
in figure 1) also need to be accurate. however, accuracy 
is not sufﬁcient to ensure acceptance and proper use of data 
science techniques. both decisions ➏and analysis results
➎also need to be transparent .
gender 
age accept 
reject gender 
age accept 
reject gender 
age accept 
reject black box 
data-driven 
decision system 2
data-driven 
decision system 3data-driven 
decision system 1
your claim is 
rejec ted 
because you 
are male 
and above 50 
... 
figure 4: different levels of transparency. 
figure 4 illustrates the notion of transparency. consider 
an application submitted by john evaluated using three data- 
driven decision systems. the ﬁrst system is a black box: it 
is unclear why john’s application is rejected. the second 
system reveals it’s decision logic in the form of a decision 
tree. applications from females and younger males are al-
ways accepted. only applications from older males get re- 
jected. the third system uses the same decision tree, but 
also explains the rejection (“because male and above 50”). clearly, the third system is most transparent. when govern- 
ments make decisions for citizens it is often mandatory to 
explain the basis for such decisions. 
deep learning techniques (like many-layered neural 
networks) use multiple processing layers with complex
structures or multiple non-linear transformations. these 
techniques have been successfully applied to automatic 
speech recognition, image recognition, and various other 
complex decision tasks. deep learning methods are often 
looked at as a “black box”, with performance measured 
empirically and no formal guarantees or explanations. a 
many-layered neural network is not as transparent as for ex- 
ample a decision tree. such a neural network may make 
good decisions, but it cannot explain a rule or criterion. 
therefore, such black box approaches are non-transparent
and may be unacceptable in some domains. 
transparency is not restricted to automated decision 
making and explaining individual decisions, it also involves 
the intelligibility, clearness, and comprehensibility of anal- 
ysis results (e.g., a process model, decision tree, regression 
formula). for example, a model may reveal bottlenecks in a 
process, possible fraudulent behavior, deviations by a small 
group of individuals, etc. it needs to be clear for the user of 
such models (e.g., a manager) how these ﬁndings where ob- 
tained. the link to the data and the analysis technique used 
should be clear. for example, ﬁltering the input data (e.g., 
removing outliers) or adjusting parameters of the algorithm 
may have a dramatic effect on the model returned. 
storytelling is sometimes referred to as “the last mile 
in data science”. the key question is: how to communi- 
cate analysis results with end-users? storytelling is about 
communicating actionable insights to the right person, at 
the right time, in the right way. one needs to know the gist 
of the story one wants to tell to successfully communicate 
analysis results (rather than presenting the whole model and
all data). one can use natural language generation to trans- 
form selected analysis results into concise, easy-to-read, in-
dividualized reports. 
to provide transparency there should be a clear link be- 
tween data and analysis results/stories. one needs to be able 
to drill-down and inspect the data from the model’s perspec- 
tive. given a bottleneck one needs to be able to drill down 
to the instances that are delayed due to the bottleneck. this 
related to data provenance : it should always be possible to 
reproduce analysis results from the original data.
the four challenges depicted in figure 1 are clearly in- 
terrelated. there may be trade-offs between fairness ,conﬁ-
dentiality ,accuracy andtransparency . for example, to en- 
sure conﬁdentiality we may add noise and de-identify data 
thus possibly compromising accuracy and transparency. 
3 example: green process 
mining 
the goal of process mining is to turn event data into in- 
sights and actions (aalst, 2016). process mining is an inte- 
gral part of data science, fueled by the availability of data 
and the desire to improve processes. process mining can 
be seen as a means to bridge the gap between data science 
and process science. data science approaches tend to be data in a 
variety of 
systems data used as 
input for 
analytics information 
systems, 
devices, etc.
   
  
 
results models 
extract, load, 
transform, clean, 
anonymize, de-
identify, etc.report, discover, 
mine, learn, check, 
predict, recommend, etc.interaction with individuals 
interpretation by analysts, managers, etc.1
2 34
56
7
event data 
(e.g., in xes 
format)data in databases, 
files, logs, etc. 
having a temporal 
dimension process models (e.g., 
bpmn, uml ad/sds, petri 
nets, workflow models) 
techniques for process discovery, 
conformance checking, and 
performance analysis results include process models 
annotated with frequencies, 
times, and deviations operational support, 
e.g., predictions, 
recommendations, 
decisions, and alerts people and devices 
generating a 
variety of events 
figure 5: the “process mining pipeline” relates observed and modeled behavior. 
process agonistic whereas process science approaches tend
to be model-driven without considering the “evidence” hid- 
den in the data. this section discusses challenges related to 
fairness, conﬁdentiality, accuracy, and transparency in the 
context of process mining. the goal is not to provide so- 
lutions, but to illustrate that the more general challenges 
discussed before trigger concrete research questions when 
considering processes and event data. 
3.1 what is process mining? 
figure 5 shows the “process mining pipeline” and can be 
viewed as a specialization of the figure 1. process mining 
focuses on the analysis of event data and analysis results
are often related to process models . process mining is a 
rapidly growing subdiscipline within both business process
management (bpm) (aalst, 2013a) and data science (aalst,
2014). mainstream business intelligence (bi), data min-
ing and machine learning tools are not tailored towards the 
analysis of event data and the improvement of processes. 
fortunately, there are dedicated process mining tools able 
to transform event data into actionable process-related in- 
sights. for example, prom (www.processmining.org ) is 
an open-source process mining tool supporting process dis- 
covery, conformance checking, social network analysis, or- 
ganizational mining, clustering, decision mining, predic- 
tion, and recommendation (see figure 6). moreover, in 
recent years, several vendors released commercial process 
mining tools. examples include: celonis process mining 
by celonis gmbh ( www.celonis.de ), disco by fluxicon 
(www.fluxicon.com ), interstage business process man- 
ager analytics by fujitsu ltd ( www.fujitsu.com ), minit
by gradient ecm ( www.minitlabs.com ), myinvenio by 
cognitive technology ( www.my-invenio.com ), perceptive 
process mining by lexmark ( www.lexmark.com ), qpr
processanalyzer by qpr ( www.qpr.com ), rialto process 
by exeura ( www.exeura.eu ), snp business process anal- 
ysis by snp schneider-neureither & partner ag ( www.snp-bpa.com ), and ppm webmethods process perfor- 
mance manager by software ag ( www.softwareag.com ). 
3.1.1 creating and managing event data 
process mining is impossible without proper event logs 
(aalst, 2011). an event log contains event data related to 
a particular process. each event in an event log refers to 
oneprocess instance , called case. events related to a case 
are ordered. events can have attributes. examples of typ- 
ical attribute names are activity, time, costs, and resource. 
not all events need to have the same set of attributes. how- 
ever, typically, events referring to the same activity have the 
same set of attributes. figure 6(a) shows the conversion of 
an csv ﬁle with four columns (case, activity, resource, and 
timestamp) into an event log. 
most process mining tools support xes (extensible
event stream) (ieee task force on process mining, 2013). 
in september 2010, the format was adopted by the ieee 
task force on process mining and became the de facto ex- 
change format for process mining. the ieee standards or- 
ganization is currently evaluating xes with the aim to turn 
xes into an ofﬁcial ieee standard. 
to create event logs we need to extract, load, trans- 
form, anonymize, and de-identify data in a variety of sys- 
tems (see ➌in figure 5). consider for example the hun- 
dreds of tables in a typical his (hospital information sys- 
tem) like chipsoft, mckesson and epic or in an erp (en- 
terprise resource planning) system like sap, oracle, and 
microsoft dynamics. non-trivial mappings are needed to 
extract events and to relate events to cases. event data needs 
to be scoped to focus on a particular process. moreover, the 
data also needs to be scoped with respect to conﬁdentiality 
issues.
3.1.2 process discovery 
process discovery is one of the most challenging process 
mining tasks (aalst, 2011). based on an event log, a process case activity resource timestamp each row 
corresponds 
to an event 
each dot 
corresponds 
to an event 208 cases 
time 
process model 
discovered for the most 
frequent activities 
conformance 
checking view 
performance 
analysis view activity was 
skipped 16 times 
average waiting 
time is 18 days 
queue length is 
currently 22 (a) (b)
(c)
(d)
(e)
(f)5987 
events 
tokens refer to 
real cases 
figure 6: six screenshots of prom while analyzing an event log with 208 cases, 5987 events, and 74 different activities. first, 
a csv ﬁle is converted into an event log (a). then, the event data can be explored using a dotted chart (b). a process model 
is discovered for the 11 most frequent activities (c). the event log can be replayed on the discovered model. this is used to 
show deviations (d), average waiting times (e), and queue lengths (f).table 1: relating the four challenges to process mining speciﬁc tasks. 
creating and 
managing
event data process 
discovery conformance
checkingperformance
analysisoperational
support
fairness
data science with-
out prejudice: how to 
avoid unfair conclusions
even if they are true? the input data may
be biased, incomplete 
or incorrect such 
that the analysis
reconﬁrms prejudices.
by resampling or 
relabeling the data,
undesirable forms of 
discrimination can
be avoided. note 
that both cases and
resources (used to 
execute activities) 
may refer to individ- 
uals having sensitive 
attributes such as 
race, gender, age, etc. the discovered model 
may abstract from
paths followed by cer- 
tain under-represented
groups of cases. 
discrimination-aware 
process-discovery 
algorithms can be 
used to avoid this. for 
example, if cases are 
handled differently
based on gender, we 
may want to ensure 
that both are equally
represented in the 
model.conformance
checking can be 
used to “blame” 
individuals, groups,
or organizations for 
deviating from some 
normative model. 
discrimination-aware 
conformance check-
ing (e.g., alignments)
needs to separate 
(1) likelihood, (2)
severity and (3) 
blame. deviations 
may need to be 
interpreted differently
for different groups of 
cases and resources.straightforward
performance measure-
ments may be unfair 
for certain classes of 
cases and resources
(e.g., not taking into
account the context).
discrimination-aware 
performance analysis
detects unfairness
and supports process
improvements taking 
into account trade-
offs between internal 
fairness (worker’s 
perspective) and 
external fairness (citi- 
zen/patient/customer’s
perspective). process-related
predictions, rec-
ommendations
and decisions
may discriminate
(un)intentionally.
this problem can
be tackled using 
techniques from
discrimination-aware 
data mining.
conﬁdentiality
data science that
ensures conﬁdentiality: 
how to answer questions 
without revealing secrets? event data (e.g., xes 
ﬁles) may reveal 
sensitive information. 
anonymization and
de-identiﬁcation can
be used to avoid 
disclosure. note
that timestamps and
paths may be unique 
and a source for 
re-identiﬁcation (e.g.,
all paths are unique).the discovered model 
may reveal sensitive 
information, espe-
cially with respect
to infrequent paths 
or small event logs. 
drilling-down from 
the model may need
to be blocked when 
numbers get too small
(cf. k-anonymity).conformance check-
ing shows diagnostics 
for deviating cases 
and resources.
access-control
is important and 
diagnostics need to be 
aggregated to avoid 
revealing compliance 
problems at the level 
of individuals. performance analysis
shows bottlenecks and 
other problems. link-
ing these problems to 
cases and resources
may disclose sensitive 
information.process-related
predictions, rec-
ommendations
and decisions may
disclose sensitive 
information, e.g.,
based on a rejection 
other properties can
be derived. 
accuracy
data science with-
out guesswork: how to 
answer questions with
a guaranteed level of 
accuracy?event data (e.g., 
xes ﬁles) may
have all kinds of 
quality problems.
attributes may be 
incorrect, imprecise,
or uncertain. for 
example, timestamps 
may be too coarse 
(just the date) or 
reﬂect the time of 
recording rather than
the time of the event’s 
occurrence.process discovery de- 
pends on many pa- 
rameters and charac-
teristics of the event 
log. process mod-
els should better show
the conﬁdence level 
of the different parts. 
moreover, additional 
information needs to 
be used better (do- 
main knowledge, un-
certainty in event data, 
etc.).often multiple
explanations are 
possible to interpret 
non-conformance.
just providing one
alignment based
on a particular cost 
function may be mis- 
leading. how robust 
are the ﬁndings?in case of ﬁtness 
problems (process
model and event log 
disagree), perfor-
mance analysis is 
based on assumptions 
and needs to deal 
with missing values 
(making results less
accurate).inaccurate process
models may lead to 
ﬂawed predictions, 
recommendations and
decisions. moreover, 
not communicating
the (un)certainty
of predictions, 
recommendations
and decisions, may
negatively impact 
processes.
transparency
data science that
provides transparency: 
how to clarify answers 
such that they become 
indisputable?provenance of event 
data is key. ide- 
ally, process mining 
insights can be related 
to the event data they 
are based on. how- 
ever, this may con- 
ﬂict with conﬁdential-
ity concerns.discovered process 
models depend on 
the event data used 
as input and the 
parameter settings and
choice of discovery 
algorithm. how to en- 
sure that the process
model is interpreted 
correctly? end-users
need to understand 
the relation between
data and model to 
trust analysis.when modeled and
observed behavior 
disagree there may be 
multiple explanations. 
how to ensure that 
conformance diagnos-
tics are interpreted
correctly?when detecting per-
formance problems, it 
should be clear how 
these were detected
and what the possi-
ble causes are. ani-
mating event logs on 
models helps to make 
problems more trans-
parent.predictions, rec-
ommendations and
decisions are based
on process models. if 
possible, these models
should be transparent. 
moreover, expla- 
nations should be 
added to predictions, 
recommendations
and decisions (“we
predict that this case
be late, because ...”). model is constructed thus capturing the behavior seen in the 
log. dozens of process discovery algorithms are available. 
figure 6(c) shows a process model discovered using prom’s 
inductive visual miner (leemans et al., 2015). techniques 
use petri nets, wf-nets, c-nets, process trees, or transition 
systems as a representational bias (aalst, 2016). these re- 
sults can always be converted to the desired notation, for 
example bpmn (business process model and notation), 
yawl, or uml activity diagrams. 
3.1.3 conformance checking
using conformance checking discrepancies between the log
and the model can be detected and quantiﬁed by replaying 
the log (aalst et al., 2012). for example, figure 6(c) shows 
an activity that was skipped 16 times. some of the discrep- 
ancies found may expose undesirable deviations, i.e., con- 
formance checking signals the need for a better control of 
the process. other discrepancies may reveal desirable de- 
viations and can be used for better process support. input 
for conformance checking is a process model having exe- 
cutable semantics and an event log. 
3.1.4 performance analysis 
by replaying event logs on process model, we can com- 
pute frequencies and waiting/service times. using align- 
ments (aalst et al., 2012) we can relate cases to paths in 
the model. since events have timestamps, we can associate 
the times in-between events along such a path to delays in 
the process model. if the event log records both start and 
complete events for activities, we can also monitor activity 
durations. figure 6(d) shows an activity that has an aver- 
age waiting time of 18 days and 16 hours. note that such 
bottlenecks are discovered without any modeling. 
3.1.5 operational support
figure 6(e) shows the queue length at a particular point in 
time. this illustrates that process mining can be used in an 
online setting to provide operational support. process min- 
ing techniques exist to predict the remaining ﬂow time for 
a case or the outcome of a process. this requires the com- 
bination of a discovered process model, historic event data, 
and information about running cases. there are also tech-
niques to recommend the next step in a process, to check 
conformance at run-time, and to provide alerts when cer- 
tain service level agreements (slas) are violated. 
3.2 challenges in process mining 
table 1 maps the four generic challenges identiﬁed in sec- 
tion 2 onto the six key ingredients of process mining brieﬂy 
introduced in section 3.1. note that both cases (i.e., process 
instances) and the resources used to execute activities may 
refer to individuals (customers, citizens, patients, workers, 
etc.). event data are difﬁcult to fully anonymize. in larger 
processes, most cases follow a unique path. in the event log 
used in figure 6, 198 of the 208 cases follow a unique path 
(focusing only on the order of activities). hence, knowing the order of a few selected activities may be used to de- 
anonymize or re-identify cases. the same holds for (pre- 
cise) timestamps. for the event log in figure 6, several 
cases can be uniquely identiﬁed based on the day the reg- 
istration activity (ﬁrst activity in process) was executed. if 
one knows the timestamps of these initial activities with the 
precision of an hour, then almost all cases can be uniquely 
identiﬁed. this shows that the ordering and timestamp data 
in event logs may reveal conﬁdential information uninten- 
tionally. therefore, it is interesting to investigate what can 
be done by adding noise (or other transformations) to event 
data such that the analysis results do not change too much. 
for example, we can shift all timestamps such that all cases 
start in “week 0”. most process discovery techniques will 
still return the same process model. moreover, the average 
ﬂow/waiting/service times are not affected by this. 
conformance checking (aalst et al., 2012) can be 
viewed as a classiﬁcation problem. what kind of cases de- 
viate at a particular point? bottleneck analysis can also be 
formulated as a classiﬁcation problem. which cases get de- 
layed more than 5 days? we may ﬁnd out that conformance 
or performance problems are caused by characteristics of 
the case itself or the people that worked on it. this allows 
us to discover patterns such as: 
•doctor jones often performs an operation without mak- 
ing a scan and this results in more incidents later in the 
process.
•insurance claims from older customers often get re-
jected because they are incomplete. 
•citizens that submit their tax declaration too late often
get rejected by teams having a higher workload. 
techniques for discrimination discovery can be used to ﬁnd 
distinctions that are not desirable/acceptable. subsequently,
techniques for discrimination prevention can be used to 
avoid such situations. it is important to note that discrimi- 
nation is not just related to static variables, but also relates 
to the way cases are handled. 
it is also interesting to use techniques from decomposed 
process mining or streaming process mining (see chap- 
ter 12 in (aalst, 2016)) to make process mining “greener”. 
for streaming process mining one cannot keep track of 
all events and all cases due to memory constraints and the 
need to provide answers in real-time (burattin et al., 2014; 
aalst, 2016; zelst et al., 2015). hence, event data need to 
be stored in aggregated form. aging data structures, queues, 
time windows, sampling, hashing, etc. can be used to keep 
only the information necessary to instantly provide answers 
to selected questions. such approaches can also be used 
to ensure conﬁdentiality, often without a signiﬁcant loss of 
accuracy. 
for decomposed/distributed process mining event data 
need to be split based on a grouping activities in the pro- 
cess (aalst, 2013b; aalst, 2016). after splitting the event 
log, it is still possible to discover process models and to 
check conformance. interestingly, the sublogs can be ana- 
lyzed separately. this may be used to break potential harm- 
ful correlations. rather than storing complete cases, one
can also store shorter episodes of anonymized case frag- 
ments. sometimes it may even be sufﬁcient to store only di-
rect successions , i.e., facts of the form “for some unknown 
case activity awas followed by activity bwith a delay of 8 hours”. some discovery algorithms only use data on direct 
successions and do not require additional, possibly sensi- 
tive, information. of course certain questions can no longer 
be answered in a reliable manner (e.g., ﬂow times of cases). 
the above examples illustrate that table 1 identiﬁes a 
range of novel research challenges in process mining. in 
today’s society, event data are collected about anything, at 
any time, and at any place. today’s process mining tools 
are able to analyze such data and can handle event logs with 
billions of events. these amazing capabilities also imply a 
great responsibility. fairness, conﬁdentiality, accuracy and 
transparency should be key concerns for any process miner. 
4 conclusion 
this paper introduced the notion of “green data science”
(gds) from four angles: fairness ,conﬁdentiality ,accuracy ,
andtransparency . the possible “pollution” caused by data 
science should not be addressed (only) by legislation. we 
should aim for positive, technological solutions to protect 
individuals, organizations and society against the negative 
side-effects of data. as an example, we discussed “green 
challenges” in process mining . table 1 can be viewed as a 
research agenda listing interesting open problems.
references
aalst, w. van der (2011). process mining: discovery, con- 
formance and enhancement of business processes .
springer-verlag, berlin. 
aalst, w. van der (2013a). business process management: 
a comprehensive survey. isrn software engineer- 
ing, pages 1–37. doi:10.1155/2013/507984. 
aalst, w. van der (2013b). decomposing petri nets for pro- 
cess mining: a generic approach. distributed and
parallel databases , 31(4):471–507. 
aalst, w. van der (2014). data scientist: the engineer of 
the future. in mertins, k., benaben, f., poler, r., 
and bourrieres, j., editors, proceedings of the i-esa 
conference , volume 7 of enterprise interoperability ,
pages 13–28. springer-verlag, berlin. 
aalst, w. van der (2016). process mining: data science in 
action . springer-verlag, berlin. 
aalst, w. van der, adriansyah, a., and dongen, b. van 
(2012). replaying history on process models 
for conformance checking and performance analy-
sis. wires data mining and knowledge discovery ,
2(2):182–192.
burattin, a., sperduti, a., and aalst, w. van der (2014). 
control-flow discovery from event streams. in ieee
congress on evolutionary computation (cec 2014) ,
pages 2420–2427. ieee computer society.
calders, t. and verwer, s. (2010). three naive bayes 
approaches for discrimination-aware classiﬁcation. 
data mining and knowledge discovery , 21(2):277– 
292.casella, g. and berger, r. (2002). statistical inference, 2nd 
edition . duxbury press. 
european commission (1995). directive 95/46/ec of the 
european parliament and of the council on the pro- 
tection of individuals with wegard to the processing 
of personal data and on the free movement of such 
data. ofﬁcial journal of the european communities, 
no l 281/31. 
european commission (2015). proposal for a regulation 
of the european parliament and of the council on 
the protection of individuals with wegard to the pro- 
cessing of personal data and on the free movement 
of such data (general data protection regulation). 
9565/15, 2012/0011 (cod).
ieee task force on process mining (2013). xes standard 
deﬁnition. www.xes-standard.org. 
kamiran, f., calders, t., and pechenizkiy, m. (2010). 
discrimination-aware decision-tree learning. in 
proceedings of the ieee international conference on 
data mining (icdm 2010) , pages 869–874. 
leemans, s., fahland, d., and aalst, w. van der (2015). 
exploring processes and deviations. in fournier, f. 
and mendling, j., editors, business process manage- 
ment workshops, international workshop on business 
process intelligence (bpi 2014) , volume 202 of lec-
ture notes in business information processing , pages 
304–316. springer-verlag, berlin. 
miller, r. (1981). simultaneous statistical inference .
springer-verlag, berlin. 
monreale, a., rinzivillo, s., pratesi, f., giannotti, f., and 
pedreschi, d. (2014). privacy-by-design in big data 
analytics and social mining. epj data science ,
1(10):1–26.
nelson, g. (2015). practical implications of sharing data: 
a primer on data privacy, anonymization, and de- 
identiﬁcation. paper 1884-2015, thotwave technolo- 
gies, chapel hill, nc.
pedreshi, d., ruggieri, s., and turini, f. (2008). 
discrimination-aware data mining. in proceedings 
of the 14th acm sigkdd international conference 
on knowledge discovery and data mining , pages 
560–568. acm. 
president’s council of advisors on science and technology 
(2014). big data and privacy: a technological per- 
spective (report to the president). executive ofﬁce of 
the president, us-pcast.
rajaraman, a. and ullman, j. (2011). mining of massive 
datasets . cambridge university press. 
ruggieri, s., pedreshi, d., and turini, f. (2010). dcube: 
discrimination discovery in databases. in proceed- 
ings of the acm sigmod intetrnational conference 
on management of data , pages 1127–1130. acm. 
zelst, s. van, dongen, b. van, and aalst, w. van der (2015). 
know what you stream: generating event streams 
from cpn models in prom 6. in proceedings of 
the bpm2015 demo session , volume 1418 of ceur
workshop proceedings , pages 85–89. ceur-ws.org. 
vigen, t. (2015). spurious correlations . hachette books. 