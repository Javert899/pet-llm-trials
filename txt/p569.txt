business process simulation:
how to get it right?
w.m.p. van der aalst, j. nakatumba, a. rozinat, and n. russell
eindhoven university of technology
p.o. box 513, nl-5600 mb, eindhoven, the netherlands.
w.m.p.v.d.aalst@tue.nl
abstract. although simulation is typically considered as relevant and
highly applicable, in reality the use of simulation is limited. many orga-
nizations have tried to use simulation to analyze their business processes
at some stage. however, few are using simulation in a structured and
eÂ®ective manner. this may be caused by a lack of training and limita-
tions of existing tools, but in this paper we will argue that there are
also several additional and more fundamental problems. first of all, the
focus is mainly on design while managers would also like to use simu-
lation for operational decision making (solving the concrete problem at
hand rather than some abstract future problem). second, there is limited
support for using existing artifacts such as historical data and workÂ°ow
schemas. third, the behavior of resources is modeled in a rather naive
manner. this paper focuses on the latter problem. it proposes a new
way of characterizing resource availability. the ideas are described and
analyzed using cpn tools. experiments show that it is indeed possi-
ble to capture human behavior in business processes in a much better
way. by incorporating better resource characterizations in contemporary
tools, business process simulation can Â¯nally deliver on its outstanding
promise.
1 introduction
the correctness, eÂ®ectiveness, and eÂ±ciency of the business processes supported
by a process-aware information system (pais) [13] are vital to the organization.
examples of paiss are workÂ°ow management systems but als other \process-
aware" systems such as enterprise resource planning systems (e.g., sap r/3,
oracle, jd edwards, etc.), call-center systems, product-data management sys-
tems, and process-centric middleware (e.g., ibm's websphere, jboss, etc.). if a
pais is conÂ¯gured based on a process deÂ¯nition which contains errors, then the
resulting process may lead to angry customers, back-log, damage claims, and loss
of goodwill. moreover, an inadequate design may also lead to processes which
perform poorly, e.g., long response times, unbalanced utilization of resources,
and low service levels. this is why it is important to analyze processes before
they are put into production (to Â¯nd design Â°aws), but also while they are run-
ning (for diagnosis and decision support). in this paper, we focus on the role ofsimulation when analyzing business processes. the goal is to identify limitations
of existing approaches and to discuss possible solutions. in particular, we will
focus on the availability of resources . it will be shown that many organizations
have a limited view on the availability of their employees and that today's sim-
ulation tools do not support the more reÂ¯ned views that are needed. the goal is
to transform simulation from a \toy for managers and consultants" into a truly
useful and versatile tool.
environment
arrival
processsubrun
settings
probabilities
for choicespriorities
service
timesnumber of
resources
fig. 1. information required for a traditional simulation.
to introduce the concept of business process simulation, let us consider fig-
ure 1. in the background a workÂ°ow speciÂ¯cation is shown using the yawl nota-
tion [4]. the process starts with task order cable . after this task is executed, task
pay deposit is enabled. however, if payment does not follow within two weeks,
tasktime-out is executed. the details of the process and the exact notation are
not important. however, it is important to see that a workÂ°ow model deÂ¯nes the
ordering of tasks, models (time) triggers, etc. the arrow above a task indicates
that the task requires a resource of a particular type, e.g., using the role concept
resources are linked to tasks. when there are choices, conditions are added to
specify when to take a particular route, etc. the yawl model in figure 1 can be
used to conÂ¯gure a pais (in this case the workÂ°ow management system yawl)
and thus enact the corresponding business process. however, the yawl model
is not suÂ±cient for simulation. the ordering of activities and information about
roles, conditions, triggers, etc. is useful for simulation purposes, but as figure 1
shows, additional information is needed. first of all, an environment needs to be
added. while in a pais the real environment interacts directly with the model,in a simulation tool the behaviorial characteristics of the environment need to
be speciÂ¯ed. for example, the arrival of new cases (i.e., process instances) needs
to be speciÂ¯ed (see box arrival process in figure 1). typically a poisson arrival
process is assumed and the analyst needs to indicate the average arrival rate. sec-
ond, the service time, also called the process time, of tasks needs to be speciÂ¯ed.
for example, one can assume that the service time is described by a beta dis-
tribution with a minimum, a maximum, an average, and a mode. note that the
simulation model needs to abstract from the actual implementation of the task
and replace the detailed behavior by stochastic distributions. similarly, choices,
priorities, etc. are replaced by probability distributions. finally, the workÂ°ow
model needs to be complemented by information about resources (e.g., number
of people having a particular role). in order to conduct experiments one also has
to specify the number of subruns, the length of each subrun, etc. based on all
this information, simulation tools can provide information about, for example,
expected Â°ow times, service levels (e.g., percentage of cases handled within two
weeks), and resource utilization.
figure 1 presents a rather classical view on business process simulation. this
is the type of simulation supported by hundreds, if not thousands, of commercial
simulation packages. some vendors provide a pure simulation tool (e.g., arena,
extend, etc.) while others embed this in a workÂ°ow management system (e.g.,
filenet, cosa, etc.) or a business process modeling tool (e.g., protos, aris,
etc.). all of these tools more or less use the information presented in figure 1 to
calculate various performance indicators. in this paper we will call this \tradi-
tional simulation". we will argue that this type of simulation is not very useful.
figure 2 shows the need to move beyond traditional simulation approaches.
paisoperational process
organization/
resources
process modelevent log
process state
resource modeldescribes
describe
configureinteract
record
usetraditional simulation 
(steady state, naive view of
resources, only indirect use of 
historic information)advanced simulation 
(transient and steady state,
refined view of resources, use
of historic and state information)
enactment analysis
fig. 2. overview of the relationship between enactment and simulation and the diÂ®er-
ent data sources.
the left-hand-side of figure 2 shows the role of a pais (e.g., a workÂ°ow
engine as well as also other types of process-oriented information systems) in
supporting operational business processes. the pais supports, controls, and
monitors operational processes. the resources within the organization performtasks in such processes and therefore also interact with the pais. the pais
can only do meaningful things if it has knowledge of the process, the resources
within the organization and the current states of active cases. moreover, a pais
often records historical information for auditing and performance analysis. the
four ellipses in the middle of figure 2 show these four types of data: (1) event
log, (2) process state, (3) process model, and (4) resource model. the event log
contains historical information about \when, how, and by whom?" in the form
of recorded events. the process state represents all information that is attached
to cases, e.g., customer order xyz consists of 25 order lines and has been in the
state \waiting for replenishment" since monday. the process model describes the
ordering of tasks, routing conditions, etc. (cf. the yawl model in figure 1). the
resource model holds information about people, roles, departments, etc. clearly,
the process state, process model, and resource model are needed to enact the
process using a pais. the event log merely records the process as it is actually
enacted.
the right-hand-side of figure 2 links the four types of data to simulation.
for traditional simulation (i.e., in the sense of figure 1) a process model is
needed. this model can be derived from the model used by the pais. moreover,
information about resources, arrival processes, processing times, etc. is added.
the arcs between the box traditional simulation and the three types of data
(event log, process model, and resource model) are curved to illustrate that
the relationship between the data used by the pais and the simulation tool is
typically rather indirect. for example, the analyst cannot use the process model
directly, but needs to transform it to another language or notation. the resource
model used for simulation is typically very simple. each activity has a single role
and for each role there are a Â¯xed number of resources available. moreover, it is
assumed that these resources are available on a full-time basis. the event logs are
not used directly. at best, they are used to estimate the parameters for some of
the probability distributions. hence, traditional simulation can be characterized
as having a weak link with the actual pais and historical data and a rather
naive view of resources. moreover, the current state is not used at all. as such,
simulation focuses on steady-state behavior and cannot be used for operational
decision making.
this paper advocates the use of more advanced notions of simulation. key
aspects of which include the establishment of a close coupling with the data
used by the pais together with the extensive use of event log and process state
information. moreover, we will not only focus on steady-state behavior but also
on transient behavior in order to also support operational decision making. this
is illustrated by the box advanced simulation in figure 2. the contribution of
this paper is twofold:
{first of all, we provide a critical analysis of current simulation approaches
and tools as summarized by figure 2. we argue that there is too much focus
on process design and that there should be more emphasis on operational
decision making using transient analysis. we also advocate the use of existing
artifacts such as workÂ°ow models, event logs, state information, etc. it is ourbelief that vital information remains unused in current approaches. in our
analysis of current simulation approaches, we also address the problem that
resources are modeled in a way that does not reÂ°ect the true behavior of
people. for example, the working speed may depend on the utilization of
people and people may prefer to work in batches.
{second, we provide a detailed analysis of the eÂ®ect of resource availability
in simulation studies. we argue that resources are modeled inadequately
because of incorrect assumptions (e.g., availability and processing speed are
much more dynamic than often assumed). using a concrete simulation model,
we prove that such assumptions lead to incorrect predictions. as a result,
the simulation model may indicate that the average Â°ow time is around one
hour while in reality the average Â°ow time is actually more than one month.
the remainder of this paper is organized as follows. first, we provide an
overview of the limitations of traditional simulation approaches. then we look
into the problem of describing resource availability. we develop a simple sim-
ulation model with which to do simulation experiments and use these results
to show the eÂ®ects of oversimplifying the availability of people. after providing
concrete suggestions for improving the modeling of resources, we discuss related
work and complementary approaches, and conclude the paper.
2 pitfalls of current simulation approaches
in the introduction, we used figure 2 to summarize some of the limitations of
contemporary simulation approaches. in this section, we describe these pitfalls
in more detail.
2.1 focus on design rather than operational decision making
simulation is widely used as a tool for analyzing business processes but it mostly
focuses on examining rather abstract steady-state situations. such analyses are
helpful for the initial design of a business process but are less suitable for op-
erational decision making and continuous improvement. to explain this we Â¯rst
elaborate on the diÂ®erence between transient analysis andsteady-state analysis .
the key idea of simulation is to execute a model repeatedly. the reason for
doing the experiments repeatedly, is to not come up with just a single value
(e.g., \the average response time is 10.36 minutes") but to provide conÂ¯dence
intervals (e.g., \the average response time is with 90 percent certainty between
10 and 11 minutes"). this is why there is not a single simulation run, but several
subruns . figure 3 shows two sets of four subruns. (typically, dozens of subruns
are used to calculate conÂ¯dence intervals and, in the case of steady-state analysis,
subruns can be obtained by partitioning one long run into smaller runs [19, 24].)
in the four subruns depicted in figure 3(a) the focus is on the initial part of the
process, i.e., starting from the initial state the \near future" is explored. in thefig. 3. for transient analysis the initial state is vital while for steady-state analysis the
choice of initial state should have no eÂ®ect on the simulation result. each graph shows
one simulation run. the x-axis denotes time while the y-axis represents the system
state. the Â¯rst four graphs (a) illustrate the importance of the initial state and the
focus of transient simulation on the initial part. the other four graphs (b) show the
focus on the steady-state behavior.
four subruns depicted in figure 3(b) the initial part is discarded and only the
later behavior is of interest. note that for steady-state analysis the initial state is
irrelevant. typically, the simulation is started \empty" (i.e., without any cases
in progress) and only when the system is Â¯lled with cases the measurements
start. figure 3(a) clearly shows that for transient analysis the initial state is
very important. if the simulation starts in a state with long queues of work,
then in the near future Â°ow times will be long and it may take some time to get
rid of the backlog as shown in the diagram.
despite the abundance of simulation tools, simulation is rarely used for op-
erational decision making. one of the reasons is the inability of traditional tools
to capture the real process (see above). however, another, perhaps more impor-
tant, reason is that existing simulation tools aim at strategic or tactical decisions.
contemporary tools tend to support simulations that start in an arbitrary ini-
tial state (without any cases in the pipeline) and then simulate the process for a
long period to make statements about the steady-state behavior. however, this
steady-state behavior does not exist (the environment of the process changes
continuously) and is thus considered irrelevant by the manager. moreover, the
really interesting questions are related to the near future. therefore, it seems
vital to also support transient analysis, often referred to as short-term simula-
tion[22, 32, 26]. the `fast-forward button' provided by short-term simulation is
a useful option, however, it requires the use of the current state. fortunately,
when using a pais it is relatively easy to obtain the current state and load this
into the simulation model.2.2 modeling from scratch rather than using existing artifacts
in practise, it is time consuming to construct a good simulation model and to
determine the input parameter. a pitfall of current simulation approaches is that
existing artifacts (models, logs, data, etc.) are not used in a direct manner. if
a pais is used, there are often models that are used to conÂ¯gure the system
(e.g., workÂ°ow schemas). today, these models are typically disconnected from
the simulation models and created separately. sometimes a business process
modeling tool is used to make an initial process design. this design can be
used for simulation purposes when using a tool like protos or aris. when the
designed process is implemented, another system is used and the connection
between the implementation model and the design model is lost. it may be
that at a later stage, when the process needs to be analyzed, that a simulation
model is built from scratch. this is a pity as the pais contains most of the
information required. as a result the process is \reinvented" again and again,
thus introducing errors and unnecessary work. the lack of reuse also applies to
other sources of information. for example, the pais may provide detailed event
logs. therefore, there is no need to \invent" processing times, arrival times, and
routing probabilities, etc. all of this information can be extracted from the logs.
note that all additional information shown in figure 1 can be derived from event
logs. in fact, in [25] it is demonstrated that complete simulation models can be
extracted from event logs.
as indicated in figure 2, simulation could use all four types of data provided
by the pais, i.e., not just the event log and process model but also the process
state and resource model. the process state can be used to enable short-term
simulation (as described before) and the resource model may be used to more
accurately describe resources. in most simulation tools, only the number of re-
sources per class is given. however, a pais holds detailed information about
authorizations, delegations, working times, etc. by using this information di-
rectly, more realistic models can be constructed.
it is interesting to note that today's data mining and business intelligence
tools are completely disconnected from simulation. these tools are merely used to
measure performance indicators and to discover correlations and trends. yet their
objectives are similar, i.e., both simulation and data mining/business intelligence
tools aim at improving operational business processes. therefore, it seems good
to combine things and exploit existing artifacts as much as possible.
2.3 incorrect modeling of resources
probably the biggest problem of current business simulation approaches is that
human resources are modeled in a very naive manner. as a result, it is not
uncommon that the simulated model predicts Â°ow times of minutes or hours
while in reality Â°ow times are weeks or even months. therefore, we list some of
the main problems encountered when modeling resources in current simulation
tools.people are involved in multiple processes. in practice there are few people
that only perform activities for a single process. often people are involved in
many diÂ®erent processes, e.g., a manager, doctor, or specialist may perform
tasks in a wide range of processes. however, simulation often focuses on a single
process. suppose a manager is involved in 10 diÂ®erent processes and spends
about 20 percent of his time on the process that we want to analyze. in most
simulation tools it is impossible to model that a resource is only available 20
percent of the time. hence, one needs to assume that the manager is there all
the time and has a very low utilization. as a result the simulation results are
too optimistic. in the more advanced simulation tools, one can indicate that
resources are there at certain times in the week (e.g., only on monday). this is
also an incorrect abstraction as the manager distributes his work over the various
processes based on priorities and workload. suppose that there are 5 managers
all working 20 percent of their time on the process of interest. one could think
that these 5 managers could be replaced by a single manager (5*20%=1*100%).
however, from a simulation point of view this is an incorrect abstraction. there
may be times that all 5 managers are available and there may be times that
none of them are available.
people do not work at a constant speed. another problem is that people
work at diÂ®erent speeds based on their workload, i.e., it is not just the dis-
tribution of attention over various processes, but also their absolute working
speed that determines their capacity for a particular process. there are various
studies that suggest a relation between workload and performance of people. a
well-known example is the so-called yerkes-dodson law [31]. the yerkes-dodson
law models the relationship between arousal and performance as an inverse u-
shaped curve. this implies that for a given individual and a given type of tasks,
there exists an optimal arousal level. this is the level where the performance
has its maximal value. thus work pressure is productive, up to a certain point,
beyond which performance collapses. although this phenomenon can be easily
observed in daily life, today's business process simulation tools do not support
the modeling of workload dependent processing times.
people tend to work part-time and in batches. as indicated earlier, people
may be involved in diÂ®erent processes. moreover, they may work part-time (e.g.,
only in the morning). in addition to their limited availabilities, people have a
tendency to work in batches (cf. resource pattern 38: piled execution [27]). in
any operational process, the same task typically needs to be executed for many
diÂ®erent cases (process instances). often people prefer to let work-items related
to the same task accumulate, and then process all of these in one batch. in most
simulation tools a resource is either available or not, i.e., it is assumed that a
resource is eagerly waiting for work and immediately reacts to any work-item
that arrives. clearly, this does not do justice to the way people work in reality.
for example, consider how and when people reply to e-mails. some people handlee-mails one-by-one when they arrive while others process their e-mail at Â¯xed
times in batch.
related is the fact that calenders and shifts are typically ignored in simulation
tools. while holidays, lunch breaks, etc. can heavily impact the performance of
a process, they are typically not incorporated in the simulation model.
priorities are diÂ±cult to model. as indicated above, people are involved in
multiple processes and even within a single process diÂ®erent activities and cases
may compete for resources. one process may be more important than another
and get priority. another phenomenon is that in some processes cases that are
delayed get priority while in other processes late cases are \sacriÂ¯ced" to Â¯nish
other cases in time. people need to continuously choose between work-items and
set priorities. although important, this is typically not captured by simulation
models.
process may change depending on context. another problem is that most
simulation tools assume a stable process and organization and that neither of
them change over time. if the Â°ow times become too long and work is accumulat-
ing, resources may decide to skip certain activities or additional resources may
be mobilized. depending on the context, processes may be conÂ¯gured diÂ®erently
and resources may be deployed diÂ®erently. in [6] it is shown that such \second
order dynamics" heavily inÂ°uence performance.
the pitfalls mentioned above illustrate that simulation techniques and tools have
a very naive view of business processes. as a result, the simulation results may
deviate dramatically from the real-life process that is modeled. one response
could be to make more detailed models. we think that this is not the best
solution. the simulation model should have the right level of detail and adding
further detail does not always solve the problem. therefore, we propose to use
the data already present in a pais more eÂ®ectively. moreover, it is vital to
characterize resources at a high abstraction level. clearly, it is not wise to model
a person as a full-time resource always available and eager to work, nor should
we attempt to make a detailed model of human behavior. in the next section,
we try to characterize resource availability using only a few parameters.
3 resource availability: how to get it right?
the previous section listed several pitfalls of contemporary simulation approaches.
some of these pitfalls have been addressed in other papers [6, 25, 26]. here, we
focus on the accurate modeling of resource availability . this can be used to cap-
ture various phenomena, e.g., people working in multiple processes or working
part-time and the tendency of people to work in batches.3.1 approach
as already indicated in this paper, there are a number of issues that need to
be considered when modeling resources. these issues deal with the way people
actually carry out their work. the Â¯rst issue is that people are not available
to work all the time but for speciÂ¯c periods of time. in most cases, people are
only part-time available (e.g., in the mornings, or only on the weekends). in this
paper, this is described as the availability (denoted by a) of the resource, and
it is the percentage of time over which a person is able to work. secondly, when
people are available to work, they divide up their work into portions which are
called chunks and the size of a chunk is denoted by c. chunk sizes may vary
among diÂ®erent people, for example, a person that is available for 50% of his
time may work whenever there is work and he did not exceed the 50% yet (i.e.,
small chunk size), or only in blocks of say half a day (i.e., large chunk size).
another case is that a person may save up work and then work for an extended
period (large c) while another person prefers to regularly check for new work
items and work on these for a shorter period of time (small c). the chunks of
work to be done are distributed over particular horizons of length h. this is the
time period over which constraints can be put in place.
horizon (h) horizon (h) horizon (h)c c c c c c c c c c carrival of 
casecompletion
of caseresource is 
workingresource is 
available
fig. 4. overview of the relation between horizon ( h) and chunk size ( c). resources are
made available in chunks of size c. in this example, not more than four chunks can be
allocated per period of length h. if all four chunks are used and still work needs to be
done, processing is delayed until the next period when new chunks are made available.
figure 4 shows the relationship between chunk size and horizon. the empty
circles represent case arrivals, i.e., the points in time where a new work-item is
oÂ®ered. the Â¯lled circles represent case completions, i.e., the points in time where
some work-item is completed. the chunks of work are divided over the horizon
(see the double headed arcs labeled with c). the periods where the resource is
actually working, is denoted by the horizontal bars. a resource can have three
states:
{inactive , i.e., the resource is not allocated to the process because there is no
work or because all available capacity has been used.{ready , i.e., the resource is allocated to the process but there is currently no
work to be done.
{busy, i.e., the resource is allocated to the process and is working on a case.
when a case arrives, and the resource is inactive and still has remaining chunks
of time (given the current horizon), then a chunk of time is allocated and the
resource starts working. if a case arrives and the resource is busy, the work is
queued until the resource becomes available. note that it may be the case that
work cannot be completed in the current horizon and is postponed to the Â¯rst
chunk in the next period of length h, as illustrated in figure 4. furthermore, if
a chunk has been started then it will be completed even though there might be
no work left (in this case the resource is in the ready state).
the main parameters of the model are as follows.
{arrival rate Â¸, i.e., the average number of cases arriving per time unit.
we assume a poisson arrival process, i.e., the time between two subsequent
arrivals is sampled from a negative exponential distribution with mean1
Â¸.
note that Â¸ >0.
{service rate Â¹, i.e., the average number of cases that can be handled per
time unit. the processing time is also sampled from a negative exponential
distribution. the mean processing time is1
Â¹andÂ¹ >0.
{utilization Â½=Â¸
Â¹is the expected fraction of time that the resource will be
busy.
{chunk size cis the smallest duration a resource is allocated to a process.
when a resource leaves the inactive state, i.e., becomes active (state ready
or busy), it will do so for at least a period c. in fact, the active period is
always a multiple of c.
{horizon his the length of the period considered ( h >0).
{availability ais the fraction of time that the resource is available for the
process (0 < aÂ·1), i.e., the resource is inactive at least 1 Â¡apercent of the
time.
not all combinations of these parameters makes sense, as is illustrated by the
following requirements.
{Â½=Â¸
Â¹Â·a, i.e., the utilization should be smaller than the availability.
{cÂ·h, i.e., the chunk size cannot be larger than the horizon.
{(aÂ¤h)mod c= 0, i.e., the maximum time a resource can be active each
period should be a multiple of c, otherwise it would never be possible to
actually use all of fraction a.
we use an example to explain the last requirement. suppose that the horizon
is 8 hours, the availability is 0.5, and the chunk size is 3 hours. in this case,
aÂ¤h= 4 hours and c= 3 hours. now it is obvious that per period only one
chunk can be allocated. hence, the eÂ®ective availability is not 4 hours but just
3 hours (i.e., eÂ®ectively a=3
8). therefore, we require that aÂ¤his a multiple of
c.arrival rate 
(È)availability
(a)chunk size 
(c)horizon
(h)
queueservice rate
(Âµ)
resourcefig. 5. cases arrive with intensity Â¸and are placed in a queue and a resource with
parameters a,c,h, and Â¹handles each case.
figure 5 summarizes the parameters used in our basic model. cases arrive
with a particular arrival rate Â¸and are then placed in a queue. a resource,
described by four main parameters (availability a, horizon h, chunk size cand
service rate Â¹), is then made available to work on the case as shown in figure 5.
a resource will work on the Â¯rst case in the queue. if the case is not completed
within a particular chunk, then it is sent back to the beginning of the queue to
wait for the next chunk to be allocated.
3.2 modeling in terms of cpn tools
we analyzed the eÂ®ects of the various resource characteristics using a simulation
model. colored petri nets (cpns) [17, 18] were used as a modeling language.
cpns allow for the modeling of complex processes. using cpn tools [18] such
models can be analyzed in various ways, i.e., simulation, state-space analysis,
etc.
cpns extend the classical petri net with data (colored tokens), time, and
hierarchy . places are typed, i.e., all tokens on a place have a value of some
common type. in cpn-terms this means that all tokens in a given place should
belong to the same color set. this implies that each place has a color set (i.e.,
type). tokens also have timestamps indicating when they can be consumed.
when producing a token, it may be given a delay . this delay may be sampled
from some probability distribution. the cpn language, also referred to as cpn-
ml, is based on the functional language (standard) ml. therefore, cpn inherits
the basic types, type constructors, basic functions, operators, and expressions
from ml. inscriptions on the arcs specify the values of the tokens to be produced.
complex models can be structured in an hierarchical manner, i.e., nodes at one
level may refer to subprocesses at a lower level. cpns are distributed over so-
called pages . one page describes a network of places and transitions and may
refer to other pages. for a more detailed introduction to cpns and cpn tools,
we refer to [17, 18].
our cpn model is a hierarchical model that is divided into 3 pages which are:
thegenerator page (which creates cases for which a task needs to be performed),
theactivation page (which models the availability of resources), and the mainpage (which models the actual execution of tasks). this cpn model is used to
clearly study the behavior of a single resource, but it can easily be extended
to more realistic situations with diÂ®erent resources (see section 3.4). in the
following we brieÂ°y describe each of the 3 pages of the cpn model1.
   
   

 	
 
 
 
 
 
  
 

 
 

	 
	   
 
	
	
  
	

 
fig. 6. thegenerator page. the time between two subsequent case arrivals is given by
the function iat(), the creation time of cases is recorded by the current model time
function mtime (), and the duration of the task is given by the function dur(). note
that queue is a list color set, i.e., a single token represents a queue of cases.
figure 6 shows the generator page of the cpn model. cases arrive through
this page and are put in a queue. we assume the arrival process to be poisson
with parameter Â¸(i.e., negative exponential inter-arrival times with mean1
Â¸).
the cases themselves are represented by tokens with a value which is a product
of three parameters: caseid ,arrival time , and duration (the processing time is
sampled from a negative exponential distribution with mean1
Â¹).
the modeling of the availability of resources is done in the activation page of
the cpn model shown in figure 7. we consider the variables introduced earlier in
this section, i.e., h,a, and c. the token in place resource info holds details about
a resource with values related to its availability a, chunk size c, and horizon h.
it is important at this point to determine the amount of work that a person can
do. this is obtained by multiplying availability by horizon. the availability can
be distributed over the period hin chunks of size c. not more than ( aÂ¤h)divc
chunks can be allocated and allocation is eager, i.e., as long that there is work to
be done and available capacity, the resource is active. when transition activate
Â¯res, then a resource with the parameters randmtime () +cbecomes available
to work. the resource will have a delay attached to it which is equivalent to the
current time plus the chunk size, i.e., the resource will be active for ctime units
and this period ends at time mtime () +c.
the actual processing of the cases is carried out in the main page shown in
figure 8. this page uses the generator andactivation pages described above.
cases come in from the generator page through the place joband a resource is
1the interested reader can look up the declarations that would initialize this model
with Â¸=1
100,Â¹=1
15, and one resource "r1" characterized by h= 1000, a= 0:2,
andc= 200 in appendix a.made available from the activation page through the place ready . the token in
place busy indicates the actual processing of a case by the resource. the length
of the processing of a case is restricted by the task duration (already sampled
during case creation) and the remaining chunk size. if cases leave place busy but
are still incomplete, because the resource is no longer available for a time period
suÂ±cient to complete the case, then these cases are put back on the queue. when
the processing of the case is completed the resource is made available to work
again, but this is only possible if there is still time left in the current chunk.
otherwise, the resource will be deactivated and is no longer available until the
next chunk is allocated. the deactivation is controlled by the activation page
shown in figure 7.
the cpn model just described speciÂ¯es the resource behaviors considered.
as indicated before, we assume a very basic setting and the model can easily
be extended. however, our goal is to show that parameters such as availability
a, chunk size c, and horizon hreally matter. most business simulation tools
do not provide such parameters and assume a= 1 (always available), c!0
(resources are only active if they are actually busy working on a case), and
h! 1 (inÂ¯nite horizon). the next subsection shows that this may result in
unrealistic simulations with huge deviations from reality.
3.3 experiments
using the cpn model, experiments were carried out to investigate the relation-
ship between the Â°ow time of cases and the main parameters related to resource
availability. monitors were used to extract numerical data during the simula-
tion. the monitor concept of cpn tools allows for the measurement of various
performance indicators without changing or inÂ°uencing the model [10]. all ex-
perimental results reported here are based on a simulation with 10 subruns, each
subrun having 10,000 cases. for each performance indicator measured, we calcu-
lated the so-called 90% conÂ¯dence interval. these are shown in the graphs but
are typically too narrow to be observed (which is good as it conÂ¯rms the validity
of the trends observed).
as discussed already, the availability aof a resource is the percentage of time
over which a person is able to work. in the cpn model, diÂ®erent availability
values were investigated while keeping the chunk size and horizon constant. the
results from the experiment are shown in figure 9. the graph was plotted to
show the values of the averages with a 90% conÂ¯dence interval and in the caption
all Â¯xed parameter values are shown. the idea behind this experiment was to
determine whether one's availability has any eÂ®ect on the Â°ow time. the result
is obvious: the more people are available, the more work they can do and the
shorter the Â°ow time is. however, one should realize that in most simulation
tools it is not possible to set the parameter a, i.e., a 100% availability ( a= 1) is
assumed. figure 9 shows that this may lead to severe discrepancies.
while the eÂ®ect of reduced availability may be obvious, the eÂ®ect of the
chunk size con the Â°ow time may be more surprising. people can divide up their
work into chunks of varying sizes. when availability is distributed over chunks,fig. 9. graph showing availability against Â°ow time ( Â¸=1
100,Â¹=1
15,Â½= 0:15,
c= 200, and h= 1000). the Â°ow time reduces as the availability increases. (the
straight line shows the trend using linear regression.)
fig. 10. graph showing chunk size against Â°ow time ( Â¸=1
100,Â¹=1
15,Â½= 0:15,
a= 0:2, and h= 1000). the Â°ow time increases as the chunk size increases.the bigger the chunk, the larger the Â°ow times of cases. this is because work
is more likely to accumulate. the results obtained from the experiments carried
out with diÂ®erent chunk sizes (while keeping all other parameters constant) are
shown in figure 10. the graph shows the values of the average Â°ow times and the
90% conÂ¯dence intervals. our Â¯ndings indeed conÂ¯rm that Â°ow time increases as
the chunk size increases. the reason is that the larger the chunk size, the longer
the periods between chunks become. figure 10 shows an important insight that
people making simulation models often do not realize.
fig. 11. graph showing the horizon against the Â°ow times ( Â¸=1
100,Â¹=1
15,Â½= 0:15,
c= 200, and a= 0:8). the Â°ow time decreases as the horizon increases.
when a horizon is large, then the distribution of chunks is more Â°exible. if
aÂ¤h=c, then only one chunk per period is possible. this chunk will typically
start in the beginning and if ais small, then for a large part of hno resource is
available. if aÂ¤his much larger than c, then more chunks are possible and these
can be more evenly distributed over the period h. note that the eÂ®ect of making
the horizon longer is similar to making the chunk size smaller. figure 11 shows
the relation between Â°ow time and horizon observed and clearly shows that
shortening the horizon may lead to longer Â°ow times. however, if the horizon
is suÂ±ciently large (in this case more than 3000), it does not seem to matter
anymore.
finally, it is important to measure the eÂ®ect of utilization on the Â°ow times
of cases. with a higher utilization, the Â°ow times obviously increase as shown
in figure 12. typically, Â°ow times dramatically increase when Â½get close to
1. however, with limited availability, the Â°ow time dramatically increases when
Â½gets close to a. figure 12 shows the average Â°ow times with 90% conÂ¯dence
intervals. note that Â½results from dividing Â¸byÂ¹. in this graph we keep Â¹con-
stant and vary Â¸to get diÂ®erent utilization values. as expected, the conÂ¯dence
intervals get wider as Â½approaches a.fig. 12. graph showing utilization against Â°ow time ( Â¹=1
15,c= 200, a= 0:8, and
h= 1000). the Â°ow time increases as utilization increases.
3.4 example
this section describes a model that deals with the handling of claims in an in-
surance company (taken from [3]). the insurance company processes claims that
result from accidents with cars where the customers of the insurance company
are involved. figure 13 shows the workÂ°ow modeled in terms of a petri net using
the yawl notation [4]. a claim reported by a customer is registered by an em-
ployee of department car damages (cd). after registration, the insurance claim
is classiÂ¯ed by a claim handler of department cd. based on this classiÂ¯cation,
either the claim is processed or a letter is sent to the customer explaining why
the claim cannot be handled (50% is processed and 50% is not handled). if the
claim can be handled, then two tasks are carried out which are check insurance
andphone garage . these tasks are executed in parallel and are handled by em-
ployees in department cd. after executing these tasks, the claim handler makes
a decision which has two possible outcomes: ok (positive) and nok (negative).
if the outcome is ok, then the claim is paid and a letter is sent to the customer.
(half of the decisions lead to a payment and the other half not.) otherwise, just
a letter is sent to the customer.
each of the tasks shown in figure 13 corresponds to an instance of the cpn
model explained in section 3.2. each task shown in the workÂ°ow model has a
number attached to it and this corresponds to the number of people (poten-
tially) available to carry out that task. for example, there is one person able
to execute task register and there are two persons able to execute task classify .
the workÂ°ow model was implemented in cpn tools and figure 14 shows the
main page of the cpn model.
initially, a base scenario was chosen with suitable values for the chunk size,
horizon, availability and utilization of the resources. based on these values, ex-
periments were carried out to determine the sensitivity of these parameters withrespect to the Â°ow time. for example, we were interested to see whether the Â°ow
time was aÂ®ected by larger chunk sizes or not. table 1 summarizes the values of
the Â°ow times obtained when experiments with diÂ®erent parameters were varied.
appendix b lists the parameters of the individual tasks, e.g., task register takes
on average 18 minutes ( Â¹a=1
18) and the time between two subsequent arrivals
is 50 minutes on average ( Â¸a=1
50). since the two choices in the model split the
Â°ow with equal probabilities, only 25% of the cases that arrive are actually payed
(i.e., 50% is classiÂ¯ed as relevant and of these 50% again 50% is rejected). tasks
have diÂ®erent processing times, but, for simplicity, all the tasks share the same
value for chunk size ( c), horizon ( h), and availability ( a). in the base scenario:
c= 5, h= 2000, and a= 0:4. the Â°ow time is with 90% conÂ¯dence within
757:6Â¡65:0 and 757 :6 + 65 :0 minutes.
table 1. results of experiments carried out to determine the eÂ®ect of varying diÂ®erent
parameters against the Â°ow time.
parameters flow time
a) base case scenario ( c= 5,h= 2000, Â¸=1
50anda= 0:4, see
appendix b for all other parameters)
757.6Â§65:0
b)i)divide the horizon by 20 ( h= 100)
1218.9 Â§72:3
ii)divide the horizon by 40 ( h= 50)
1247.8 Â§51:8
c)i)multiply the chunk size by 5 ( c= 25)
1158.7 Â§47:2
ii)multiply the chunk size by 20 ( c= 100)
1698Â§139
iii)multiply the chunk size by 80 ( c= 400)
1950Â§83:7
iv)multiply the chunk size by 160 ( c= 800)
2025Â§99
d)i)decrease availability and arrival rate by 2 ( a= 0:2,Â¸=1
100)
1634Â§105
ii)decrease availability and arrival rate by 4 ( a= 0:1,Â¸=1
200)
3420.32 Â§252
the results shown in table 1 indeed conÂ¯rm that the parameters for chunk
size ( c), horizon ( h), and availability ( a) have dramatic eÂ®ects on the Â°ow times.
based on the initial values, variations were made and diÂ®erent Â°ow time values
were obtained. for example, when the chunk size was increased from c= 5 to
c= 100 the Â°ow time more than doubled. when the availability and the horizon
were varied, the eÂ®ects were as expected. for example, when the availability
and arrival rate decrease by a factor 4 (i.e., the relative utilization Â½=aremains
unchanged) the Â°ow time goes up from approx. 757 to approx. 3420. our exper-iments conÂ¯rm that the parameters identiÂ¯ed in this paper are relevant. in fact,
it is easy to see that the eÂ®ects accumulate when the workÂ°ow is larger.
3.5 lesson learned
there are a number of lessons to be learned from our experiments and cpn
model. it is important to note that the modeling of resources is typically done
in a naive way. there are issues characterized by parameters such as a,c, and
hthat dramatically aÂ®ect performance and these have to be considered in order
to make simulations more realistic.
{first of all, it is important not to assume that people are always available
and eager to work when cases arrive. in real-life situations, this is not true
because people are available for only speciÂ¯c times and may let work accu-
mulate before commencing it. this heavily impacts performance as shown
in figure 9.
{secondly, when people are available to work, they will do this work in chunks
whose size may vary between diÂ®erent people. the bigger the chunk size, the
longer the Â°ow times of cases. so, even if the availability is the same, the
Â°ow time heavily depends on this parameter and it cannot be ignored as
shown in figure 10.
{chunks are divided over a particular horizon and so the larger the horizon,
the shorter the Â°ow times because of increased Â°exibility. increasing the
length of the horizon corresponds to making chunks (relatively) smaller.
{utilization of people is also an important factor that greatly aÂ®ects the Â°ow
times of cases. when it is high, then the Â°ow times increase.
{the example in section 3.4 shows that these eÂ®ects may accumulate in larger
workÂ°ows. the typical assumptions made in today's simulation tools (i.e.,
a= 1, c!0, and h! 1 ), may result in Â°ow times of minutes or hours
while with more realistic settings for a,c, and hthe Â°ow time may go up to
weeks or months and actually coincide with the actual Â°ow times observed.
4 complementary approaches and related work
simulation has been used for the analysis of business processes since the seven-
ties [28]. in fact, the simulation language simula was developed in the sixties
and inÂ°uenced the development of general purpose programming languages [11].
hence, it is fair to say that simulation is one of the earliest and most estab-
lished applications of computing. while the initial focus was on programming
languages extended with simulation capabilities, gradually more and more sim-
ulation packages became available that oÂ®ered some graphical environment to
design business processes. these languages provide simulation building blocks
that can be composed graphically (e.g. arena). today, most business process
modeling tools provide some form of simulation (cf. protos and aris). more-
over, the more mature workÂ°ow management systems also provide simulationcapabilities (cf. filenet, flower, websphere, cosa, etc.). in parallel with
the development of simulation tools and embedding of simulation capabilities
in larger systems, the analysis of simulation data and the setting up of experi-
ments was investigated in detail [19{21, 24, 28]. in some cases it is possible to use
analytical models [9], however, in most cases one needs to resort to simulation.
the use of simulation was also stimulated by management approaches such
as business process reengineering [15, 12], business process improvement [16],
business process intelligence [14], etc. when reengineering a process from scratch
or when improving an existing process design, simulation can be very valuable [7].
despite the interest in simulation and the potential applicability of simulation,
its actual use by end-users is limited.
in section 2 we mentioned some of the main pitfalls of simulation. the core
contribution of this paper is to provide an overview of these problems and to
address one particular problem in detail (resource availability).
the results presented complement our earlier work on \short-term simula-
tion", i.e., the analysis of transient behavior using the actual state as a starting
point. the idea of doing short-term simulation was raised in [22] using a setting
involving protos (modeling), exspect (simulation), and cosa (workÂ°ow man-
agement). this idea was revisited in [32], but not implemented. recently, the
approach has been implemented using prom [2], yawl [1], and cpn tools [18]
(cf. [26]). processes are modeled and enacted using yawl and yawl provides
the four types of data mentioned in figure 2. this information is taken by prom
to create a reÂ¯ned simulation model that includes information about control-Â°ow,
data-Â°ow, and resources. moreover, temporal information is extracted from the
log to Â¯t probability distributions. prom generates a colored petri net that can
be simulated by cpn tools. moreover, cpn tools can load the current state to
allow for transient analysis. interestingly, both the real behavior and the sim-
ulated behavior can be analyzed and visualized using prom. this means that
decision makers view the real process and the simulated processes using the same
type of dashboard. this further supports operational decision making [26].
the approach presented in [26] heavily relies on process mining techniques
developed in the context of prom [2]. of particular importance is the work
presented in [25] where simulation models are extracted from event logs. process
mining [5] is a tool to extract non-trivial and useful information from process
execution logs. these event logs are the starting point for various discovery and
analysis techniques that help to gain insight into certain characteristics of the
process. in [25] we use a combination of process mining techniques to discover
multiple perspectives (namely, the control-Â°ow, data, performance, and resource
perspective) of the process from historical data, and we integrate them into a
comprehensive simulation model that can be analyzed using cpn tools.
when discussing the factors inÂ°uencing the speed at which people work, we
mentioned the yerkes-dodson law [31]. some authors have been trying to op-
erationalize this \law" using mathematical models or simulation models. for
example, in [8] both empirical data and simulation are used to explore the re-
lationship between workload and shop performance. also related is the workpresented in [29] where the authors present a diÂ®erent view on business pro-
cesses, namely describing work as a practice, a collection of psychologically and
socially situated collaborative activities of the members of a group. in this view,
people are concurrently involved in multiple processes and activities. however,
in this work modeling aims at describing collaboration rather than focusing on
performance analysis.
finally, we would like to mention the work reported in [23] where the ef-
fectiveness of workÂ°ow management technology is analyzed by comparing the
process performance before and after introduction of a workÂ°ow management
system. in this study sixteen business processes from six dutch organizations
were investigated. interestingly, the processes before and after were analyzed us-
ing both empirical data and simulated data. this study showed how diÂ±cult it
is to calibrate business process simulation models such that they match reality.
these and other real-life simulation studies motivated the work reported in this
paper.
5 conclusion
although simulation is an established way of analyzing processes and one of the
oldest applications of computing (cf. simula), the practical relevance of busi-
ness process simulation is limited. the reason is that it is time-consuming to
construct and maintain simulation models and that often the simulation results
do not match with reality. hence, simulation is expensive and cannot be trusted.
this paper summarizes the main pitfalls. moreover, it addresses one particular
problem in detail, namely the availability of resources. it is shown that resources
are typically modeled in a naive manner and that this highly inÂ°uences the sim-
ulation results. the fact that people may be involved in multiple processes, and
that they tend to work in batches, has dramatic eÂ®ects on the key performance
indicators of a process.
in this paper, we provide a simple model to characterize resource availabil-
ity. using this model important insights into the eÂ®ectiveness of resources are
provided. moreover, it is shown that these characteristics can be embedded in
existing simulation approaches.
using figure 2, we discussed the role of diÂ®erent information sources and
how information systems and simulation tools can be integrated. this enables
new ways of process support. for example, we are working on predictions and
recommendations in the context of a pais. using simulation, we can predict
when a running case is Â¯nished. based on historical information, we calibrate
the model and do transient analysis from the current state loaded from the pais.
similarly, we can provide recommendations. for example, by using simulation
and historical data, we can predict the execution path that is most likely to lead
to a fast result. initial ideas with respect to prediction and recommendation have
been implemented in prom [2, 30].references
1.w.m.p. van der aalst, l. aldred, m. dumas, and a.h.m. ter hofstede. design
and implementation of the yawl system. in a. persson and j. stirna, editors,
advanced information systems engineering, proceedings of the 16th international
conference on advanced information systems engineering (caise'04) , volume
3084 of lecture notes in computer science , pages 142{159. springer-verlag, berlin,
2004.
2.w.m.p. van der aalst, b.f. van dongen, c.w. gÃ¤ unther, r.s. mans, a.k. alves
de medeiros, a. rozinat, v. rubin, m. song, h.m.w. verbeek, and a.j.m.m.
weijters. prom 4.0: comprehensive support for real process analysis. in j. kleijn
and a. yakovlev, editors, application and theory of petri nets and other models of
concurrency (icatpn 2007) , volume 4546 of lecture notes in computer science ,
pages 484{494. springer-verlag, berlin, 2007.
3.w.m.p. van der aalst and k.m. van hee. workÂ°ow management: models, methods,
and systems . mit press, cambridge, ma, 2002.
4.w.m.p. van der aalst and a.h.m. ter hofstede. yawl: yet another workÂ°ow
language. information systems , 30(4):245{275, 2005.
5.w.m.p. van der aalst, h.a. reijers, a.j.m.m. weijters, b.f. van dongen, a.k.
alves de medeiros, m. song, and h.m.w. verbeek. business process mining: an
industrial application. information systems , 32(5):713{732, 2007.
6.w.m.p. van der aalst, m. rosemann, and m. dumas. deadline-based escalation
in process-aware information systems. decision support systems , 43(2):492{511,
2007.
7.r. ardhaldjian and m. fahner. using simulation in the business process reengi-
neering eÂ®ort. industrial engineering , pages 60{61, july 1994.
8.j.w.m. bertrand and h.p.g. van ooijen. workload based order release and pro-
ductivity: a missing link. production planning and control , 13(7):665{678, 2002.
9.j.a. buzacott. commonalities in reengineerd business processes: models and
issues. management science , 42(5):768{782, 1996.
10.cpn group, university of aarhus, denmark. cpn tools home page.
http://wiki.daimi.au.dk/cpntools/.
11.o.j. dahl and k. nygaard. simula: an algol based simulation language.
communications of the acm , 1:671{678, sept 1966.
12.t.h. davenport. process innovation: reengineering work through information
technology . harvard business school press, boston, 1993.
13.m. dumas, w.m.p. van der aalst, and a.h.m. ter hofstede. process-aware infor-
mation systems: bridging people and software through process technology . wiley
& sons, 2005.
14.d. grigori, f. casati, m. castellanos, u. dayal, m. sayal, and m.c. shan. business
process intelligence. computers in industry , 53(3):321{343, 2004.
15.m. hammer and j. champy. reengineering the corporation . nicolas brealey pub-
lishing, london, 1993.
16.j. harrington. business process improvement: the breakthrough strategy for total
quality . mcgraw-hill, 1991.
17.k. jensen. coloured petri nets. basic concepts, analysis methods and practi-
cal use . eatcs monographs on theoretical computer science. springer-verlag,
berlin, 1992.
18.k. jensen, l.m. kristensen, and l. wells. coloured petri nets and cpn tools
for modelling and validation of concurrent systems. international journal on
software tools for technology transfer , 9(3-4):213{254, 2007.19.j. kleijnen and w. van groenendaal. simulation: a statistical perspective . john
wiley and sons, new york, 1992.
20.a.m. law and d.w. kelton. simulation modeling and analysis . mcgraw-hill,
new york, 1982.
21.m. pidd. computer modelling for discrete simulation . john wiley and sons, new
york, 1989.
22.h.a. reijers and w.m.p. van der aalst. short-term simulation: bridging the gap
between operational control and strategic decision making. in m.h. hamza,
editor, proceedings of the iasted international conference on modelling and
simulation , pages 417{421. iasted/acta press, anaheim, usa, 1999.
23.h.a. reijers and w.m.p. van der aalst. the eÂ®ectiveness of workÂ°ow manage-
ment systems: predictions and lessons learned. international journal of infor-
mation management , 25(5):458{472, 2005.
24.s.m. ross. a course in simulation . macmillan, new york, 1990.
25.a. rozinat, r.s. mans, m. song, and w.m.p. van der aalst. discovering col-
ored petri nets from event logs. international journal on software tools for
technology transfer , 10(1):57{74, 2008.
26.a. rozinat, m. wynn, w.m.p. van der aalst, a.h.m. ter hofstede, and c. fidge.
workÂ°ow simulation for operational decision support using yawl and prom.
bpm center report bpm-08-04, bpmcenter.org, 2008.
27.n. russell, w.m.p.van der aalst, a.h.m. ter hofstede, and d. edmond. workÂ°ow
resource patterns: identiÂ¯cation, representation and tool support. in o. pastor
and j. falcao e cunha, editors, proceedings of the 17th conference on advanced
information systems engineering (caise'05) , volume 3520 of lecture notes in
computer science , pages 216{232. springer-verlag, berlin, 2005.
28.r.e. shannon. systems simulation: the art and science . prentice-hall, engle-
wood cliÂ®s, 1975.
29.m. sierhuis and w.j. clancey. modeling and simulating work practice: a method
for work systems design. ieee intelligent systems , 17(5):32{41, 2002.
30.b. weber, b.f. van dongen, m. pesic, c.w. gÃ¤ unther, and w.m.p. van der
aalst. supporting flexible processes through recommendations based on his-
tory. beta working paper series, wp 212, eindhoven university of technology,
eindhoven, 2007.
31.c.d. wickens. engineering psychology and human performance . harper, 1992.
32.m.t. wynn, m. dumas, c.j. fidge, a.h.m. ter hofstede, and w.m.p. van der
aalst. business process simulation for operational decision support. in a. ter
hofstede, b. benatallah, and h.y. paik, editors, bpm 2007 international work-
shops (bpi, bpd, cbp, prohealth, refmod, semantics4ws) , volume 4928 of lec-
ture notes in computer science , pages 66{77. springer-verlag, berlin, 2008.
a declarations for cpn model in section 3.2
the colset, variable and function declarations of the cpn model have been listed
in the ml language.
a.1 colset declarations
colset cid = int timed;
colset tm = int;colset work= int;
colset case = product cid * tm * work timed;
colset queue = list case;
colset res= string timed;
colset hor = int;
colset av = int with 1..100;
colset chunk = int;
colset info = product hor * av * chunk;
colset rwc = product res * work * chunk timed;
colset rt = product res * tm timed;
colset ri = product res * info timed;
colset cr = product case * rt timed;
a.2 variable declarations
var i:cid;
var t,t1,t2,done:tm;
var w,w1,w2:work;
var r:res;
var h:hor;
var a:av;
var c,c1:chunk;
var q:queue;
var hac : info;
val rinit = [("r1",(1000,20,200))];
a.3 function declarations
fun x1([]) =[] | x1((x,(h,a,c))::r) = (x,0,c)::x1(r);
fun x2([]) =[] | x2((x,y)::r) = x::x2(r);
fun mtime() = intinf.toint(time()):int;
fun dur()= floor(exponential(1.0/15.0));
fun iat()= floor(exponential(1.0/100.0));
fun min(x,y) = if x<y then x else y;
b task parameters for base scenario described in
section 3.4task parameters
a)register resources ra= 1
arrival rate Â¸a=1
50
service rate Â¹a=1
18
utilization Â½a= 0:36
b)classify resources rb= 2
arrival rate Â¸b=1
50
service rate Â¹b=1
36
utilization Â½b= 0:36
c)phone garage resources rc= 3
arrival rate Â¸c=1
100
service rate Â¹c=1
100
utilization Â½c= 0:33
d)check insurance resources rd= 2
arrival rate Â¸d=1
100
service rate Â¹d=1
70
utilization Â½d= 0:35
e)decide resources re= 2
arrival rate Â¸e=1
100
service rate Â¹e=1
70
utilization Â½e= 0:35
f)pay resources rf= 1
arrival rate Â¸f=1
200
service rate Â¹f=1
70
utilization Â½f= 0:35
g)send letter resources rg= 2
arrival rate Â¸g=1
50
service rate Â¹g=1
36
utilization Â½g= 0:36