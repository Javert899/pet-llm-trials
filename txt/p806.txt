nováticanováticanováticanováticanovática , founded in 1975, is the oldest periodical publication
amongst those especialized in information and communication
technology (ict) existing today in spain. it is published by atiatiatiatiati
(((((asociación de técnicos de informáticaasociación de técnicos de informáticaasociación de técnicos de informáticaasociación de técnicos de informáticaasociación de técnicos de informática ) ) ) ) ) which also
publishes reicis reicis reicis reicis reicis ( revista española de inovación, calidad  e
ingeniería del software ).
<http://www.ati.es/novatica/>
<http://www.ati.es/reicis/>
ati ati ati ati ati is a founding member of cepis cepis cepis cepis cepis ( council of european
professional informatics societies ), the spain´s representative
in ifip ifip ifip ifip ifip ( international federation for information processing ),
and a member of clei clei clei clei clei (centro latinoamericano de estudios en
informática) and cecuacecuacecuacecuacecua ( confederation of europeancomputer
user associations ). it has a collaboration agreement with acmacmacmacmacm
(association for computing machinery ) as well as with diverse
spanish organisations in the ict field.
editorial board
guillem alsina gonzález, rafael fernández calvo (presidente del consejo), jaime fernández martínez,luís fernández sanz, josé antonio gutiérrez de mesa, silvia leal martín, dídac lópez viñas, francescnoguera puig, joan antoni pastor collado, víktu pons i colomer, moisés robles gener, cristina vigil díaz,
juan carlos vigo lópez
chief editor
llorenç pagés casas <pages@ati.es>layoutjorge llácer gil de ramales
translations
grupo de lengua e informática de ati <http://www.ati.es/gt/lengua-informatica/>administrationtomás brunete, maría josé fernández, enric camarero
section editors
artificial intelligence
vicente botti navarro, julián inglada (dsic-upv), <{vbotti,viglada}@dsic.upv.escomputational linguisticsxavier gómez guinovart (univ. de vigo), <xgg@uvigo.es>manuel palomar (univ. de alicante), <mpalomar@dlsi.ua.es>
computer archiecture
enrique f. torres moreno (universidad de zaragoza), <enrique.torres@unizar.es>josé flich cardo (universidad politécnica de valencia, <jflich@disca.upv.es>computer graphicsmiguel chover sellés (universitat jaume i de castellón), <chover@lsi.uji.es>roberto vivó hernando (eurographics, sección española), <rvivo@dsic.upv.es>
computer languages
óscar belmonte fernández (univ. jaime i de castellón), <belfern@lsi.uji.es>inmaculada coma tatay (univ. de valencia), <inmaculada.coma@uv.es>e-governmentfrancisco lópez crespo (mae), <flc@ati.es>sebastià justicia pérez (diputación de barcelona) <sjusticia@ati.es>
free software
jesús m. gonzález barahona (gsyc - urjc), <jgb@gsyc.es>israel herráiz taberner o (universidad politéncia de madrid), <isra@herraiz.org>
human -computer interactionpedro m. latorre andrés (universidad de zaragoza, aipo), <platorre@unizar.es>francisco l. gutierrez vela (universidad de granada, aipo), <fgutierr@ugr.es>ict and tourism
andrés aguayo maldonado, antonio guevara plaza (universidad de málaga),
<{aguayo, guevara}@lcc.uma.es>informatics and philosophyjosé angel olivas varela (escuela superior de informática, uclm), <joseangel.olivas@uclm.es>roberto feltrero oreja (uned), <rfeltrero@gmail.com>informatics profession
rafael fernández calvo (ati), rfcalvo@ati.es>, miquel sarriès griñó (ati), <miguel@ati.es>
information access and retrievaljosé maría gómez hidalgo (optenet), <jmgomezh@yahoo.es>enrique puertas sanz (universidad europea de madrid), <enrique.puertas@uem.es>information systems auditingmarina touriño troitiño, <marinatourino@marinatourino.com>
sergio gómez-landero pérez (endesa), 
<sergio.gomezlandero@endesa.es>
it governance
manuel palao garcía-suelto (ati), <manuel@palao.com>,miguel garcía-menéndez (itti) <
mgarciamenendez@ittrendsinstitute.org >
knowledge managementjoan baiget solé (cap gemini ernst & young), <joan.baiget@ati.es>
language and informatics
m. del carmen ugarte garcía (ati), <cugarte@ati.es>law and  tecnologyisabel hernando collazos  (fac. derecho de donostia, upv), <isabel.hernando@ehu.es>elena davara fernández de marcos (davara & davara), <edavara@davara.com>networking and telematic services
juan carlos lópez lópez (uclm), <juancarlos.lopez@uclm.es>
ana pont sanjuán (upv), <apont@disca.upv.es>personal digital environmentandrés marín lópez (univ. carlos iii), <amarin@it.uc3m.es>diego gachet páez (universidad europea de madrid), <gachet@uem.es>software modeling
jesus garcía molina (dis-um), <jmolina@um.es>
gustavo rossi (lifia-unlp , argentina), <gustavo@sol.info.unlp.edu.ar>
students´ worldfederico g. mon trotti (ritsi), <gnu.fede@gmail.com>mikel salazar peña (area de jovenes profesionales, junta de ati madrid), <mikeltxo_uni@yahoo.es>real time systems
alejandro alonso muñoz, juan antonio de la puente alfaro (dit-upm),
<{aalonso,jpuente}@dit.upm.es>roboticsjosé cortés arenas (sopra group), <joscorare@gmail.com>juan gonzález gómez (universidad carlos iii ), <juan@iearobotics.comsecurity
javier areitio bertolín (univ. de deusto), <jareitio@deusto.es>
javier lópez muñoz (etsi informática-uma), <jlm@lcc.uma.es>software engineeringluis fernández sanz, daniel rodríguez garcía (universidad de alcalá),<{luis.fernandezs,daniel.rodriguez}@uah.es>tecnologies and business
didac lópez viñas (universitat de girona), <didac.lopez@ati.es>
alonso álvarez garcía (tid) <aag@tid.es>tecnologies for educationjuan manuel dodero beardo (uc3m), <dodero@inf.uc3m.es>césar pablo córcoles briongo (uoc), <ccorcoles@uoc.edu>.teaching of computer science
cristóbal pareja flores (dsip -ucm), <cpareja@sip.ucm.es>
j. ángel velázquez iturbide (dlsi i, urjc), angel.velazquez@urjc.es>technological trendsjuan carlos vigo (ati), <juancarlosvigo@atinet.es>gabriel martí fuentes (interbits), <gabi@atinet.es>web standards
encarna quesada ruiz (virati) <encarna.quesada@gmail.com>
josé carlos del arco prieto (tcp sistemas e ingeniería), <jcarco@gmail.com>)
copyrightcopyrightcopyrightcopyrightcopyright © ati 2014© ati 2014© ati 2014© ati 2014© ati 2014
the opinions expressed by the autors are their exclusive responsability
editorial office, advertising and  madrid office
plaza de españa 6, 2ª planta, 28008 madridtlfn.914029391; fax.913093685 <novatica@ati.es>layout and comunidad valenciana officeav. del reino de valencia 23, 46005 valenciatlfn. 963740173 <novatica_prod@ati.es>
acounting, subscriptions and catalonia office
calle ävila 48-50, 3a planta, local 9, 08005 barcelonatlfn.934125235; fax 934127713 <secregen@ati.es>andalucía  officeofficeofficeofficeoffice <secreand@ati.es>
galicia officeofficeofficeofficeoffice <secregal@ati.es>
subscriptións and sales  <novatica.subscripciones@atinet.es>
advertising plaza de españa 6, 2ª planta, 28008 madrid
tlnf.914029391; fax.913093685 <novatica@ati.es>legal depósit:  b 15.154-1975 -- issn: 0211-2124; coden novaec
cover page:  "mineral, vegetable, animal" - concha arias pérez  / © ati
layout dising: fernando agresta / © ati 2003
    special english edition 2013-2014    special english edition 2013-2014    special english edition 2013-2014    special english edition 2013-2014    special english edition 2013-2014
    annual selection of articles    annual selection of articles    annual selection of articles    annual selection of articles    annual selection of articles
editorial
ati: boosting the future > 02
from the chief editor´pen
process mining: taking advantage of information overload > 02
llorenç pagés casas
monograph
process mining
guest editors: antonio valle-salas and anne rozinat
presentation. introduction to process mining > 04
antonio valle-salas, anne rozinatprocess mining: the objectification of gut instinct - making business
processes more transparent through data analysis
> 06
anne rozinat, wil van der aalst
process mining: x-ray your business processes > 10
wil van der aalstthe process discovery journey
> 18
josep carmonausing process mining in itsm
> 22
antonio valle-salasprocess mining-driven optimization of a consumer loan approvals process
> 30
arjel bautista, lalit wangikar, s.m. kumail akbardetection of temporal changes in business processes
using clustering techniques
> 39
daniela luengo, marcos sepúlveda
summary
novática special english edition - 2013-2014 annual selection of articles2
ati: boosting the futureati: boosting the futureati: boosting the futureati: boosting the futureati: boosting the future    editorial
from the chief editor´s pen
process mining: taking advantage of information overload
in its 2014 general assembly, ati, the 47-year
old spanish association which publishes
novática,novática,novática,novática,novática,  set the foundations for a new
stage in the development of its activities. thebig changes occurred in the profession’sstructure and the huge advances in technologyin the fields of information and communicationmake necessary a transition towards a newmodel in the functioning of our associationwhich combines agility, efficiency and newways of participation for our members. withspecial mention of our aim of internationalreach and presence reinforced with the recentapproval of a new category of internationalmembership at ati.
the action plan approved in the general
assembly includes as the main priorities:
internationalizationinternationalizationinternationalizationinternationalizationinternationalization: this is going to
be one of our cornerstones for the futurewith a special focus on ati’s expansion inlatin american spanish speaking countries.
services for membersservices for membersservices for membersservices for membersservices for members : ati members
will continue to be the centre of our attention.for this purpose we are planning to create a
variety of offers of professional activities andtraining intended to be attractive both in contentand cost. all of them having the ati trust markas a guarantee.
brand and communicationbrand and communicationbrand and communicationbrand and communicationbrand and communication: we aim
to further develop and enhance ourcommunication channels. that’s to say thenewsletter we publish on a weekly basis,increasing participation in social networksas well as promotion and spreading of ouractivities. with a special emphasis on
nováticanováticanováticanováticanovática , the improvement of its digital
version and the search for collaborativepartnerships to increase its spread.
sustainabilitysustainabilitysustainabilitysustainabilitysustainability: we will continue to
streamline our financial structure to ensurethe viability of our activities by creating amodel which allows us to keep them attractivewithout a further impact on costs andmembership fees.
professionalismprofessionalismprofessionalismprofessionalismprofessionalism: we are planning to
define a structure and concept of the itprofession and of the quality of itsdevelopment as a framework for ourinternational activities.
networkingnetworkingnetworkingnetworkingnetworking: this is without a doubt
one of our main reasons for the existence ofour association and therefore we shouldkeep improving our capabilities to establishrelationships among our members based ontheir interests and preferences.
changes in ati bylawschanges in ati bylawschanges in ati bylawschanges in ati bylawschanges in ati bylaws: we will
create a working group to prepare a proposalfor a new associative framework that alignswith the aforementioned goals of gainingrelevance and influence in national andinternational contexts whereas ourorganization becomes more agile andresponsive to it professionals.
to sum up, we can say that after our general
assembly, ati is fully prepared to continuewith its role of being the it associationleader in spain representing ourprofessionals worldwide.
dìdac lópez viñasdìdac lópez viñasdìdac lópez viñasdìdac lópez viñasdìdac lópez viñas
president of atipresident of atipresident of atipresident of atipresident of ati
in the so called "information era" almost every
simple event can be digitized.  in fact, speaking
in business terms, we could say that all of
them will be digitized without a doubt because
it is easy and cheap.
but, is it really cheap to do that? not
necessarily. being able to browse through
thousands of millions of event records is not
synonymous with success in business. by
contrast, it can lead you to failure if you don’t
have appropriate tools to manage the glut of
information.
in other words, we are not going here to
discuss the value of having that plethora of
information but how to take advantage of it
while avoiding its associated problems.
data mining, business intelligence are (relatively)
new approaches to tackle this issue and we have
broadly covered these topics in 
nováticanováticanováticanováticanovática  in the
last years. nevertheless, let me say that "mining
data" or "applying intelligence to business" sound
like incomplete approaches. " do you aim to
extract just ‘data’ from all that information ?"
you could object to the former solution. " you
apply ‘intelligence’ to business, but howsystematically ?" you could argue with the
partisans of the later.
in that context, process mining appears to
be the missing link or at least one of the
most important of them. this is the feeling
you get when you are introduced to this new
discipline in business terms. indeed it looks
plausible to think that all business events
whose historical data are being stored on a
daily basis respond to business models and
patterns. and, besides, it looks really feasible
to think that artificial intelligence techniques
could help you to surf into how your data
is connected in order to discover process
models and patterns you are not aware of.
the deeper you go into the subject the more
you feel this is definitely a very promissory
approach. especially considering that in the
business arena the concept of "process" is
more and more placed at a higher level than
"data" or even "data collection". nowadays,
the emergent trend to set out corporate
governance goals and business compliance
requirements use the concept of ‘business
process’ as their elementary unit of work.all those reasons have led us to select this
monograph entitled " process miningprocess miningprocess miningprocess miningprocess mining " whose
guest editors have been antonio valle-salasantonio valle-salasantonio valle-salasantonio valle-salasantonio valle-salas
(managing partner of g2) and anne rozinatanne rozinatanne rozinatanne rozinatanne rozinat
(co-founder of fluxicon) for the present special
english edition of nováticanováticanováticanováticanovática .
we thank very much to the guest editors and
the authors for their highly valuable
contribution to the monograph. sincere thanks
must be extended to arthur cookarthur cookarthur cookarthur cookarthur cook , jimjimjimjimjim
holderholderholderholderholder , roger shlomo-harrisroger shlomo-harrisroger shlomo-harrisroger shlomo-harrisroger shlomo-harris  and
william centrellawilliam centrellawilliam centrellawilliam centrellawilliam centrella   for their excellent  work
in the edition of the articles.
i hope you enjoy this issue as much as we did
during our process of edition.
llorenç pagés casasllorenç pagés casasllorenç pagés casasllorenç pagés casasllorenç pagés casas
chief editor of nováticachief editor of nováticachief editor of nováticachief editor of nováticachief editor of nováticainternational federation  
for  
information processing 
 
hofstrasse 3, a-2361 laxenburg, austria 
phone: +43 2236 73616  fax: +43 2236 736169 
e-mail: ifip@ifip.org    http://www.ifip.org 
 
ifip is the global forum of ict experts that plays an active role 
in developing and offering expertise to all stakeho lders of the 
information and knowledge society  4novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph guest editorspresentation. introduction
to process mining
during the last few decades information
technology has touched every part of ourlives. from cellular phones to the mostadvanced medical information processingsystems, vending machines and plcs inproduction lines, computerized componentsare everywhere. all these componentsgenerate vast amounts of information thatare growing exponentially, relatively fewyears ago the challenge was finding digitizedinformation, whereas now the problem isbeing able to process and give meaning to allthe information we generate.
in recent years we have seen how the
information analysis industry has proposedvarious approaches to this problem. someof them have been covered in one way oranother in previous editions of 
nováticanováticanováticanováticanovática :
starting with vldb  (very large databases)
in volume 91 (1991) and datawarehouse
approaches attempting to discover patternsin these data stores with 
data mining  in
volume 138 (1999), then followed
knowledge management  in volume 155
(2002). we realized how complex theproblem was in the monograph on 
the
internet of things  in volume 209 (2011)
and how we could exploit this information involume 211 on 
business intelligence  (2011).
finally, the industry is also moving in adirection not yet covered in 
nováticanováticanováticanováticanovática  but
certain to be addressed in the near future:
big data .
in the present volume of nováticanováticanováticanováticanovática  we
address a particularly interesting topic withinthis broad range of techniques for dataanalysis: 
process mining ..... this is a variant of
data mining in which we focus on analyzingthe information generated by the processesthat have been computerized and whoseexecutions have been traced. as 
anneanneanneanneanne
rozinat rozinat rozinat rozinat rozinat and wil van der aalstwil van der aalstwil van der aalstwil van der aalstwil van der aalst  explain in
the opening article, we will see that the firsttraces are found in the late nineteenth century,although in terms of modern science werefer to the seminal work of myhill / nerod(1958), or viterbi algorithms (1978).
in the late 90s there were already some specific
research teams in universities around theworld, especially the university of coloradoand 
technische universiteit eindhoven .
these teams developed their researchdefining algorithms and methods that allowthe treatment of process execution traces fordiscovery, analysis and representation of the
underlying processes.
however no tools that implemented these
algorithms with appropriate degrees ofusability had yet reached the market. by theend of 2003 the 
processmining.org
specialized community (a working group ofthe tu/e) was created, and in early 2004 thefirst version of prom was developed, a genericand open source framework for processmining that has become the primary tool forresearchers and analysts, now at version 6.3and including more than 500 plugins thatimplement state of the art in this field.
in 2009 a task force of the ieee focused
on process mining was created that now hasmembers from over 20 countries includingsoftware vendors (such as software ag,hp, ibm or fluxicon, among many others),consulting firms and analysts (processsphere, gartner and deloitte, among others)and a wide range of educational and researchinstitutions (tu/e, universitat politècnicade catalunya or universität zu berlin, toname but a few). one of the key objectivesof this task force is to spread the concepts,techniques and benefits of process mining.in 2011 they published the process miningmanifesto, a document signed by more than50 professionals translated into 12languages. we cannot include here the fulltext of the manifesto, but the reader will findthe reference in the links section of thismonograph.for this present edition of 
nováticanováticanováticanováticanovática  we
have been privileged to have a group ofauthors that give us different perspectiveson the matter. we begin with an introductoryarticle in which 
anne rozinatanne rozinatanne rozinatanne rozinatanne rozinat  and wil vanwil vanwil vanwil vanwil van
der aalstder aalstder aalstder aalstder aalst  set the context for process mining
concepts and state, in a very enlighteningprocess mining message, that it allows us toget an objective vision of our processes.
in the second paper 
wil van der aalstwil van der aalstwil van der aalstwil van der aalstwil van der aalst
guides us through the different uses we can
make of process mining: to create of a modelof the process, to check the compliance ofthe model or to improve an existing model.here another key message is presented: theuse of process mining as x-rays that allow usto see "inside" the process, based on theanalysis of real data from the execution of allcases (as opposed to the statistical samplingwe would do in an audit, for example).
in the next article you will find the 
josepjosepjosepjosepjosep
carmonacarmonacarmonacarmonacarmona ’s vision of the task of discovering
a process from its traces. here josep makesan entertaining approach to how we coulduse the mining process to decrypt themessage of an alien explaining his visit toplanet earth, while showing us the anatomyof the discovery process.
the introductory papers will be followed by
a set of articles focusing on case studies.first 
antonio valle-salasantonio valle-salasantonio valle-salasantonio valle-salasantonio valle-salas ’’’’’ article presents
an application of process mining in a specificindustry, focusing on the processes in an itantonio valle-salas1, anne
rozinat2
managing partner of g2; co-founder of
fluxicon
<avalle@gedos.es>, <anne@fluxicon.com><avalle@gedos.es>, <anne@fluxicon.com><avalle@gedos.es>, <anne@fluxicon.com><avalle@gedos.es>, <anne@fluxicon.com><avalle@gedos.es>, <anne@fluxicon.com>
antonio valle-salas  is managing partner of g2 and a specialist consultant in itsm (information
technology service management) and it governance. he graduated as a technical engineer inmanagement informatics from upc ( universitat politécnica de catalunya ) and holds a number of
methodology certifications such as itil service manager from exin (examination institute for information
science), certified information systems auditor (cisa) from isaca, and cobit based it governancefoundations from it governance network, plus more technical certifications in the hp openview family
of management tools. he is a regular collaborator with itsmf (it service management forum) spain
and its catalan chapter, and combines consulting and project implementation activities with frequentcollaborations in educational activities in a university setting (such as upc or the universitat pompeu
fabra ) and in the world of publishing in which he has collaborated on such publications as it governance:
a pocket guide, metrics in it service organizations, gestión de servicios ti.  una introducción a itil ,
and the translations into spanish of the books itil v2 service support and itil v2 service delivery.
anne rozinat  has more than ten years of experience with process mining technology and obtained her
phd cum laude in the process mining group of prof. wil van der aalst at the eindhoven university of
technology in the netherlands. currently, she is a co-founder of fluxicon and blogs at <http://
www.fluxicon.com/blog/>.5process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesuseful references of "process mining"useful references of "process mining"useful references of "process mining"useful references of "process mining"useful references of "process mining"department and showing the different uses we
can make of these techniques in the world ofit service management (itsm)
then 
arjel d. bautistaarjel d. bautistaarjel d. bautistaarjel d. bautistaarjel d. bautista , lalit m.lalit m.lalit m.lalit m.lalit m.
wangikarwangikarwangikarwangikarwangikar  and syed kumail akbarsyed kumail akbarsyed kumail akbarsyed kumail akbarsyed kumail akbar  present
the work done to optimize the loan approvalprocess of a dutch bank, a remarkable workthat was awarded the bpi challenge 2012prize.
finally 
daniela luengodaniela luengodaniela luengodaniela luengodaniela luengo  and marcosmarcosmarcosmarcosmarcos
sepúlvedasepúlvedasepúlvedasepúlvedasepúlveda  give us a research perspective
on one of the challenges stated in themanifesto: dealing with the concept drift.this term is used to refer to the situation inwhich the process is changing while it isbeing used. detecting these changes andincluding these features in the analysis isessential when working on rapidly changingenvironments because, otherwise, it couldlead to erroneous conclusions in analysis.
these authors have contributed with their
articles to give a clearer vision of what processmining is, what it is useful for and what itsfuture is. process mining is a relatively newscience but is already reaching the level ofmaturity required to become standardpractice in companies and organizations, asreflected in the articles’ practical uses.however, there are still many challengesahead and a long way to go: will we be ableto overcome the problems introduced by theconcept drift? can we use process miningnot only for knowing the past of a processbut also to predict its future? will weimplement these techniques in themanagement systems of business processesin order to provide them with predictivesystems or support operators?
we are sure we will see great advances in this
area in the near future.
in addition to the materials referenced by the
authors in their articles, we offer the followingones for those who wish to dig deeper into thetopics covered by the monograph:
w.m.p. van der aalst. w.m.p. van der aalst. w.m.p. van der aalst. w.m.p. van der aalst. w.m.p. van der aalst. process mining:
discovery, conformance and enhancement
of business processes.  springer verlag, 2011.
isbn 978-3-642-19344-6.
ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.
process mining manifesto  (en 12 idiomas).
<http://www.win.tue.nl/ieeetfpm/doku.php?id=shared: process_mining_manifesto>.fluxicon tu/eprocess mining group.fluxicon tu/eprocess mining group.fluxicon tu/eprocess mining group.fluxicon tu/eprocess mining group.fluxicon tu/eprocess mining group.
introduction to process mining: turning (big)
data into value  (video). <http://www.
youtube.com/watch?v=7oat7matu_u>.
fluxicon.fluxicon.fluxicon.fluxicon.fluxicon. process mining news . <http://
fluxicon.com/s/newsarchive>.
tu/e workgroup.tu/e workgroup.tu/e workgroup.tu/e workgroup.tu/e workgroup.  <http://www.
processmining.org>.
fluxiconfluxiconfluxiconfluxiconfluxicon ..... process mining blog.  <http://
fluxicon.com/blog/>.
ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.ieee task force on process mining.
<http://www.win.tue.nl/ieeetfpm/doku.
php?id=start>.linkedin. linkedin. linkedin. linkedin. linkedin. process mining  ( ( ( ( (community)
<http://www.linkedin.com/groups/process-
mining-1915049>.
tu/e.tu/e.tu/e.tu/e.tu/e. health analytics using process
mining . <http://www.healthcare-analytics-
process-mining.org>.6novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph1. introduction
the archive of the united states naval
observatory stored all the naval logbooks of
the us navy in the 19th century.  these
logbooks contained daily entries relating to
position, winds, currents and other details of
thousands of sea voyages.  these logbooks
lay ignored and it had even been suggested
that they be thrown away until mathew
fontaine maury came along.
maury (see figure 1) was a sailor in the us
navy and from 1842 was the director of the
united states naval observatory.  he
evaluated the data systematically and created
illustrated handbooks which visually mapped
the winds and currents of the oceans. these
were able to serve ships’ captains as a
decision-making aid when they were planning
their route.
in 1848 captain jackson of the w. h. d. c.
wright was one of the first users of maury’s
handbooks on a trip from baltimore to rio
de janeiro and returned more than a month
earlier than planned.  after only seven years
from the production of the first edition
maury’s sailing directions had saved the
sailing industry worldwide about 10 million
dollars per year [1].
the it systems in businesses also conceal
invaluable data, which often remains
completely unused.  business processes
create the modern day equivalent of "logbook
entries", which detail exactly which activities
were carried out when and by whom, (see
figure 2).  if, for example, a purchasing
process is started in an sap system, every
step in the process is indicated in the
corresponding sap tables. similarly, crm
systems, ticketing systems and even legacy
systems record historical data about the
processes.
these digital traces are the byproduct of the
increasing automation and it support of
business processes [2].
2. from random samples to
comprehensive analysis
before maury’s manual on currents and tides,
sailors were restricted to planning a route
based solely on their own experience. this is
also the case for most business processes:
nobody really has a clear overview of how
the processes are actually executed. instead,figure 1.  matthew fontaine maury (source: wikipedia).process mining: the objectification
of gut instinct - making business
processes more transparent
through data analysisanne rozinat1, wil van der
aalst2
1co-founder of fluxicon, the netherlands;
2technical university of eindhoven, the
netherlands
<anne@fluxicon.com>,
<w.m.p.v.d.aalst@tue.nl>
abstract: big data existed in the 19th century. at least that might be the conclusion you would draw by
reading  the story of matthew maury. we draw a parallel with the first systematic evaluations of
seafaring logbooks and we show how you can quickly and objectively map processes based on the
evaluation of log files in it systems.
keywords: big data, case study, log data, process mining, process models, process visualization,
systematic analysis.
authors
anne rozinat has more than eight years of experience with process mining technology and obtained
her phd cum laude in the process mining group of prof. wil van der aalst at the eindhoven university of
technology in the netherlands. currently, she is a co-founder of fluxicon and blogs at <http://
www.fluxicon.com/blog/>.
wil van der aalst is a professor at the technical university in eindhoven and with an h-index of over 90
points the most cited computer scientist in europe. well known through his work on the workflow
patterns, he is the widely recognized "godfather" of process mining. his personal website is <http://
www.vdaalst.com>.
there are anecdotes, good feeling and many
subjective (potentially contradicting)
opinions which have to be reconciled.the systematic analysis of digital log traces
through so-called process mining techniques
[3] offers enormous potential for all
7process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articles
figure 2. it-supported processes record in detail which activities were
executed when and by whom.organizations currently struggling with
complex processes.  through an analysis of
the sequence of events and their time stamps,
the actual processes can be fully and
objectively reconstructed and weaknesses
uncovered.  the information in the it logs
can be used to automatically generate process
models, which can then be further enriched
by process metrics also extracted directly
out of the log data (for example execution
times and waiting times).
typical questions that process mining can
answer are:
what does my process actually look like?
where are the  bottlenecks?
are there deviations from the prescribed
or described process?
in order to optimize a process, one must first
understand the current process reality - the
‘as-is’ process.  this is usually far from
simple, because business processes arethe manual discovery through classical workshops
and interviews is costly and time-consuming,
remaining incomplete and subjective
“
”
figure 3. process visualization of the refund process for cases were started via the call center (a) and via the internet portal (b).
in the case of the internet cases missing information has to be requested too often. in the call center-initiated process, however,
the problem does not exist.8novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographlike maury did with the naval log books, objective process maps
can be derived that show how processes
actually work in the real world
performed by multiple persons, often
distributed across different organizational
units or even companies.
everybody only sees a part of the process.
the manual discovery through classical
workshops and interviews is costly and time-
consuming, remaining incomplete and
subjective. with process mining tools it is
possible to leverage existing it data from
operational systems to quickly and
objectively visualize the as-is processes as
they are really taking place.
in workshops with process stakeholders one
can then focus on the root cause analysis
and the value-adding process improvement
activities.
3. a case study
in one of our projects we have analyzed a
refund process of a big electronics
manufacturer. the following processdescription has been slightly changed to
protect the identity of the manufacturer.
the starting point for the project was the
feeling of the process manager that the
process had severe problems. customer
complaints and the inspection of individual
cases indicated that there were lengthy
throughput times and other inefficiencies in
the process.
the project was performed in the phases:
first, the concrete questions and problems
were collected, and the it logs of all cases
from the running business year were extracted
from the corresponding service platform.
then, in an interactive workshop involving
the process managers the log data were
analyzed.
for example, in figure 3 you see a simplified
fragment of the beginning of the refund process.
on the left side (a) is the process for all cases
that were initiated via the call center. on theright side (b) you see the same process fragment
for all cases that were initiated through the
internet portal of the manufacturer. both
process visualizations were automatically
constructed using fluxicon’s process mining
software disco based on the it log data that
had been extracted.
the numbers, the thickness of the arcs, and
the coloring all illustrate how frequently
each activity or path has been performed.
for example, the visualization of the call
center-initiated process is based on 50 cases
(see left in figure 3). all 50 cases start with
activity order created. afterwards, the request
is immediately approved in 47 cases. in 3 cases
missing information has to be requested from
the customer. for simplicity, only the main
process flows are displayed here.
what becomes apparent in figure 3 is that,
although missing information should only
occasionally be requested from the customer,“
”
figure 4. screenshot of the process mining software disco in the performance analysis view. it becomes apparent that the
shipment through the forwarding company causes a bottleneck.9process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesreferencesthis happens a lot for cases that are started
via the internet portal: for 97% of all cases
(77 out of 83 completed cases) this additional
process step was performed. for 12 of the 83
analyzed cases (ca. 14%) this happened even
multiple times (in total 90 times for 83
cases).
this process step costs a lot of time because
it requires a call or an email from the service
provider. in addition, through the external
communication that is required, the process
is delayed for the customer, who in a refund
process has already had a bad experience.
therefore, the problem needs to be solved.
an improvement of the internet portal (with
respect to the mandatory information in the
form that submits the refund request) could
ensure that information is complete when
the process is started.
another analysis result was a detected
bottleneck in connection with the pick-ups
that were performed through the forwarding
company. the process fragment in figure 4
shows the average waiting times between the
process steps based on the timestamps in
the historical data.
also such waiting times analyses are
automatically created by the process mining
software. you can see that before and after
the process step shipment via forwarding
company a lot of time passes. for example,
it takes on average ca. 16 days between
shipment via forwarding company and
product received. the company discovered
that the root cause for the long waiting times
was that products were collected in a palette
and the palette was shipped only when it was
full, which led to delays particularly for
those products that were placed in an almost
empty palette. also the actual refund process
at the electronics manufacturer was taking
too long (on average ca. 5 days). for the
customer the process is only completed when
she has her money back.
as a last result of the process mining analysis,
deviations from the required process were
detected. it is possible to compare the log data
(and therewith the actual process) objectively
and completely against required business rules,
and to isolate those cases that show deviations.
specifically, we found that (1) in one case the
customer received the refund twice, (2) in two
cases the money was refunded without ensuring
that the defect product had been received by
the manufacturer, (3) in a few cases an
important and mandatory approval step in the
process had been skipped.
4. state of the art
process mining, which is still a young and
relatively unknown discipline, is being made
available by the first professional software
tools on the market and supported by[1] tim zimmermann. the race: extreme sailing and
its ultimate event: nonstop, round-the-world, no
holds barred. mariner books, 2004. isbn-10:
0618382704.
[2] w. brian arthur. the second economy. mckinsey
quarterly, 2011.
[3] wil m.p. van der aalst. process mining:
discovery, conformance and enhancement of busi-
ness processes. springer-verlag, 2011. isbn-10:
3642193447.
[4] alberto manuel. process mining - ana aeroportos
de portugal, 2012. bptrends, <www.bptrends.
com>.
[5] ieee task force on process mining. <http://
www.win.tue.nl/ieeetfpm/>.
[6] ieee task force on process mining. process
mining manifesto. business process management
workshops 2011, lecture notes in business
information processing, vol. 99, springer-verlag,
2011.
[7] anne rozinat. how to reduce waste with process
mining, 2011. bptrends, <www.bptrends.com>.
[8] mark a. thornton. general circulation and the
southern hemisphere, 2005. <http://www.
lakeeriewx.com/meteo241/researchtopictwo/
projecttwo.html>.published case studies [4 |. the ieee task
force on process mining [5] was founded in
2009 to increase the visibility of process
mining. in autumn 2011, it published a
process mining manifesto [6], which is
available in 13 languages.
companies already generate vast quantities
of data as a byproduct of their it-enabled
business processes.  this data can be directly
analyzed by process mining tools. like maury
did with the naval log books, objective
process maps can be derived that show how
processes actually work in the real world [7].
developments in the field of big data are
helping to store and access this data to
analyze it effectively.
matthew fontaine maury’s wind and current
books were so useful that by the mid-1850s,
their use was even made compulsory by
insurers [8] in order to prevent marine
accidents and to guarantee plain sailing.
likewise, in business process analysis and
optimization, there will come a point when
we can not imagine a time when we were ever
without it and left to rely on our gut feeling.10novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph1. process mining spectrum
process mining aims to discover, monitor
and improve real processes by extracting
knowledge from event logs readily available
in today’s information systems [1][2].
although event data are omnipresent,
organizations lack a good understanding of
their actual processes. management
decisions tend to be based on powerpoint
diagrams, local politics, or management
dashboards rather than an careful analysis of
event data. the knowledge hidden in event
logs cannot be turned into actionable
information. advances in data mining made
it possible to find valuable patterns in large
datasets and to support complex decisions
based on such data. however, classical data
mining problems such as classification,
clustering, regression, association rule
learning, and sequence/episode mining are
not process-centric.
therefore, business process management
(bpm) approaches tend to resort to hand-
made models. process mining research
aims to bridge the gap between data mining
and bpm. metaphorically, process mining
can be seen as taking x-rays to diagnose/
predict problems and recommend treat-
ment.
an important driver for process mining is the
incredible growth of event data [4][6]. event
data is everywhere – in every sector, in every
economy, in every organization, and in every
home one can find systems that log events.
for less than $600, one can buy a disk drive
with the capacity to store all of the world’s
music [6]. a recent study published in
science, shows that storage space grew from
2.6 optimally compressed exabytes (2.6 x 10
18  bytes) in 1986 to 295 compressed exabytes
in 2007. in 2007, 94 percent of all
information storage capacity on earth was
digital. the other 6 percent resided in books,
magazines and other non-digital formats.
this is in stark contrast with 1986 when only
0.8 percent of all information storage capacity
was digital. these numbers illustrate the
exponential growth of data.process mining:
x-ray your business processeswil van der aalst
technical university of eindhoven, the
netherlands
<w.m.p.v.d.aalst@tue.nl>
abstract: recent breakthroughs in process mining research make it possible to discover, analyze, and
improve business processes based on event data. activities executed by people, machines, and soft-
ware leave trails in so-called event logs. events such as entering a customer order into sap, checking
in for a flight, changing the dosage for a patient, and rejecting a building permit have in common that
they are all recorded by information systems. over the last decade there has been a spectacular
growth of data. moreover, the digital universe and the physical universe are becoming more and more
aligned. therefore, business processes should be managed, supported, and improved based on event
data rather than subjective opinions or obsolete experiences. the application of process mining in
hundreds of organizations has shown that both managers and users tend to overestimate their knowledge
of the processes they are involved in. hence, process mining results can be viewed as x-rays showing
what is really going on inside processes. such x-rays can be used to diagnose problems and suggest
proper treatment. the practical relevance of process mining and the interesting scientific challenges
make process mining one of the "hot" topics in business process management (bpm). this article
provides an introduction to process mining by explaining the core concepts and discussing various
applications of this emerging technology.
keywords: business intelligence, business process management, data mining, management,
measurement, performance, process mining,
author
wil van der aalst is a professor at the technical university in eindhoven and with an h-index of over 90
points the most cited computer scientist in europe. well known through his work on the workflow
patterns, he is the widely recognized "godfather" of process mining. his personal website is <http://
www.vdaalst.com>.© 2013 acm, inc. van der aalst, w.m.p. 2012.
process mining. communications of the acm
cacm volume 55 issue 8 (august 2012) pages 76-
83, <http://doi.acm.org/10.1145/2240236.2240
257>. included here by permission.
the further adoption of technologies such
as rfid (radio frequency identification),
location-based services, cloud computing,
and sensor networks, will further accelerate
the growth of event data. however,
organizations have problems effectively using
such large amounts of event data. in fact,
most organizations still diagnose problems
based on fiction (powerpoint slides, visio
diagrams, etc.) rather than facts (event data).
this is illustrated by the poor quality of
process models in practice, e.g., more than
20% of the 604 process diagrams in sap’s
reference model have obvious errors and
their relation to the actual business processes
supported by sap is unclear [7]. therefore,
it is vital to turn the massive amounts of
event data into relevant knowledge and
reliable insights. this is where process mining
can help.
the growing maturity of process mining is
illustrated by the process mining manifesto
[5] recently released by the ieee task for-
ce on process mining. this manifesto is
supported by 53 organizations and 77 process
mining experts contributed to it. the active
contributions from end-users, tool vendors,consultants, analysts, and researchers
illustrate the significance of process mining
as a bridge between data mining and busi-
ness process modeling.
starting point for process mining is an event
log. each event in such a log refers to an
activity (i.e., a well-defined step in some
process) and is related to a particular case
(i.e., a process instance). the events
belonging to a case are ordered and can be
seen as one "run" of the process. event logs
may store additional information about
events. in fact, whenever possible, process
mining techniques use extra information such
as the resource (i.e., person or device)
executing or initiating the activity, the
timestamp of the event, or data elements
recorded with the event (e.g., the size of an
order).
event logs can be used to conduct three
types of process mining as shown in figure
1 [1]. the first type of process mining is
discovery. a discovery technique takes an
event log and produces a model without
using any a-priori information. process
discovery is the most prominent process11process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesfigure  1. the three basic types of process mining explained in terms of input and output.conformance checking can be used to check if reality,
as recorded in the log, conforms to the model and vice versa
mining technique. for many organizations it
is surprising to see that existing techniques
are indeed able to discover real processes
merely based on example behaviors recorded
in event logs.
the second type of process mining is
conformance. here, an existing process
model is compared with an event log of the
same process. conformance checking can
be used to check if reality, as recorded in the
log, conforms to the model and vice versa.
the third type of process mining is
enhancement. here, the idea is to extend or
improve an existing process model using
information about the actual process
recorded in some event log. whereas
conformance checking measures the
alignment between model and reality, this
third type of process mining aims at changing
or extending the a-priori model. for instance,
by using timestamps in the event log one can
extend the model to show bottlenecks, service
levels, throughput times, and frequencies.
2. process discovery
as shown in figure 1, the goal of process
discovery is to learn a model based on some
event log. events can have all kinds of
attributes (timestamps, transactional
information, resource usage, etc.). these
can all be used for process discovery.however, for simplicity, we often represent
events by activity names only. this way, a
case (i.e., process instance) can be
represented by a trace describing a sequence
of activities.
consider for example the event log shown in
figure 1 (example is taken from [1]). this
event log contains 1,391 cases, i.e., instances
of some reimbursement process. there are
455 process instances following trace acdeh.
activities are represented by a single
character:  = register request, b = examine
thoroughly, c = examine casually, d = check
ticket, e = decide, f = reinitiate request, g =
pay compensation, and h = reject request.
hence, trace acdeh  models a reimbursement
request that was rejected after a registration,
examination, check, and decision step. 455
cases followed this path consisting of five
steps, i.e., the first line in the table
corresponds to 455 x 5 = 2,275 events. the
whole log consists of 7,539 events.
process discovery techniques produce
process models based on event logs such as
the one shown in figure 2. for example, the
classical -algorithm produces model m1
for this log. this process model is represented
as a petri net. a petri net consists of places
and transitions. the state of a petri net, also
referred to as marking, is defined by thedistribution of tokens over places.
a transition is enabled if each of its input
places contains a token. for example, 
 is
enabled in the initial marking of 
 , because
the only input place of 
 contains a token
(black dot). transition 
 in 
  is only
enabled if both input places contain a token.
an enabled transition may fire thereby
consuming a token from each of its input
places and producing a token for each of its
output places. firing 
 in the initial marking
corresponds to removing one token from
start and producing two tokens (one for
each output place). after firing 
, three
transitions are enabled: 
, 
, and 
.
firing 
 will disable 
 because the token
is removed from the shared input place (and
vice versa). transition 
 is concurrent with
 and 
, i.e., it can fire without disabling
another transition. transition 
 becomes
enabled after 
 and 
 or 
 have occurred.
after executing 
 three transitions become
enabled: 
, 
, and 
. these transitions
are competing for the same token thus
modeling a choice. when 
 or 
 is fired,
the process ends with a token in place end.
if 
 is fired, the process returns to the state
just after executing 
.
note that transition 
 is concurrent with
 and 
. process mining techniques need
to be able to discover such more advanced
process patterns and should not be restricted
to simple sequential processes.
it is easy to check that all traces in the event
log can be reproduced by 
 . this does not
hold for the second process model in figure
2. 
  is only able to reproduce the most
frequent trace 
 . the model does
not fit the log well because observed traces
such as 
  are not possible according
to 
 . the third model is able to reproduce
the entire event log, but 
  also allows for
traces such as 
  and 
 .
“”12novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph
figure 2. one event log and four potential process models (m1, m2, m3 and m4) aiming to describe the observed behavior.therefore, we consider 
  to be
"underfitting"; too much behavior is allowed
because 
  clearly overgeneralizes the
observed behavior. model 
  is also able to
reproduce the event log. however, the model
simply encodes the example traces in the
log. we call such a model "overfitting" as the
model does not generalize behavior beyond
the observed examples.
in recent years, powerful process mining
techniques have been developed that can
automatically construct a suitable process
model given an event log. the goal of such
techniques is to construct a simple model
that is able to explain most of the observed
behavior without "overfitting" or
"underfitting" the log.3. conformance checking
process mining is not limited to process
discovery. in fact, the discovered process is
merely the starting point for deeper analysis.
as shown in figure 1, conformance
checking and enhancement relate model and
log. the model may have been made by hand
or discovered through process discovery.
for conformance checking, the modeled
behavior and the observed behavior (i.e.,
event log) are compared. when checking the
conformance of 
  with respect to the log
shown in figure 2 it is easy to see that only
the 455 cases that followed 
  can be
replayed from begin to end. if we try to
replay trace 
 , we get stuck afterexecuting 
  because 
 is not enabled.
if we try to replay trace 
 , we get
stuck after executing the first step because
 is not (yet) enabled.
there are various approaches to diagnose
and quantify conformance. one approach is
to find an optimal alignment between each
trace in the log and the most similar behavior
in the model. consider for example process
model 
 , a fitting trace 
 , a
non-fitting trace 
 , and the
three alignments shown in table 1. 
shows a perfect alignment between 
 and
: all moves of the trace in the event log
(top part of alignment) can be followed by13process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesmoves of the model (bottom part of
alignment). 
 shows an optimal alignment
for trace 
 in the event log and model 
 .
the first two moves of the trace in the event
log can be followed by the model. however,
 is not enabled after executing just 
 and
. in the third position of alignment 
, we
see a 
 move of the model that is not
synchronized with a move in the event log. a
move in just the model is denoted as
). in the next three moves model and
log agree. in the seventh position of
alignment 
 there is just a move of the
model and not a move in the log: 
 ).
 shows another optimal alignment for
trace 
 . here there are two situations
where log and model do not move together:
) and 
 ). alignments 
 and
 are both optimal if the penalties for
"move in log" and "move in model" are the
same. in both alignments there are two 
steps and there are no alignments with less
than two
  steps.
conformance can be viewed from two angles:
(a) the model does not capture the real
behavior ("the model is wrong") and (b)
reality deviates from the desired model "the
event log is wrong"). the first viewpoint is
taken when the model is supposed to be
descriptive, i.e., capture or predict reality.
the second viewpoint is taken when the
model is normative, i.e., used to influence or
control reality.
there are various types of conformance and
creating an alignment between log and model
is just the starting point for conformance
checking [1]. for example, there are various
fitness (the ability to replay) metrics. a
model has fitness 1 if all traces can be
replayed from begin to end. a model has
fitness 0 if model and event log "disagree" on
all events. process models 
 , 
  and
 have a fitness of 1 (i.e., perfect fitness)
with respect to the event log shown in figu-
re 2 model 
  has a fitness 0.8 for the
event log consisting of 1,391 cases.intuitively, this means that 80% of the events
in the log can be explained by the model.
fitness is just one of several conformance
metrics.
experiences with conformance checking in
dozens of organizations show that real-life
processes often deviate from the simplified
visio or powerpoint representations used by
process analysts.
4. model enhancement
it is also possible to extend or improve an
existing process model using the alignment
between event log and model. a non-fitting
process model can be corrected using the
diagnostics provided by the alignment. if the
alignment contains many 
 ) moves,
then it may make sense to allow for the
skipping of activity 
 in the model.
moreover, event logs may contain
information about resources, timestamps,
and case data. for example, an event referring
to activity "register request" and case
"992564" may also have attributes describing
the person that registered the request (e.g.,
"john"), the time of the event (e.g., "30-11-
2011:14.55"), the age of the customer (e.g.,
"45"), and the claimed amount (e.g., "650
euro"). after aligning model and log it is
possible to replay the event log on the model.
while replaying one can analyze these
additional attributes.
for example, as figure 3 shows, it is
possible to analyze waiting times in-between
activities. simply measure the time difference
between causally related events and compu-
te basic statistics such as averages, variances,
and confidence intervals. this way it is
possible to identify the main bottlenecks.
information about resources can be used to
discover roles, i.e., groups of people
frequently executing related activities. here,
standard clustering techniques can be used.
it is also possible to construct social networks
based on the flow of work and analyze
resource performance (e.g., the relation
between workload and service times).
standard classification techniques can be
used to analyze the decision points in the
process model. for example, activity 
 ("de-cide") has three possible outcomes ("pay",
"reject", and "redo"). using the data known
about the case prior to the decision, we can
construct a decision tree explaining the
observed behavior.
figure 3 illustrates that process mining is
not limited to control-flow discovery.
moreover, process mining is not restricted
to offline analysis and can also be used for
predictions and recommendations at
runtime. for example, the completion time
of a partially handled customer order can be
predicted using a discovered process model
with timing information.
5. process mining creates
value in several ways
after introducing the three types of process
mining using a small example, we now focus
on the practical value of process mining. as
mentioned earlier, process mining is driven
by the exponential growth of event data. for
example, according to mgi, enterprises
stored more than 7 exabytes of new data on
disk drives in 2010 while consumers stored
more than 6 exabytes of new data on devices
such as pcs and notebooks [6].
in the remainder, we will show that process
mining can provide value in several ways. to
illustrate this we refer to case studies where
we used our open-source software package
prom [1]. prom was created and is
maintained by the process mining group at
eindhoven university of technology.
however, research groups from all over the
world contributed to it, e.g., university of
padua, universitat politècnica de catalunya,
university of calabria, humboldt-
universität zu berlin, queensland university
of technology, technical university of
lisbon, vienna university of economics
and business, ulsan national institute of
science and technology, k.u. leuven,
tsinghua university, and university of
innsbruck. besides prom there are about 10
commercial software vendors providing
process mining software (often embedded
in larger tools), e.g., pallas athena, soft-
ware ag, futura process intelligence,
fluxicon, businesscape, iontas/verint,
fujitsu, and stereologic.a petri net consists of places and transitions.
the state of a petri net, also referred to as marking, is defined
by the distribution of tokens over places
table 1. examples of alignment between the traces in the event log and the model.“
”14novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographit is also possible to extend or improve an existing process
model using the alignment between event log and model “”
figure 3. the process model can be extended using event attributes such as timestamps, resource information and case data. the model
also shows frequencies, e.g. 1,537 times a decision was made and 930 cases where rejected.!
5.1. provide insights
in the last decade, we have applied our
process mining software prom in over 100
organizations. examples are municipalities
(about 20 in total, e.g., alkmaar, heusden,
and harderwijk), government agencies (e.g.,
rijkswaterstaat, centraal justitieel incasso
bureau, and the dutch justice department),
insurance related agencies (e.g., uwv),
banks (e.g., ing bank), hospitals (e.g., amc
hospital and catharina hospital),
multinationals (e.g., dsm and deloitte),
high-tech system manufacturers and their
customers (e.g., philips healthcare, asml,
ricoh, and thales), and media companies
(e.g., winkwaves). for each of these
organizations, we discovered some of their
processes based on the event data they
provided. in each discovered process, there
were parts that surprised some of the
stakeholders. the variability of processes is
typically much bigger than expected. such
insights represent a tremendous value as
surprising differences often point to waste
and mismanagement.5.2. improve performance
as explained earlier, it is possible to replay
event logs on discovered or hand-made
process models. this can be used for
conformance checking and model
enhancement. since most event logs contain
timestamps, replay can be used to extend the
model with performance information.
figure 4 illustrates some of the performan-
ce-related diagnostics that can be obtained
through process mining. the model shown
was discovered based on 745 objections
against the so-called woz ("waardering
onroerende zaken") valuation in a dutch
municipality. dutch municipalities need to
estimate the value of houses and apartments.
the woz value is used as a basis for
determining the real-estate property tax. the
higher the woz value, the more tax the
owner needs to pay. therefore, many citizens
appeal against the woz valuation and assert
that it is too high.
each of the 745 objections corresponds to aprocess instance. together these instances
generated 9,583 events all having
timestamps. figure 4 shows the frequency
of the different paths in the model. moreover,
the different stages of the model are colored
to show where, on average, most time is
spent. the purple stages of the process take
most time whereas the blue stages take the
least time. it is also possible to select two
activities and measure the time that passes
in-between these activities.
as shown in figure 4, on average, 202.73
days pass in-between the completion of activity
"oz02 voorbereiden" (preparation) and the
completion of "oz16 uitspraak" (final
judgment). this is longer than the average
overall flow time which is approx. 178 days.
about 416 of the objections (approx. 56%)
follow this route; the other cases follow the
branch "oz15 zelf uitspraak" which, on avera-
ge, takes less time.
diagnostics as shown in figure 4 can be
used to improve processes by removing
15process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesfigure 4. performance analysis based on 745 appeals against the woz valuation.
bottlenecks and rerouting cases. since the
model is connected to event data, it is possible
to "drill down" immediately and investigate
groups of cases that take more time than
others [1].
5.3. ensure conformance
replay can also be used to check
conformance as is illustrated by figure 5.
based on 745 appeals against the woz
valuation, we also compared the normative
model and the observed behavior: 628 of the
745 cases can be replayed without
encountering any problems. the fitness of
the model and log is 0.98876214 indicating
that almost all recorded events are explained
by the model. despite the good fitness,
prom clearly shows all deviations. for
example, "oz12 hertaxeren" (reevaluate
property) occurred 23 times while this was
not allowed according to the normative model
(indicated by the "-23" in figure 5). again
it is easy to "drill down" and see what these
cases have in common.the conformance of the appeal process just
described is very high (about 99% of events
are possible according to the model). we
also encountered many processes with a
very low conformance, e.g., it is not
uncommon to find processes where only
40% of the events are possible according to
the model. for example, process mining
revealed that asml’s modeled test process
strongly deviated from the real process [9].
the increased importance of corporate
governance, risk and compliance manag-
ement, and legislation such as the sarbanes-
oxley act (sox) and the basel ii accord,
illustrate the practical relevance of
conformance checking. process mining can
help auditors to check whether processes are
executed within certain boundaries set by
managers, governments, and other stake
holders [3]. violations discovered through
process mining may indicate fraud,
malpractice, risks, and inefficiencies. for
example, in the municipality where we
analyzed the woz appeal process, wediscovered misconfigurations of their
eistream workflow management system.
people also bypassed the system. this was
possible because system administrators
could manually change the status of cases
[8].
5.4. show variability
hand-made process models tend to provide
an idealized view on the business process
that is modeled. often such a "powerpoint
reality" has little in common with the real
processes that have much more variability.
however, to improve conformance and per-
formance, one should not abstract away this
variability.
in the context of process mining we often see
spaghetti-like models such as the one shown
in figure 6. the model was discovered
based on an event log containing 24,331
events referring to 376 different activities.
the event log describes the diagnosis and
treatment of 627 gynecological oncologyoften such a ‘powerpoint reality’ has little in common with
the real processes that have much more variability.
however, to improve conformance and performance,
one should not abstract away this variability“
”16novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographfigure 6. process model discovered for a group of 627 gynecological oncology patients.
figure 5. conformance analysis showing deviations between eventlog and process.
17process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesreferencespatients in the amc hospital in amsterdam.
the spaghetti-like structures are not caused
by the discovery algorithm but by the true
variability of the process.
although it is important to confront
stakeholders with the reality as shown in
fig. 6, we can also seamlessly simplify spag-
hetti-like models. just like using electronic
maps it is possible to seamlessly zoom in and
out [1]. while zooming out, insignificant
things are either left out or dynamically
clustered into aggregate shapes – like streets
and suburbs amalgamate into cities in google
maps. the significance level of an activity or
connection may be based on frequency,
costs, or time.
5.5. improve reliability
process mining can also be used to improve
the reliability of systems and processes. for
example, since 2007 we have been involved
in an ongoing effort to analyze the event logs
of the x-ray machines of philips healthcare
using process mining [1]. these machines
record massive amounts of events. for
medical equipment it is essential to prove
that the system was tested under realistic
circumstances. therefore, process discovery
was used to construct realistic test profiles.
philips healthcare also used process mining
for fault diagnosis. by learning from earlier
problems, it is possible to find the root cause
for new problems that emerge. for example,
using prom, we have analyzed under which
circumstances particular components are
replaced. this resulted in a set of signatures.
when a malfunctioning x-ray machine
exhibits a particular "signature" behavior,
the service engineer knows what component
to replace.
5.6. enable prediction
the combination of historic event data with
real-time event data can also be used to
predict problems. for instance, philips
healthcare can anticipate that an x-ray tube
in the field is about to fail by discovering
patterns in event logs. hence, the tube can
be replaced before the machine starts to
malfunction.
today, many data sources are updated in
(near) real-time and sufficient computing
power is available to analyze events as they
occur. therefore, process mining is not
restricted to off-line analysis and can also be
used for online operational support. for a
running process instance it is possible to
make predictions such as the expected
remaining flow time [1].
6. conclusion
process mining techniques enable
organizations to x-ray their business processes,
diagnose problems, and get suggestions for
treatment. process discovery often provides[1] w. van der aaalst. process mining: discovery,
conformance and enhancement of business
processes. springer-verlag, berlin, 2011. isbn:
978-3-642-19345-3.
[2] w. van der aaalst. using process mining to
bridge the gap between bi and bpm. ieee computer
44, 12, pp. 77–80, 2011.
[3] w. van der aaalst, k. van hee, j.m. van werf,
m. verdonk. auditing 2.0: using process mining to
support tomorrow’s auditor. ieee computer 43, 3,
pp. 90–93, 2010.
[4] m. hilbert, p.lopez. the world’s technological
capacity to store, communicate, and compute
information. science 332, 6025, pp. 60–65, 2011.
[5] tfpm task force on process mining. process
mining manifesto. business process management
workshops, f. daniel, k. barkaoui, and s. dustdar,
eds. lecture notes in business information
processing series, vol. 99. springer-verlag, berlin,
pp. 169–194, 2012.
[6] j. manyika, m. chui, b. brown, j. bughin, r.
dobbs,  c. roxburgh, a. byers. big data: the next
frontier for innovation, competition, and
productivity. mckinsey global institute, 2011.
<http://www. mckinsey.com/insights/business_
technology/big_data_the_next_frontier_for_
innovation>.
[7] j. mendling, g. neumann, w. van der aalst.
understanding the occurrence of errors in process
models based on metrics. proceedings of the
otm conference on cooperative information
systems (coopis 2007). en f. curbera, f. leymann,
and m. weske, eds. lecture notes in computer
science series, vol. 4803. springer-verlag, berlin,
pp. 113–130, 2007.
[8] a. rozinat, w. van der aalst. conformance
checking of processes based on monitoring real
behavior. information systems 33, 1, pp. 64–95,
2008.
[9] a. rozinat, i. de jong, c. günther, w. van der
aalst. process mining applied to the test process
of wafer scanners in asml. ieee transactions on
systems, man and cybernetics, part c 39, 4, pp.
474–479, 2009.new and surprising insights. these can be used
to redesign processes or improve management.
conformance checking can be used to see
where processes deviate. this is very relevant
as organizations are required to put more
emphasis on corporate governance, risks, and
compliance. process mining techniques offer a
means to more rigorously check compliance
while improving performance.
this article introduced the basic concepts and
showed that process mining can provide value
in several ways. the reader interested in process
mining is referred to the first book on process
mining [1] and the process mining manifesto
[5] which is available in 12 languages. also
visit <www.processmining.org> for sample
logs, videos, slides, articles, and software.
the author would like to thank the members
of the ieee task force on process mining
and all that contributed to the process
mining manifesto and the prom framework.18novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph1. introduction
the speed at which data grows in it systems
[1] makes it crucial to rely on automation in
order to enable enterprises and institutions
to manage their processes. automated
techniques open the door for dealing with
large amounts of data, a mission unthinkable
for a human’s capabilities. in this paper we
discuss one of these techniques: the
discovery of process models. we now
illustrate the main task behind process
discovery by means of a (hopefully) funny
example.
2. a funny example: the visit
of an alien
imagine that an alien visits you (see figure
1) and, by some means, it wants to
communicate the plan it has regarding its
visit to the earth. for obvious reasons, we
cannot understand the alien’s messages, that
look like the one shown in figure 2.
although not knowing the meaning of each
individual letter in the message above, one
may detect that there are some patterns,
e.g., a repetition for the sequence i a c d m
e (first and last six letters in the sequence).
so the question is: how can we represent the
behavior of the aliens without knowing exactly
the meaning of each single piece of
information?the process discovery journeyjosep carmona
software department, technical university
of catalonia, spain
<jcarmona@lsi.upc.edu>
abstract: process models are an invaluable element of an it system: they can be used to analyze,
monitor, or improve the real processes that provide the system’s functionality. technology has enabled
it systems to store in file logs the footprints of process executions, which can be used to derive the
process models corresponding with the real processes, a discipline called process discovery. we
provide an overview of the discipline together with some of the alternatives that exist nowadays.
keywords: formal methods, process discovery, software engineering.
author
josep carmona received his ms and phd degrees in computer science from the technical university
of catalonia, in 1999 and 2004, respectively. he is an associate professor in the software department
of the same university. his research interests include formal methods, concurrent systems, and process
and data mining. he has co-authored more than 50 research papers in conferences and journals.
figure 1. our imaginary alien.process discovery may be a good solution
for this situation: a process discovery
algorithm will try to produce a (formal)
model of the behavior underlying a set of
sequences. for instance, the following for-
mal model in the business process modeling
notation (bpmn) [2] shown in figure 3
represents very accurately the behavior
expressed in the alien’s sequences. for those
not familiar with the bpmn notation, the
model above describes the following process:
after i occurs, then  (‘x’ gateway) either
branch b followed by x occurs, or branch a
followed by c and d in parallel(‘+’ gateway),
and then m occurs. both branches activate
e which in turn reactivates i. clearly, even
without knowing anything about the actions
taken from the alien, the global structuring
of these activities becomes very apparent
from a simple inspection of the bpmn model.
now imagine that at some point the meaning
of each letter is decrypted: evaluate the
amount of energy in the earth (i), high
energy (b), invade the earth (x), low energy
(a), gather some human samples (c), learn
the human reproduction system (d), teach
humans to increase their energy resources
(m), communicate the situation to the aliens
in the closest ufo (e). in the presence of
this new information, the value of the model
obtained is significantly incremented
(although maybe one may not be relaxed
after realizing the global situation that the
model brings into light).
figure 2. a message sent by the alien.i a c d m e i b x e i a d c m e i b x e i a c d m e 3. anatomy of a simple process
discovery algorithm
the previous example illustrates one of the
main tasks of a process discovery algorithm:
given a set of traces (called log)
corresponding to a particular behavior under
study, derive a formal model which represents
faithfully the process producing these tra-
ces. in its simplest form, process discovery
algorithms focus on the control-flow
perspective of the process, i.e., the ordering
activities are performed in order to carry out
the process tasks. the previous example has
considered this perspective.
a log must contain enough information to
extract the sequencing of the activities that
are monitored. typically, a trace identifier,
an activity name and a time stamp are
required to enable the corresponding
sequencing (by the time stamp) for the
activities belonging to a given trace
(determined by the trace identifier). other
information may be required if the discovery
engine must take into account additional
information, like resources (what quantity
was purchased?), activity originator (who
performed that activity?), activity duration
(how long does activity x last?), among
others. an example of a discovery algorithm
that takes into account other dimension is
the social network miner [3], that derives
the network of collaborators that carry out a
given process.19process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesthe core of a process discovery algorithm is
the ability to extract the necessary
information required to learn a model that
will represent the process. process discovery
is often an unsupervised learning task, since
the algorithm is usually exposed only to
positive examples, i.e., successful executions
of the process under study: in the example of
the introduction, we were only exposed to
what the alien plans to do, but we do not
know what the alien does not plan to do.
this complicates the learning task, since
process discovery algorithms are expected
to produce models that are both precise (the
model produced should not deviate much
from the behavior seen) and general (the
model should generalize the patterns
observed in the log) [4]. obviously, the
presence of negative examples would help
the discovery algorithm into improving these
two quality metrics, but negative information
is often not available on it logs.
how to learn a process model from a set of
traces? various algorithms exist nowadays
for various models (see section 4).
however, let us use the alien’s example to
reason on the discovery of the bpmn model
above. if we focus on the first letter of the
sequence (i), it is sometimes followed by a
and sometimes by b, and always (except for
the first occurrence) preceded by e. these
observations can be expressed graphically as
shown in figure 4.
in bpmn notation, the or-exclusive relation
between the occurrences of either a or b
after i is modeled by using the ‘x’ gateway.
the precedence between e and i is modeled
by an edge connecting both letters in the
model. symmetrically, e is preceded either
by m or by x. also, following a both c and
d occur in any order. the well-known alpha
algorithm [5] can find most of these pair-wise ordering relations in the log, and one
may use them to craft the bpmn model as
table 1 illustrates.
table 1 can be read as follows: if in the log
a precedes b always but b is unique (there
is no other letter preceded by a), then a
directed arc between a and b is created. if in
contrast there is always more than one letter
preceded by a, then an ‘+’ gateway is inserted
between a and the letters preceded by a.
the sometimes relation can be read similarly.
hence one can scan the log to extract these
relations (worst-case quadratic in the size of
the log) and use the table to create the
bpmn model. however, this is a very
restrictive way of discovery since other
relations available in the bpmn notation
can also be hidden in the log, like the inclu-
sive-or relation, but the algorithm does not
consider them. process discovery
algorithms are always in a trade-off between
the complexity of the algorithm and the
modeling capacity: the algorithm proposed
in this section could be extended to
consider also inclusive-or gateways, but
that may significantly complicate the
algorithm. below we address informally
these and other issues.
4. algorithms and models
there are several models that can be obtained
through different process discovery
algorithms: petri nets, event-driven process
chains, bpmn, c-nets, heuristic nets,
business process maps, among others.
remarkably, most of these models are
supported by replay semantics that allow
one to simulate the model in order to certify
its adequacy in representing the log.
to describe each one of these models is out
of the scope of this article, but i can brieflycomment on petri nets, which is a model
often produced by discovery algorithms, due
to its formal semantics and ability to represent
concurrency. for the model of our running
example, the corresponding petri net that
would be discovered by most of the petri net
discovery algorithms will be as shown in
figure 5.
those readers familiar with petri nets will
find a perfect match between the underlying
behavior of the petri net and the alien’s
trace. notice that while in the bpmn model,
apart from the units of information (in this
case letters of the alphabet), there are other
model components (gateways) whose
semantics define the way the model
represents the log traces.
the same happens with the petri net above,
where the circles correspond to the global
behavior of the model, which is distributed
among the net (only some circles are marked).
while the discovery algorithm for bpmn
needs to find both the connections and
gateways, the analogous algorithm for petri
nets must compute the circles and
connections.
several techniques exist nowadays to
accomplish the discovery of petri nets,
ranging from the log-ordering relations
extracted by the alpha algorithm, down to
very complex graph-based structures that
are computed on top of an automaton
representing the log traces.
what process discovery algorithm/modeling
notation to choose? this is in fact a very
good question that can only be answered
partially: there is no one model that is better
than the rest, but instead models that are
better than others only for a particular type
of behaviors. actually, deciding the bestfigure 3. a formal model of behavior in the alien’s sequences in bpmn.the core of a process discovery algorithm is the ability
to extract the necessary information required to learn a
model that will represent the process“
”
 
i x a 
b x + 
 c 
d + 
 m 
x e 20novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographmodeling notation for a log is a hard problem
for which research must provide techniquesin the next decade (a problem called
representational bias selection ). from a
pragmatic point of view, one must selectthose process modeling notations one isfamiliar with, and expect the discoveryalgorithms for that notation to be goodenough for the user needs.
as said before, other perspectives different
from the control-flow view may be consideredby process discovery algorithms: time,resources, organizational, etc.the reference book [6] may be consulted in
order to dig into these other processdiscovery algorithms.
5. tools5. tools5. tools5. tools5. tools
process discovery is a rather new discipline,
if compared with related areas such as datamining or machine learning. in spite of this,one can find process mining tools both inacademia (mostly) but also in industry.
the following classification is by no means
exhaustive, but instead reports some of theprominent tools one can use to experiencewith process discovery tools:
academia: the prom framework, from
technical university of eindhoven (tu/e) is
the reference tool nowadays. it is the result ofa great academic collaboration among severaluniversities in the world to gather algorithmicsupport for process mining (i.e., not onlyprocess discovery).  additionally, differentgroups have developed several academic stand-alone tools that incorporate modern processdiscovery algorithms.
industry: some important companies
have invested an effort into building process
discovery tools, e.g., fujitsu (apd), butalso medium-sized or start-ups that are morefocused on process mining practices, e.g.,
table 1. bpmn model built from patterns in the alien’s messages.i a c d m e i b x e i a d c m e i b x e i a c d m e  
 
figure 4. patterns observed in the alien’s messages.
 
i a 
b x c 
d m 
e 
figure 5.   petri net for the model of our running example.
21process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesreferences
[1] s. rogers. data is scaling bi and analytics-data
growth is about to accelerate exponentially - get
ready. information and management - brookfield,
21(5):p. 14, 2011.
[2] d. miers, s.a. white. bpmn modeling and
reference guide: understanding and using bpmn.
future strategies inc., 2008. isbn-10: 0977752720.
[3] w. m. p. van der aalst, h. reijers, m. song.
discovering social networks form event logs.
computer supported cooperative work, 14(6):pp.
549-593, 2005.
[4]  a. rozinat, w. m. p. van der aalst. conformance
checking of processes based on monitoring real
behavior. information systems, 33(1):pp. 64-95, 2008.
[5] w.m.p. van der aalst, a. weijters, l. maruster.
workflow mining: discovering process models from
event logs. ieee transactions on knowledge and
data engineering, 16 (9):pp. 1128–1142, 2004.
[6] w.m.p. van der aalst. process mining: discovery,
conformance and enhancement of business
processes. springer, 2011. isbn-10: 3642193447.actually, deciding the best modeling notation for
a log is a hard problem for which research must
provide techniques in the next decade“
”
pallas athena (reflectone), fluxicon (dis-
co), perspective software (bpmone, futu-
ra reflect), software ag (aris process
performance manager), among others.
6. challenges
the task of process discovery may be
aggravated if some of the aspects below are
present:
log incompleteness: the log often contains
only a fraction of the total behavior
representing the process. therefore, the
process discovery algorithm is required to
guess part of the behavior that is not present
in the log, which may be in general a difficult
task.
noise: logged behavior may sometimes
represent infrequent exceptions that are not
meant to be part of the process. hence,
process discovery algorithms may be
hampered when noise is present, e.g., in
control-flow discovery some relations
between the activities may become
contradictory. to separate noise from the
valid information in a log is a current research
direction.
complexity: due to the magnitude of
current it logs, it is often difficult to use
complex algorithms that may either require
loading the log into memory in order to
derive the process model, or apply techniques
whose complexity are not linear on the size
of the log. in those cases, high level strategies
(e.g., divide-and-conquer) are the only
possibility to derive a process model.
§visualization: even if the process
discovery algorithm does its job and can
derive a process model, it may be hard for a
human to understand it if it has more than a
hundred elements (nodes, arcs). in those
cases, a hierarchical description, similar to
the google maps application were one can
zoom in or out of a model’s part, will enable
the understanding of a complex process
model.
acknowledgements
i would like to thank david antón for creating the
alien’s drawing used in this paper.22novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph1. roles and responsibilities
in itsm models
all models, standards or frameworks used in
the itsm industry are process oriented.
this is because process orientation helps
structure the related tasks and allows the
organization to formalize the great variety of
activities performed daily: which activities
to execute and when, who should carry them
out, who owns what responsibilities over
those tasks, which tools or information
systems to use and what are the expected
objectives and outcomes of the process.
one model commonly used to represent the
different components of the process is the
itoco model [1] figure 1 that represents
the fundamental elements of a process:
inputs, outputs, tasks, control parameters
and outcomes.
this model allows us to differentiate between
three different roles needed for the correct
execution of any process: process operators,
who are responsible for executing the
different tasks; process managers, who
warrantee that the process execution meets
the specifications and ensure that both inputs
and outputs match the expectations (within
the specified control parameters); and process
owners, who use a governance perspective
to define the process, its outcomes and the
applicable controls and policies, as well as
being responsible to obtain and allocate the
resources needed for the right execution of
the process.
the process manager’s job is the execution
of the control activities (also called the
control process) over the managed process,
acting on the deviations or the quality
variations of the results, and managing the
allocated resources to obtain the best
possible results. therefore, this role requires
a combination of skills from diverse
professional disciplines such as auditing,
consulting and, chiefly, continuous
improvement.
2. itsm process management
the itsm industry has traditionally used a
number of methodological tools to enable
the process manager do the job:
definition of metrics and indicators
(usually standardized from the adopted
frameworks).
usage of balanced scorecards to showusing process mining in itsmantonio valle-salas
managing partner of g2
<avalle@gedos.es>
abstract: when it comes to information systems, ranging from copiers to surgical equipment or
enterprise management systems, all the information about the processes executed using those systems
are frequently stored in logs. specifically for it service management processes (itsm), it is quite
common for the information systems used to execute and control those processes to keep structured
logs that maintain enough information to ensure traceability of the related activities. it would be interesting
to use all that information to get an accurate idea of ??how the process looks like in reality, to verify if
the real process flow matches the previous design, and to analyze the process to improve it in order
to become more effective and efficient. this is the main goal of process mining. this paper explores
the different capabilities of process mining and its applicability in the it service management area.
keywords: change management, itsm, process management tools, process mining, service desk,
services.
author
antonio valle-salas is managing partner of g2 and a specialist consultant in itsm (information
technology service management) and it governance. he graduated as a technical engineer in
management informatics from upc (universitat politécnica de catalunya) and holds a number of
methodology certifications such as itil service manager from exin (examination institute for information
science), certified information systems auditor (cisa) from isaca, and cobit based it governance
foundations from it governance network, plus more technical certifications in the hp openview family
of management tools. he is a regular collaborator with itsmf (it service management forum) spain
and its catalan chapter, and combines consulting and project implementation activities with frequent
collaborations in educational activities in a university setting (such as upc or the universitat pompeu
fabra) and in the world of publishing in which he has collaborated on such publications as it governance:
a pocket guide, metrics in it service organizations, gestión de servicios ti. una introducción a itil,
and the translations into spanish of the books itil v2 service support and itil v2 service delivery.
and follow those indicators.
definition of management reports (daily,
weekly, monthly).
usage of various kinds of customer and/or
user satisfaction surveys.
performance of internal or external
compliance audits.
these tools allow the process manager to
gain knowledge about the behavior of the
processes she is in charge of, and to make
decisions to set the correct course of tasks
and activities. however these tools are
commonly rather rigid whereas the process
manager needs a deeper analysis of the
process behaviour.
still, there are two key aspects of any
continuous improvement model: to know
what the current situation is - as the starting
point for the improvement trip - and to
understand what the impact of the
improvement initiatives will be on the process
and its current situation. both aspects are
represented in figure 2.
at these initial stages many questions ariseregarding the daily activities of the process
manager, namely:
which is the most common flow?
what happens in some specific type of
request?
how long are the different cases at each
state of the flow?
can we improve the flow?
where is the flow stuck?
which are the most repeated activities?
are there any bottlenecks?
are the process operators following the
defined process?
is there segregation of duties in place?
moreover, in itsm we usually find that
most processes defined using frameworks
do not fully match the real needs of daily
operations; a standard and rigid approach to
processes does not meet the needs of those
activities in which the next steps are not
known in advance [2].
one clear case of this type of processes in
itsm is the problem management process.
here, to be able to execute the diagnostics
and identification of root causes, the operator23process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articleswill have to decide the next step according to
the results of the previous analysis. thus,
problem management is, by nature, a non-
structured process whose behavior will totally
differ from a strict process such as request
management.
3. process mining & itsm
the first and most delicate task when using
process mining techniques is obtaining a log
of good quality, representative of the process
we want to analyze, and with enough
attributes to enable filtering and driving
subsequent analysis steps as shown in figu-
re 3.
fortunately enough, most itsm process
management tools have logs that allow the
actions executed by the various process actors
to be traced. these logs (e.g. figure 4) are
usually between maturity levels iv and v on
the scale proposed by the process mining
manifesto [3].
the following steps of discovery and
representation are those in which the use of
process mining techniques provides
immediate value.
the designed processes are usually different
to the real execution of activities. this iscaused by various factors, amongst which
we find too generalist process designs (to try
to cover non-structured processes), flexibility
of the management tools (that are frequently
configured to allow free flows instead of
closed flows) and process operator’s
creativity (they are not always comfortable
with a strict process definition).
for this reason, both the process owner and
the process manager usually have an idealized
view of the process, and so are deeply
surprised the first time they see a graphic
representation of the process from the
analysis of the real and complete information.
for instance, as mentioned in usmbok
[4], the different types of request a user can
log into a call center will be covered by a
single concept of service request that will
then follow a different flow or pathway as
shown in figure 5. this flow will be "fitted"
within a common flow in the corresponding
module of the management tool used by the
service desk team.
in order to fit this wide spectrum of different
types of requests into a relatively general
flow we usually avoid a closed definition of
the process and its stages (in the form of a
deterministic automat) but we allow an openflow as shown in figure 6 in which any
operator decides at any given time the next
state or stage of the corresponding life cycle
[2].
that is why, when we try to discover and
represent these types of activities, we find
what in process mining jargon is called "spag-
hetti model" as shown in figure 7.  in this
model, even with a reduced number of cases,
the high volume and heterogeneous
transactions between states makes the
diagram of little (if any) use.
therefore, to facilitate analysis, we need to
use some techniques to divide the problem
into smaller parts [5]. we can use clustering,
or simply filtering the original log, in order to
select the type of pathway we want to analyze.
previous to the discovery and representation
tasks, it is recommended that the log is
enriched with any available information that
will later allow segmenting the data set
according to the various dimensions of
analysis.
for instance, in this case we will need to
have an attribute indicating the request type
or pathway to be able to break down the
model by requests, segmenting the data setthe role of the process manager requires a combination
of skills from diverse professional disciplines such as auditing,
consulting and, chiefly, continuous improvement
figure 1. the itoco model.“
”
tasks
procedures
work instructions
rolescontrol, 
quality 
speciﬁcations,
policies
resources capabilitiesinputs outputs
resources capabilities24novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographplan
do
checkactset 
goals
gap analysiscurrent situation
analysis
de?nition of 
improvement 
scenariosexecute
improvement
initiativesresults
evaluation
figure 2. continuous improvement cycle.
figure 3.  sequence of process mining steps.
figure 4. sample log.definition25process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesthe first and most delicate task when using process mining
techniques is obtaining a log of good quality, representative
of the process we want to analyze
figure 5. pathways, according to usmbok.
closedin 
progress
pending
solvedregistered
closedin 
progress
pending
solvedregistered
figure 6. strict flow vs. relaxed flow.“
”
26novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph
and carrying on the analysis of a specific
kind of request (see figure 8).
on the other hand, we need to remember that
process mining techniques are independent of
the process activity. instead, they focus in
analyzing the changes of state.
at this point, we can be creative and think
about the process flow as any "change of state
within our information system", so we can use
these techniques to analyze any otherfigure 7. spaghetti model.
transitions such as the task assignment flow
amongst different actors, the escalation flow
amongst different specialist groups or (even
lesser related to the common understanding of
a process) ticket priority changes and re
classifications (see figure 9).
finally, at the analysis stage it is time to
answering questions about the process
behaviour. to do this we have a broad array
of tools:
enrich the visual representation: for examplein figure 9 we can observe that longer
transactions between operators are represented
in a thicker line, or in figure 8 we show most
frequent states in darker color.
graphs and histograms: to represent
volume or time-related information. typical
situations of this kind of analysis are graphic
representations of the number of open cases
over time (backlog evolution) and histograms
showing the distribution of duration and/or
events per case.
in more analytic fields, we can obtain a
figure 8. filtered spaghetti model.27process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesfigure 9. social map: how cases flow through the different process operators.to facilitate analysis we need to use some
techniques to divide the problem into smaller parts
diagram showing a márkov chain for our
process (see figure 10). it will depict the
probability for a particular transition to
happen, to help answer questions like
"what is the probability that a closed ticket
will be re-opened?" we can also
complement this information with case
attributes: affected item, contact person,
request type, organization etc.  so that the
model for analysis is richer.
so far we have covered methodological tools
and mechanisms intended for quantitative and
statistical analysis of processes and their
behaviour. however, there is another side of
the analysis focusing in the specific area of
execution, answering questions such as "are
there clear patterns of behavior in my process?",
"is the process execution meeting previous
definitions or corporate policies?" [6].
to answer the first question we will use
the concept of "variant". we can describe
a variant as the set of cases executed
following the same trace of sequence of
events. thus, it is possible that some types
of requests are always completed in a
common pattern. we will easily check this
by analyzing the variants of our process asshown in figure 11 (right side), where we
see 79% of cases following the same flow:
registered à completed / validation à
closed.
to answer the second questions about
process conformance we must have a formal
model of the process to compare with its real
execution. once we have this piece, we can
carry out different approaches to the problem
of validation of conformance, as described
by anne rozinat in her paper conformance
checking of processes based on monitoring
real behavior [7]:
fitness analysis: answers the question "is
the observed process complying with the
process flow specified in the model?"
appropriateness analysis: answers the
question "does the process model describe
the observed process appropriately?"
nevertheless, calculating some fitness index
to a particular model will not be enough
when doing analysis or audits; in those
situations we will need ways to do complex
consultations of the log [8]. to be able to
know in which situations activity a is executed
before activity b, or when did operator x
executes activities a and b, will be of greatimportance to unveil violations of business
rules or policies that govern the execution of
the process.
if these techniques are applied to itsm
processes, we can provide an interesting
application to ensure segregation of duties
in change management for those
organizations needing to comply with sox
or similar regulations. next step would be
continuously monitoring of these rules [9].
4. conclusions
process mining is presented as a set of tools
that facilitate to a large degree process
owners’ and process managers’ tasks, from
acquiring knowledge about the real behaviour
of the process to audits and continuous
improvement. they allow many analyses
that would be practically impossible or
extremely costly to perform using traditional
strategies like reporting, dashboarding or
measuring indicators.
although, generally speaking, one of the
biggest difficulties we find in process mining
is the lack of information or logs to analyze,
in the specific area of it service management“”
28novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph
figure 11. process variants.startreassigned
registeredin progress
solvedclosed0,816
0,1840,804
0,1960,901
0,403
0,597 0,9270,0730,099
figure 10. simplified márkov chain.29process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesreferences
[1] jan van bon. it service management global best
practices, volume 1. nl, nl: van haren publishing,
2008.
[2] rob england. plus! the standard+case
approach. wellington, nz: createspace, 2013.
[3] ieee task force on process mining. process
mining manifesto (in 12 languages). <http://www.
win.tue.nl/ieeetfpm/doku.php?id=shared:
process_mining_manifesto>.
[4] ian m. clayton. usmbok - the guide to the
universal service management body of knowledge.
ca, us: service management 101, 2012.
[5] marco aniceto vaz, jano moreira de souza,
luciano terres, pedro miguel esposito. a case
study on clustering and mining business processes
from a university, 2011.
[6] wil m.p. van der aalst et al. auditing 2.0: using
process mining to support tomorrow’s auditor, 2010.
<http://bpmcenter.org/wp-content/uploads/
reports/2010/bpm-10-07.pdf>.
[7] anne rozinat, w.m.p. van der aalst. conformance
checking of processes based on monitoring real
behavior, 2008. <http://wwwis.win.tue.nl/
~wvdaalst/publications/p436.pdf>.
[8] w.m.p. van der aalst, h.t. de beer, b.f. van
dongen. process mining and verification of
properties: an approach based on temporal logic,
2005.
[9] linh thao ly, stefanie rinderle-ma, david
knuplesch, peter dadam. monitoring business
process compliance using compliance rule graphs,
2011. <http://dbis.eprints.uni-ulm.de/768/1/
paper.pdf>.this is not a problem. here itsm process
management tools keep logs that can be
used for process mining and can define
auditable fields to be traced giving us
different perspectives or dimensions of
analysis.
therefore, process mining stands out as a
powerful and adequate tool to support itsm
practices in its permanent quest for
improvement opportunities, both for
processes and services of the management
system.30novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph1. introduction1. introduction1. introduction1. introduction1. introduction
as the role of big data gains prevalence in
this information-driven era [1][2][3],businesses the world over are constantlysearching for ways to take advantage ofthese potentially valuable resources. the2012 business processing intelligencechallenge (bpic, 2012) is an exercise inanalyzing one such data set using acombination of commercial, proprietary, andopen-source tools, and combining these withcreative insights to better understand therole of process mining in the modernworkplace.
1.1. approach and scope1.1. approach and scope1.1. approach and scope1.1. approach and scope1.1. approach and scope
the situation depicted in bpic 2012 focuses
on the loan and overdraft approvals process ofa real-world financial institution in thenetherlands. in our analysis of this information,we sought to understand the underlying busi-ness processes in great detail and at multiplelevels of granularity. we also sought to identifyany opportunities for improving efficiency andeffectiveness of the overall process. specifically,we attempted to investigate the following areasin detail:
develop a thorough understanding of the
data and the underlying process.
 understand critical activities and decision
points.
map the lifecycle of a loan application
from start to eventual disposition.
identify any resource-level differences in
performance and opportunities for processinterventions.
as newcomers to process mining, we at
ckm advisors wanted to use thisopportunity to put into practice ourlearning in this discipline. we alsoattempted to combine process mining toolswith traditional analytical methods to builda more complete picture. we are certainthat with experience, our approach willbecome more refined and increasinglydriven by methods developed specificallyfor process mining.
our attempt was to be as broad as possible
in our analysis and delve deep where wecould. while we have done detailed analysisin a few areas, we have not covered allpossible areas of process mining in ouranalysis. any areas that we did not cover (forexample, social network analysis) are drivensolely by our own comfort and familiarityprocess mining-driven
optimization of a consumer
loan approvals processarjel bautista, lalit wangikar,
s.m. kumail akbar
ckm advisors, 711 third avenue, suite
1806, new york, ny, usa
<{abautista,lwangikar,sakbar}@ckmadvisors.com><{abautista,lwangikar,sakbar}@ckmadvisors.com><{abautista,lwangikar,sakbar}@ckmadvisors.com><{abautista,lwangikar,sakbar}@ckmadvisors.com><{abautista,lwangikar,sakbar}@ckmadvisors.com>
abstract: an event log (262,200 events; 13,087 cases) of the loan and overdraft approvals process
from a bank in the netherlands was analyzed using a number of analytical techniques. through a
combination of spreadsheet-based approaches, process mining capabilities and exploratory analytics,
we examined the data in great detail and at multiple levels of granularity. we present our findings onhow we developed a deep understanding of the process, assessed potential areas of efficiency
improvement and identified opportunities to make knowledge-based predictions about the eventual
outcome of a loan application. we also discuss unique challenges of working with such data, andopportunities for enhancing the impact of such analyses by incorporating additional data elements.
keywords: big data, business process intelligence, data analytics, process mining.
authors
arjel bautista  is a consultant at ckm advisors, involved in the development of innovative process re-
engineering and analytical research techniques within the firm. in his projects, arjel has deployed a
combination of state-of-the-art data mining tools and traditional strategic analysis to solve a variety ofproblems relating to business processes. he has also developed strategies for the analysis of
unstructured text and other non-traditional data sources. arjel holds masters and doctorate degrees in
chemistry from yale university, and a bachelor’s degree in biochemistry from uc san diego.
lalit wangikar is a partner at ckm advisors. as a consultant, lalit has advised clients primarily in the
financial services sector, insurance, and payment services industries. his primary area of expertise isuse of big data and analytics for driving business impact across all key business areas such as
marketing, risk, operations and compliance. he has worked with clients in north america, uk, singapore
and india. prior to joining ckm advisors, lalit ran decision analytics practice for exl service / inductis.prior to that he worked as a consultant with deloitte consulting and mitchell madison group where he
advised clients in banking and capital markets verticals.
syed m. kumail akbar  is a consultant at ckm advisors where he is a member of the analytics team
and assists in data mining, process mapping and predictive analytics. in the past, he has worked on
strategy and operations projects in the financial services industry. before joining ckm, syed worked
as a research assistant in both the quantitative analysis center and the physics department at wesleyan
university. he also co-founded possibilities pakistan, a non-governmental organization dedicated to
providing access to college counseling for high school students in pakistan. syed holds a ba in physicsand mathematics-economics from wesleyan university.
with the subject matter, and not necessarily
a limitation of the data.
2. materials and methods2. materials and methods2. materials and methods2. materials and methods2. materials and methods
2.1. understanding the data2.1. understanding the data2.1. understanding the data2.1. understanding the data2.1. understanding the data
the data captures process events for
13,087 loan / overdraft applications over asix month period, between october 2011and march 2012. the event log iscomprised of a total of 262,200 eventswithin these cases, starting with a customersubmitting an application and ending witheventual conclusion of that applicationinto an approval, cancellation or rejection(declined).  each application contains asingle attribute, amount_req, whichindicates the amount requested by theapplicant. for each event, the extract showsthe type of event, lifecycle stage (schedu-le, start, complete), a resource indicator
and time of completion.
the events themselves describe steps along
the approvals process and are classifiedintothree major types. table 1 table 1 table 1 table 1 table 1 shows the event
types and our understanding of what theevents mean.
by itself, the event log is a complicated mass
of information from which it is difficult todraw logical conclusions. therefore, as otherresearchers have noted [4][5], it is necessaryto subject the log to some degree ofpreprocessing in order to reduce its overallcomplexity, make visual connections betweenthe steps contained within, and aid inanalyzing and optimizing the businessconcepts at hand.31process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesalthough we were provided a rigorously pre-
processed event log that could be analyzed
in process mining tools quiet readily, we
processed the data further to build tailored
extracts for various analytical purposes.
2.2. tools used for analysis
disco: we procured an evaluation version
of disco 1.0.0 (fluxicon) and used it in the
exportation of data into formats suitable for
spreadsheet analysis. disco was especially
helpful in facilitating visualization of typical
process flows and exceptions.
microsoft excel: we used excel 2010
(microsoft) to foster deeper exploration intothe preprocessed data. excel was especially
helpful for performing basic and advanced
mathematical functions and data sorting,
two capabilities notably absent from the
disco application.
cart: we used an evaluation version of
the cart implementation (salford systems)
for conducting preliminary segmentation
analysis of the loan applications to assess
opportunities for prioritizing work effort.
3. understanding the process
in detail
3.1. simplifying the event log
upon obtaining the bpic 2012 event log,we first attempted to reduce its overall
complexity by identifying and removing
redundant events. for the purposes of this
analysis, an event is considered redundant if
it occurs concurrently with or subsequently
after another event, such that the time
between the two events is minimal (a few
seconds at most) with respect to the time
frame of the case as a whole.
initial analysis of the raw data in disco
revealed a total of 4,366 event order variants
among the 13,087 cases represented. we
surmised that removal of even one sequence
of redundant events could result in aas other researchers have noted, it is necessary to
subject the log to some degree of preprocessing
in order to reduce its overall complexity
type description
“a_” 
application 
eventsrefers to states of the application itself. after a customer initiates an 
application, bank resources follow up to complete the application where 
needed and facilitate decisions on applications.
“o_” 
offer eventsrefers to states of an offer communicated to the customer.
“w_” 
work eventsrefers to states of work items that occur during the approval process. 
these events capture most of the manual effort exerted by bank’s 
resources during the application approval process. the events describe 
efforts during various stages of the application process. 
-w_afhandelen leads: following up on incomplete initial 
submissions
-w_completeren aanvraag: completing pre-accepted applications
-w_nabellen offertes: follow up after transmitting offers to 
qualified applicants
-w_valideren aanvraag: assessing the application
-w_nabellen incomplete dossiers: seeking additional information 
during assessment phase
-w_beoordelen fraude: investigating suspect fraud cases
w_wijzigen contractgegevens: modifying approved contracts
table 1. event names and descriptions.
figure 1. standardized case flow for approved applications.“
”32novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographtable 2. potential redundancies in the event log.redundant events occurrence 
a_partlysubmitted immediately after a_submitted in all 13,087 cases. 
o_selected 
o_created both in quick succession prior to o_sent for the 5,015 cases 
selected to receive offers. in certain cases, o_cancelled 
(974 instances), a_finalized (2,907 instances) or 
w_nabellen offertes-schedule (1 instance) occur between 
o_selected and o_created in the offer creation 
process. 
o_accepted 
a_registered 
a_activated all three occur, in random order, with a_approved for the 
2,246 successful applications. in certain cases, o_accepted 
is interspersed among these events. 
 
significant reduction in the number of
variants. this simplification is compounded
further when the number of removed variants
is multiplied by others occurring downstream
of the initial event.
additionally, we eliminated two o-type events
(o_cancelled and o_declined)
which occur simultaneously with a_
cancelled and a_declined,
respectively. w-type events were not
considered for removal, as their transition
phases are crucial for calculating work time
spent per case. with the redundant events
removed from the event log, the number of
variants was reduced to 3,346 – an improvement
from the unfiltered data set of nearly 25%.
such consolidation can aid in simplifying the
process data and facilitating quicker analysis.
the variant complexity could be further reduced
by interviewing process experts at the bank to
help consolidate events that occur together
and sequencing variations not critical for busi-
ness analysis.
3.2. determining standard case
flow
we next sought to determine the standard
case flow for a successful application, against
which all other cases could then be
compared. we did this by loading the
simplified project into disco and filtering all
cases for the attribute a_approved. we
then set both the activities and paths
thresholds to the most rigorous level (0%),
which resulted in an idealized depiction of
the path from submission to approval (see
figure 1).
3.3. understanding application
outcomes
before launching into a more detailed review
of the data, we found it necessary to define
endpoint outcomes for all 13,087
applications. using the standardized case
flow (see figure 1), we determined that all
applications are subject to one of four fates
at each stage of the approvals process:
advancement to next stage: theapplication proceeds to the next stage of the
process.
approved: applications that are approved
and where the customer has accepted the
bank’s offer are considered a success and are
tagged as approved, with the end point
depicted by the event a_approved.
cancelled: the application is cancelled
by the bank or at the request of the customer.
cancelled applications have a final endpoint
of a_cancelled.
denied: the applicant, after having been
subject to review, is deemed unfit to receive
the requested loan or overdraft. denied
applications have a final endpoint of
a_declined.
we leveraged disco’s filtering algorithm to
define a set of possible endpoint behaviors.
399 cases were classified unresolved as they
were in progress at the time the data was
collected (i.e., did not contain endpoints of
a_declined, a_cancelled or
a_approved).
figure 2 shows a high-level process flow that
marks how the cases are disposed at each of the
key process steps. this analysis provides us
useful insights on the overall business impact
of this process as well as overall case flow
through critical process steps.
we observe several baseline performance
characteristics from figure 2:
~26% of applications are instantly declined
(3,429 out of 13,087); indicating tight
screening criteria for moving an application
beyond the starting point.
~24% of the remaining (2,290 out of
9,658) are declined after initial lead follow
up, indicating a continuous risk selection
process at play.
754 of the 3,254 applications that go to
validation stage (~23%) are declined,
indicating possibilities for tightening upfront
scrutiny at application or offer stages.
4. assessing process perfor-
mance4.1. case-level analysis
4.1.1. case endpoint vs. overall duration
in an effort to evaluate how the fate of a
particular case changes with overall duration,
we prepared a plot of these two variables and
overlaid upon it the cumulative amount of
work time amassed over the life of these
cases. we excluded 3,429 cases that are
instantly declined on initial application
submission, as no effort is spent on these.
we endeavored to visualize the point at
which exertion of additional effort yields
minimal or no return in the form of completed
(closed) applications.
figure 3 shows a lifecycle view of all
applications, indexed to the time of
submission. as shown in the figure, within
the first seven days applications continue to
move forward or are declined. at day 7, the
number of approved cases begins to rise,
suggesting this is the minimal number of
days required to fulfill the steps in the stan-
dard case flow (see figure 1).
approvals continue until ~day 23, at which
point >80% of all cases that are eventually
approved have been closed and registered.
there is a significant jump in the number of
cancelled applications at day 30, as inactive
cases receiving no response from the
applicant after stalling in the bottleneck
stages completeren aanvraag or nabellen
offertes are cancelled, likely according to
bank policies.
this raises the interesting question of when
the bank should stop any proactive efforts to
convert an application to a loan, and whether
the bank should treat customers differently
based on behaviors that indicate likelihood
of eventual approval. for example, the bank
exerts an additional 380+ person days of
effort between days 23 and 31, only to
cancel a majority of pending cases at the
conclusion of this period. with additional
data on customer profitability or lifetime
value and comparative cost of additional
effort, one can determine an optimal point33process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesin the process where additional effort on
cases that have not reached a certain stage
carries no positive value.
4.1.2. segmenting cases by amount
requested
as each case is associated with an amount
requested by the applicant, we found it
appropriate to arrange them into segmentsof roughly equal number, sorted by total
requested value. we first removed the
instantly declined cases by filtering them
through disco, as these are immediately
resolved upon submission and do not have
any additional effort or steps in the process.
the resultant 9,658 cases (which include
those in progress) were then split into deciles
of 965-966 cases each. each decile wasfurther segmented by classifying the cases
according to eventual outcome, and the
ensuing trends were examined for correlation
of approval percentage with amounts
requested (see figure 4).
we immediately observed the highest
approval percentages in deciles 3 and 6,
whose cases contained request ranges offigure 2. key process steps and application volume flow.
figure 3. distribution of cases by eventual outcome and duration, with cumulative work effort. gray: remaining in
progress, blue: cumulative declined, red: cumulative cancelled, green: cumulative approved. excludes 3,472 instantly
declined cases.1. a_submitted 
13,087 cases 
(100% of submissions) 2. a_partlysubmitted 
13,087 cases 
(100% of submissions) 3. a_preaccepted 
7,367 cases 
(56% of submissions) 4. a_accepted 
5,113 cases 
(39% of submissions) 
5. a_finalized 
5,015 cases 
(38% of submissions) 6. o_selected 
o_created 
o_sent 
5,015 cases 
(38% of submissions) 7. o_sent_back 
3,254 cases 
(25% of submissions) 8. a_approved 
a_activated 
a_registered 
2,246 cases 
(17% of submissions) afhandelen 
leads 
declined instantly: 3,429 
declined after call: 2,290 
cancelled: 1 
unresolved: 0 declined: 1,085 
cancelled: 1,100 
unresolved: 69 completeren 
aanvraag 
declined: 29 
cancelled: 66 
unresolved: 3 
valideren 
aanvraag 
declined: 48 
cancelled: 1,482 
unresolved: 231 declined: 754 
cancelled: 158 
unresolved: 96 finalization 
of applications 
customer response 
to mailed offers key process steps and distribution of application volume 34novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph
these results suggest that an office of specialists performing
single activities may be better suited to handle a larger amount of cases
than an army of resources charged with a myriad of tasks
figure 4. endpoints of cases (left axis), segmented by amounts requested by the applicant. green:
approved, red: cancelled, blue: declined, violet: in progress.5,000-6,000 and 10,000-14,000,
respectively. the exact reason for this
pattern is unclear; however, we speculate
that typical applicants will often choose a
"round" number upon which to base their
requests (indeed, this is reflected in the
three most frequent request values in the
data set: 5,000, 10,000 and 15,000).
perhaps a certain risk threshold change in
the bank’s approval process causes a step
change in approval percentages.
4.2. event-level analysis
4.2.1. calculating event duration
we sought to gain a detailed understanding
of the work activities embedded in the
approvals process, specifically those that
contribute a significant amount of time or
resources toward resolution. the format of
data made available in this case was not
readily amenable to this analysis.
we used excel to manipulate the event-level
data as provided and defined work time
(presumably actual effort expended by human
resources) for each event as the duration from
start to finish (start / completetransitions, respectively). in contrast, wait time
was defined as the latency between event
scheduling and commencement (schedu-
le / start), or the time elapsed between
two instances of a single activity type as well as
between complete of one event and
start of another:
as shown in table 3, two activities,
completeren aanvraag and nabellen
offertes, contribute a significant amount to
the total case time represented in the event
log. the accumulated wait time attributed
to each of these two events can reach as high
as 30+ days per case, as the bank presumably
makes numerous attempts to reach the
applicant until contact is made.
on closer inspection of the data, we realized
that the bank attempts to contact the
customer multiple times per day until day
30 in order to complete the application, as
well as to follow up on offers that have been
extended but not yet replied to.
4.2.2.  initial vs. follow-up activities
the average work time spent performingeach event changes whether the bank is
conducting it for the first time, or following
up on a previous step in a particular case
(see figure 5).
some differences in initial and follow-up
instances are minimal (valideren aanvraag),
while others are more pronounced
(beoordelen fraude). in the case of valideren
aanvraag, the bank is likely to be as thorough
as possible during the validation process,
regardless of how many times it has previously
viewed an application. on the other hand,
when investigating suspect cases for fraud,
the bank may already have come to a
preliminary conclusion regarding the
application and is merely using the follow-
up instance to justify its decision.
follow-up instances for those events in which
the bank must contact the applicant often
have smaller average work times than their
initial counterparts, as these activities are
those most likely to become trapped in
repeating loops, perhaps due to non-
responsive customers. one can leverage such
event data to understand customer behavior“
”35process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articles
these results suggest that an office of specialists performing
single activities may be better suited to handle a larger amount of cases
than an army of resources charged with a myriad of tasks
and assess potential usefulness of such data
for work prioritization.
4.3. resource-level analysis
4.3.1. specialist vs. generalist-driven
work activities
we profiled 48 resources that handled at
least 100 total events (excluding resource
112, as this resource does not handle work
events outside of scheduling) and computed
work volume by number of events handled
by each. we observed nine resources that
spent >50% of their effort on valideren
aanvraag, and a distinct group that mostly
performed activities of completeren
aanvraag, nabellen offertes and nabellen
incomplete dossiers. it appears validation is
performed by a dedicated team of specialists
focused on this work type, while customer-
facing activities such as completeren
aanvraag, nabellen offertes and nabellen
incomplete dossiers might require similar
skills that are performed by another
specialized group.
we next examined the performance of
resources identified as specialists (>50% of
work events of one single type) or contributors(25-50%) and compared them with those who
played only minor roles in similar activities. to
do this, we took the total work time accumulated
in an activity by resources belonging to a
particular category and calculated averages
based on the total number of work events
performed in that category. two activities,
nabellen offertes and valideren aanvraag, did
not contain specialists and contributors,
respectively, and so these categories were
omitted from the comparisons for these
activities.
as depicted in figure 6, specialists spent
less time per event instance than their
counterparts, in some cases performing tasks
up to 80% more efficiently than minor
players. the performance of contributors is
far less consistent, however, exhibiting ave-
rage work times / case that are both higher
(afhandelen leads, nabellen offertes) and
lower (completeren aanvraag, nabellen
incomplete dossiers) than those of the minor
players. these results suggest that an office
of specialists performing single activities
may be better suited to handle a larger
amount of cases than an army of resources
charged with a myriad of tasks.4.4. leveraging behavioral data
for work effort prioritization
one of the objectives of process mining is to
identify opportunities for driving process
effectiveness; that is, achieving better busi-
ness outcomes for the same or less effort in
a shorter or equal time period. in particular,
we sought to use process event data collected
on an application to better prioritize work
efforts. specifically, we set out to understand
if this could be done on the fifth day since
the application was submitted.
to do this, we created an application-level
data set for 5,255 cases that lasted >4 days
and where the end outcome is known. for
these applications, we captured all events
from submission until the end of day 4 and
used them to calculate the following:
what stage the application had reached,
and if it had been completed.
how much effort had already gone into the
application.
how many events of each kind had already
been logged.
if the application required lead follow up.
we attempted to find key segments in this“
”
figure 5. comparison of average work times, initial vs. follow-up event instances.36novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph afhandelen 
leads beoordelen 
fraude completeren 
aanvraag nabellen 
offertes nabellen 
incomplete 
dossiers valideren 
aanvraag 
work 
time:       
approved  13,659 23 45,909 68,473 89,204 121,099 
cancelled  14,601 2 119,497 94,601 25,633 7,775 
declined 67,560 
 2,471 63,052 30,870 26,993 29,946 
wait 
time:       
approved 198,916 8,456 1,873,537 34,972,224 5,980,887 10,537,938 
cancelled 300,062 28,763 16,582,465 42,630,195 2,006,774 678,105 
declined 986,421 236,115 3,294,367 13,542,054 1,001,354 3,227,252 
 
table 4. potential time savings associated with conversion of current generalists to single-activity specialists. (*) none of
the resources performing nabellen offertes were identified as specialists; therefore mean efficiency for area contributors was
used instead.
population that were highly likely to be
approved and accepted or highly likely to
be cancelled or declined. we did this by
subjecting the data to segmentation using
the classification and regression tree
(cart) technique (see figure 7).
the partial tree above shows two segments
with <6% approval rates: terminal nodes 1
and 14, consisting of a total of 1,018 cases
with only 49 eventual approvals. node 14,
consisting of 818 cases, shows incomplete
applications where the bank could not pre-
pare an offer for the customers by the end of
day 4. such "slow-moving" applications had
a <6% chance of being approved, compared
to an average of 42% for the entire group of
5,255. node 1 has applications that are
touched by 3 or fewer resources; with 112
being one of them. this might be another
indicator for a slow-moving application. such
applications have virtually no likelihood of
being approved in the end.
one could repeat this analysis at different
stages in the lifecycle of the application to help
with effort prioritization. this preliminary
analysis indicates significant potential to redu-
ce effort on cases that might not reach the
desired end state. further analysis with
customer demographics, application details,
and additional information on resources who
work on such cases will help refine the findings
and suggest specific action steps to improve
process effectiveness.
5. discussion
5.1. working with data challenges
5.1.1. managing event complexity
the optimization of the loan approvals
process is an exercise in streamlining each
step of the end-to-end operation. one nota-ble point that creates challenges in building
a streamlined process view with automated
process mining tools is the amount and
complexity of data captured. if such data is
not used with accompanying business
judgment, one can get lost in apparent
complexity (>4,000 process variants for a
process that has 6-7 key steps). we
illustrated this point above in our discussion
regarding redundant events. we recommend
dealing with such complexities at the time of
analysis, using process knowledge and good
business judgment, by performing additional
data pre-processing steps.
it is also critical to scrutinize event data up
front to understand all quirks and to build
ways of addressing these. for example, a
comparison of the number of start and
complete transitions for w-type events
in the data set reveals the existence of 1,037
more complete transitions than start
transitions. as the time stamps for these
events are unique with respect to others in
the same case id, they have the potential to
greatly confuse the summation of work and
wait times for a particular case and for
resources within the institution. we denoted
these as systems errors and worked with the
first complete following a start as
the "correct" one for a given work event type.
in a real project, we would validate our
assumption by deeper review of how such
instances arise in the system and using that
understanding to treat these observations
correctly in our analysis.
as described in section 3.1, the event
log would also benefit from consolidation
of events that happen concurrently, such
as those that occur when successful
applications are approved (a_approved,a_registered and a_activated).
this would not only decrease the overall
file size (which becomes important as the
volume of data grows), but also reduce the
complexity of the initial log.
5.2. potential benefits of resource
5.2.1. re-deployment recasting gene-
ral ists as specialists
as mentioned previously, the tasks involved
in the loan approvals process are performed
by a mixture of specialists and generalists.
through our analysis we concluded that the
bank might benefit from specialization of
labor, whereby current resources are
reassigned to single posts in order to maximize
efficiency. in table 4, we show potential
gains to be made through such restructuring.
if the bank can improve performance of
everyone executing a task to the same levels
as specialists, we estimate a substantial
overall time saving.
we also evaluated the potential savings
associated with downsizing the overall pool
of resources assigned to these tasks. using
the average amount of work time for resources
handling >100 total events (approximately
16,000 minutes; again excluding resource
112), we estimate opportunity to reduce the
work effort by 35%:
5.3. the power of additional
information additional
5.3.1. case-level attributes
in its raw form, the bpic 2012 event log is a
gold mine of information that, once decoded,
provides a detailed view of a consumer loan
approvals process. however, this information
would be greatly strengthened by the addition
of a few key data points. as each case carries
with it a single attribute – the amount requested37process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articles
figure 6. work time per event, specialists / contributors vs. minor players.
by the applicant – we have no way of knowing
why certain cases are approved while others
with identical request amounts and paths are
rejected. therefore it would be useful to know
customer demographics, current or past
relationships with the customers, and additional
details about the resources that execute these
processes. with this information, we can build
specific recommendations for changing the
process and more accurately estimate likely
benefits of such changes.
5.3.2. customer profitability and
operating costs
a final set of data notably absent from the
provided bpic 2012 log are the overall costs
associated with the loan approvals process
and value of each loan application to the
bank. it would be worthwhile to understand
how much it costs to operate each resource,
and whether this cost varies based on the
activities they perform or the number of
events they participate in. this information
would also allow us to calculate an average
acquisition cost for each applicant, and
subsequently understand the minimum
threshold below which it does not make
economic sense to approve an incoming
loan request.
6. conclusions
through comprehensive analysis of the bpic2012 event log, we converted a fairly complex
data set into a clearly interpretable, end-to-
end workflow for a loan and overdraft
approvals process. we examined the data at
multiple levels of granularity, uncovering
interesting insights at all levels. through our
work we uncovered potential improvements
in a number of areas, including revision of
automated processes, restructuring of key
resources, and evaluation of current case
handling procedures. indeed, future analysis
would be greatly aided by the inclusion of
additional data, such as customer information,
governing policies, operating costs and relative
customer value.
as part of our analysis, we performed a
rudimentary predictive exercise whereby we
determined the current status of cases at
various days in the approvals process and
quantified their chances of approval,
cancellation, or denial. this allowed us to
estimate the fate of a case based on its
performance and tailor the overall process
to minimize stalling at traditional case
bottlenecks. while preliminary in its nature,
this opens the door to more elaborate future
modeling exercises, perhaps driven by
sophisticated computer algorithms.
while we covered several areas in this exercise,
there are others where we did not conductdetailed analysis. the bank would find
significant additional benefits from exploring
such additional areas, for example, social
network analysis.
in conclusion, the procedures highlighted
by the bpic 2012 elaborate the role and
importance of process mining in the modern
workplace. steps that were previously
elucidated only after years of practice and
observation can now be examined using a
sample set of existing data. as the era of big
data continues its march toward the busi-
ness world, we foresee process mining as a
central player in the charge toward turning
questions into solutions and problems into
sustainable profit.
acknowledgements
we are grateful to the financial institution that made
this data available for study. special thanks to
fluxicon for providing us with an evaluation copy of
disco and an accompanying copy of the bpic 2012
data set. we also thank tom metzger, nicholas
hartman, rolf thrane and pierre buhler for helpful
discussions and insights. our thanks also to salford
systems, who made their software available in a
demonstration version.38novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographreferences
[1] w. van der aalst, a. adriansyah, a.k. alves de
medeiros, f. arcieri, t. baier et al. process mining
manifesto. business process management
workshops 2011, lecture notes in business
information processing, vol. 99. springer-verlag,
2011.
[2] j. manyika, m. chui, b. brown, j. bughin, r. dobbs,
c. roxburgh, a. byers. big data: the next frontier for
innovation, competition, and productivity. mckinsey
global institute, 2011. <http://www.mckinsey.com/
insights/business_ technology/big_data_the_next_
frontier_for_ innovation>.
[3] r. adduci, d. blue, g. chiarello, j. chickering, d.
mavroyiannis et al. big data: big opportunities to
create business value. technical report, information
intelligence group, emc corporation, 2011.
figure 7. partial view, cart segmentation tree.it is also critical to scrutinize event data up front to understand
all quirks and to build ways of addressing these “”
[4] r.p.j.c. bose, w.m.p. van der aalst. analysis of
patient treatment procedures: the bpi challenge
case study. first international business process
intelligence challenge, 2011. <http://bpmcenter.org/
wp-content/uploads/reports/2011/bpm-11-
18.pdf>.
[5] w.m.p. van der aalst. process mining: discovery,
conformance and enhancement of business
processes. springer, 2011. isbn-10: 3642193447.39process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesdetection of temporal changes
in business processes
using clustering techniques
1. introduction
in a globalized and hyper-connected world,
organizations need to be constantly
changing in order to adapt to the needs of
their business environment, which implies
that their business processes must also be
constantly changing.
to illustrate this, consider the case of a
toy store whose sales practices may change
radically depending on whether they are
executed over the christmas period or
during vacations, principally due to the
changes in the volume of the demand. for
the christmas season, they might employ
a process that prioritizes efficiency and
volume (throughput), while during the
vacation season they might use a process
which focuses on the quality of their
customer service.
in this example, it is easy to identify the
periods in which demand changes, and thus
it is possible that the process manager (the
person in charge of the process management)
will have a clear understanding of the changes
that the sales process undergoes over time.
however, if an organization has a process
that occurs in various independent offices,
for example, offices that are located in
different geographic locations, the evolution
of the changes in the process at each office
will not be so evident to the central process
manager. moreover, the changes in each
office could be different and could happen
at different moments in time.
understanding the changes that are
occurring in the different offices could
help to better understand how to improve
the overall design of the process. being
able to understand these changes and
model the different versions of the process
allow the process manager to have more
accurate and complete information in order
to make coherent decisions which result in
better service or efficiency.
to achieve the aforementioned goals, various
advances have been made in the discipline
of business process management (bpm), a
discipline which combines knowledge about
information technology and management
techniques, which are then applied to busi-
ness operation processes, with the goal of
improving efficiency [1]. within bpm,
process mining has positioned itself as anemerging discipline, providing a set of tools
that help to analyze and improve business
processes [1], based on analyzing event logs
stored by information systems during the
execution of a process. however, despite the
advances made in this field, there still exists
a great challenge, which consists of
incorporating the fact that processes change
over time, a concept which is known in the
literature as concept drift [2].
depending on the nature of the change, it
is possible to distinguish different types of
concept drift, including: sudden drift
(sudden and significant change to the
definition of the process), gradual drift
(gradual change to the definition of the
process, allowing for the simultaneous
existence of the two definitions), and
incremental drift (the evolution of the
process that occurs through small,
consecutive changes to the definition of
the model).despite the existence of all these drift types,
the existing techniques of process mining
are limited to finding the points in time when
the process changes, centering principally
on sudden drift changes. the problem with
this limitation is that in practice it is not as
frequent for business processes to show a
sudden change in definition.
if we apply the existing process mining
approaches to processes that have different
kinds of changes other than sudden drift,
we could end up with results which make
little sense to the business.
in this document we propose a new approach,
which allows the discovery of the various
versions of a process when there are different
kinds of drift, helping to understand how
the process behaves over time. to accomplish
this task, existing process mining clustering
techniques are used, but with time
incorporated as an additional factor to theabstract: nowadays, organizations need to be constantly evolving in order to adjust to the needs of
their business environment. these changes are reflected in their business processes, for example:
due to seasonal changes, a supermarket’s demand will vary greatly during different months of the
year, which means product supply and/or re-stocking needs will be different during different times of
the year. one way to analyze a process in depth and understand how it is really executed in practice
over time, is on the basis of an analysis of past event logs stored in information systems, known as
process mining. however, currently most of the techniques that exist to analyze and improve processes
assume that process logs are in a steady state, in other words, that the processes do not change over
time, which in practice is quite unrealistic given the dynamic nature of organizations. this document
presents in detail the proposed technique and a set of experiments that reflect how our proposal
delivers better results than existing clustering techniques.
keywords: concept drift, clustering, process mining, temporal dimension.
authors
daniela luengo was born in santiago, chile. she is an industrial civil engineer with a major in information
technology from pontificia universidad católica de chile. she also received the academic degree of
master of science in engineering from the same university. additionally she has an academic certificate
in physical activity, sport, health and education. from 2009 to 2013 she worked at the information
technology research center of the pontificia universidad católica de chile (cetiuc), as an analyst
and consultant in the area of process management excellence, performing process mapping, process
improvement and several researches. her current interests are focused on applying her knowledge in
the public sector of her country.
marcos sepúlveda was born in santiago, chile. he received his ph.d. on computer science from the
pontificia universidad católica de chile in 1995. he made a postdoctoral research in the eth zürich,
switzerland. he is an associate professor in the computer science department at the pontificia univer-
sidad católica de chile since 2001. he is also the director of the information technology research
center of his university (cetiuc). his research interests are process mining, business process modeling,
business intelligence, and information systems management.daniela luengo, marcos
sepúlveda
computer science department, school of
engineering, pontificia universidad católi-
ca de chile, santiago (chile)
<dlluengo@uc.cl>,<marcos@ing.puc.cl>40novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph“
”
control-flow perspective to generate the
different clusters. trace clustering
techniques are used, which unlike other
metrics-based techniques that measure the
distance between complete sequences
having linear complexity, allowing for the
delivery of results in a shorter time span
[3].
the focus of our work contributes to the
process analysis, allowing the process manager
to have a more realistic vision of how the
process behaves over different periods of time.
with this approach it is possible to determine
the different versions of the process, the
characteristics of each process, and to identify
in which moment the changes occur.
this article is organized in the following
way. section 2 presents the related work.
section 3 describes base and the modified
version of the clustering method. section 4
presents experiments and results and finally
the conclusions and future work are presented
in section 5.
2. related work
process mining is a discipline that has
attracted major interest recently. this disci-
pline assumes that the historical information
about a process stored in information
systems can be found in a dataset, known as
an event log [4]. this event log contains past
information about the activities that were
performed in each step of the process, whereeach row of the log is composed of at least
one identifier (id) associated with each indi-
vidual execution of the process, the name of
the activity performed, the timestamp (day
and time when the activity occurred), and,
optionally, additional information like the
person who carried out the activity or other
such information. additionally, in the
literature [3] an ordered list of activities
invoked by a specific execution of the process
is defined as a trace of the execution.
currently, one of the problems in process
mining is that the developed algorithms
assume the existence of information relative
to a unique version of the process in the
event log. however, this is often not the case,
which is why applying the process mining
algorithms to these logs leads to fairly
unrepresentative and/or very complicated
results, which contribute little to the job of
analyzing and improving the processes.
2.1. clustering of the event log
to resolve the aforementioned problems in
process mining, clustering techniques have
been proposed for dividing the event log
before the process mining techniques are
applied [5]. this would consist of dividing
the event log into homogenous clusters; in
order to later apply the process mining
techniques independently to each cluster
and thus obtaining more representative
models. figure 1 shows the stage of log
preprocessing and then uses a discoverytechnique as an example of a process mining
technique.
to produce this clustering, it is necessary to
define a way of representing the traces, so
that it becomes possible to group them later
according to previously determined criteria
of similarity. there currently exist various
clustering techniques in process mining
[6][3]. the majority of them principally
consider information about the control-flow
of activities. these techniques can be
classified into two categories:
1) techniques that transform the traces into a
vector space, in which each trace is converted
into a vector. the dividing of the log can be
done using a variety of clustering techniques in
the vector space, like for example: bag of
activities, k-gram model [6], and trace
clustering [5]. however, these techniques have
the problem of lacking contextual information,
which some have attempted to correct with the
trace clustering based on conserved patterns
technique [3].
2) techniques that operate with the whole
trace. these techniques use metrics of
distance like levenshtein and generic edit
distance [6], together with standard
clustering techniques, assigning a cost to the
difference between traces.
the existing techniques for both categories,
despite improving clustering through the
creation of structurally similar trace clusters,
 
discovery
eventlog clusteringclustera
clusterbmodela
modelb
preprocessingstageof theeventlog
discovery
figure 1. preprocessing stage of the event log.currently, one of the problems in process mining is that
the developed algorithms assume the existence of information relative
to a unique version of the process in the event log41process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articles“
”
do not consider the temporal dimension of
the process’s execution, nor how the process
changes over time.
2.2. the concept drift challenge
in bpm, concept drift refers to a situation in
which a process has experienced changes in its
design within an analyzed period (yet the exact
moment in which the changes were produced
is unknown). these changes can be due to a
variety of factors, but are mainly due to the
dynamic nature of the processes [7].
the study of concept drift in the area of
process mining has centered on process
changes in the control-flow perspective, and
can be of two kinds, momentary changes or
permanent changes, depending on the
duration of the change.
when changes occur over short and
infrequent periods, they are considered
momentary changes. these changes are also
known in processes jargon as process noise
or process anomalies.
on the other hand, permanent changes occur
over more prolonged periods of time and/or
when a considerable amount of instances is
affected by the changes, which signals
changes in the design process.
our interest centers on the permanent changes
in the control-flow perspective, which can be
divided into the following four categories:
sudden drift: this refers to drastic
changes, meaning the way in which the
process execution changes suddenly from
one moment to the next.
recurring drift: when a process suffers
periodic changes, meaning a way of performing
the process is repeated again later.
gradual drift: this refers to changes which
are not drastic, but rather at a moment when
two versions of the process overlap, which
corresponds to the transition from one version
of the process to another.incremental drift: this is when a process
has small incremental changes. these types
of changes are more frequent in organizations
that adopt agile bpm methodologies.
to solve the concept drift problem, new
approaches have evolved to analyze the
dynamic nature of the processes.
bose [2] proposes methods to manage
concept drift by showing how the process
changes are indirectly reflected in the event log
and that the detection of these changes is
feasible by examining the relationship between
activities. different metrics have been defined
to measure the relationship between activities.
based on these metrics, a statistical method
was proposed whose basic idea is to consider
a successive series of values and investigate if
a significant difference between two series
exists. if it does, this would correspond to a
process change.
stocker [8] also proposes a method to manage
concept drift which considers the distances
between pairs of activities of different traces as
a structural feature, in order to generate
chronologically subsequent clusters.
bose and stocker’s approaches are limited
to determining the moment in time when the
process changes, and thus center on sudden
changes and leave out other types of changes.
to resolve this, in an earlier article [9] we
proposed an approach that makes use of
clustering techniques to discover the changes
that a process can experience over time, but
without limiting ourselves to one particular
kind of change. in that approach, the
similarity among two traces is defined by the
control-flow information and by the moment
in which each trace begins to operate.
in this article, we present an extension of the
earlier work [9], after incorporating a new form
of measuring the distance between two traces.3. extending clustering tech-
niques to incorporate the tem-
poral variable
as was explained in the last section, the
existing approaches for dealing with concept
drift are not sufficiently effective at finding
the versions of a process when the process
has undergone different types of changes.
to solve this problem, we look to the trace
clustering technique proposed by bose [3]
and based on conserved patterns, which
allows clustering the event log considering
each trace’s sequence of activities.
our work is based on this technique and
extends it by incorporating an additional
temporal variable to the other control-flow
variables used for clustering.
3.1. trace clustering based on
conserved patterns
the basic idea proposed by bose [3] is to
consider subsequences of activities that
repeat in multiple traces as feature sets for
the implementation of clustering. unlike
the k-gram approach that considers
subsequences of fixed size, in this approach
the subsequences can be of different
lengths. when two instances have a
significant number of subsequences in
common, it is assumed that they have
structural similarity and these instances
are assigned to the same cluster.
there are six types of subsequences,
therefore, we will only give a formal definition
of mr, since these are the subsequences
that we used to develop our approach,
however the work could be extended to use
the other subsequences.
maximal repeat (mr): a maximal
repeat in a sequence t is defined as a
subsequence that occurs in a maximal pair
in t. intuitively, an mr corresponds to a
subsequence of activities that is repeated
more than once in the log.
table 1 shows an example where existing mr
in a sequence are determined. this technique
constructs a unique sequence starting from the
event log, which is obtained by connecting all
the traces, but with a delimiter placed among
them. then, the mr definition is applied to
this unique sequence. the set of all mr
discovered in the sequence with more than one
activity, is called a feature set.the study of concept drift in the area of process mining
has centered on process changes in the control-flow perspective,
and can be of two kinds, momentary changes or
permanent changes, depending on the duration of the change
sequence maximal repeat feature set 
bbbcd-bbbc-caa {a, b, c, bb, bbbc} {bb, bbbc} 
table 1. example of maximal repeat and feature set.42novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph“
”
 structural 
features
timebc1bc
ccac
ccacstructural 
featuresbased on the feature set, a matrix is created
that allows the calculation of the distance
between the different traces. each row of
the matrix corresponds to a trace and each
column corresponds to a feature of the
feature set. the values of the matrix
correspond to the number of times that each
feature is found in the various traces (see
table 2). we will call this matrix the
structural features matrix.
this pattern-based clustering approach uses
"agglomerative hierarchical clustering" as
a clustering technique, with the minimum
variance criterion [15], and using the
euclidian distance to measure the difference
between two traces, defined in as follows:

n
1i2
bi ai )t-(t =b) (a, dist 
where
dist (a, b) = distance between trace a and
trace b.
n = number of features in de feature set.
tai  = number of times de feature in the
appears in the trace a.
3.2. clustering technique to
include the temporal variable
to identify the various types of changes that
can occur in business processes we must
find a way to identify all the versions of a
process. if we only look at the structural
features (control-flow) then we leave out
information regarding temporality. both tem-poral and structural features are very important
since the structure indicates how similar one
instance is to another and the temporality
shows how close in time the two instances are.
our approach looks to identify the different
forms of implementing the process using both
features (structural and temporal) at the same
time, as is illustrated in figure 2.
in order to mitigate the effects of external
factors that are difficult to control, we use
only the beginning of each process instance
as the temporal variable.
for each trace, we store the time that have
elapsed since a reference timestamp in the
time dimension, for example, the number of
days (or hours, minutes or seconds, depending
on the process) elapsed since january 1st,
1970, to the timestamp in which the trace’s
first activity begins (see table 3).
in this new approach, "agglomerative
hierarchical clustering" with the minimum
variance criterion [15] is also used as a
clustering technique.
to calculate the distance between two tra-
ces we use the euclidian distance, but
modified in order to consider at the same
time the structural and temporal features.
first, we define jit as the feature i of the
trace j. if the feature i cannot be found in the
trace j, its value will be 0, otherwise its value
will be the number of times that the featurei appears in the trace j.
)1 (njt  corresponds to the temporal feature
of the trace j and its value is the number of
days (or hours, minutes or seconds,
depending on the process) that have
elapsed since a reference timestamp. the
index (n+1) is given to indicate that it is to
be added to the structural features.
we define l as the set of all the log traces,
and the expression )(ji ljt max  represents
the largest number of times the feature i
appears in an event log trace. in the same
way, )(ji ljt min  corresponds to the
smallest number of times the feature i appears
in an event log trace.
) ()1(  nj ljt min  and ) ()1(  nj ljt max
correspond to the earliest and latest time in
which an event log trace begin. also we
define ),(ba de and ),(badt as the
structural and temporal distance between
the trace a and the trace b, respectively.the basic idea proposed by bose is to consider
subsequences of activities that repeat in multiple traces
as feature sets for the implementation of clustering
figure 2. example of the relevance of considering time in the analysis.trace \ feature set bb bbbc 
bbbcd 2 1 
bbbc 2 1 
caa 0 0 
 table 2. structural features matrix.
where:
n = number of features from the structural features set.
even though both distances, ed and td are
normalized, since the domain of ed is greater
than the one of td, we define mine , maxe ,
mint and maxt as:
 



n
i jiljjiljbi ai
et tt tbad
12
)(min )(max),(  
2
)1( )1()1( )1(
) (min ) (max),(




 
njljnjljnb na
tt tt tbad  
43process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesbase approach: trace clustering based
on conserved patterns.
our approach: extended trace clustering,
where the temporal dimension is incorporated.
in this technique, the weight of the temporal
dimension,, can be given different values,
which vary between 0 and 1.
3) for each of the clusters a discovery process
is carried out, generating two new models
(b a m and m ).
the approach’s performance can be
measured at two points:
a)conformance 1: the metrics are measured
between any of the original models and one of
the generated clusters.
b)conformance 2: metrics are measured
between any of the original models and one of
the generated models.
the metrics used to measure conformance
1 are the following:
accuracy: indicates the number of
instances correctly classified in each clus-
ter, according to what is known about the
original events log. these values are between
0% and 100%, where 100% indicates that the
clustering was exact.
fitness: indicates how much of the
observed behavior in an event log, (for
example, cluster ac) is captured by the
original process model (for example, model
2m) [12]. these values are between 0 and 1,
where 1 means the model is capable of
representing all the log traces.
precision: quantifies whether the ori-
ginal model allows for behavior completely
unrelated to what is seen in the event log.
these values are between 0 and 1, where 1
means that the model does not allow
behavior additional to what the traces
indicate [13].
the metrics used to measure conformance
2 are the following [14]:
behavioral precision (pb)
structural precision (ps)
behavioral recall (rb)
structural recall (rs)
these metrics quantify the precision and
generality of one model with respect to
another. the values of these four metrics
are between 0 and 1, where 1 is the best
possible value.
table 4 summarizes the results of applying
the base approach and the new approach
(varying the value of the parameter µ), to the
different synthetic event logs created. this
table shows the accuracy metric, which
indicates the percentage of correctly
classified traces.for each log, the highest accuracy is reached
with our approach, but with different µ values
(varying between 0.2 and 0.9). the accuracy
varies with different µ values because in
each log the relevance of the temporal
dimension versus the structure of the process
is not the same.
we use log (f) to make a more in-depth
analysis of the results, measuring all the
metrics defined both in conformance 1 and
conformance 2 (see table 5).
all metrics calculated for log (f) show good
results when the parameter µ has a value of
0.5 or 0.6. when the five metrics are averaged,
the highest overall average is obtained with
µ equal to 0.5.
5. conclusions and future work
in this article we present the limitations of
current clustering techniques for process
mining, which center on grouping structurally
similar executions of a process. by focusing
just on the structure of the executions, the
process’s evolution over time (concept
drift) is left out. new techniques have been
developed to address this, but these also
present limitations since they focus on
finding the points in which the process
changes, which is limited to just one kind of
change, sudden drift.
we present an approach that extends
current clustering techniques in order to
find the different versions of a process
that changes overt time (in multiple ways,
i.e., different types of concept drifts),
allowing for a better understanding of the
variations that occur in the process and
how, in practice, it is truly being performed
over time.
our work focuses on the identification of
models associated with each version of the
process. the technique we propose is a tool
that helps business managers to make
decisions. for example, it can help determi-
ne if the changes produced in the implement-
ation of the process are really those that are
expected, and based on this, take the proper
action if they discover abnormal behaviors.
also, by understanding and comparing the
different versions of a process, good and bad
practices can be identified, which are highly
useful when the time comes to improve or
standardize the process.
in this document we present a set of metrics to
measure the performance of our approach, i.e.,
whether the approach is able to cluster data in
the same way the data was created. thus, these
metrics require a priori process information,
which is not feasible for real cases.
each metric measures a different aspect,
which when used together, allow for a multi-mine and emax correspond to the minimum
and maximum (normalized) distance between
all traces, considering only structural features.
mint   and maxt  correspond to the minimum
and maximum (normalized) distance between
all traces, considering only temporal features.
the new way of measuring the distance between
two traces, ),(ba dist , incorporates the
parameter µ, which we will call the weight of
the temporal dimension, which serves to weigh
the structural and temporal features.
additionally, this new way of measuring the
distance adjusts ed and td in such a way that
the weight of both distances are equivalent.
 
e ee e
min maxmin badba dist ),() 1(),( 
the weight of the temporal dimension, ,
can have values between 0 and 1, according
to the relevance given to the temporal
feature.
4. evaluation
we analyzed the proposed technique using
six event logs obtained from different
synthetic processes, which were created with
cpn tools [10][11]. in order to measure
their performance we used the guide tree
miner plug-in [3] available in prom 6.1 as
well as a modified version of this plug-in that
incorporates the proposed changes.
the evaluation was carried out using different
metrics to measure the new approach’s
classification effectiveness versus the base
approach.
4.1. experiments and results
figure 3 shows the sequence of steps
performed in the experiments.
1) to create the synthetic log, a simulation
was used based on two designed models, m1
and m2, using cpn tools.
2) the method starts by applying the clustering
technique on the synthetic log received, which
generates a given number of clusters. in this
case, two clusters (b a c and c ). two
clustering techniques are applied:t tt t
min maxmin bad
),(
44novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monograph
figure 3. steps for performing the experimental tests.
table 4. accuracy metric calculated for the six synthetic test logs.approach µ log (a) log (b) log (c) log (d) log (e) log (f) 
base - 57% 38% 55% 86% 99% 53% 
0.0 59% 52% 49% 82% 59% 51% 
0.1 65% 52% 49% 82% 59% 68% 
0.2 87% 52% 63% 100% 59% 64% 
0.3 87% 52% 63% 100% 59% 62% 
0.4 95% 52% 63% 100% 59% 63% 
0.5 100% 52% 63% 100% 59% 95% 
0.6 100% 52% 73% 100% 100% 94% 
0.7 96% 89% 73% 100% 100% 67% 
0.8 78% 88% 73% 74% 84% 55% 
0.9 79% 77% 77% 68% 77% 78% new 
1.0 81% 78% 68% 73% 72% 55% 
 45process mining monograph
monograph novática special english edition - 2013/2014 annual selection of articlesfaceted vision that makes the analysis more
complete.
one key aspect of our clustering approach is
the value it is given to the weight of the
temporal dimension, parameter µ, which is
closely related to the nature of the process.
high µ values give a greater importance to
time when carrying out the clustering,
whereas low µ values give more importance
to the structural features of the process.
the experiments results show that the
approach proposed in this document has a
better performance and that exists at least a
value for the parameter µ that gives better
results in comparison to only using the
structural clustering technique (trace
clustering based on patterns). this is
achieved because our approach is capable of
grouping the log traces in such a way so as to
identify structural similarity and temporal
proximity at the same time.
one of the metrics used is accuracy. in some
experiments, this metric reached 100%,
meaning all traces were classified correctly.
when a 100% accuracy was not reached, itwas because there are processes whose
different versions are similar amongst
themselves, and therefore there are traces
that can correspond to more than one version
of the process, which makes the classification
not exactly the same as what is expected.
our future work in this line of investigation
is to test the new approach with real
processes. we also want to work on
developing the existing algorithms so that
they are capable of automatically determining
the optimal number of clusters. in order to
do so, it will be necessary to define new
metrics that will allow us to calculate the
optimal number of clusters without a priori
information of the process versions.table 5. different metrics for analyzing log (f)approach µ accuracy fitness precision average 
 average 
 overall 
average 
base - 53% 0.93 0.78 0.77 0.81 0.76 
0.0 51% 0.93 0.73 0.73 0.70 0.72 
0.1 68% 0.93 0.81 0.86 0.86 0.83 
0.2 64% 0.92 0.80 0.83 0.82 0.80 
0.3 62% 0.92 0.80 0.80 0.78 0.78 
0.4 63% 0.92 0.79 0.83 0.86 0.81 
0.5 95% 0. 95 0.85 0.97 0.96 0. 94 
0.6 94% 0.95 0.86 0.95 0.94 0.93 
0.7 67% 0.92 0.84 0.84 0.89 0.83 
0.8 55% 0.94 0.87 0.88 0.94 0.84 
0.9 78% 0.94 0.81 0.89 0.95 0.87 new 
1,0 55% 0.93 0.87 0.88 0.95 0.84 
 46novática special english edition - 2013/2014 annual selection of articlesmonograph process mining
monographreferences
[1] w. van der aalst. process mining, discovery,
conformance and enhancement of business processes.
springer, 2011. isbn 978-3-642-19345-3.
[2] r.j. bose, w. van der aalst, i. zliobaite, m.
pechenizkiy. handling concept drift in process
mining. 23rd international conference on advanced
information systems engineering. london, 2011.
[3] r. bose, w. van der aalst. trace clustering
based on conserved patterns : towards achieving
better process models. business process
management workshops, pp. 170-181, 2010. berlin:
springer heidelberg.
[4] w. van der aalst, b. van dongen, j. herbst, l.
maruster, g. schimm, a. weijters. data & knowledge
engineering, pp. 237-267, 2003.
[5] m. song, c. günther, w. van der aalst. trace
clustering in process mining. 4th workshop on bu-
siness process intelligence (bpi 08), pp. 109-120.
milano, 2009.
[6] r. bose, w. van der aalst. context aware trace
clustering: towards improving process mining
results. siam, pp. 401-412, 2009.
[7] w. van der aalst, a. adriansyah, a.k. alves de
medeiros, f. arcieri, t. baier, t. blickle et al. process
mining manifesto, 2011.
[8] t. stocker. time-based trace clustering for
evolution-aware security audits. proceedings of the
bpm workshop on workflow security audit and
certification, pp. 471-476. clermont-ferrand, 2011.
[9] d. luengo, m. sepúlveda. applying clustering in
process mining to find different versions of a business
process that changes over time. lecture notes in
business information processing, pp. 153-158, 2011.
[10] a.v. ratzer, l. wells, h.m. lassen, m. laursen,
j. frank, m.s. stissing et al. cpn tools for editing,
simulating, and analysing coloured petri nets.
proceedings of the 24th international conference on
applications and theory of petri nets pp. 450-462.
eindhoven: springer-verlag, 2003.
[11] a.k. alves de medeiros, c. günther. process
mining: using cpn tools to create test logs for
mining algorithms. proceedings of the sixth workshop
and tutorial on practical use of coloured petri nets
and the cpn tools, pp. 177–190, 2005.
[12] a. rozinat, w. van der aalst. conformance
testing: measuring the fit and appropriateness of
event logs and process models. business process
management workshops, pp. 163-176, 2006.
[13] j. muñoz-gama, j. carmona. a fresh look at
precision in process conformance. proceeding
bpm’10, proceedings of the 8th international
conference on business process management, pp.
211-226, 2010.
[14] a. rozinat, a.k. alves de medeiros, c. günther,
a. weijters, w. van der aalst. towards an evaluation
framework for process mining algorithms. genetics,
2007.
[15] j. ward. hierarchical grouping to optimize an
objective function. journal of the american
statistical association, pp. 236-244, 1963. 
 clei  --- www. clei. org  --- es una  asociación  sin fines de lucro  con casi 40 años de existencia, y 
que reúne a más de 100 universidades, centros de investigación y asociaciones de profesionales 
latinoamericanas,  y algunas extra -territoriales (españolas y estadounidenses ) interesadas en 
promover la investigación y doce ncia en informática .   
clei  invita anualmente   a presentar trabajos que reporten resultados de investigación y/o 
experiencia  originales  durante su conferencia latinoamericana en  informática (clei) . en l a 
conferencia , que se realiza típicamente en o ctubre, participan aprox. 1000 personas, y se recibe n 
para sus s imposios ,  más de 500 artículos en castellano , inglés y portugués, de los cuales el 33% 
son aceptados.    
a partir del 2012, las memorias de la clei son publicadas en ieee xplore .  además, números 
especiales de l clei electronic journal  son dedicad os a trabajos seleccionados de esta 
conferencia . 
simposios clei  
 simposio latinoamericano de ingeni ería del software  
 simposio latinoamericano de informática y sociedad  
 simposio latinoamericano de  investigación  de operaciones e inteligencia 
artificial  
 simposio latinoamericano de infraestructura, hardware y software  
 simposio latinoamericano de sistemas innovadores de datos  
 simposio latinoamericano de teoría computacional  
 simposio latinoamericano de computación gráfica, realidad virtual y 
procesamiento de imágenes  
 simposio latinoamericano de sistemas de información de gran escala  
eventos asociados  (realizados en paralelo anual o bienalmente )  
 congreso iberoamericano de e ducación sup erior en computación (ciesc ) 
 concurso latinoamericano  de tesis de maestría (cltm ) 
 congreso de la mujer latinoamerica na en la computación (lawcc ) 
 simposio de historia de la  informática en américa latina y el. caribe  (shialc)  
 latin america networking conference (lanc)  
 workshop en nomenclatura y acreditación en programas de computación   
 
