123
international journal on software
tools for technology transfer
 
issn 1433-2779
 
int j softw tools technol transfer
doi 10.1007/s10009-015-0399-5scientific workflows for process
mining: building blocks, scenarios, and
implementation
alfredo bolt, massimiliano de leoni &
wil m. p. van der aalst
123
your article is published under the creative
commons attribution license which allows
users to read, copy, distribute and make
derivative works, as long as the author of
the original work is cited. you may self-
archive this article on your own website, an
institutional repository or funder’s repository
and make it publicly available immediately.int j softw tools technol transfer
doi 10.1007/s10009-015-0399-5
sw
scientiﬁc workﬂows for process mining: building blocks,
scenarios, and implementation
alfredo bolt1·massimiliano de leoni1·wil m. p. van der aalst1
© the author(s) 2015. this article is published with open access at springerlink.com
abstract over the past decade process mining has emerged
as a new analytical discipline able to answer a variety of
questions based on event data. event logs have a very par-
ticular structure; events have timestamps, refer to activities
and resources, and need to be correlated to form process
instances. process mining results tend to be very different
from classical data mining results, e.g., process discovery
may yield end-to-end process models capturing different per-
spectives rather than decision trees or frequent patterns. a
process-mining tool like prom provides hundreds of differ-
ent process mining techniques ranging from discovery and
conformance checking to ﬁltering and prediction. typically, a
combination of techniques is needed and, for every step, there
are different techniques that may be very sensitive to para-
meter settings. moreover, event logs may be huge and may
need to be decomposed and distributed for analysis. these
aspects make it very cumbersome to analyze event logs man-
ually. process mining should be repeatable and automated .
therefore, we propose a framework to support the analysis of
process mining workﬂows. existing scientiﬁc workﬂow sys-
tems and data mining tools are not tailored towards process
mining and the artifacts used for analysis (process models
and event logs). this paper structures the basic building
blocks needed for process mining and describes various
analysis scenarios. based on these requirements we imple-
mented rapidprom , a tool supporting scientiﬁc workﬂows
for process mining. examples illustrating the different sce-
narios are provided to show the feasibility of the approach.
b wil m. p . van der aalst
w.m.p.v.d.aalst@tue.nl
1department of mathematics and computer science,
eindhoven university of technology, eindhoven,
the netherlandskeywords scientiﬁc workﬂows ·process mining ·large
scale process analysis ·rapidprom
1 introduction
scientiﬁc workﬂow management (swfm) systems help
users to design, compose, execute, archive, and share work-
ﬂows that represent some type of analysis or experiment.
scientiﬁc workﬂows are often represented as directed graphs
where the nodes represent “work” and the edges repre-
sent paths along which data and results can ﬂow between
nodes. next to “classical” swfm systems such as taverna
[23], kepler [ 33], galaxy [ 20], clowdflows [ 27], and jabc
[40], one can also see the uptake of integrated environments
for data mining, predictive analytics, business analytics,
machine learning, text mining, reporting, etc. notable exam-
ples are rapidminer [ 22] and knime [ 4]. these can be
viewed as swfm systems tailored towards the needs of data
scientists.
traditional data-driven analysis techniques do not con-
sider end-to-end processes. people are process models by
hand [e.g., petri nets, uml activity diagrams, or business
process modeling notation (bpmn) models], but this mod-
eled behavior is seldom aligned with real-life event data.
process mining aims to bridge this gap by connecting end-to-
end process models to the raw events that have been recorded.
process-mining techniques enable the analysis of a wide
variety of processes using event data. for example, event
logs can be used to automatically learn a process model
(e.g., a petri net or bpmn model). next to the automated
discovery of the real underlying process, there are process-
mining techniques to analyze bottlenecks, to uncover hidden
inefﬁciencies, to check compliance, to explain deviations,
to predict performance, and to guide users towards “better”
123a. bolt et al.
processes. hundreds of process-mining techniques are avail-
able and their value has been proven in many case studies. seefor example the twenty case studies on the webpage of the
ieee task force on process mining [ 24]. the open source
process mining framework prom [58] provides hundreds of
plug-ins and has been downloaded over 100,000 times. thegrowing number of commercial process mining tools (disco,
perceptive process mining, celonis process mining, qpr
processanalyzer, software ag/aris ppm, fujitsu inter-stage automated process discovery, etc.) further illustrates
the uptake of process mining.
for process mining typically many analysis steps need
to be chained together. existing process mining tools do not
support such analysis workﬂows. as a result, analysis may be
tedious and it is easy to make errors. repeatability and prove-nance are jeopardized by manually executing more involvedprocess mining workﬂows.
this paper is motivated by the observation that tool sup-
port for process mining workﬂows is missing. none of theprocess mining tools (prom, disco, perceptive, celonis,
qpr, etc.) provides a facility to design and execute analysis
workﬂows. none of the scientiﬁc workﬂow management sys-tems including analytics suites like rapidminer and knime
support process mining. yet, process models and event logsare very different from the artifacts typically considered.
therefore, we propose the framework to support process min-
ing workﬂows depicted in fig. 1.
this paper considers four analysis scenarios where
process mining workﬂows are essential:
–result (sub-)optimality often different process mining
techniques can be applied and a priori it is not clearwhich one is most suitable. by modeling the analysisworkﬂow, one can just perform all candidate techniques
on the data, evaluate the different analysis results, and
pick the result with the highest quality (e.g., the processmodel best describing the observed behavior).
–parameter sensitivity different parameter settings and
alternative ways of ﬁltering can have unexpected effects.therefore, it is important to see how sensitive the results
are (e.g., leaving out some data or changing a parameter
setting a bit should not change the results dramatically).it is important to not simply show the analysis result with-out having some conﬁdence indications.
–large-scale experiments each year new process mining
techniques become available and larger data sets need tobe tackled. for example, novel discovery techniques need
to be evaluated through massive testing and larger event
fig. 1 overview of the
framework to support processmining workﬂows
analysis scenarios for process mining
result (sub-)
optimalityparameter sensitivitylarge-scale
experimentsrepeating questions
categories of building blocks
event data extraction
import event data
(imported)
generate event data
from model
(genered)event data
transformation
add data to event
data (added)
filter event data
(filtered)
split event data
(splited)
merge event data
(merged)process model
extraction
import process
model (importm)
discover process
model from event
data (discm)
select process
model form
collection
(selectm)process model and
event data analysis
analyze process
model (analyzem)
evaluate process
model using event
data (evaluam)
compare process
models
(comparem)
analyze event data
(analyzeed)
generate report(generr)process model
transformations
repair process
model (repairm)
decompose process
model (decompm)
merge process
models (mergem)process model
enhancement
enrich process
model using event
data (enrichm)
improve process
model (improvem)
implementation
rapidprom
prom rapidminer
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
logs need to be decomposed to make analysis feasible.
without automated workﬂow support, these experimentsare tedious, error-prone, and time consuming.
–repeating questions it is important to lower the threshold
for process mining to let non-expert users approach it.questions are often repetitive, e.g., the same analysis isdone for a different period or a different group of cases.
process mining workﬂows facilitate recurring forms of
analysis.
a ss h o w ni nf i g . 1these scenarios build on process mining
building blocks grouped into six categories:
–event data extraction building blocks to extract data
from systems or to create synthetic data.
–event data transformation building blocks to pre-
process data (e.g., splitting, merging, ﬁltering, and
enriching) before analysis.
–process model extraction building blocks to obtain
process models, e.g., through discovery or selection.
–process model and event analysis building blocks to
evaluate event logs and models, e.g., to check the internalconsistency or to check conformance with respect to an
event log.
–process model transformations building blocks to repair,
merge or decompose process models.
–process model enhancement building blocks to enrich
event logs with additional perspectives or to suggestprocess improvements.
building blocks can be chained together to support spe-
ciﬁc analysis scenarios. the suggested approach has beenimplemented thereby building on the process mining frame-
work prom and the workﬂow and data mining capabilities of
rapidminer . the resulting tool is called, rapidprom , which
supports process mining workﬂows. prom was selected
because it is open source and there is no other tool that sup-
ports as many process mining building blocks. rapidminerwas selected because it allows for extensions that can be
offered through a marketplace. rapidprom is also offered
as such an extension and the infrastructure allows us to mixprocess mining with traditional data mining approaches, textmining, reporting, and machine learning. overall, rapid-
prom offers comprehensive support for any type of analysis
involving event data and processes.
the remainder of this paper is organized as follows: sec-
tion 2discusses related work and positions our framework.
an initial set of process-mining building blocks is describedin sect. 3. these building blocks support the four analysis
scenarios described in sect. 4. the rapidprom implementa-
tion is presented in sect. 5. section 6evaluates the approach
by showing concrete examples. finally, sect. 7concludes the
paper.2 related work
over the past decade, process mining has emerged as a new
scientiﬁc discipline at the interface between process models
and event data [ 45]. conventional business process man-
agement (bpm) [ 46,63] and workﬂow management (wfm)
[31,51] approaches and tools are mostly model-driven with
little consideration for event data. data mining (dm) [ 21],
business intelligence (bi), and machine learning (ml) [ 35]
focus on data without considering end-to-end process mod-
els. process mining aims to bridge the gap between bpm
and wfm on the one hand and dm, bi, and ml on theother hand. a wealth of process discovery [29,53,62] and
conformance checking [1,2,48] techniques has become avail-
able. for example, the process mining framework prom [ 58]
provides hundreds of plug-ins supporting different types ofprocess mining ( http://www.processmining.org ).
this paper takes a different perspective on the gap between
analytics and bpm/wfm. we propose to use workﬂow tech-nology for process mining rather than the other way around.
to this end, we focus on particular kinds of scientiﬁc work-
ﬂows composed of process mining operators.
differences between scientiﬁc and business workﬂows
have been discussed in several papers [ 3]. despite uniﬁcation
attempts (e.g., [ 38]) both domains have remained quite dis-
parate due to differences in functional requirements, selectedpriorities, and disjoint communities.
obviously, the work reported in this paper is closer to sci-
entiﬁc workﬂows than business workﬂows (i.e., traditionalbpm/wfm from the business domain). numerous scientiﬁc
workﬂow management (swfm) systems have been devel-
oped. examples include taverna [ 23], kepler [ 33], galaxy
[20], clowdflows [ 27], jabc [ 40], vistrails, pegasus, swift,
e-bioflow, view, and many others. some of the swfm
systems (e.g., kepler and galaxy) also provide repositoriesof models. the website http://www.myexperiment.org lists
over 3500 workﬂows shared by its members [ 19]. the diver-
sity of the different approaches illustrates that the ﬁeld is
evolving in many different ways. we refer to the book [ 41]
for an extensive introduction to swfm.
an approach to mine process models for scientiﬁc work-
ﬂows (including data and control dependencies) was pre-sented in [ 65]. this approach uses “process mining for
scientiﬁc workﬂows” rather than applying scientiﬁc work-
ﬂow technology to process mining. the results in [ 65] can
be used to recommend scientiﬁc workﬂow compositionsbased on actual usage. to our knowledge, rapidprom
is the
only approach supporting “scientiﬁc workﬂows for process
mining”. the demo paper [ 34] reported on the ﬁrst imple-
mentation. in the meantime, rapidprom has been refactored
based on various practical experiences.
123a. bolt et al.
there are many approaches that aim to analyze reposi-
tories of scientiﬁc workﬂows. in [ 64], the authors provide
an extensible process library for analyzing jabc workﬂows
empirically. in [ 14] graph clustering is used to discover sub-
workﬂows from a repository of workﬂows. other analysisapproaches include [ 16,32], and [ 61].
scientiﬁc workﬂows have been developed and adopted in
various disciplines, including physics, astronomy, bioinfor-
matics, neuroscience, earth science, economics, health, andsocial sciences. v arious collections of reusable workﬂows
have been proposed for all of these disciplines. for example,
in [42] the authors describe workﬂows for quantitative data
analysis in the social sciences.
the boundary between data analytics tools and scientiﬁc
workﬂow management systems is not well-deﬁned. toolslike rapidminer [ 22] and knime [ 4] provide graphical
workﬂow modeling and execution capabilities. even the
scripting in r [ 25] can be viewed as primitive workﬂow sup-
port. in this paper we build on rapidminer as it allows us tomix process mining with data mining and other types of ana-
lytics. earlier we developed extensions of prom for chaining
process mining plug-ins together, but these were merely pro-totypes. we also realized a prototype using an integration
between knime and prom. however, for reasons of usabil-
ity, we opted for rapidminer as a platform to expose processmining capabilities.
3 deﬁnition of the process-mining building blocks
to create scientiﬁc workﬂows for process mining we need todeﬁne the building blocks, which are, then, connected witheach other to create meaningful analysis scenarios. this sec-tion discusses a taxonomy and a repertoire of such building
blocks inspired by the so-called “bpm use cases”, which
were presented in [ 46]. the process-mining building blocks
(pmbb) are characterized by two main aspects. first, they
areabstract as they are not linked to any speciﬁc technique
or algorithm. second, they represent logical units of work,i.e., they cannot be conceptually split while maintaining their
generality. this does not imply that concrete techniques that
implement process-mining building blocks cannot be com-posed by micro-steps, according to the implementation anddesign that was used.
the process-mining building blocks can be chained, thus
producing process-mining scientiﬁc workﬂows to answer avariety of process-mining questions.
each process-mining building block takes a number of
inputs and produces certain outputs. the input elements rep-resent the set (or sets) of abstract objects required to perform
the operation. the process-mining building block compo-
nent represents the logical unit of work needed to processthe inputs and produce the outputs. inputs and outputs areindicated through circles, whereas a process-mining building
block is represented by a rectangle. arcs are used to connectthe blocks to the inputs and outputs. a generic example of a
building block interacting with inputs and outputs is shown
in fig. 2.
two process-mining building blocks aandbare chained
if one or more outputs of aare used as an inputs in b.a s
mentioned, inputs and outputs are depicted by circles. the
letter inside a circle speciﬁes the type of the input or output.the following types of inputs and outputs are considered in
this paper:
–process models , which are a representation of the behav-
ior of a process, are represented by letter “m” .h e r ew e
abstract from the notation used, e.g., petri nets, heuristicsnest, bpmn models are concrete representation lan-
guages.
–event data sets , which contain the recording of the
execution of process instances within the informationsystem(s), regardless of the format. they are represented
by letter “e” . xes is currently the de-facto standard for-
mat to store events.
1
–information systems , which supports the performance of
processes at runtime. they are represented by the label
“s”. information systems may generate events used for
analysis and process mining results (e.g., prediction) may
inﬂuence the information system.
–sets of parameters to conﬁgure the application of
process-mining building blocks (e.g., thresholds, wei-ghts, ratios, etc.). they are represented by letter “p” .
–results that are generated as outputs of a process-mining
building blocks. this can be as simple as a number ormore complex structures like a detailed report. in princi-
ple, the types enumerated above in this list (e.g., process
models) can also be results. however, it is worth differen-tiating those speciﬁc types of outputs from results which
are not process mining speciﬁc (like a bar chart). results
are represented by letter “r” .
–additional data sets that can be used as input for certain
process-mining building blocks. these are represented
by the letter “d” . such an additional data set can be used
to complement event data with context information (e.g.,one can use weather or stock-market data to augment the
event log with additional data).
the remainder of this section provides a taxonomy of
process-mining building blocks grouped into six different
categories. for each category, several building blocks areprovided. they were selected because of their usefulness
1xes (extensible event stream) is an xml-based standard for event
logs http://www.xes-standard.org . it provides a standard format for the
interchange of event log data between tools and application domains.
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
fig. 2 generic example of a
building block transforming a
process model (m) and event
data (e) into process analyticsresults (r) and an annotatedprocess model
for the deﬁnition of many process-mining scientiﬁc work-
ﬂows. the taxonomy is not intended to be exhaustive; there
will be new process-mining building blocks as the discipline
evolves. section 5discusses how these building blocks can be
implemented into concrete operators and provides examplesof these operators implemented in rapidprom.
3.1 event data extraction
event data are the cornerstone of process mining. in order
to be used for analysis, event data has to be extracted andmade available. all of the process-mining building blocks of
this category can extract event data from different sources.
figure 3shows some process-mining building blocks that
belong to this category.
fig. 3 process-mining building blocks related to event data extractionimport event data (imported) information systems store
event data in different format and media, from ﬁles in a hard
drive to databases in the cloud. this building block represents
the functionality of extracting event data from any of thesesources. some parameters can be set to drive the event-dataextraction. for example, event data can be extracted from
ﬁles in standard formats, such as xes, or from transactional
databases.
generate event data from model (genered) in a number of
cases, one wants to assess whether a certain technique returns
the expected or desired output (i.e., synthetic event data). for
this assessment, controlled experiments are necessary whereinput data are generated in a way that the expected output of
the technique is clearly known. given a process model m,
this building block represents the functionality of generatingevent data that record the possible execution of instancesofm. this is an important function for, e.g., testing a new
discovery technique. v arious simulators have been developed
to support the generation of event data.
3.2 event data transformation
sometimes, event data sets are not sufﬁciently rich to enable
certain process-mining analyses. in addition, certain data-set portions should be excluded, because they are irrelevant,
123a. bolt et al.
fig. 4 process-mining building blocks related to event data transfor-
mations
out of the scope of the analysis or, even, noise. therefore, a
number of event data transformations may be required before
doing further analysis. this category comprises the buildingblocks to provide functionalities to perform the necessaryevent data transformations. figure 4shows the repertoire of
process-mining building blocks that belong to this category.
add data to event data (added) in order to perform a cer-
tain analysis or to improve the results, the event data can
be augmented with additional data coming from differentsources. for instance, if the process involves citizens, theevent data can be augmented with data from the municipal-
ity data source. if the level of performance of a process is
suspected to be inﬂuenced by the weather, event data canincorporate weather data coming from a system storing such
a kind of data. if the event data contain a zip code, then
other data ﬁelds such as country or city can be added to theevent data from external data sources. this building block
represents the functionality of augmenting event data using
external data, represented as a generic data set in the ﬁgure.
filter event data (filtered) several reasons may exist to
ﬁlter out part of the event data. for instance, the process
behavior may exhibit concept drifts over time. in those situa-
tions, the analysis needs to focus on certain parts of the event
data instead of all of it. one could ﬁlter the event data and
use only those events that occurred, e.g., in year 2015. as asecond example, the same process may run at different geo-graphical locations. one may want to restrict the scope of the
analysis to a speciﬁc location by ﬁltering out the event data
referring to different locations. this motivates the importanceof being able to ﬁlter event data in various ways.
split event data (splited) sometimes, the organization gen-
erating the event data is interested in comparing the process’performances for different customers, ofﬁces, divisions,
involved employees, etc. to perform such comparison, theevent data need to be split according to a certain criterion,
e.g., according to organizational structures, and the analy-
sis needs to be iterated over each portion of the event data.finally, the results can be compared to highlight difference.alternatively, the splitting of the data may be motivated by
the size of the data. it may be intractable to analyze all data
without decomposition or distribution. many process-miningtechniques are exponential in the number of different activi-
ties and linear in the size of the event log. if data are split in a
proper way, the results of applying the techniques to the dif-ferent portions can be fused into a single result. for instance,
work [ 47] discusses how to split event data while preserv-
ing the correctness of results. this building block representsthe functionality of splitting event data into overlapping ornon-overlapping portions.
merge event data (merged) this process-mining building
block is the inverse of the previous: data sets from differ-
ent information systems are merged into a single event dataset. this process-mining building block can also tackle the
typical problems of data fusion, such as redundancy and
inconsistency.
3.3 process model extraction
process mining revolves around process models to represent
the behavior of a process. this category is concerned with
providing building blocks to mine a process model from eventdata as well as to select or extract it from a process-model
collection. figure 5lists a number of process-mining building
blocks belonging to this category.
import process model (importm) process models can be
stored in some media for later retrieval to conduct someanalyses. this building block represents the functionality of
loading a process model from some repository.
fig. 5 process-mining building blocks related to process model extrac-
tion
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
discover process model from event data (discm) process
models can be manually designed to provide a normative
deﬁnition for a process. these models are usually intuitive
and understandable, but they might not describe accurately
what happens in reality. event data represent the “real behav-ior” of the process. discovery techniques can be used to minea process model on the basis of the behavior observed in the
event data (cf. [ 45]). here, we stay independent of the speciﬁc
notations and algorithms. examples of algorithms are thealpha miner [ 53], the heuristics miner [ 62] or, more recent
techniques like the inductive miner [ 29]. this building block
represents the functionality of discovering a process modelfrom event data. this block, as many others, can receive a
set of parameters as an input to customize the application of
the algorithms.
select process model from collection (selectm) organiza-
tions can be viewed as a collection of processes and resources
that are interconnected to form a process ecosystem .t h i s
collection of processes can be managed and supported bydifferent approaches, such as aris [ 36] or apromore [ 28].
to conduct certain analyses, one needs to use some of these
models and not the whole collection. in addition, one cangive a criterion to retrieve a subset of the collection. thisbuilding block represents the functionality of selecting one
or more process models from a process-model collection.
3.4 process model and event analysis
organizations normally use process models for the discus-
sion, conﬁguration, and implementation of processes. inrecent years, many process mining techniques are also usingprocess models for analysis. this category groups process-
mining building blocks that can analyze process models or
event logs and provide analysis results. figure 6shows some
process-mining building blocks that belong to this category.
analyze process model (analyzem) process models may
contain a number of structural problems. for instance, themodel may exhibit undesired deadlocks, activities that are
never enabled for execution, variables that are used to drive
decisions without previously taking on a value, etc. sev-eral techniques have been designed to verify the soundnessof process models against deadlocks and other problems
[52]. this building block refers to design-time properties:
the process model is analyzed without considering how theprocess instances are actually being executed. the checking
of the conformance of the process model against real event
data is covered by the next building block ( evaluam ). unde-
sired design-time properties happen for models designed by
hand but also for models automatically mined from event
data. indeed, several discovery techniques do not guaranteeto mine process models without structural problems. this
fig. 6 process-mining building blocks related to process model and
event analysis
building block provides functionalities for analyzing process
models and detecting structural problems.
evaluate process model using event data (evaluam) besides
structural analysis, process models can also be analyzed
against event data. compared with the previous building
block ( analyzem ), this block is not concerned with a design-
time analysis. conversely, it makes a-posteriori analysis
where the adherence of the process model is checked with
respect to the event data, namely how the process has actuallybeen executed. in this way, the expected or normative behav-ior as represented by the process model is checked against
the actual behavior as recorded in the event data. in the liter-
ature, this is referred to as conformance checking (cf. [ 45]).
this can be used, for example, in fraud or anomaly detection.
replaying event data on process models has many possible
uses: aligning observed behavior with modeled behavior iskey in many applications. for example, after aligning event
data and model, one can use the time and resource infor-
mation contained in the log for performance analysis. thiscan be used for bottleneck identiﬁcation or to gather infor-
mation for simulation analysis or predictive techniques. this
building block represents the functionality of analyzing or
evaluating process models using event data.
compare process models (comparem) processes are not sta-
tic as they dynamically evolve and adapt to the businesscontext and requirements. for example, processes can behave
differently over different years, or at different locations. such
differences or similarities can be captured through the com-parison of the corresponding process models. for example,
123a. bolt et al.
the degree of similarity can be calculated. approaches that
explicitly represent conﬁguration or variation points [ 49]
directly beneﬁt from such comparisons. building block com-
parem is often used in combination with splited that splits
the event data into sublogs and discm that discovers a model
per sublog.
analyze event data (analyzeed) instead of directly creating
a process model from event data, one can also ﬁrst inspect thedata and look at basic statistics. moreover, it often helps to
simply visualize the data. for example, one can create a so-
called dotted chart [ 45] exploiting the temporal dimension
of event data. every event is plotted in a two-dimensionalspace where one dimension represents the time (absolute or
relative) and the other dimension may be based on the case,
resource, activity or any other property of the event. thecolor of the dot can be used as a third dimension. see [ 26]
for other approaches combining visualization with other ana-
lytical techniques.
generate report (generr) to consolidate process models
and other results, one may create a structured report. thegoal is not to create new analysis results, but to present theﬁndings in an understandable and predictable manner. gen-
erating standard reports helps to reduce the cognitive load
and helps users to focus on the things that matter most.
3.5 process model transformations
process models can be designed or, alternatively, discovered
from event data. sometimes, these models need to be adjusted
for follow-up analyses. this category groups process-mining
building blocks that provide functionality to change the struc-ture of a process model. figure 7shows some process-mining
building blocks that belong to this category.
repair process model (repairm) process models may need
to be repaired in case of consistency or conformance prob-lems. repairing can be regarded from two perspectives:
fig. 7 process-mining building blocks related to process model trans-
formationsrepairing structural problems and repairing behavioral prob-
lems. the ﬁrst case is related to the fact that models cancontain undesired design-time properties such as deadlocks
and livelocks (see also the analyze process model building
block discussed in sect. 3.4). repairing involves modifying
the model to avoid those properties. techniques for repair-ing behavioral problems focus on models that are structurally
sound but that allow for undesired behavior or behavior that
does not reﬂect reality. see also the evaluate process model
using event data building block discussed in sect. 3.4, which
is concerned with discovering the conformance problems.
this building block provides functionality for both types ofrepair.
decompose process model (decompm) processes running
within organizations may be extremely large, in terms of
activities, resources, data variables, etc. as mentioned, manytechniques are exponential in the number of activities. the
computation may be improved by splitting the models into
fragments, analogously to what was mentioned for splittingthe event log. if the model is split according to certain crite-ria, the results can be somehow amalgamated and, hence,
be meaningful for the entire model seen as a whole. for
instance, the work on decomposed conformance checking[47] discusses how to split process model to make process
mining possible with models with hundreds of elements (such
as activities, resources, data variables), while preserving thecorrectness of certain results (e.g., the fraction of deviat-
ing cases does not change because of decomposition). this
block provides functionalities for splitting process modelsinto smaller fragments.
merge process models (mergem) process models may also
be created from the intersection (i.e., the common behavior)
or union of other models. this building block provides func-
tionalities for merging process models into a single processmodel. when process discovery is decomposed, the resulting
models need to be merged into a single model.
3.6 process model enhancement
process models just describing the control-ﬂow are usually
not the ﬁnal result of process mining analysis. process models
can be enriched or improved using additional data to providebetter insights into the real process behavior that it repre-
sents. this category groups process-mining building blocks
that are used to enhance process models. figure 8shows a
summary of the process-mining building blocks that belongto this category.
enrich process model using event data (enrichm) the back-
bone of any process models contains basic structural infor-mation relating to control-ﬂow. however, the backbone can
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
fig. 8 process-mining building blocks related to process model
enhancement
be enriched with additional perspectives derived from event
data to obtain better analysis results. for example, event fre-quency can be annotated in a process model to identify the
most common paths followed by process instances. timing
information can also be used to enrich a process model tohighlight bottlenecks or long waiting times. this enrichment
does not have an effect on the structure of the process model.
this building block represents the functionality of enrich-ing process models with additional information contained inevent data.
improve process model (improvem) besides being enriched
with data, process models can also be improved. for example,performance data can be used to suggest structural mod-iﬁcations to improve the overall process performance. it
is possible to automatically improve models using causal
dependencies and observed performance. the impact ofsuch modiﬁcations could be simulated in “what-if scenar-
ios” using performance data obtained in the previous steps.
this building block represents the functionality of improvingprocess models using data from other analysis results.
4 analysis scenarios for process mining
this section identiﬁes generic analysis scenarios that are not
domain-speciﬁc and, hence, that can be applied to different
contexts. the analysis scenarios compose the basic process-mining building blocks and, hence, they remain independent
of any speciﬁc operationalization of a technique. in fact, as
mentioned before, the building blocks may employ differentconcrete techniques, e.g., there are dozens of process discov-ery techniques realizing instances of building block discm
(fig. 5).
as depicted in fig. 1, we consider four analysis scenar-
ios: (a) result (sub-)optimality ,( b ) parameter sensitivity ,( c )
large-scale experiments , and (d) repeating questions . these
are described in the remainder of this section.
as discussed in this section and validated in sect. 6,t h e
same results could also be achieved without using scientiﬁc
workﬂows. however, the results would require a tedious anderror-prone work of repeating the same steps ad nauseam .4.1 result (sub-)optimality
this subsection discusses how process-mining building
blocks can be used to mine optimal process models accord-
ing to some optimality criteria. often, in process discovery,optimality is difﬁcult (or even impossible) to achieve. oftensub-optimal results are returned and it is no known what is
“optimal”.
consider for example the process discovery task. the
quality of a discovered process model is generally deﬁned
by four quality metrics [ 1,2,45,48]:
–replay ﬁtness quantiﬁes the ability of the process model
to reproduce the execution of process instances as
recorded in event data.
–simplicity captures the degree of complexity of a process
model, in terms of the numbers of activities, arcs, vari-
ables, gateways, etc.
–precision quantiﬁes the degree with which the model
allows for too much behavior compared to what was
observed in the event data.
–generalization quantiﬁes the degree with which the
process model is capable to reproduce behavior that isnot observed in the event data but that potentially should
be allowed. this is linked to the fact that event data often
are incomplete in the sense that only a fraction of thepossible behaviors can be observed.
traditionally, these values are normalized between 0 and 1,
where 1 indicates the highest score and 0 the lowest.
the model of the highest value within a collection of (dis-
covered) models is such that it can mediate among thosecriteria. often, these criteria are in competing: higher scorefor one criterion may lower the score of a second criterion.
for instance, to have a more precise model, it is necessary to
sacriﬁce the behavior observed in the event data that is lessfrequent, thus partly hampering the replay-ﬁtness score.
figure 9shows a suitable scientiﬁc workﬂow for min-
ing a process model from event data that is sub-optimalwith respect to a score deﬁned by speciﬁc criteria. the opti-
mization is done by ﬁnding the parameters that returns a
sub-optimal model.
event data are loaded from an information system and
used ntimes as input for a discovery technique using dif-
ferent parameter values. the nresulting process models are
evaluated using the original event data and the model that hasthe best score is returned. please note that the result is likely
to be sub-optimal: narbitrary parameter values are chosen
out of a much larger set of possibilities. if nis sufﬁciently
large, the result is sufﬁciently close to the optimal. this sci-
entiﬁc workﬂow is still independent of the speciﬁc algorithm
used for discovery; as such, the parameter settings are alsogeneric.
123a. bolt et al.
fig. 9 result (sub-)optimality in process model discovery: process-
mining scientiﬁc workﬂow for mining an optimal model in terms of a
deﬁned scoring criteria
figure 10a illustrates a scientiﬁc workﬂow that tries to
account for generalization. for this purpose, a k-fold cross
validation approach is used. in this approach, the process
instances recorded in the event data are randomly split into
kfolds, through building block split event data (splited).
for each of the ktimes, a different fold is taken aside: the
other k−1 folds are used for discovery and the “elected”
fold is used for evaluation through conformance checking.this corresponds to block fold(i)with 1 ≤i≤k. finally,
through the process-mining building block select process
model from collection (selectm), the model with the best
score is returned as output. figure 10b enters inside the block
fold(i)showing how fold e
iis used for evaluation and folds
e1,..., ei−1,ei+1,enare used for discovery (after being
merged).
scientiﬁc workﬂows can also be hierarchically deﬁned:
in turn, the discover process-mining building block (discm)
in fig. 9can be an entire scientiﬁc sub-workﬂow. the two
scientiﬁc workﬂows shown in figs. 9and 10do not exclude
each other. process-mining building block discover process
model from event data (discm) can be replaced by the entire
workﬂow in fig. 10a, thus including some generalization
aspects in the search for a sub-optimal process model.
4.2 parameter sensitivity
parameters are used by techniques to customize their behav-
ior, e.g., adapting to the noise level in the event log. these
parameters have different ways of affecting the results pro-
duced, depending on the speciﬁc implementation of thetechnique or algorithm. some parameters can have more rel-
fig. 10 process-mining main scientiﬁc workﬂow based on k-fold cross
validation. amain workﬂow. bprocess-mining sub-workﬂow for any
fold(i)
evance than others (i.e., they have a more substantial effect
on the results). there are many ways to evaluate the sensi-
tivity of a certain parameter for a given algorithm. figure 11
shows an example of this analysis scenario. here the parame-ter value is varied across the range. for each of the discoveredmodels, a score is computed. the results are ﬁnally plotted
on a cartesian coordinate system where the x-axis is associ-
ated with the potential parameter’s values and the y-axis is
associated with the model’s score.
alternatively, the sensitivity analysis can also focus on the
ﬁltering part, while keeping the same conﬁguration of para-meter(s) for discovery. in other words, we can study how the
discovered model is affected by different ﬁltering, namely
different values of the parameter(s) that customize the appli-cation of ﬁltering.
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
fig. 11 parameter sensitivity in process discovery techniques: process
mining workﬂow for comparing the effects of different parameter valuesfor a given discovery technique
fig. 12 parameter sensitivity in event data ﬁltering: process-mining
scientiﬁc workﬂow for comparing the effect of different event-data ﬁl-tering conﬁgurations on the discovered model
figure 12shows an example of this analysis scenario in the
process mining domain, using process-mining building blockto analyze the differences and similarities of results obtained
by discovery techniques from event data that were ﬁltered
using different parameter values. in this example, event dataare loaded and ﬁltered several times using different parameter
settings, producing several ﬁltered event data sets. each of
these ﬁltered event data sets is input for the same discoverytechnique using the same conﬁguration of parameter(s).
4.3 large-scale experiments
empirical evaluation is often needed (and certainly recom-
mended) when testing new process mining algorithms. in
case of process mining, many experiments need to be con-ducted to prove that these algorithms or techniques can beapplied in reality and that the results are as expected. this
is due to the richness of the domain. process models canhave a wide variety of routing behaviors, timing behavior,
and second-order dynamics (e.g., concept drift). event logs
can be large or small and contain infrequent behavior (some-times called noise) or not. hence this type of evaluation hasto be conducted on a large scale. the execution and evalu-
ation of such large-scale experiment results is a tedious and
time-consuming task: it requires intensive human assistanceby conﬁguring each experiment’s run and waiting for the
results at the end of each run.
this can be greatly improved by using process mining
workﬂows, as only one initial conﬁguration is required. there
are many examples for this analysis scenario within the
process mining domain. two of them are presented next.
4.3.1 assessment of discovery techniques through massive
testing
when developing new process discovery techniques, several
experiments have to be conducted to test the robustness of
the approach. as mentioned, many discovery techniques useparameters that can affect the result produced. it is extremely
time-consuming and error prone to assess the discovery tech-
niques using several different combinations of parametervalues and, at the same time, testing on a dozen of differ-
ent event-data sets.
figure 13shows the result of a large-scale experiment
using nevent data sets and mdifferent parameter settings
that produces n×mresulting process models. in this exam-
ple, the same discovery technique with different parameters
is used. however, one can consider the discovery algorithmto employ as an additional parameter. therefore, the mdif-
ferent parameter settings can indicate mdifferent discovery
algorithms. after mining n×mmodels, the best model is
considered.
fig. 13 exhaustive testing of a discovery technique: large-scale exper-
iments using different types of event data and parameter combinationsare needed to evaluate a discovery technique
123a. bolt et al.
fig. 14 decomposed process discovery: a generic example using event
data splitting, model composition and a speciﬁed discovery technique
4.3.2 decomposed process discovery
existing process mining techniques are often unable to han-
dle “big event data” adequately. decomposed process mining
aims to solve this problem by decomposing the processmining problem into many smaller problems, which can be
solved in less time and using less resources.
in decomposed process discovery, large event data sets are
decomposed in sublogs, each of which refers to a subset of
the process’ activities. once an appropriate decomposition is
performed, the discovery can be applied to each cluster. thisresults in as many process models as the number of clusters;these models are ﬁnally merged to obtain a single process
model. see for example the decomposed process mining
technique described in [ 59] which presents an approach that
clusters the event data, applies discovery techniques to each
cluster, and merges the process models.
figure 14shows a process-mining workﬂow that splits the
event data into nsubsets, then uses a discovery algorithm to
discover models for each of these subsets, and ﬁnally merges
them into a single process model.
4.4 repeating questions
whereas the previous scenarios are aimed at (data) scien-
tists, process mining workﬂows can also be used to lower thethreshold for process mining. after the process mining work-
ﬂow has been created and tested, the same analysis can be
repeated easily using different subsets of data and differenttime-periods. without workﬂow support this implies repeat-
ing the analysis steps manually or using hardcoded scripts
that perform them over some input data. the use of scientiﬁcworkﬂows is clearly beneﬁcial: the same workﬂow can bereplayed many times using different inputs where no further
conﬁguration is required.
there are many examples of this analysis scenario within
the process-mining domain. two representative examples are
described next.
4.4.1 periodic benchmarking
modern organizations make large investments to improve
their own processes for better performance in terms of costs,
time, or quality. in order to measure these improvements,organizations have to evaluate their performance periodi-
cally. this requires them to evaluate performance of the new
time-period and compare it with the previous periods. per-formance can improve or degrade in different time-periods.
obviously, the returned results require human judgments and,
hence, cannot be fully automated by the scientiﬁc workﬂow.
figure 15shows an example of this analysis scenario using
different process-mining building blocks. let us assume that
we want to compare period τ
kwith period τk−1. for period
τk, the entire event data are loaded and, then, ﬁltered so as
to only keep portion eτkthat refers to the period τk.u s i n g
portion eτk, a process model mτkis discovered. for period
τk−1, the entire event data are loaded and, then, ﬁltered so as
to only keep the portion eτk−1that refers to the period τk−1.
finally, an evaluation is computed about the conformance
between model mτkand event-data portion eτkand between
mτkand eτk−1. each evaluation will return valuable results,
which are compared to ﬁnd signiﬁcant changes.
fig. 15 periodic performance benchmark: process mining workﬂow
for comparing the performance of the process in two different time-periods ( tandt−1)
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
4.4.2 report generation over collections of data sets
scientiﬁc workﬂows are useful when generating several
reports for different portions of event data, e.g., different
groups of patients or customers. since the steps are the sameand the only difference is concerned with using differentportions of events, this can be easily automated, even when
dozens of subsets need to be taken into consideration.
from this, it follows that this scenario shares common
points with large-scale experiments. however, some differ-
ences exist. the report-generation scenario is characterized
by a stable workﬂow with a deﬁned set of parameters,whereas in the large-scale experiments scenario, parame-
ters may change signiﬁcantly in the different iterations. in
addition, the input elements used in report-generation sce-narios are similar and comparable event data sets. this canbe explained by the desire that reports should have the same
structure. in case of large-scale experiments, event data sets
may be heterogenous. it is thus worthwhile repeating theexperiments using diverse and dissimilar event data sets as
input.
figure 16illustrates a potential scientiﬁc workﬂow to gen-
erate reports that contain process-mining results. for the sake
of explanation, the process mining workﬂow is kept simple.
the report is assumed to contain three objects: the results
r
edof the analysis of the input event data, the discovered
process model m, and the results rmof the evaluation of
such a model against the input event data. process-mining
building block generate report takes these three objects as
input and combines them into a reporting document r.
5 implementation
our framework to support process mining workﬂows shown
in fig. 1is supported by rapidprom . rapidprom was imple-
fig. 16 report generation workﬂowmented using prom [ 58] and rapidminer [ 22]. the building
blocks deﬁned in sect. 3have been implemented in rapid-
prom as operators . most of the building blocks have been
realized using rapidminer-speciﬁc wrappers of plug-ins of
the prom framework [ 58]. prom is a framework that allows
researchers to implement process mining algorithms in astandardized environment, which provides a number of facil-
ities to support programmers. nowadays, it has become the
de-facto standard for process mining. prom can be freely
downloaded from http://www.promtools.org . the extension
of rapidminer to provide process-mining blocks for scien-
tiﬁc workﬂows using prom is also freely available. at thetime of writing, rapidprom provides 37 process mining oper-
ators, including several process-discovery algorithms and
ﬁlters as well as importers and exporters from/to differentprocess-modeling notations. the operators are deﬁned asatomic steps; however, they can be composed into (sub)
processes natively in rapidminer. a (sub) process is the
equivalent of a collapsed group of operators, but it can alsobe executed as an atomic block itself. this is allowed by
rapidminer’s native concurrency management, which sepa-
rates input from output object representations (i.e., a modiﬁedinput does not affect any other parallel operators that use the
same input).
the ﬁrst version of rapidprom was presented during
the bpm 2014 demo session [ 34]. this initial version suc-
cessfully implemented basic process-mining functionalities
and has been downloaded 4020 times since its release in
july 2014 until april 2015 (on average, over 400 monthlydownloads). however, process mining is a relatively new dis-
cipline, which is developing and evolving rapidly. therefore,
various changes and extensions were needed to keep up withthe state-of-the-art. the new version incorporates implemen-
tations of various new algorithms, which did not exist in the
ﬁrst version.
the rapidprom extension is hosted both at http://www.
rapidprom.org and in the rapidprom extension manager
server, which can be directly accessed through the rapid-
miner marketplace. after installation, the rapidprom opera-tors are available for use in any rapidminer workﬂow, which
allows to combine process-mining with other data-mining
techniques. figure 17shows an example of a process-mining
scientiﬁc workﬂow implemented using rapidprom opera-
tors. many of these operators implement a process-mining
building block. the process mining workﬂow shown infig. 17is used in sect. 6.1to obtain a sub-optimal process
model from event data.
readers are referred to http://www.rapidprom.org for
detailed installation, setup and troubleshooting instructions.
table 1shows the prom import plugins implemented
in rapidprom v ersion 2. these ﬁve operators are comple-
mented with rapidminer native operators to export visualresults and data tables, in a way that most ﬁnal results of
123a. bolt et al.
fig. 17 example of a process-mining workﬂow in rapidminer through the rapidprom extension: the workﬂow transforms event data (input)
into a sub-optimal process model (output)
table 1 import/export operators
building block operator name operator description
imported read log (path) imports an event log from a
speciﬁed path
imported read log (ﬁle) takes a ﬁle object (usually
obtained from a “loopﬁles” operator) andtransforms it to an eventlog
importm read pnml imports a petri net in a
petri net modelinglanguage (pnml) format
from a speciﬁed path
– export event log exports an event log in
different formats
– export pnml exports a petri net in
pnml format
process mining workﬂows can be exported and saved out-
side rapidminer.
table 2shows a list of prom discovery plugins imple-
mented in rapidprom as discovery operators. these nine
operators (usually referred to as miners ) are the most com-
monly used discovery techniques for process mining. thesediscovery operators produce different models using different
techniques and parameters to ﬁne-tune the resulting model.
table 3shows a list of prom visualization plugins imple-
mented in rapidprom as visualization operators. these fourvisualization plugins are accompanied by renderers that
allow one to inspect both intermediate and ﬁnal results duringand after the execution of process mining workﬂows.
table 4shows a list of prom conversion plugins imple-
mented in rapidprom as conversion operators. these fourconversion plugins are intended for converting models into
other model formats. this way we improve the chances that
a produced model can be used by other operators. for exam-ple, if a heuristics net is discovered from an event log usingthe heuristics miner, then the replay log on petri net (con-
formance) operator cannot be executed unless a conversion
to petri net is performed (which is supported).
table 5shows a list of log processing operators imple-
mented in rapidprom. some of these eight operators use
prom functionalities to perform their tasks, but others weredeveloped speciﬁcally for rapidprom, as the prom frame-
work generally does not use ﬂat data tables to represent event
data. these operators are used to modify an event log byadding attributes, events, or converting it to data tables, andvice versa.
table 6shows a list of prom plugins implemented in
rapidprom as analysis operators.
6 evaluation
this section shows a number of instantiations of scientiﬁcworkﬂows in rapidprom, highlighting the beneﬁts of using
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
table 2 discovery operatorsbuilding block operator name operator description
discm alpha miner [ 53] discovers a petri net. fast but results are not always
reliable because of overﬁtting issues
discm ilp miner [ 54] discovers a petri net by solving integer linear
programming (ilp) problems. result have perfect
ﬁtness but generally poor precision. slow on largelogs
discm genetic miner [ 43] discovers a heuristics net using genetic algorithms.
depending on the parameter settings it can be slowor fast
discm evolutionary tree miner [ 11] discovers a process tree using a guided genetic
algorithms based on model quality dimensions.guarantees soundness but cannot represent allpossible behavior due to its block-structured nature
discm heuristics miner [ 62] discovers a heuristics net using a probabilistic
approach. good when dealing with noise. fast
discm inductive miner [ 29] discovers a process tree or petri net. good when
dealing with infrequent behavior and large logs.soundness is guaranteed
discm social network miner [ 50] discovers a social network from the event log
resources. different social networks can beobtained: similar task, handover of work, etc.
discm transition system miner [ 44] discovers a transition system using parameters to
simplify the space-state exploration
discm fuzzy miner [ 18] discovers a fuzzy model. good when dealing with
unstructured behavior. fast
table 3 visualization operatorsbuilding block operator name operator description
analyzeed dotted chart [ 37] shows the temporal distribution of events
within traces
enrichm inductive visual miner [ 30] process exploration tool that shows an
annotated interactive model for quickexploration of a log
enrichm animate log in fuzzy instance [ 17] shows an animated replay of a log
projected over a fuzzy instance
enrichm pompom petri net visualizer that emphasizes those
parts of the process that correspond tohigh-frequent events in a given log
table 4 conversion operatorsbuilding block operator name operator description
– reachability graph to petri net converts a reachability
g r a p hi n t oap e t r in e t
– petri net to reachability graph converts a petri net into a
reachability graph
– heuristics net to petri net converts a heuristics net
into a petri net
– process tree to petri net converts a process tree
into a petri net
123a. bolt et al.
table 5 log processing operators
building block operator name operator description
added add table column to event log adds a single data table column as trace attribute to a given event log
added add trace attributes to event log adds all columns of a data table (except case id) as trace attributes to
a given event log
added add event attributes to event log adds all columns of a data table (except case id and event id) as event
attributes to a given event log
added add events to event log adds events to a given event log from selected columns on a data
table
mergeed merge event logs merges two event logs
added add artiﬁcial start and end event adds an artiﬁcial start event to the beginning, and an artiﬁcial end
event to the ending of each trace
– event log to exampleset converts an event log into a data table (exampleset)– exampleset to event log converts a data table (exampleset) into an event log
table 6 analysis operators
building block operator name operator description
analyzem woflan [ 60] analyzes the soundness of a petri net using the woﬂan software
(www.swmath.org/software/7028 )
– select fuzzy instance selects the best fuzzy instance from a fuzzy modelrepairm repair model [ 15] replays an event log in a petri net and repairs this net to improve
ﬁtness
repairm reduce silent transitions reduces a petri net by removing invisible transitions (and places) that
are not used
analyzeed feature prediction [ 13] produces predictions of business process features using decision trees
enrichm replay log on petri net (performance) [ 48] replays a log on a petri net and generates performance metrics such
as throughput time, waiting time, etc.
enrichm replay log on petri net (conformance) [ 48] replays a log on a petri net and generates conformance metrics such
as ﬁtness
scientiﬁc workﬂows for process mining. they are speciﬁc
examples of the analysis scenarios discussed in sect. 4
6.1 evaluating result optimality
the ﬁrst experiment is related to the result optimality analy-
sis scenario described in sect. 4.1. in this experiment, we
implemented a process mining workﬂow using rapidprom
to extract the model that scores higher with respect to thegeometric average of precision and replay ﬁtness.
2the geo-
metric average of replay ﬁtness and precision seems to be
better than the arithmetic average since it is necessary tohave a strong penalty if one of the criteria is low.
for this experiment, we employed the inductive miner -
infrequent discovery technique [ 29] and used different values
for the noise threshold parameter. this parameter is deﬁned
in a range of values between 0 and 1. this parameter allows
for ﬁltering out infrequent behavior contained in event data
2the geometric average of aand bused in this article is deﬁned as
2*a*b/(a+b) and it is meant to penalize very low scores.in order to produce a simpler model: the lower the value is
for this parameter (i.e., close to 0), the larger the fraction ofbehavior observed in the event data that the model allows. to
measure ﬁtness and precision, we employ the conformance-
checking techniques reported in [ 1,2]. all techniques are
available as part of the rapidprom extension. a summaryof the concrete operators used for each building block is pre-
sented in table 7.
this experiment instantiates the analysis scenario
described in sect. 4.1 and depicted in fig. 9. the model
table 7 operators used in the result (sub) optimality experiment
building block operator name
imported read log (ﬁle)
discm inductive miner
evaluam r e p l a yl o go np e t r in e t
selectm optimize parameter
(rapidminer native operator)
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
fig. 18 comparison of process models that are mined with the default
parameters and with the parameters that maximize the geometric aver-age of replay ﬁtness and precision. the process is concerned withroad-trafﬁc ﬁne management and models are represented using thebpmn notation. amodel mined using the inductive miner with defaultvalue of the noise-threshold parameter, which is 0.2. the geometric
average of ﬁtness and precision is 0.708. bmodel mined using the induc-
tive miner with one of the best values of the noise-threshold parameter,which is 0.7. this value was obtained as a result of this experiment. thegeometric average of ﬁtness and precision is 0.912
obtained with the default value of the parameter is com-
pared with the model that (almost) maximizes the geometric
average of ﬁtness and precision. to obtain this result, wedesigned a scientiﬁc workﬂow where several models arediscovered with different values of the noise threshold para-
meter. finally, the workﬂow selects the model with the
highest value of the geometric average among those dis-covered. as input, we used an event-data log that records
real-life executions of a process for road-trafﬁc ﬁne manage-
ment, which is employed by a local-police force in italy [ 12].
this event data refer to 150,370 process-instance executions
and record the execution of around 560,000 activities.
figure 18b shows the model generated using the opti-
mal parameters obtained through our scientiﬁc workﬂow,whereas fig. 18a illustrates the model generated using default
parameters.
there are clear differences between the models. for
example, in the default model, parallel behavior dominates
the beginning of the process. instead, the “optimal model”
presents simpler choices. another example concerns the ﬁnalpart of the model. in the default model, the latest process
activities can be skipped through. however, in the optimal
model, this is not possible. the optimal model has a replayﬁtness and precision of 0.921 and 0.903 respectively, withgeometric average 0.912. it scores better than the model
obtained through default parameters, where the replay ﬁtness
and precision is 1 and 0.548, respectively, with geometricaverage 0.708. the optimal model was generated with value
0.7 for the noise threshold parameter.
6.2 evaluating parameter sensitivity
as second experiment illustrating the beneﬁts of using scien-
tiﬁc workﬂows for process mining, we conducted an analysisof the sensitivity of the noise threshold parameter of the
inductive miner-infrequent . we used again the event data
of the road-trafﬁc ﬁne management process also used insect. 6.1. this experiment operationalizes the analysis sce-
nario discussed in sect. 4.2and depicted in fig. 11.i nt h i s
experiment, we implemented a process mining workﬂowusing rapidprom to explore the effect of this parameter in
the ﬁnal quality of the produced model. in order to do so,
we discovered 41 models using different parameter valuesbetween 0 and 1 (i.e., a step-size 0.025) and evaluated theirquality through the geometric average of replay ﬁtness and
precision used before. a summary of the concrete operators
used for each building block is presented in table 8.
figure 19shows the results of these evaluations, showing
the variation of the geometric average for different values of
thenoise threshold parameter.
by analyzing the graph, the models with higher geometric
average are produced when the parameter takes on a value
between 0.675 and 0.875. the worst model is obtained whenvalue 1 is assigned to the parameter.
123a. bolt et al.
table 8 operators used in the result (sub) optimality experiment
building block operator name
imported read log (ﬁle)
discm inductive miner
comparem replay event log on petri net, create
chart from value array(rapidminer native operator)
fig. 19 parameter sensitivity analysis: variation of the geometric aver-
age of ﬁtness and precision when varying the value of the noise threshold
parameter
6.3 performing large scale experiments
as mentioned before, the use of scientiﬁc workﬂows is
very beneﬁcial for conducting large-scale experiments withmany event logs. when assessing a certain process-mining
technique one cannot rely on a single event log to draw con-
clusions.
for instance, here we want to study how the noise thresh-
oldparameter inﬂuences the quality of the discovered model,
in terms of geometric average of ﬁtness and precision. insect. 4.2, the experiment was conducted using a single event
log, but rapidprom allows us to do this for any number ofevent logs using the same operators. to illustrate this, we use
11 real-life event logs and produce the corresponding process
models using different parameter settings.
table 9shows the results of this evaluation, where each
cell shows the geometric average of the replay ﬁtness and the
precision of the model obtained using a speciﬁc parameter
value (column) and event data (row). every event log usedin this experiment is publicly available through the digital
object identiﬁers (dois) of the included references. to use
some of them for discovery, we had to conduct pre-processingsteps in the following cases: the hospital event data set [ 55]
was extremely unstructured. to provide reasonable results
and to allow for conformance checking using alignments,we ﬁltered the event log to retain the 80% most frequentbehavior before applying the mining algorithm. the same
ﬁltering was done for the ﬁve coselog event logs [ 5–9].
the actual results in table 9are not very relevant for this
paper. it just shows that techniques can be evaluated on a
large scale by using scientiﬁc workﬂows.
6.4 automatic report generation
to illustrate the fourth analysis scenario we used event data
related to the study behavior and actual performance of stu-
dents of the faculty of mathematics and computer science at
eindhoven university of technology (tu/e). tu/e providesvideo lectures for many courses to support students who are
unable to attend face-to-face lectures for various reasons. the
event data record the views of video lectures and the examattempts of all tu/e courses.
first of all, students generate events when they watch lec-
tures. it is known how long and when they watch a particular
table 9 summary of a few large-scale experimental results: evaluating the geometric average of replay ﬁtness and precision of models discovered
with the inductive miner using different values of the noise threshold parameter (columns) and different real-life sets of event data (rows)
event data nt=0nt=0.1 nt=0.2 nt=0.3 nt=0.4 nt=0.5 nt=0.6 nt=0.7 nt=0.8 nt=0.9 nt=1a v e r a g e
bpi2012 [ 56] 0.314 0.730 0.430 0.450 0.508 0.474 0.675 0.683 0.674 0.679 0.644 0.569
bpi2013 [ 39] 0.847 0.826 0.778 0.863 0.458 0.458 0.458 0.458 0.458 0.458 0.453 0.592
bpi2014 [ 57] 0.566 0.720 0.708 0.613 0.616 0.654 0.626 0.414 0.530 0.527 0.490 0.588
hospital [ 55] 0.153 0.111 0.546 0.473 0.338 0.172 0.280 0.342 0.392 0.515 0.517 0.349
road fines [ 12] 0.689 0.633 0.708 0.721 0.909 0.909 0.744 0.912 0.912 0.710 0.498 0.758
coselog 1 [ 5] 0.143 0.366 0.389 0.576 0.687 0.710 0.737 0.668 0.673 0.649 0.594 0.563
coselog 2 [ 6] 0.095 0.191 0.146 0.233 0.127 0.167 0.250 0.177 0.218 0.180 0.362 0.195
coselog 3 [ 7] 0.182 0.352 0.573 0.640 0.170 0.209 0.628 0.632 0.585 0.732 0.657 0.487
coselog 4 [ 8] 0.190 0.448 0.488 0.640 0.623 0.163 0.553 0.621 0.546 0.518 0.670 0.496
coselog 5 [ 9] 0.160 0.199 0.445 0.517 0.522 0.628 0.634 0.145 0.246 0.222 0.602 0.393
coselog r. [ 10] 0.520 0.860 0.838 0.869 0.859 0.377 0.868 0.868 0.883 0.861 0.656 0.769
average 0.350 0.494 0.549 0.599 0.528 0.447 0.586 0.538 0.556 0.550 0.558
we use ntto indicate the value of the noise threshold parameter of application of the algorithm
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
lecture of a particular course. these data can be preprocessed
so that low-level events are collapsed into lecture views. sec-ond, students generate events when they take exams and the
result is added to the event.
for each course, we have generated a report that includes
the results of the application of various data-mining andprocess-mining techniques. this generation is automatic in
the sense that the scientiﬁc workﬂow takes a list of courses as
input and produces as many reports as the number of coursesin the list.
the report contains three sections: course information,
core statistics and advanced analysis.
figure 20shows a small part of the report generated
for the course on business information systems (2ii05). in
the ﬁrst section, the report provides information about thecourse, the bachelor or master programs which it belongsto, as well as the information about the overall number of
views of the course’s video lectures. in the second section
(only a small fragment is shown), some basic distributionsare calculated. for example, statistics are reported about the
division per gender, nationality, and ﬁnal grade. the thirdsection is devoted to process-mining results. the results of
applying conformance checking using the event data and theideal process model where a student watches every video
lecture and in the right order, namely he/she watches the
i
thvideo lecture only after watching the (i−1)thvideo
lecture. as expected, the results show a positive correlationbetween higher grades and higher compliance with the nor-
mative process just mentioned: the more a student watches all
video lectures in the right order, the higher the correspondinggrade will be. in addition to showing the conformance infor-
mation, the report always embeds a dotted chart. the dotted
chart is similar to a gantt chart (see building block ana-
lyzeed ). the dotted chart shows the distribution of events
for the different students over time. this way one can see
the patterns and frequency with which students watch videolectures.
note that reports like the one shown in fig. 20are very
informative for both professors and students. by using rapid-
prom we are able to automatically generate reports for allcourses (after data conversion and modeling the desired
process mining workﬂow).
fig. 20 fragments of the automatically generated report using rapidprom
123a. bolt et al.
7 conclusions
this paper presented a framework for supporting the design
and execution of process mining workﬂows. as argued, sci-
entiﬁc workﬂow systems are not tailored towards the analysisof processes based on models and logs. tools like rapid-miner and knime can model analysis workﬂows but do
not provide any process mining capabilities. the focus of
these tools is mostly on traditional data mining and reportingcapabilities that tend to use tabular data. also more classi-
cal scientiﬁc workﬂow management (swfm) systems like
kepler and taverna do not provide dedicated support forartifacts like process models and event logs. process min-
ing tools like prom, disco, perceptive, celonis, qpr, etc.
do not provide any workﬂow support. the inability to modeland execute process mining workﬂows was the primary moti-vation for developing the framework presented in this paper.
we proposed generic process mining building blocks
grouped into six categories. these can be chained together tocreate process mining workﬂows. we identiﬁed four broader
analysis scenarios and provided conceptual workﬂows for
these. the whole approach is supported using rapidprom
which is based on prom and rapidminer. rapidprom has
been tested in various situations and in this paper we demon-
strated this using concrete instances of the four analysisscenarios. rapidprom is freely available via http://www.
rapidprom.org and the rapidminer market place.
future work aims at extending the set of process mining
building blocks and evaluating rapidprom in various casestudies. we continue to apply rapidprom in all four areas
described. moreover, we would like to make standard work-
ﬂows available via infrastructures like myexperiment andopenml. we are also interested a further cross-fertilizations
between process mining and other analysis techniques avail-
able in tools like rapidminer and knime (text mining,clustering, predictive analytics, etc.).
acknowledgments the authors thank ronny mans for his seminal
work on the initial version of rapidprom.
open access this article is distributed under the terms of the creative
commons attribution 4.0 international license ( http://creativecomm
ons.org/licenses/by/4.0/ ), which permits unrestricted use, distribution,
and reproduction in any medium, provided you give appropriate credit
to the original author(s) and the source, provide a link to the creativecommons license, and indicate if changes were made.
references
1. adriansyah, a., munoz-gama, j., carmona, j., van dongen, b.f.,
van der aalst, w.m.p .: alignment based precision checking. in: larosa, m., pnina, s. (eds.) business process management work-shops. lecture notes in business information processing, vol. 132,pp. 137–149. springer, heidelberg (2013)2. adriansyah, a., munoz-gama, j., carmona, j., van dongen, b.f.,
van der aalst, w.m.p .: measuring precision of modeled behavior.inf. syst. e-bus manag. 13(1), 37–67 (2015)
3. barga, r., gannon, d.: scientiﬁc versus business workﬂows. in:
taylor, i.j., deelman, e., gannon, d.b., shields, m. (eds.) work-ﬂows for e-science, pp. 9–16. springer, berlin (2007)
4. berthold, m.r., cebron, n., dill, f., gabriel, t.r., koetter, t.,
meinl, t., ohl, p ., sieb, c., thiel, k., wiswedel, b.: knime:the konstanz information miner. in: preisach, c., burkhardt, h.,schmidt-thieme, l., decker, r. (eds.) data analysis, machinelearning and applications, studies in classiﬁcation, data analy-sis, and knowledge organization, pp. 319–326. springer, berlin(2008)
5. buijs, j.c.a.m.: environmental permit application process
(wabo), coselog project, municipality 1, (2014). doi: 10.4121/uuid:
c45dcbe9-557b-43ca-b6d0-10561e13dcb5
6. buijs, j.c.a.m.: environmental permit application process
(wabo), coselog project, municipality 2, (2014). doi: 10.4121/uuid:
34b4f6f4-dbe0-4857-bf75-5b9e1138eb87
7. buijs, j.c.a.m.: environmental permit application process
(wabo), coselog project, municipality 3, (2014). doi: 10.4121/uuid:
a8ed945d-2ad8-480e-8348-cf7f06c933b3
8. buijs, j.c.a.m.: environmental permit application process
(wabo), coselog project, municipality 4, (2014). doi: 10.4121/uuid:
e8c3a53d-5301-4afb-9bcd-38e74171ca32
9. buijs, j.c.a.m.: environmental permit application process
(wabo), coselog project, municipality 5, (2014). doi: 10.4121/uuid:
c399c768-d995-4086-adda-c0bc72ad02bc
10. buijs, j.c.a.m.: receipt phase of an environmental permit appli-
cation process (wabo), coselog project. (2014). doi: 10.4121/uuid:
a07386a5-7be3-4367-9535-70bc9e77dbe6
11. buijs, j.c.a.m., van dongen, b.f., van der aalst, w.m.p .: on the
role of ﬁtness, precision, generalization and simplicity in processdiscovery. in: meersman, r., panetto, h., dillon, t., rinderle-ma,s., dadam, p ., zhou, x., pearson, s., ferscha, a., bergamaschi,s., cruz, i.f. (eds.) on the move to meaningful internet systems(otm), volume 7565 of lecture notes in computer science, pp.305–322. springer, berlin (2012)
12. de leoni, m., mannhardt, f.: road trafﬁc ﬁne
management process. (2015). doi: 10.4121/uuid:
270fd440-1057-4fb9-89a9-b699b47990f5
13. de leoni, m., van der aalst, w.m.p ., dees, m.: a general frame-
work for correlating business process characteristics. in: sadiq, s.,soffer, p ., vlzer, h. (eds.) business process management, volume8659 of lecture notes in computer science, pp. 250–266. springer,new y ork (2014)
14. diamantini, c., potena, d., storti, e.: mining usage patterns from
a repository of scientiﬁc workﬂows. in: proceedings of the 27th
annual acm symposium on applied computing, sac ’12, p 152–157, new y ork, ny , 2012. acm (2012)
15. fahland, d., van der aalst, w.m.p .: model repair aligning process
models to reality. inf syst 47, 220–243 (2015)
16. garijo, d., alper, p ., belhajjame, k., corcho, ó., gil, y ., goble,
c.a.: common motifs in scientiﬁc workﬂows: an empirical analy-sis. future gener. comput. syst. 36, 338–351 (2014)
17. günther, c.w.: process mining in flexible environments, phd
thesis. technische universiteit eindhoven, eindhoven, the nether-lands (2009)
18. günther, c.w., van der aalst, w.m.p .: fuzzy mining adaptive
process simpliﬁcation based on multi-perspective metrics. in:alonso, g., dadam, p ., rosemann, m. (eds.) business processmanagement, volume 4714 of lecture notes in computer science,pp. 328–343. springer, berlin (2007)
19. goble, c.a., bhagat, j., aleksejevs, s., cruickshank, d.,
michaelides, d., newman, d., borkum, m., bechhofer, s., roos,m., li, p ., de roure, d.: myexperiment: a repository and social
123scientiﬁc workﬂows for process mining: building blocks, scenarios, and implementation
network for the sharing of bioinformatics workﬂows. nucl. acids
res. 38(suppl 2), w677–w682 (2010)
20. goecks, j., nekrutenko, a., taylor, j., the galaxy team.: galaxy:
a comprehensive approach for supporting accessible, reproducible,and transparent computational research in the life sciences.genome biol 11(8), r86 (2010)
21. hand, d.j., smyth, p ., mannila, h.: principles of data mining. mit
press, cambridge, ma (2001)
22. hofmann, m., klinkenberg, r.: rapidminer: data mining use
cases and business analytics applications. chapman andhall/crc, florida (2013)
23. hull, d., wolstencroft, k., stevens, r., goble, c.a., pocock, m.r.,
li, p ., oinn, t.: taverna: a tool for building and running workﬂowsof services. nucl. acids res. 34, 729–732 (2006)
24. ieee task force on process mining. process mining case studies.
http://www.win.tue.nl/ieeetfpm/doku.php?id=shared:process_mining_case_studies (2013)
25. ihaka, r., gentleman, r.: r: a language for data analysis and graph-
ics. j. comput. graph. stat. 5(3), 299–314 (1996)
26. keim, d., andrienko, g., fekete, j.-d., grg, c., kohlhammer, j.,
melanon, g.: visual analytics: deﬁnition, process, and challenges.in: kerren, a., stasko, j.t., fekete, j.-d., north, c. (eds.) infor-
mation visualization, volume 4950 of lecture notes in computer
science, pp. 154–175. springer, berlin (2008)
27. kranjc, j., podpean, v ., lavra, n.: clowdﬂows: a cloud based sci-
entiﬁc workﬂow platform. in: flach, p .a., de bie, t., cristianini,n. (eds.) machine learning and knowledge discovery in data-bases, volume 7524 of lecture notes in computer science, pp.816–819. springer, berlin (2012)
28. la rosa, m., reijers, h.a., van der aalst, w.m.p ., dijkman,
r.m., mendling, j., dumas, m., garcía-bañuelos, l.: apromore:an advanced process model repository. expert syst. appl. 38(6),
7029–7040 (2011)
29. leemans, s.j.j., fahland, d., van der aalst, w.m.p .: discovering
block-structured process models from event logs—a constructiveapproach. in: colom, j.-m., desel, j. (eds.) application and theoryof petri nets and concurrency, volume 7927 of lecture notes incomputer science, pp. 311–329. springer, berlin (2013)
30. leemans, s.j.j., fahland, d., van der aalst, w.m.p .: exploring
processes and deviations. in: fournier, f., mendling, j. (eds.) busi-ness process management workshops, volume 202 of lecturenotes in business information processing, pp. 304–316. springer,heidelberg (2015)
31. leymann, f., roller, d.: production workﬂow—concepts and
techniques. prentice hall ptr, upper saddle river, nj (2000)
32. littauer, r., ram, k., ludäscher, b., michener, w., koskela, r.:
trends in use of scientiﬁc workﬂows: insights from a public repos-
itory and recommendations for best practice. ijdc 7(2), 92–100
(2012)
33. ludäscher, b., altintas, i., berkley, c., higgins, d., jaeger, e.,
jones, m.b., lee, e.a., tao, j., zhao, y .: scientiﬁc workﬂowmanagement and the kepler system. concurr comput. pract. exp.18(10), 1039–1065 (2006)
34. mans, r.s., van der aalst, w.m.p ., v erbeek, h.m.w.: support-
ing process mining workﬂows with rapidprom. in: limonad, l.,weber, b. (eds.) proceedings of the bpm demo sessions 2014 co-located with the 12th international conference on business processmanagement (bpm), volume 1295 of ceur workshop proceed-ings, pp 56–60. ceur-ws.org (2014)
35. mitchell, t.m.: machine learning. mcgraw hill series in com-
puter science. mcgraw-hill, new y ork (1997)
36. scheer, a.-w., nüttgens, m.: aris architecture and reference mod-
els for business process management. in: van der aalst, w.m.p .,desel, j., oberweis, a. (eds.) business process management, vol-ume 1806 of lecture notes in computer science, pp. 376–389.springer, berlin (2000)37. song, m., van der aalst, w.m. p .: supporting process mining by
showing events at a glance. in: proceedings of the 17th annualworkshop on information technologies and systems (wits), pp.139–145 (2007)
38. sonntag, m., karastoyanova, d., deelman, e.: bridging the gap
between business and scientiﬁc workﬂows: humans in the loopof scientiﬁc workﬂows. in: sixth international conference on e-science, e-science 2010, 7–10 dec 2010, brisbane, qld, pp. 206–213 (2010)
39. steeman, w.: bpi challenge 2013. (2013). doi: 10.4121/
500573e6-accc-4b0c-9576-aa5468b10cee
40. steffen, b., margaria, t., nagel, r., joerges, s., kubczak, c.:
model-driven development with the jabc. in: bin, e., ziv, a., ur,s. (eds.) hardware and software, v eriﬁcation and testing, volume4383 of lecture notes in computer science, pp. 92–108. springer,berlin (2007)
41. taylor, i.j., deelman, e., gannon, d.b., shields, m.: workﬂows for
e-science: scientiﬁc workﬂows for grids. springer, berlin (2007)
42. turner, k.j., lambert, p .s.: workﬂows for quantitative data analy-
sis in the social sciences. int. j. softw. tools technol. transf. 17(3),
321–338 (2015)
43. van der aalst, w.m.p ., alves de medeiros, a.k., weijters,
a.j.m.m.: genetic process mining. in: ciardo, g., darondeau, p .
(eds.) applications and theory of petri nets 2005, volume 3536of lecture notes in computer science, pp. 48–69. springer, berlin(2005)
44. van der aalst, w.m.p ., schonenberg, m.h., song, m.: time pre-
diction based on process mining. inf. syst. 36(2), 450–475 (2011).
special issue: semantic integration of data, multimedia, and ser-vices
45. van der aalst, w.m.p .: process mining: discovery, conformance
and enhancement of business processes, 1st edn. springer, berlin(2011)
46. van der aalst, w.m.p .: a decade of business process management
conferences: personal reﬂections on a developing discipline. in:barros, a., gal, a., kindler, e. (eds.) business process manage-ment, volume 7481 of lecture notes in computer science, pp.1–16. springer, berlin (2012)
47. van der aalst, w.m.p .: decomposing process mining problems
using passages. in: haddad, s., pomello, l. (eds.) application andtheory of petri nets, volume 7347 of lecture notes in computerscience, pp. 72–91. springer, berlin (2012)
48. van der aalst, w.m.p ., adriansyah, a., van dongen, b.f.: replay-
ing history on process models for conformance checking andperformance analysis. wiley interdisciplinary reviews. data min.knowl. disc. 2(2), 182–192 (2012)
49. van der aalst, w.m.p ., dreiling, a., gottschalk, f., rosemann, m.,
jansen-vullers, m.h.: conﬁgurable process models as a basis for
reference modeling. in: bussler, c.j., haller, a. (eds.) businessprocess management workshops, volume 3812 of lecture notesin computer science, pp. 512–518. springer, berlin (2006)
50. van der aalst, w.m.p ., song, m.: mining social networks: uncover-
ing interaction patterns in business processes. in: desel, j., pernici,b., weske, m. (eds.) business process management, volume 3080of lecture notes in computer science, pp. 244–260. springer,berlin (2004)
51. van der aalst, w.m.p ., ter hofstede, a.h.m., kiepuszewski, b.,
barros, a.p .: workﬂow patterns. distrib. parallel. databases 14(1),
5–51 (2003)
52. van der aalst, w.m.p ., van hee, k.m., ter hofstede, a.h.m.,
sidorova, n., v erbeek, h.m.w., v oorhoeve, m., wynn, m.t.:soundness of workﬂow nets: classiﬁcation, decidability, and analy-sis. form. aspect. comput. 23(3), 333–363 (2011)
53. van der aalst, w.m.p ., weijters, a.j.m.m., maruster, l.: workﬂow
mining: discovering process models from event logs. ieee trans.knowl. data eng. 16(9), 1128–1142 (2004)
123a. bolt et al.
54. van der werf, j.m.e.m., van dongen, b.f., hurkens, c.a.j., sere-
brenik, a.: process discovery using integer linear programming.in: van hee, k.m., v alk, r. (eds.) applications and theory ofpetri nets, volume 5062 of lecture notes in computer science,pp. 368–387. springer, berlin (2008)
55. van dongen, b.f.: real-life event logs - hospital log. (2011). doi: 10.
4121/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54
56. van dongen, b.f.: bpi challenge 2012. (2012). doi: 10.4121/uuid:
3926db30-f712-4394-aebc-75976070e91f
57. van dongen, b.f.: bpi challenge 2014. (2014). doi: 10.4121/uuid:
d5ccb355-ca67-480f-8739-289b9b593aaf
58. van dongen, b.f., de medeiros, a.k.a., v erbeek, h.m.w., wei-
jters, a.j.m.m., van der aalst, w.m.p .: the promframework: anew era in process mining tool support. in: ciardo, g., darondeau,p . (eds.) applications and theory of petri nets, volume 3536 oflecture notes in computer science, pp. 444–454. springer, berlin(2005)
59. van der aalst, w.m.p .: decomposing petri nets for process min-
ing: a generic approach. distrib. parallel databases 31(4), 471–507
(2013)
60. v erbeek, h.m.w., basten, t., van der aalst, w.m.p .: diagnos-
ing workﬂow processes using woﬂan. comput. j. 44(4), 246–279
(2001)61. wassink, i.h.c., van der v et, p .e., wolstencroft, k., neerincx,
p .b.t., roos, m., rauwerda, h., breit, t.m.: analysing scientiﬁcworkﬂows: why workﬂows not only connect web services. in: 2009ieee congress on services, part i, services i 2009, los ange-les, ca, july 6–10, pp 314–321 (2009)
62. weijters, a.j.m.m., van der aalst, w.m.p .: rediscovering work-
ﬂow models from event-based data using little thumb. integr.comput. aided eng. 10(2), 151–162 (2003)
63. weske, m.: business process management—concepts, lan-
guages, architectures, 2nd edn. springer, heidelberg (2012)
64. wickert, a., lamprecht, a.-l.: jabcstats: an extensible process
library for the empirical analysis of jabc workﬂows. in: margaria,t., steffen, b. (eds.) leveraging applications of formal methods,v eriﬁcation and v alidation. specialized techniques and applica-tions, volume 8803 of lecture notes in computer science, pp.449–463. springer, berlin (2014)
65. zeng, r., he, x., li, j., liu, z., van der aalst, w.m.p .: a method
to build and analyze scientiﬁc workﬂows from provenance throughprocess mining. in: 3rd workshop on the theory and practice ofprovenance, tapp’11, heraklion, crete, greece, 20–21 june 2011(2011)
123