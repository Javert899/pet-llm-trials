from place nets to local process models
viki peeva, lisa l. mannel, and wil m. p. van der aalst
rwth aachen university, 52062 aachen germany
{peeva, mannel, wvdaalst }@pads.rwth-aachen.de
abstract. standard process discovery algorithms find a single process
model that describes all traces in the event log from start to end as best as
possible. however, when the event log contains highly diverse behavior,
they fail to find a suitable model, i.e., a so-called ”flower” or ”spaghetti”
model is returned. in these cases, discovering local process models can
provide valuable information about the event log by returning multiple
small process models that explain local behavior. in addition to explain-
ability, local process models have also been used for event abstraction,
trace clustering, outcome prediction, etc. existing approaches that dis-
cover local process models do not scale well on event logs with many
events or activities. hence, in this paper, we propose a novel approach
for discovering local process models composed of so-called place nets, i.e.,
petri net places with the corresponding transitions. the place nets may
correspond to state- or language-based regions, but do not need to. the
goal however is to build multiple models, each explaining parts of the
overall behavior. we also introduce different heuristics that measure the
model’s frequency, simplicity, and precision. the algorithm is scalable on
large event logs since it needs only one pass through the event log. we
implemented our approach as a prom plugin and evaluated it on several
data sets.
keywords: local process models ·process mining ·process discovery.
1 introduction
the main goal of process mining is to help people analyze and improve pro-
cesses. one subarea of process mining is process discovery which automatically
creates process models from available event logs [1]. process discovery tech-
niques [13,15,26,27] try to explain and visualize the process from start to end,
while other algorithms like sequence and episode mining [19,21] try to mine small
patterns that frequently happen in the event log. this paper will focus on lo-
cal process model discovery which was first introduced in [24] as an individual
branch and was positioned between process discovery and pattern mining. local
process models are able to describe complex constructs in contrast to sequences
and episodes, but keep the local perspective introduced in pattern mining, which
separates them from process discovery. this way, instead of describing a process
with one overall model, a set of models is used.2 v. peeva et al.
some processes we want to analyze are too diverse to have a clear structure.
thus, making it almost impossible to discover an end-to-end model, resulting in
a discovery of a so-called ”flower” or ”spaghetti” models. hence, one straight-
forward use-case of local process model discovery is when traditional process
discovery approaches fail to produce an understandable model. however, the
importance of local process models is not constrained to processes where pro-
cess discovery fails to produce a good model. despite the limited number of
approaches that offer local process model discovery [2,24], local process models
have been used in event abstraction [18], classification of traces [20], clustering
of resources [8], as sub-part of end-to-end discovery algorithms [12,17], and in
different use-case studies [7,11].
in this paper, we introduce a novel approach for discovering local process
models. we are inspired by region-theory discovery algorithms. we assume that
the possible regions are already available to us, and instead of building one end-
to-end model, we combine the regions in smaller local process models. we accept
the regions in the form of place nets, that we can get from any of the existing
process discovery approaches. our proposed algorithm is available as a plugin
in prom1[25] (figure 1) that allows the input to be defined as a set of place
nets or a petri net. the first notable difference between our approach and the
existing ones is that we build the local process models as petri nets instead of
process trees as in [2,24]. this allows us to find constructs like long-term depen-
dencies that are not possible in process trees. the next significant difference is
speed and feasibility. we show that in contrast to the existing approaches, we
are able to handle event logs with many activities or events and we return re-
sults much faster. previous approaches rely on pruning infrequent local process
models early on to gain on speed. thus, forcing them to return only frequently
appearing models. and although we are able to return frequent models, we are
not constrained to find only those, since our speed arises from passing the event
log only once and not pruning out infrequent patterns. in the future, this would
allow for even broader usage and application of local process models in other
areas of process analysis. to summarize, our contribution is threefold:
–we introduce an entirely new technique to build local process models that
is completely based on petri nets.
–we offer a technique that is feasible on event logs with many activities or
events because it is linear in the size of the event log.
–we do not limit the results to frequent local process models.
we continue the paper by presenting some related work in section 2, and
preliminaries in section 3. in section 4, we present the approach for local process
model discovery. section 5 explains our evaluation strategy and the results we
get. section 6 concludes the paper by summarizing and giving an outlook for
future work.
1the plugin ”localprocessmodeldiscoverybycombiningplaces” is available in prom
6.11 and the nightly buildsfrom place nets to local process models 3
(a) table with local process models
 (b) visualizing a local process model
fig. 1. implementation of our approach as a prom plugin.
2 related work
as previously mentioned, local process model discovery is positioned in-between
traditional process discovery and episode and sequence mining. although process
discovery approaches [4,6,13,15,26,27] are highly valuable for process analysis,
the purpose of local process models is different. local process models try to ex-
plain subsequences of the traces like in episode and sequence mining but can
discover much more complex constructs as compared to traditional process dis-
covery. to take advantage of all the different discovery techniques, we can use
the output they produce as input for our algorithm. to be flexible regarding the
existing and future discovery methods, we only require that the input is a petri
net or a set of place nets no matter how or which algorithm produced them.
to the best of our knowledge, there are two existing techniques for mining
local process models from event logs. both [24] and [2] mine local process models
by recursively extending process trees.
the approach in [24] was the first to discover local process models and con-
sists of four main steps. in the first step, for each activity in the event log a
process tree containing exactly one leaf node that represents the activity is cre-
ated. this set is the first set of candidate local process models. in the second
step, each of the local process models from the candidate set is evaluated on the
event log with different quality metrics, and only a subset of them that satisfy
certain thresholds are selected in the third step. in the fourth step, the selected
local process models (process trees) are expanded by replacing one of the leaves
with each process tree operator (sequence, loop, parallel, and exclusive choice)
and adding the replaced leaf as one child and an activity not already present in
the process tree as a second child. the expanded local process models become
candidates in the next step, and the procedure is repeated until the maximal size
of the local process model is achieved or none of the candidates pass the selection
phase. each process tree is evaluated on the entire event log and extended with
all activities, making the approach infeasible for event logs with many events or
activities.
the approach in [2] was inspired by [24], and also recursively extends process
trees. however, they create new process trees by combining two existing process4 v. peeva et al.
trees, called seeds, that differ only in one leaf node. in the combined process
tree this differentiating node is replaced with a process tree operator and the
two nodes are added as children to the new operator. in addition to [24] they do
not reevaluate the process trees on the entire event log but on projections of the
seeds. additionally, they define compact and maximal process trees and strive
to return only such local process models.
both approaches use monotonicity of model frequency for pruning, but still
struggle to return results in reasonable time on mid-sized event logs. to handle
this problem, [23] extends the work in [24] by mining local process models for
specific subsets of activities decided via heuristics. [22] allows for mining local
process models of a specific interest using utility functions, and with [3] the
work in [2] is extended to discover patterns that are frequent for a given context.
although not in the focus of this paper, our algorithm can adopt both utility
functions and in-context search without impacting our running time significantly.
3 preliminaries
in this section, we introduce important background information needed for un-
derstanding the rest of the paper. we start with some general notations, and we
continue with topic-specific definitions.
general. we use sets ( {a, b, ... }), multisets ([ a2, b, ... ]), sequences ( ⟨a, b, ... ⟩), and
tuples (( a, b, ... )) as usually defined. given a set x,x∗represents the set of
all sequences over x, andm(x) is the set of all multisets over x. given a
sequence σ=⟨s1, s2, ...sn⟩, we access the i-th element of the sequence with σ[i],
i.e.,σ[i] =si, for 1 ≤i≤n. we extend σwith an additional element sn+1
by writing σ·sn+1. we call the sequence σ′asubsequence ofσ, if and only if
σ′=⟨sl, sl+1, ...sm⟩and 1 ≤l < m ≤n(we write σ′⊑σorσ′=σ[l, m] if the
indices are known). we call σ′arelaxed subsequence (we write σ′⊏
eσ) if and only
if for some k≥1 there is σ′=⟨si1, si2, ...sik⟩such that 1 ≤i1< i2< ... < i k≤n,
i.e., we drop any number of elements from σ(at most n−1) and keep the order
for the rest. we write {σ′opσ}or [σ′opσ] where op ∈ {⊑ ,⊏
e,⊑k,⊏
ek}, to
denote the set or multiset of all sequences σ′that satisfy the given operator
in regard to σ. we use ⊑kand⊏
ekwhen we are interested in subsequences
respectively relaxed subsequences of a particular length. to recalculate sets or
multisets from other sets, multisets or sequences, we use the {·}and [·] operators.
we use f(x) ={f(x)|x∈x}(respectively f(σ) =⟨f(s1), f(s2), ...f(sn)⟩) to
apply the function fto every element in the set x(the sequence σ) and f↾x
(respectively σ↾x) to denote the projection of the function f(respectively the
sequence σ) on the set x.
process mining. the collected data used for process analysis is given in the
form of event logs . hence, in definition 1, we formally define traces andevent
logs. note that although traces are usually defined as sequences of events, in this
work, we are interested only in the activity the events represent.from place nets to local process models 5
definition 1. given the universe of activities a, we define ρ∈ a∗as a trace ,
andl∈m(a∗)as an event log .
in definition 2, we define labeled petri nets . note that a transition t∈twith
l(t) =τis called silent, and that there may be duplicate transitions t1, t2∈t
such that l(t1) =l(t2).
definition 2 (labeled petri net). alabeled petri net n= (p, t, f, a, l )
is a tuple, where pis a set of places and tis a set of transitions such that
p∩t=∅.f⊆(p×t)∪(t×p)is the flow relation, a⊆ a is a set of
activities, and l:t− →a∪ {τ}the labeling function.
now given a labeled petri net n= (p, t, f, a, l ), for each element
x∈p∪twe define the preset ofxto be •x={y|(y, x)∈f}, and the
postset ofxto be x•={y|(x, y)∈f}. we additionally define the set
n={(ti, to)|∃p∈p((ti, to)∈ •p×p•)}to denote all pairs of transitions in the
netn, that are directly connected via a place. we call each such pair a passage .
a labeled petri net can be in a given state with the help of markings . given
a labeled petri net n= (p, t, f, a, l ), we define a marking masm∈m(p),
and with [ ] we denote the empty marking . every element in the marking m
represents a token in one of the places in p. the state can change by following
thefiring rule . we say that a transition t∈tisenabled in the marking mif
and only if there is a token in each place in the preset of t, i.e.,•t⊆m(we write
m[t⟩). a transition tcanfirein marking mif and only if it is enabled inm. by
firing, the transition changes the marking to m′= (m\ •t)∪t•. in this case,
we can write mt− →m′. to denote getting from mtom′by firing a sequence of
transitions σ=⟨t1, ..., t n⟩ ∈t∗such that mt1− →m1t2− →m2···tn− →m′, we write
mσ− →m′.
in definition 3, we define a union of two labeled petri nets and we extend
for multiple labeled petri nets. then in definition 4 we define what it means for
a labeled petri net to be connected .
definition 3 (union of labeled petri nets). given two labeled
petri nets n1= (p1, t1, f1, a1, l1)and n2= (p2, t2, f2, a2, l2)we de-
fine their union asn1∪n2=n= (p, t, f, a, l )where p=p1∪p2,
t=t1∪t2,f=f1∪f2,a=a1∪a2, and l:t− →a∪ {τ,⊥} is the
mapping l(t) =

l1(t),ift∈t1\t2
l2(t),ift∈t2\t1
l1(t),ift∈t1∩t2∧l1(t) =l2(t)
⊥,otherwise.
the union is a valid union if there is no t∈tsuch that l(t) =⊥. we writesn
i=1ni= (···((n1∪n2)∪n3)··· ∪ nn)to denote the union of the set of
labeled petri nets {n1, . . . , n n}.
definition 4 (connected labeled petri net). a labeled petri net
n= (p, t, f, a, l )isconnected , if and only if for each two different elements
x, x′∈p∪tthere exists a sequence ⟨y1, . . . , y n⟩such that n≥2,(yi, yi+1)∈f
or(yi+1, yi)∈ffor1≤i < n andy1=xandyn=x′.6 v. peeva et al.
local process models. our algorithm discovers a set of local process models , that
we represent with labeled petri nets. we discover these local process models from
anevent log and a set of place nets . to represent our input, in definition 5 we
define a place net as a labeled petri net with only one place.
definition 5 (place net). aplace net is a labeled petri net
np= ({p}, t, f, a, l ), where {p}is a set of places containing one place only,
andtis a set of transitions such that p∩t=∅.f⊆({p} ×t)∪(t× {p})is
the flow relation, a⊆ a is a set of activities, and l:t− →a∪ {τ}the labeling
function.
next, with the help of definitions 3 and 4, in definition 6 we define a local
process model as aunion of place nets.
definition 6 (local process model). given a set of place nets
npi= ({pi}, ti, fi, ai, li)for1≤i≤k, their union is a local process model ,
lpm =sk
i=1npi, if and only if lpm is avalid union and a connected labeled
petri net.
what makes our local process models local is the behavior. therefore, we
define the term locality orlocal distance to be the maximal length of the trace’s
subsequences we want our local process models to explain. since local process
models are a subset of labeled petri nets, markings ,enabled transitions andfiring
rule, also hold for them. the opportunity to change states and fire transitions
makes it possible local process models to describe behavior. since we want to
discover models that explain selected parts of the behavior in an event log, we
need to somehow align the two. thus, in definition 7 we define how a local
process model can replay a sequence of activities. in addition, we want to be
able to skip some of the activities during the replay, so we also define relax
replay (definition 8).
definition 7 (replay). given a local process model lpm = (p, t, f, a, l )
and a sequence of activities ρ=⟨a1, a2, . . . a n⟩, we say lpm replays ρif and
only if there exists a sequence of transitions σ=⟨t1, t2, . . . , t m⟩ ∈t∗such that
l(σ)↾a=ρand[ ]σ− →[ ].
definition 8 (relaxed replay). given a local process model
lpm = (p, t, f, a, l )and a sequence of activities ρ=⟨a1, a2, ...a n⟩, we
saylpm relax replays ρif and only if there exists at least one relaxed
subsequence ρ′∈[ρ′⊏
eρ]thatlpm canreplay .
by defining replay andrelax replay to require starting and ending in an empty
marking, makes the subset of place nets np= ({p}, t, f, a, l ) for which •p⊆p•
orp• ⊆ • punsuitable for our local process models . hence, in the continuation
we will discard place nets of this type.
in addition, we use replay andrelax replay to define the language (defi-
nition 9) and relaxed language (definition 10) for a given local process model
lpm .from place nets to local process models 7
definition 9 (language). given a local process model lpm = (p, t, f, a, l ),
we define l(lpm ) ={ρ∈a∗|∃σ∈t∗(l(σ)↾a=ρ∧[ ]σ− →[ ])}to be the language
oflpm .
definition 10 (relaxed language). given a local process model
lpm = (p, t, f, a, l ), we define the relaxed language oflpm as
lrlx(lpm ) ={ρ∈a∗|∃ρ′∈l(lpm )(ρ′⊏
eρ)}.
we conclude this section, by defining how a local process model can be com-
pact in regard to a sequence of activities (definition 11).
definition 11 (compact local process model). given a lo-
cal process model lpm = ( p, t, f, a, l )and a sequence of activities
ρ=⟨a1, a2, ..., a n⟩ ∈ l rlx(lpm ), we say lpm is compact with respect to ρif
and only if it holds that ∃σ∈t∗([ ]σ− →[ ]∧l(σ)↾a⊏
eρ∧∀p∈p(∃t∈{σ}(p∈ •t∪t•))).
4 approach
our algorithm combines place nets into local process models. hence, as input we
require place nets and an event log for which we want to build the local process
models. however, for nplace nets, there are 2n−1 non-empty candidate local
process models. even if we remove the ones that do not satisfy definition 6,
our search space would still be enormous. additionally, some of the local process
models we build, can be too complicated or not satisfy basic quality expectations.
therefore, we propose a framework with three modules (figure 2). since our
search space directly depends on the number of place nets we use, we use the first
module for filtering and adapting the place nets to limit their number. however,
at the same time the quality of the built local process models directly depends
on the quality of the chosen place nets, so we want to choose these wisely. after
the place nets are chosen, the second module introduces the main algorithm for
building local process models. the goal of the algorithm is to consider different
subsets of place nets, construct their union and check whether it can relax replay
subsequences of the traces in the given event log. although we restrict the set
of place nets we use, we can still end up with a lot of local process models.
therefore, we also provide a module for evaluating and ranking the found local
process models with different metrics. in the following, we introduce each of the
modules, with the main focus on the combination algorithm (the second module
in figure 2).
4.1 place net adaptation and filtering (paf)
we use an ”oracle” to get the place nets from which we build our local process
models. any algorithm that returns a labeled petri net or a set of place nets
based on an event log can be considered an oracle. the oracle can return many
place nets, so for efficiency reasons, we want to limit the number of those we
use for building local process models. on the other side, the set of place nets we8 v. peeva et al.
additional
parametersplace netsevent
log
lpmsplace net adaptation
and filtering
(paf )building
local process models
(b-lpms )local process models
evaluation and ranking
(lpms-er )places lpms
fig. 2. top view of our framework for local process model discovery
use restricts our models to a representational bias defined by them. hence, to
promote simplicity and higher relax replay frequency (see definition 8), we rank
the place nets by giving preference to nets with fewer transitions that can relax
replay more subsequences. for place nets that rank the same on the previous
criteria, we use the lexicographic order of the included transitions. afterward,
we keep the khighest ranking place nets to build local process models, where k
is a user-defined parameter. for simplicity in the continuation of the paper, we
assume that given the set of place nets pthat we return nowspis a valid union.
otherwise, we keep track of all the subsets where label disagreements exist and
do not use multiple contradicting place nets in one local process model.
4.2 building local process models (b-lpms)
this module covers the part of the framework that combines place nets into local
process models, thus, making it the main contribution of this paper. to explain
the approach, we give a high-level pseudo-code in algorithm 1. there are three
main steps that make up the gist of the algorithm, and get us from a set of place
nets, to a set of local process models that describe the event log:
1. focus on locality by iterating all subsequences in the event log of certain
length (line 2).
2. build local process models for each subsequence separately (line 3).
3. store the built local process models in a single structure (line 4).
the high-level algorithm looks pretty straightforward. however, optimizing the
traversal of the event log on line 2, the particulars of the global storage and how
wecreate local process models that relax replay the window, is what makes the
algorithm not only feasible but also efficient.
focus on locality. we want our local process models to describe what happens
within some local distance in the event log. hence, with a sliding window we
get subsequences of certain length, that we call windows . the sliding window
size represents the locality we are interested in, and we accept it as an input
parameter. we formally define the sliding window in definition 12.from place nets to local process models 9
algorithm 1: combining places in local process models
input : l- event log; d- local distance; p- set of place nets;
output: lpm - set of local process models
1lpm ←[ ]; // initialize the global storage
2forall w∈[ρ′⊑dρ|ρ∈l]do
// for each subsequence of lof length dfind subsets of pthat
relax replay wand satisfy some additional constraints ac
(e.g., compactness)
3 lpms← {sp′|p′⊆p∧w∈ lrlx(sp′)∧ac(sp′, w)};
4 lpm ←lpm ∪lpms ; // add lpms to the global storage
definition 12 (sliding window). given a trace ρ=⟨a1, a2, ..., a n⟩and local-
ityd >0, we define the function wd(i, ρ) =ρ[i, i+d−1],if1≤i≤n−d+ 1
⟨⟩, otherwise
to be a sliding window . each generated subsequence for a concrete iandρwe
call a window .
the sliding window helps us to iterate the event log, and focus on a local level.
however, for each window, we need to efficiently and exhaustively (consider-
ing our representational bias and limitations) combine places into local process
models that can relax replay that window.
building local process models for one window. at this point, we have our
set of place nets p={np1, np2, . . . , n pk}and a sequence of activities, i.e., our
window w. our goal is to find subsets of p,p′⊆p, to form local process models,
lpm =sp′, that satisfy definition 6 considering the following constraints:
–lpm canrelax replay w(w∈ lrlx(lpm ))
–lpm iscompact in regard to w(see definition 11)
additionally, we want to be time efficient. therefore, given a trace
ρ=⟨a1, a2, . . . , a n⟩we consider that two consecutive windows wd(m, ρ) and
wd(m+ 1, ρ), share d−1 of their elements. the models found for this over-
lapping sequence shared by both windows, are the same. hence, it is important
that we do not recalculate these models, which in turn defines the goal to reuse
local process models shared between consecutive windows.
idea. the core idea is to create new local process models by extending exist-
ing ones with an additional place net such that the relaxed subsequence of the
window that they can replay increases in length. we start with the empty local
process model that can somehow replay the empty trace, and we want to extend
it with carefully selected place nets such that two activities from the window can
be replayed. in the next step, we would extend those local process models by
adding an additional place net such that the newly created local process models
can replay three of the activities in the window. we continue as long as there10 v. peeva et al.
(a) existing local process model lpm
 (b) new local process model lpm ∪np
fig. 3. extension of a local process model with a new place net.
are still unprocessed activities in the window. for example, let us consider the
window w7(4, ρ) in figure 3a where ρis a trace. we have built a local process
model lpm such that ⟨a4, a6, a7⟩ ∈ l (lpm )2. since ⟨a4, a6, a7⟩⊏
ew7(4, ρ),
w7(4, ρ)∈ lrlx(lpm ). we now want to extend lpm with an additional place
netnp= ({p}, tp, fp, ap, lp). what is specific for npis that it should be able to
replay a7such that a token is put in p, and also replay one of the unprocessed
activities ( a8,a9ora10) such that the token is removed from p. hence, the newly
built local process model lpm ∪npis empty after replaying four activities from
the window, and the used firing sequence is an extension of the firing sequence
used for replaying ⟨a4, a6, a7⟩onlpm . in our case the new activity is a9and
we visualize this in figure 3b. to know whether we can extend lpm with np
we have to check that we do not break the replay of ⟨a4, a6⟩. hence, we need
the firing sequence σfor which we replayed ⟨a4, a6⟩, to ensure that σcan still
fire when the new place net is added. to know where to connect the place net
and the local process model such that a7can be replayed we need the marking
mafter firing σi.e., [ ]σ− →m. at the end, we also store the two indices indin
andindout in the window for which the last extension happened. note that
σ↾a⊏
ew[1, indin ] and w[1, indout ]∈ lrlx(lpm ).
algorithm. we now present an algorithm that builds local process models given
a set of place nets pand a window w. we explained that at every step we
extend existing local process models with new place nets. to be aware of the
extension path from which we got to a particular local process model and how
to continue extending it, we organize the local process models in a tree structure
that we call local tree . each node in the local tree represents a local process
model lpm = (p, t, f, a, l ) that can relax replay w. there is an edge between
two nodes nandn′when the local process model represented by n′was built by
extending the local process model in nwith an additional place net. we formally
define our local tree in definition 13.
definition 13 (local tree). alocal tree lt= (n, e )is a pair, where nis
a set of nodes and ea set of edges such that:
–a node n = ( lpm, σ, m, indin, indout )is a tuple, where
lpm = (p, t, f, a, l )is a local process model, σ∈t∗is a sequence
2note that this doesn’t have to be the only one such local process model.from place nets to local process models 11
of transitions, m∈m(p)is the marking [ ]σ− →m, and indin, indout ∈n
are the indices for which the last extension happened.
–an edge e= (n, n′)is a pair of nodes.
in algorithm 2 we give the pseudo-code of the entire procedure. as input we
are given the set of place nets pand the window w. we start by initializing the
local tree to contain only a root node that represents the empty local process
model (line 1). then we traverse all activity pairs of the window, and for each
pair, we extend existing local process models in ltwith additional place nets.
we get suitable place nets by filtering those that can replay the currently consid-
ered two events w[i] and w[j], and suitable nodes by filtering those that contain
a local process model in a marking in which w[i] can be replayed (lines 5 and 6).
afterward, we restrict that local process models are extended with a place net
only if the place net does not add a new constraint on an already used transition
(line 9). then, we find a common transition of the place net and the local pro-
cess model that can replay w[i]. if there are no such transitions and the node is
not the root, the extension can not happen (line 12). if there are multiple such
transitions we randomly choose one (line 14). we create a new node n′(line 15)
that represents the local process model built by adding the place net npto the
local process model in the node n, in a marking after replaying w[i]. we add the
newly created node in the tree and connect it with the node from which it was
created (lines 16 and 17). we finish by adding the local process model to the
final set if after replay of w[j] we end in the empty marking (lines 18 and 19).
fulfillment of constraints and goals. in the following, we give some intuitions
that connect the design of the algorithm to the constraints and the goal. the
first constraint is that each returned local process model satisfies definition 6. in
thepaf module we assumed thatspis a valid union. hence, the union of any
subset p′⊆pis also a valid union. that the local process model is connected is
satisfied by requiring t′̸=∅when the place net we add is not the first in the local
process model (line 12). the constraint that each created local process model
can relax replay wis satisfied by combining lines 5, 6, 9 and 18. the filterings
of the nodes and place nets, ensure that a local process model is extended with
a new place net only when the newly created local process model replays one
more activity of the window than its base local process model. in line 9 we make
sure we do not break the successful replay of the base local process model, and
with line 18 we make sure that there is at least one unprocessed activity in the
window, after whose replay the local process model ends in an empty marking.
definition 11 is also satisfied because of the filtering in line 5. a place net is added
to a local process model only if a token can be put in the place it represents
and removed from it, by replaying two activities. therefore each place is marked
at some point of the replay. finally, our goal to reuse local process models
between consecutive windows, is satisfied by the way we organize our nodes in
the tree, i.e., how we create edges (line 17). given two nodes nandn′such that
(n, n′)∈lt.e , we know that n.lpm.l (n.σ) =n′.lpm.l (n′.σ)[1,|n′.σ| −1].
hence, the most distant ancestor of n′apart from the root, is some node n∗that12 v. peeva et al.
algorithm 2: building local process models for a window
input : w- window; p- set of place nets;
output: lpms - set of local process models
// in the pseudo-code we use a dot notation for accessing elements
of an object, similar as in object-oriented programming.
1lt←(n={root = (∅,⟨⟩,[ ],0,0)}, e=∅); // initialize the storage
2d=|w|; // length of the window
3forj←1toddo
4 fori←1toj−1do
// for each pair of events get suitable place nets and nodes
5 p′← {np∈p|⟨w[i], w[j]⟩ ∈ l (np)};
6 n′← {n∈n|n.lpm.l (n.σ)↾n.lpm.a ·w[i]∈ l(n.lpm )} ∪ {root}
// try to extend lpm in each node with each place net
7 forn= (lpm, σ, m, indin, indout )∈n′do
8 fornp= ({p}, tp, fp, ap, lp)∈p′do
9 ifp• ∩{σ} ̸=∅then
10 continue ; // no new constraint
11 t′← {t′∈lpm.t ∩ •p|lpm.m [t′⟩ ∧lpm.l (t′) =w[i]};
12 ifn̸=root∧t′=∅then
13 continue ; // no common transition
14 t←rt′// choose any transition
15 n′←(lpm ∪np, σ·t,(m\ •t)∪t•, i, j); // create node
16 lt.n ←lt.n ∪n′; // add node
17 lt.e ←lt.e∪(n, n′); // add edge
// add n′.lpm in final set if w∈ lrlx(n′.lpm )
18 if∃t∈n′.lpm.t (n′.mt− →[ ]∧n′.lpm.l (t) =w[j])then
19 lpms ←lpms ∪ {n′.lpm };
20return lpms
is a child of the root. then, n∗.σ=⟨n′.σ[1]⟩. therefore, if we want to remove
all local process models that replay w[1], we just need to remove all children of
the root n∗for which n∗.lpm.l (n∗.σ[1]) = w[1] (have in mind that |n∗.σ|= 1
for the children of the root).
example. to clarify how the algorithm works given its input, we additionally pro-
vide an example. for simplicity we assume that t=l(t) for each transition. given
the set of place nets p(see figure 4a) and the window w=⟨b, a, x, a, d ⟩we build
local process models by following algorithm 2. we first initialize the local tree
lt= ({root},∅) and the resulting set lpms =∅. then we iterate through the
window with the indices iandj. we start with i= 1 and j= 2. since w[i] =b
andw[j] =a, we get p′=∅so we continue. for i= 1 and j= 3 ( w[i] =b
andw[j] =x) we filter p′={np2},n′={root}. since p2• ∩{root.σ }=∅
andt′={b}, we create the node n1 = ({np2},⟨b⟩,[p21],1,3) and add it as
child to the root node ( lt= ({root, n 1},{(root, n 1)})). because [ p21]x− →[ ] and
w[j] =xwe add the local process model to the final set ( lpms ={{np2}}).
we skip i= 2, j= 3 and all pairs for j= 4, since p′=∅for them. for i= 1from place nets to local process models 13
p1b d
p2bx
y
p3x
yd
(a) example place nets
lpm1:
b d
p1
lpm2:
bx
y p2
lpm3:x
yd
p3
lpm4:
bx
yd
p2 p3
(b) found local process models
fig. 4. place nets and local process models for the example
andj= 5 we calculate p′={np1},n′={root}. given p1• ∩{root.σ }=∅and
t′={b}, we create the node n2 = ({np1},⟨b⟩,[p11],1,5), add it as child to
the root node and to the final set ( lt= ({root, n 1, n2},{(root, n 1),(root, n 2)})
and lpms ={{np2},{np1}}). we again skip i= 2, j= 5
since p′=∅. for i= 3 and j= 5 we calculate p′={np3},
n′={root, n 1}. for np3androot,p3• ∩{root.σ }=∅andt′={x}so we
create n3 = ({np3},⟨x⟩,[p31],3,5). for np3and n1,p3• ∩{n1.σ}=∅and
t′={x}so we create n4 = (s{np2, np3},⟨b, x⟩,[p31],3,5). we add n3
as child to the root node and n4 as child to n1. our local tree now is
lt= ({root, n 1, n2, n3, n4},{(root, n 1),(root, n 2),(root, n 3),(n1, n4)}) and fi-
nal set lpms ={{np2},{np1},{np3},s{np2, np3}}. we do nothing for i= 4
andj= 5 since p′=∅. the final set lpms is given in figure 4b.
choice and concurrency. after processing the window w, the tree contains all
local process models lpm for which w∈ lrlx(lpm ) and given the used firing
sequence σit holds that ∀t∈σ(mt− →m′=⇒m∩m′=∅), i.e., concurrency
is not considered. to build the concurrency constructs, we combine nodes from
different branches in the local tree and take the union of the local process mod-
els that the nodes contain. the number of transitions that can be concurrent
directly depends of the number of nodes we combine. to avoid an explosion of
possibilities, the number of concurrent transitions can not be too large since
we try all possible node combinations. because of the place nets, the choice
construct is embedded in our input, so no additional processing is needed.
silent and duplicate transitions. the presented algorithm handles the duplicate
transitions as all other transitions. however, in the case of silent transitions we
convert the set of place nets to a set of paths. each path is a valid and connected
union of one or multiple place nets connected via silent transitions. then, on14 v. peeva et al.
root
(empty)0
place net 1
(count)11
place net 2
(count)33
place net 3
(count)44place net 2
(count)22
place net 3
(count)55
fig. 5. global tree structure. with red, we denote the place nets in the global tree,
blue the local process models, and the count in green is for the number of windows the
local process model (starting in the root and ending in that node) can relax replay.
line 5 we check whether the sequence consisted of the two activities, is in the
language of the path and the path is compact for the sequence.
collecting local process models on a global level. the local process models
we want to store in the global storage are just sets of place nets. hence, to
represent them efficiently, we use a tree structure as shown in figure 5. every
node in the tree stores one place net. at the same time each node also represents
exactly one local process model by taking the union of the place nets in the path
from that node to the root. hence, in each node we also keep the number of
windows the corresponding local process model can relax replay. any additional
information about the local process model that we might want to store in the
future, can be stored in the same way as the relax replay count.
structuring the tree this way we share place nets between the stored local
process models. to also make the structure efficiently extendable, we want each
path in the tree to represent a unique local process model. therefore, we in-
troduce a rank function. the rank function rank :p7→ngives priority to each
place net which in turn determines the order in which the place nets appear in
the tree path representing the local process model. in figure 6 we illustrate the
problem when a local process models{np1, np2}needs to be added to the tree
in figure 6a.
after processing each window, we add all discovered local process models to
the global storage. at the end, after processing all windows, the tree will contain
each local process model we find together with the number of windows each local
process model can relax replay.
4.3 local process models evaluation and ranking (lpms-er)
our exhaustive search can end up in a large number of local process models.
hence, we need to limit the number of local process models we return and first
show the ones we classify as more relevant. one simple restriction is to limit thefrom place nets to local process models 15
root
np1 np2
(a) initial global tree
root
np1
np2np2 (b) adding np2tonp1
root
np1 np2
np1 (c) adding np1tonp2
fig. 6. difficulty in the global tree for adding the same local process models multiple
times when we do not use a rank function.
minimal and maximal number of places and transitions a local process model
can have.
to measure the quality of our local process models we propose different
heuristics. all metrics are calculated for an event log l, a local distance d, and a
local process model lpm = (p, t, f, a, l ). with wl= [w∈s
ρ∈l[ρ′⊑dρ]]
we define the multiset of all windows in lwith length d, and with
sl
lpm = [s∈[w′⊏
ew]|w∈wl∧s∈ l(lpm )] a multiset of the sequences re-
played by lpm during relax replay of the windows.
–fitting windows evaluation calculates the fraction of windows a local process
model can relax replay (equation (1)).
fw(lpm, l ) =|{w∈wl|w∈lrlx(lpm )}|
|wl|(1)
this metric is in a way an adaptation for calculating fitness for the local
process models. we never expect one local process model to explain the
entire event log, so to make the metric comparable, we compare the values
to the best scoring local process model.
–passage coverage evaluation calculates the fraction of the passages used in
the relax replay of the fitting windows (equation (2)).
pc(lpm, l ) =|{(t1,t2)∈lpm|∃s∈sl
lpm(∃i∈{1,...,|s|−1]}(si=l(t1)∧si+1=l(t2)))}|
|lpm|(2)
the values are in the interval (0 ,1], where we get 1 when all the local process
model passages are used at least once. this metric is similar to precision since
lower values mean that the local process model allows more behavior than
seen in the event log.
–passage repetition evaluation calculates whether multiple place nets of the
local process model contain the same passages (equation (3)). we define
#(t1,t2)=|{p∈p|(t1, t2)∈ •p×p•}to be the number of place nets in lpm
that have the passage ( t1, t2).
pr(lpm, l ) =|lpm|·|lpm|−p
(t1,t2)∈lpm#(t1,t2)
|lpm|·|lpm|−|lpm|(3)16 v. peeva et al.
table 1. information about the event logs used in our analysis
event log alias trace variants count activities count total event count
bpic2012 [9] 4366 24 182467
bpic2019 [10] 11973 42 338247
rtfm [14] 231 13 2353
sepsis [16] 846 16 13775
artificial small 2 7 45
artificial big 96 13 1624
this metric tries to express the simplicity of the local process model. the
value of 1 denotes that each passage is contained by only one place net, and
0 denotes that all passages are contained in all place nets.
–transition coverage evaluation calculates in how many of the relax replayed
windows in which a transition tcan be used, that transition is actually
used during the replay. the average value over all transitions is returned.
(equation (4)).
tc(lpm, l ) =1
|t|·x
t∈t|[s∈sl
lpm|∃i∈{1,...,|s|}(s[i]=l(t))]|
|[w∈wl|w∈lrlx(lpm )∧∃i∈{1,...,|w|}(w[i]=l(t))]|(4)
the values for the metric are in the interval (0 ,1]. low values indicate that
we use only a few transitions in our local process model during the relax
replay, meaning our model is more complex than necessary.
we finish by ranking the found local process models, by taking the average
score of the presented evaluation metrics. the higher the average score, the
better the rank of the local process model.
5 evaluation and results
in this section, we evaluate our method on real and artificial event logs (see
table 1). we split the evaluation into several parts. we start by discussing how
quality is defined and measured for local process models, and the challenges
around it. then, we compare the results of our algorithm with several process
discovery approaches and presented related work on a specific event log. after-
ward, we present the running time our algorithm has on different event logs, and
the effect different parameters have on it. we end the evaluation section by com-
paring the running time with the running time of existing approaches discussed
in related work [2,24]. for all experiments, we use the plugin we implemented in
prom and the est miner [15] as a place oracle. to allow for reproducibility of
the experiments, we provide the artificial event logs and the sets of place nets
we use at https://github.com/vikipeeva/placesandeventlogs.
5.1 quality definition and challenges
calculating the quality of local process models is challenging because of all the
different ways it can be looked at. from one side we can look at the qualityfrom place nets to local process models 17
of each individually returned local process model or the quality of all of them
as a group. if we use local process model discovery when traditional process
discovery fails, the desired result would be a minimal set of local process models
that cover the entire event log with as little overlaps between them as possible.
this is discussed in [5] where one event log is analyzed by hand and compared
to the results from [24]. both [24] and [2], nor their future work offer this as a
possibility and neither our algorithm. however, as discussed in the introduction,
that is not the only usage of local process models. if we are interested in what
happens when patients are cured, when companies lose money, when employees
resign, etc., then we might be interested in finding local process models in regard
to some utility functions or different contexts. this is to some degree investigated
in [22] and [3] accordingly. our work, currently does not support this type of
local process mining, however, it is orthogonal to the current work, and can be
integrated in the algorithm. with the previous information in mind we see how
challenging is to give quality comparison on hundreds returned local process
models between different approaches, especially when the most straight-forward
comparison - event log coverage - is not available for any of them. hence, for us,
the goal was the new approach we propose to be more feasible than the existing
ones in regard to running time and number of local process models found, and
extendable towards event log coverage and utility mining.
5.2 discoverability of constructs
to illustrate the need for local process model discovery, and why the approaches
proposed in [24] and [2] are not enough, we give an event log whose traces are
generated by repeating the pattern axdbxe . we additionally add noise (from
the alphabet l,m,n) between the different occurrences of the pattern and in
smaller amount in-between the pattern itself. an example trace in such event
log would be ⟨m, a, x, n, d, b, x, e, l ⟩. we ran the generated event log with
α++ miner [26], inductive miner [13], ilp miner [27], the local process model
discovery approaches proposed in [24] and [2], and the approach proposed in
this paper. we present the results we get in figure 7. ilp and α++ miner
returned a spaghetti-like models, while the inductive miner returned a model
with mostly flowery behavior. in none of these models the pattern is clearly
visible. the approaches in [24] and [2] although returning local process models
that represent parts of the pattern, are not able to return a local process model
that describes the pattern accurately. in contrast, our algorithm finds a local
process model that completely describes the pattern (figure 1b) in addition to
the other local process models that we find. by finding this model we show that
we are able to skip in-between noise, and that we can discover constructs like
long-term dependencies which the approaches in [2] and [24] cannot because of
the representational bias of process trees.18 v. peeva et al.
(a)α++ miner
 (b) inductive miner
 (c) ilp miner
(d) approach in [24]
(e) approach in [2]
fig. 7. process models for an event log focusing on the pattern axdbxe .
5.3 running time vs parameters
the main parameters that we can control are the number of place nets we use
and the size of the local distance. other important parameter is the cardinality
of the concurrency, i.e, what is the maximal number of transitions we allow to be
in a concurrent construct. hence, we show diagrams to see how these parameters
affect the running time of the algorithm.
in figure 8, we show the running time for place net counts of [50 ,75] and
locality of [5 ,7,10,12]. as expected, given a fixed amount of place nets used, the
running time increases as the locality increases, and also the other way around,
given a fixed locality, the running time increases as the number of place nets
used increases. we can notice that for 50 place nets the algorithm finishes in less
than five minutes for all event logs and different localities except for bpic2019
and locality 12. however, when considering 75 place nets, for all event logs
except artificial small the limit of ten minutes is reached at locality 12. what
is interesting to see is that both artificial big andrtfm have a larger running
time for place net count of 75 and localities 5, 7 and 10 than bpic2012 and
bpic2019 although the latter are much larger event logs, both in the number
of events and number of activities they contain (see table 1). this shows the
impact the linearity of our algorithm has in regard to the size of the event log,
and the importance of how we choose which place nets to use.
regarding our concurrency cardinality parameter, we see in figure 9 that
we are able to handle concurrency constructs with 4 transitions for 50 place
nets, and 3 transitions for 75 place nets. however, we notice that by adding the
possibility for just one more transition, the running time exceeds 10 minutes.from place nets to local process models 19
fig. 8. diagram that shows the effect different settings for the count of place nets and
local distance have on the running time.
fig. 9. diagram that shows how the concurrency cardinality parameter affects the
running time.
5.4 comparison to other approaches
in this section, we focus on the comparison of our approach to the ones presented
in [24] and [2] in regard to the running time. we run all algorithms on real and
artificial event logs with time limit of 10 minutes on a pc with i7-1.8ghz, 16gb
ram and windows 10. we use the provided default settings of the plugins where
for the approach of tax et al. the default settings also include the log projections
explained in [23]. the only setting we vary for our algorithm is the number of
places used (50, 75 and 100). we present the results at table 2. we see that for
the artificial event logs our approach is comparable in the time needed to return
results to the one in [24] when we use 50 places. however, when it comes to
real event logs, our approach is notably faster than the other two. for example,
on the bpic2019 event log the other approaches do not return results at all
because of memory problems, while we are able to build a large amount of local20 v. peeva et al.
table 2. results comparison to [24] and [2]
event logour approach approach in [24] approach in [2]
#places runtime #lpms runtime #lpms runtime #lpms
bpic201250 4s 284
90s 454 out of time 75 20s 2473
100 23s 6484
bpic201950 28s 3190
out of memory out of memory 75 48s 7617
100 out of time /
rtfm50 15s 8967
out of time out of time 75 368s 90862
100 out of time /
sepsis50 2s 18
56s 4627 125s 375 75 4s 3384
100 22s 14951
artificial big50 40s 8979
70s 56110 out of time 75 536s 65383
100 out of time /
artificial small50 2s 5123
2s 2665 16s 126 75 9s 15623
100 25s 34844
process models in less than a minute when we use less than 75 places. for the
bpic2012 event log, [2] needs more than 10 minutes to return results and [24]
investigates 454 candidate local process models in 90 seconds. this is less than
what we can discover and it needs four times more time than our approach. the
sepsis andrtfm event logs further confirm these results, which shows that our
algorithm is able to handle large event logs much better, while returning a large
amount of local process models.
6 conclusion and outlook
in this paper, we introduced a novel way of discovering local process models. we
proposed a first solution to the problem, which can be further investigated and
extended. our first goal was to have an algorithm that can find local process
models for large event logs, and we achieved this by building local process models
through one pass of the event log. different quality dimensions that we discussed
are returning minimal number of local process models that cover the entire event
log or mining using utility functions. these are compelling directions that we
plan to investigate as future work. another point is that we get the place nets
from which we build local process models from an oracle which currently is a
regular process discovery algorithm. hence, how to generate place nets valuable
for local process model discovery or build the local process models without using
place nets is something that warrants further research. the algorithm we propose
is able to process large event logs and is flexible to support improvements for
the above mentioned topics without destroying the linear complexity on the size
of the event log.
acknowledgments: we thank the alexander von humboldt (avh) stiftung for
supporting our research.from place nets to local process models 21
references
1. van der aalst, w.m.p.: process mining - data science in action, second edition.
springer (2016). https://doi.org/10.1007/978-3-662-49851-4
2. acheli, m., grigori, d., weidlich, m.: efficient discovery of compact maximal be-
havioral patterns from event logs. in: giorgini, p., weber, b. (eds.) advanced infor-
mation systems engineering - 31st international conference, caise 2019, rome,
italy, june 3-7, 2019, proceedings. lecture notes in computer science, vol. 11483,
pp. 579–594. springer (2019). https://doi.org/10.1007/978-3-030-21290-2 36
3. acheli, m., grigori, d., weidlich, m.: discovering and analyzing contextual be-
havioral patterns from event logs. ieee transactions on knowledge and data
engineering (2021). https://doi.org/10.1109/tkde.2021.3077653
4. bergenthum, r., desel, j., lorenz, r., mauser, s.: process mining based on re-
gions of languages. in: alonso, g., dadam, p., rosemann, m. (eds.) business pro-
cess management, 5th international conference, bpm 2007, brisbane, australia,
september 24-28, 2007, proceedings. lecture notes in computer science, vol. 4714,
pp. 375–383. springer (2007). https://doi.org/10.1007/978-3-540-75183-0 27
5. brunings, m., fahland, d., van dongen, b.f.: defining meaningful local process
models. in: van der aalst, w.m.p., bergenthum, r., carmona, j. (eds.) proceed-
ings of the international workshop on algorithms & theories for the analysis of
event data 2020 satellite event of the 41st international conference on applica-
tion and theory of petri nets and concurrency petri nets 2020, virtual workshop,
june 24, 2020. ceur workshop proceedings, vol. 2625, pp. 6–19. ceur-ws.org
(2020)
6. carmona, j., cortadella, j., kishinevsky, m.: a region-based algorithm for discov-
ering petri nets from event logs. in: dumas, m., reichert, m., shan, m. (eds.) busi-
ness process management, 6th international conference, bpm 2008, milan, italy,
september 2-4, 2008. proceedings. lecture notes in computer science, vol. 5240,
pp. 358–373. springer (2008). https://doi.org/10.1007/978-3-540-85758-7 26
7. deeva, g., weerdt, j.d.: understanding automated feedback in learning pro-
cesses by mining local patterns. in: daniel, f., sheng, q.z., motahari, h.
(eds.) business process management workshops - bpm 2018 international work-
shops, sydney, nsw, australia, september 9-14, 2018, revised papers. lecture
notes in business information processing, vol. 342, pp. 56–68. springer (2018).
https://doi.org/10.1007/978-3-030-11641-5 5
8. delcoucq, l., lecron, f., fortemps, p., van der aalst, w.m.p.: resource-centric
process mining: clustering using local process models. in: hung, c., cern´ y, t., shin,
d., bechini, a. (eds.) sac ’20: the 35th acm/sigapp symposium on applied
computing, online event, [brno, czech republic], march 30 - april 3, 2020. pp.
45–52. acm (2020). https://doi.org/10.1145/3341105.3373864
9. van dongen, b.f.: bpi challenge 2012 (2012)
10. van dongen, b.f.: bpi challenge 2019 (2019)
11. kirchner, k., markovic, p.: unveiling hidden patterns in flexible medical treat-
ment processes - a process mining case study. in: dargam, f.c.c., delias, p.,
linden, i., mareschal, b. (eds.) decision support systems viii: sustainable
data-driven and evidence-based decision support - 4th international confer-
ence, icdsst 2018, heraklion, greece, may 22-25, 2018, proceedings. lecture
notes in business information processing, vol. 313, pp. 169–180. springer (2018).
https://doi.org/10.1007/978-3-319-90315-6 1422 v. peeva et al.
12. leemans, s.j.j., tax, n., ter hofstede, a.h.m.: indulpet miner: combining dis-
covery algorithms. in: panetto, h., debruyne, c., proper, h.a., ardagna, c.a.,
roman, d., meersman, r. (eds.) on the move to meaningful internet systems.
otm 2018 conferences - confederated international conferences: coopis, c&tc,
and odbase 2018, valletta, malta, october 22-26, 2018, proceedings, part i.
lecture notes in computer science, vol. 11229, pp. 97–115. springer (2018).
https://doi.org/10.1007/978-3-030-02610-3 6
13. leemans, s., fahland, d., aalst, w.: discovering block-structured process mod-
els from event logs: a constructive approach. in: colom, j.m., desel, j. (eds.)
applications and theory of petri nets 2013. lncs, vol. 7927, pp. 311–329. springer
(2013). https://doi.org/10.1007/978-3-642-38697-8 17
14. de leoni, m.m., mannhardt, f.: road traffic fine management process (2015)
15. mannel, l.l., van der aalst, w.m.p.: finding complex process-structures by ex-
ploiting the token-game. in: donatelli, s., haar, s. (eds.) application and theory of
petri nets and concurrency - 40th international conference, petri nets 2019,
aachen, germany, june 23-28, 2019, proceedings. lecture notes in computer sci-
ence, vol. 11522, pp. 258–278. springer (2019). https://doi.org/10.1007/978-3-030-
21571-2 15
16. mannhardt, f.: sepsis cases - event log (dec 2016).
https://doi.org/10.4121/uuid:915d2bfb-7e84-49ad-a286-dc35f063a460
17. mannhardt, f., de leoni, m., reijers, h.a., van der aalst, w.m.p., toussaint, p.j.:
guided process discovery - a pattern-based approach. inf. syst. 76, 1–18 (2018).
https://doi.org/10.1016/j.is.2018.01.009
18. mannhardt, f., tax, n.: unsupervised event abstraction using pattern abstrac-
tion and local process models. in: gulden, j., nurcan, s., reinhartz-berger, i.,
gu´ edria, w., bera, p., guerreiro, s., fellmann, m., weidlich, m. (eds.) joint
proceedings of the radar tracks at the 18th international working confer-
ence on business process modeling, development and support (bpmds), and
the 22nd international working conference on evaluation and modeling meth-
ods for systems analysis and development (emmsad), and the 8th interna-
tional workshop on enterprise modeling and information systems architectures
(emisa) co-located with the 29th international conference on advanced infor-
mation systems engineering 2017 (caise 2017), essen, germany, june 12-13,
2017. ceur workshop proceedings, vol. 1859, pp. 55–63. ceur-ws.org (2017),
http://ceur-ws.org/vol-1859/bpmds-06-paper.pdf
19. mannila, h., toivonen, h., verkamo, a.: discovery of frequent episodes in event
sequences. data mining and knowledge discovery 1(3), 259–289 (1997)
20. pijnenborg, p., verhoeven, r., firat, m., laarhoven, h.v., genga, l.: to-
wards evidence-based analysis of palliative treatments for stomach and
esophageal cancer patients: a process mining approach. in: 2021 3rd in-
ternational conference on process mining (icpm). pp. 136–143 (2021).
https://doi.org/10.1109/icpm53251.2021.9576880
21. srikant, r., agrawal, r.: mining sequential patterns: generalizations and perfor-
mance improvements. in: apers, p.m.g., bouzeghoub, m., gardarin, g. (eds.)
advances in database technology - edbt’96, 5th international conference on
extending database technology, avignon, france, march 25-29, 1996, proceed-
ings. lecture notes in computer science, vol. 1057, pp. 3–17. springer (1996).
https://doi.org/10.1007/bfb0014140from place nets to local process models 23
22. tax, n., dalmas, b., sidorova, n., van der aalst, w.m.p., norre, s.: interest-
driven discovery of local process models. inf. syst. 77, 105–117 (2018).
https://doi.org/10.1016/j.is.2018.04.006
23. tax, n., sidorova, n., van der aalst, w.m.p., haakma, r.: heuristic approaches for
generating local process models through log projections. in: 2016 ieee symposium
series on computational intelligence, ssci 2016, athens, greece, december 6-9,
2016. pp. 1–8. ieee (2016). https://doi.org/10.1109/ssci.2016.7849948
24. tax, n., sidorova, n., haakma, r., van der aalst, w.m.p.: mining
local process models. j. innov. digit. ecosyst. 3(2), 183–196 (2016).
https://doi.org/10.1016/j.jides.2016.11.001
25. verbeek, e., buijs, j.c.a.m., van dongen, b.f., van der aalst, w.m.p.: prom
6: the process mining toolkit. in: rosa, m.l. (ed.) proceedings of the business
process management 2010 demonstration track, hoboken, nj, usa, september
14-16, 2010. ceur workshop proceedings, vol. 615. ceur-ws.org (2010), http:
//ceur-ws.org/vol-615/paper13.pdf
26. wen, l., van der aalst, w.m.p., wang, j., sun, j.: mining process models
with non-free-choice constructs. data min. knowl. discov. 15(2), 145–180 (2007).
https://doi.org/10.1007/s10618-007-0065-y
27. van zelst, s.j., van dongen, b.f., van der aalst, w.m.p., verbeek, h.m.w.: dis-
covering workflow nets using integer linear programming. computing 100(5), 529–
556 (2018). https://doi.org/10.1007/s00607-017-0582-5