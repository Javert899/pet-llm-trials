discovery of frequent episodes in event logs
maikel leemans and wil m.p. van der aalst
eindhoven university of technology, p.o. box 513, 5600 mb, eindhoven,
the netherlands. m.leemans@tue.nl,w.m.p.v.d.aalst@tue.nl
abstract. lion's share of process mining research focuses on the discov-
ery of end-to-end process models describing the characteristic behavior of
observed cases. the notion of a process instance (i.e., the case) plays an
important role in process mining. pattern mining techniques (such as frequent
itemset mining, association rule learning, sequence mining, and traditional
episode mining) do not consider process instances. an episode is a collection
of partially ordered events. in this paper, we present a new technique (and
corresponding implementation) that discovers frequently occurring episodes
in event logs thereby exploiting the fact that events are associated with cases.
hence, the work can be positioned in-between process mining and pattern
mining. episode discovery has its applications in, amongst others, discovering
local patterns in complex processes and conformance checking based on
partial orders. we also discover episode rules to predict behavior and discover
correlated behaviors in processes. we have developed a prom plug-in that
exploits ecient algorithms for the discovery of frequent episodes and episode
rules. experimental results based on real-life event logs demonstrate the
feasibility and usefulness of the approach.
1 introduction
process mining provides a powerful way to analyze operational processes based on
event data. unlike classical purely model-based approaches (e.g., simulation and
verication), process mining is driven by \raw" observed behavior instead of assump-
tions or aggregate data. unlike classical data-driven approaches, process mining is
truly process-oriented and relates events to high-level end-to-end process models [ 1].
in this paper, we use ideas inspired by episode mining [ 2] and apply these to the dis-
covery of partially ordered sets of activities in event logs .event logs serve as the starting
point for process mining. an event log can be viewed as a multiset of traces [1]. each
trace describes the life-cycle of a particular case(i.e., a process instance ) in terms of the
activities executed. often event logs store additional information about events, e.g.,
theresource (i.e., person or device) executing or initiating the activity, the timestamp
of the event, or data elements (e.g., cost or involved products) recorded with the event.
each trace in the event log describes the life-cycle of a case from start to completion.
hence, process discovery techniques aim to transform these event logs into end-to-end
process models . often the overall end-to-end process model is rather complicated be-
cause of the variability of real life processes. this results in \spaghetti-like" diagrams.
therefore, it is interesting to also search for more local patterns in the event log { using
episode discovery { while still exploiting the notion of process instances. another useful
application of episode discovery is conformance checking based on partial orders [ 3].since the seminal papers related to the apriori algorithm [ 4,5,6], many pattern
mining techniques have been proposed. these techniques do not consider the ordering
of events [4] or assume an unbounded stream of events [5, 6] without considering
process instances. mannila et al. [ 2] proposed an extension of sequence mining [ 5,6]
allowing for partially ordered events. an episode is a partially ordered set of activities
and it is frequent if it is \embedded" in many sliding time windows. unlike in [ 2], our
episode discovery technique does not use an arbitrary sliding window. instead, we
exploit the notion of process instances. although the idea is fairly straightforward,
as far as we know, this notion of frequent episodes was never applied to event logs.
numerous applications of process mining to real-life event logs illustrate that
concurrency is a key notion in process discovery [ 1,7,8]. one should avoid showing
all observed interleavings in a process model. first of all, the model gets too complex
(think of the classical \state-explosion problem"). second, the resulting model will
be overtting (typically one sees only a fraction of the possible interleavings). this
makes the idea of episode mining particularly attractive.
the remainder of this paper is organized as follows. section 2 positions the work in
existing literature. the novel notion of episodes and the corresponding rules are dened
in section 3. section 4 describes the algorithms and corresponding implementation in
the process mining framework prom . the approach and implementation are evaluated
in section 5 using several publicly available event logs. section 6 concludes the paper.
2 related work
the notion of frequent episode mining was rst dened by mannila et al. [ 2]. in their
paper, they applied the notion of frequent episodes to (large) event sequences. the
basic pruning technique employed in [ 2] is based on the frequency of episodes in an
event sequence. mannila et al. considered the mining of serial and parallel episodes
separately, each discovered by a distinct algorithm. laxman and sastry improved on
the episode discovery algorithm of mannila by employing new frequency calculation
and pruning techniques [ 9]. experiments suggest that the improvement of laxman
and sastry yields a 7 times speedup factor on both real and synthetic datasets.
related to the discovery of episodes or partial orders is the discovery of end-to-end
process models able to capture concurrency explicitly. the algorithm [10] was
the rst process discovery algorithm adequately handling concurrency. many other
discovery techniques followed, e.g., heuristic mining [ 11] able to deal with noise and
low-frequent behavior. the heuristicsminer is based on the notion of causal nets
(c-nets). several variants of the algorithm have been proposed [ 12,13]. moreover,
completely dierent approaches have been proposed, e.g., the dierent types of
genetic process mining [ 14,15], techniques based on state-based regions [16,17], and
techniques based on language-based regions [18,19]. another, more recent, approach
isinductive process mining where the event log is split recursively [ 20]. the latter
technique always produces a block-structured and sound process model. all the
discovery techniques mentioned are able to uncover concurrency based on example
behavior in the log. additional feature comparisons are summarised in table 1.
the episode mining technique presented in this paper is based on the discovery
of frequent item sets. a well-known algorithm for mining frequent item sets and
association rules is the apriori algorithm by agrawal and srikant [4]. one of thepitfalls in association rule mining is the huge number of solutions. one way of dealing
with this problem is the notion of representative association rules, as described by
kryszkiewicz [ 21]. this notion uses user specied constraints to reduce the number
of `similar' results. both sequence mining [ 5,6] and episode mining [ 2] can be viewed
as extensions of frequent item set mining.
exploits process instancesmines end-to-end modelsoundness guaranteedsequencechoiceconcurrencysilent (tau) transitionsduplicate activities
agrawal, sequence mining [4] --n.a. +----
manilla, episode mining [2] --n.a. +-+--
leemans m., episode discovery +-n.a. +-+-+
van der aalst, -algorithm [10] ++ -+++--
weijters, heuristics mining [11] ++ -+++--
de medeiros, genetic mining [14, 15] ++ -+++++
sol e, state regions [16, 17] ++ -+++--
bergenthum, language regions [18, 19] ++ -+++--
leemans s.j.j., inductive [20] +++++++-
table 1. feature comparison of discussed discovery algorithms
3 event logs, episodes, and episode rules
this section denes basic notions such as event logs, episodes and rules. note that
our notion of episodes is dierent from the notion in [2] which does not consider
process instances.
3.1 event logs
activities and traces letabe the alphabet of activities. a trace is a list (sequence)
t=ha1;:::;aniof activities ai2aoccurring at time index irelative to the other
activities in t.
event log an event log l= [t1;:::;tm] is a multiset of traces ti. note that the
same trace may appear multiple times in an event log. each trace corresponds to
an execution of a process, i.e., a case orprocess instance . in this simple denition
of an event log, an event refers to just an activity . often event logs store additional
information about events, such as timestamps .
3.2 episodes
episode an episode is a partial ordered collection of events. episodes are depicted using
the transitive reduction of directed acyclic graphs, where the nodes represent events,
and the edges imply the partial order on events. note that the presence of an edge
implies serial behavior. figure 1 shows the transitive reduction of an example episode.formally, an episode = (v;;g) is a triple, where vis a set of events (nodes), is
a partial order on v, andg:v7!a is a left-total function from events to activities,
thereby labelling the nodes/events [ 2]. for two vertices u;v2vwe haveu<v iu
vandu6=v. in addition, we dene gto be the multiset of activities/labels used: g=
[g(v)jv2v]. note that ifjvj1, then we got an singleton or empty episode. for the
rest of this paper, we ignore empty episodes. we call an episode parallel when=;.
a
(a1)
b
(b)c
(c)a
(a2)
d
(d)
fig. 1. shown is the transitive reduction of the partial order for an example episode. the
circles represent nodes (events), with the activity labelling imposed by ginside the circles,
and an event id beneath the nodes in parenthesis. in this example, events a1andbcan
happen in parallel (as can a2andd), but event ccan only happen after both a1andb
have occurred.
subepisode and equality an episode= (v0;0;g0) is a subepisode of = (v;;g),
denoted, i there is an injective mapping f:v07!vsuch that:
(8v2v0:g0(v) =g(f(v)))^(8v;w2v0^v0w:f(v)f(w))
an episode equals episode , denoted=i^. an episode
is a strict subepisode of , denoted, i^6=.
episode construction two episodes = (v;;g) and= (v0;0;g0) can be `merged'
to construct a new episode = (v;;g).is the smallest (i.e., smallest
setsvand) such that and. as shown below, such an episode 
always exists.
the smallest sets criteria implies that every event v2vand ordered pair
v;w2v^vwmust have a witness in and/or. formally, =i
there exists injective mappings f:v7!vandf0:v07!vsuch that:
g=g[g0activity witness
=f(f(v);f(w))j(v;w)2 g[f (f0(v);f0(w))j(v;w)2 0g order witness
occurrence an episode = (v;;g) occurs in an event trace t=ha1;:::;ani,
denotedvt, i there exists an injective mapping h:v7!f1;::;ngsuch that:
(8v2v:g(v) =ah(v))^(8v;w2v^vw:h(v)h(w))
in figure 2 an example of an \event to trace map" hfor occurrence checking is given.event indices:
episode:trace:
a (a1)
b
(b)c
(c)a(a2)
d
(d)a1
b2
a3
c4
a5
d6
mapping 1a (a1)
b
(b)c
(c)a(a2)
d
(d)a1
b2
a3
c4
a5
d6
mapping 2
fig. 2. shown are two possible mappings h(the dotted arrows) for checking occurrence of
the example episode in a trace. the shown graphs are the transitive reduction of the partial
order of the example episode. note that with the left mapping ( mapping 1 ) also an episode
with the partial order a1<b occurs in the given trace, in the right mapping ( mapping 2 )
the same holds for an episode with the partial order b<a 1.
frequency the frequency freq() of an episode in an event log l= [t1;:::;tm]
is dened as:
freq() =j[tijti2l^vti]j
jlj
given a frequency threshold minfreq , an episode is frequent i freq()
minfreq . during the actual episode discovery, we use the fact given in lemma 1.
lemma 1 (frequency and subepisodes). if an episode is frequent in an
event logl, then all subepisodes withare also frequent in l. formally, we
have for a given :
(8:freq()freq())
activity frequency the activity frequency actfreq (a) of an activity a2ain an
event logl= [t1;:::;tm] is dened as:
actfreq (a) =j[tijti2l^a2ti]j
jlj
given a frequency threshold minactfreq , an activity ais frequent i actfreq (a)
minactfreq .
trace distance given episode = (v;;g) occurring in an event trace t=
ha1;:::;ani, as indicated by the event to trace map h:v7!f1;::;ng. then the
trace distance tracedist (;t) is dened as:
tracedist (;t) = maxfh(v)jv2vg minfh(v)jv2vg
in figure 2, the left mapping yields tracedist (;t) = 6 1 = 5, and the right
mapping yields tracedist (;t) = 6 2 = 4.given a trace distance interval [ mintracedist ;maxtracedist ], an episode is
accepted in trace twith respect to the trace distance interval i mintracedist
tracedist (;t)maxtracedist .
informally, the conceptual idea behind a trace distance interval is that we are
interested in a partial order on events occurring relatively close in time.
3.3 episode rules
episode rule an episode rule is an association rule )withstating that
after seeing , then likely the larger episode will occur as well.
the condence of the episode rule )is given by:
conf()) =freq()
freq()
given a condence threshold minconf , an episode rule )is valid i
conf())minconf . during the actual episode rule discovery, we use lemma 2.
lemma 2 (condence and subepisodes). if an episode rule )is valid
in an event log l, then for all episodes 0with0the event rule 0)is
also valid in l. formally:
(80:conf())conf(0)))
episode rule magnitude let the graph size size() of an episode be denoted as the
sum of the nodes and edges in the transitive reduction of the episode. the magnitude
of an episode rule is dened as:
mag()) =size()
size()
intuitively, the magnitude of an episode rule )represents how much episode
`adds to' or `magnies' episode . the magnitude of an episode rule allows smart
ltering on generated rules. typically, an extremely low (approaching zero) or high
(approaching one) magnitude indicates a trivial episode rule.
4 realization
the denitions and insights provided in the previous section have been used to
implement a episode (rule) discovery plug-in in prom . to be able to analyze real-life
event logs, we need ecient algorithms. these are described next.
notation: in the listed algorithms, we will reference to the elements of an episode
= (v;;g) as:v,:and:g.4.1 frequent episode discovery
discovering frequent episodes is done in two phases. the rst phase discovers parallel
episodes (i.e., nodes only), the second phase discovers partial orders (i.e., adding the
edges). the main routine for discovering frequent episodes is given in algorithm 1.
algorithm 1: episodes discovery
input: an event log l, an activity alphabet a, a frequency threshold minfreq .
output: a set of frequent episodes  
description: two-phase episode discovery. each phase alternates by generating
new candidate episodes ( cl), and recognizing frequent candidates in the event
log (fl).
proof of termination: note that candidate episode generation with fl=;will
yieldcl=;. since each iteration the generated episodes become strictly larger
(in terms of vand), eventually the generated episodes cannot occur in any
trace. therefore, always eventually fl=;, and thus we will always terminate.
episodediscovery (l;a;minfreq )
(1) =;
(2) // phase 1: discover parallel episodes
(3)l= 1// tracks the number of nodes
(4)cl=f(v;=;;g=fv7!ag)jjvj= 1^v2v^a2ag
(5) whilecl6=;
(6) fl=recognizefrequentepisodes (l;cl;minfreq )
(7)  = [fl
(8) cl=generatecandidateparallel (l;fl)
(9) l=l+ 1
(10) // phase 2: discover partial orders
(11)l= 1// tracks the number of edges
(12)cl=f(v=:v;=f(v;w)g;g=:g)j2 ^v;w2:v^v6=wg
(13) whilecl6=;
(14) fl=recognizefrequentepisodes (l;cl;minfreq )
(15)  = [fl
(16) cl=generatecandidateorder (l;fl)
(17) l=l+ 1
(18) return 
4.2 episode candidate generation
the generation of candidate episodes for each phase is an adaptation of the well-known
apriori algorithm over an event log. given a set of frequent episodes fl, we can con-
struct a candidate episode by combining two partially overlapping episodes and
fromfl. note that this implements the episode construction operation =.
for phase 1, we have flcontains frequent episodes with lnodes and no edges.
a candidate episode will havel+ 1 nodes, resulting from episodes andthat
overlap on the rst l 1 nodes. this generation is implemented by algorithm 2.
for phase 2, we have flcontains frequent episodes with ledges. a candidate
episodewill havel+ 1 edges, resulting from episodes andthat overlap on the
rstl 1 edges and have the same set of nodes. this generation is implemented
by algorithm 3. note that, formally, the partial order is the transitive closure of
the set of edges being constructed, and that the edges are really only the transitive
reduction of this partial order.algorithm 2: candidate episode generation { parallel
input: a set of frequent episodes flwithlnodes.
output: a set of candidate episodes cl+1withl+ 1 nodes.
description: generates candidate episodes by merging overlapping episodes and(i.e.,
=). for parallel episodes, overlapping means: sharing l 1 nodes.
generatecandidateparallel (l;fl)
(1)cl+1=;
(2) fori= 0tojflj 1
(3) forj=itojflj 1
(4) =fl[i]
(5) =fl[j]
(6) if80il 2 ::g(:v[i]) =:g(:v[i])
(7) = (v= (:v[0::l 1][:v[l 1]);=;;g=:g[:g)
(8) cl+1=cl+1[fg
(9) else
(10) break
(11) returncl+1
algorithm 3: candidate episode generation { partial order
input: a set of frequent episodes flwithledges.
output: a set of candidate episodes cl+1withl+ 1 edges.
description: generates candidate episodes by merging overlapping episodes and(i.e.,
=). for partial order episodes, overlapping means: sharing all nodes and l 1 edges.
generatecandidateorder (l;fl)
(1)cl+1=;
(2) fori= 0tojflj 1
(3) forj=i+ 1tojflj 1
(4) =fl[i]
(5) =fl[j]
(6) if:v=:v^:g=:g^:[0::l 2] =:[0::l 2]
(7) = (v=:v;= (:e[0::l 1][:e[l 1]);g=:g)
(8) cl+1=cl+1[fg
(9) else
(10) break
(11) returncl+1
4.3 frequent episode recognition
in order to check if a candidate episode is frequent, we check if freq()minfreq .
the computation of freq() boils down to counting the number of traces twith
vt. algorithm 4 recognizes all frequent episodes from a set of candidate episodes
using the above described approach. note that for both parallel and partial order
episodes we can use the same recognition algorithm.
algorithm 4: recognize frequent episodes
input: an event log l, a set of candidate episodes cl, a frequency threshold minfreq .
output: a set of frequent episodes fl
description: recognizes frequent episodes, by ltering out candidate episodes that do not occur
frequently in the log. note: iffl=;, thencl=;.
recognizefrequentepisodes (l;cl;minfreq )
(1) support = [0;:::; 0] withjsupportj=jclj
(2) foreacht2l
(3) fori= 0tojclj 1
(4) ifoccurs (cl[i];t)then support [i] =support [i] + 1
(5)fl=;
(6) fori= 0tojclj 1
(7) ifsupport [i]
jljminfreq thenfl=fl[fcl[i]g
(8) returnflchecking whether an episode occurs in a trace t=ha1;:::;aniis done via
checking the existence of the mapping h::v7!f1;::;ng. this results in checking
the two propositions shown below. algorithm 5 implements these checks.
{checking whether each node v2:vhas a unique witness in trace t.
{checking whether the (injective) mapping hrespects the partial order indicated
by:.
for the discovery of an injective mapping hfor a specic episode and tracet
we use the following recipe. first, we declare the class of models h:a7!p (n)
such that for each activity a2awe get the set of indices iat whicha=ai2t.
next, we try all possible models derivable from h. a modelh::v7!f1;::;ng
is derived from hby choosing an index i2h(f(v)) for each node v2:v. with
such a model h, we can perform the actual partial order check against :.
algorithm 5: this algorithm implements occurrence checking via recursive
discovery of the injective mapping has per the occurrence denition.
input: an episode , a tracet.
output: true ivt
description: implements occurrence checking based on nding an occurrence proof in the form of
a mappingh::v7!f1;::;ng.
occurs (= (v;;g);t)
(1) return checkmodel (;fa7!fija=ai2tgja2ag;;)
input: an episode , a class of mappings h:a 7! p (n), and an intermediate mapping
h::v7!f1;::;ng.
output: true i there is a mapping h, as per the occurrence denition, derivable from h
description: recursive implementation for nding hbased on the following induction principle: base
case ( if-part): every v2vis mapped ( v2domh). step case ( else-part): (ih) nvertices are mapped,
step by adding a mapping for a vertex v =2domh. (i.e., induction to the number of mapped vertices.)
checkmodel (= (v;;g);h;h )
(1) if8v2v:v2domh
(2) return (8(v;w)2:h(v)h(w))
(3) else
(4) pick v2vwithv =2domh
(5) return (9i2h(g(v)) :
checkmodel (;h[g(v)7!h(g(v))nfig];h[v7!i]))
4.4 pruning
using the pruning techniques described below, we reduce the number of generated
episodes (and thereby computation time and memory requirements) and lter out un-
interesting results. these techniques eliminate less interesting episodes by ignoring in-
frequent activities and skipping partial orders on activities with low temporal locality.
activity pruning based on the frequency of an activity, uninteresting episodes
can be pruned in an early stage. this is achieved by replacing the activity alphabet
abyaa, with
(8a2a:actfreq (a)minactfreq ), on line 4 in algorithm 1. this pruning
technique allows the episode discovery algorithm to be more resistant to logs with
many infrequent activities, which are indicative of exceptions or noise.trace distance pruning the pruning of episodes based on a trace distance
interval can be achieved by adding the trace distance interval check to line 2 of
algorithm 5. note that if there are two or more interpretations for h, with one passing
and one rejected by the interval check, then we will nd the correct interpretation
thanks to the9on line 5.
4.5 episode rule discovery
the discovery of episode rules is done after discovering all the frequent episodes. for
all frequent episodes , we consider all frequent subepisodes withfor the
episode rule ).
for eciently nding potential frequent subepisodes , we use the notion of \dis-
covery tree", based on episode construction. each time we recognize a frequent episode
created from combining frequent episodes and", we recognize as a child of and
". similarly,and"are the parents of . see figure 3 for an example of a discovery tree.
a
cbc
a
cba
c
ba
c
"
 
fig. 3. part of an example discovery tree. each block denotes an episode. the dashed
arrows between blocks denote a parent-child relationship. in this example we have, amongst
others:,","and(not shown as a parent-child relation).
using the discovery tree we can walk from an episode along the discovery
parents of. each time we nd a parent with, we can consider the parents
and children of . as result of lemma 2, we cannot apply pruning in either direction
of the parent-child relation based on the condence conf()). this is easy to
see for the child direction. for the parent direction, observe the discovery tree in
figure 3 and . if for episode we would stop before visiting the parents of
, we would never consider (which has ).
4.6 implementation considerations
we implemented the episode discovery algorithm as a prom 6 plug-in (see also
figure 4), written in java. since the occurs () algorithm (5) is the biggest bottleneck,
this part of the implementation was considerably optimized.5 evaluation
this section reviews the feasibility of the approach using both synthetic and real-life
event data.
5.1 methodology
we ran a series of experiments on two type of event logs. the rst event log,
bigger-example.xes , is an articial event log from the chapter 5 of [ 1] and available
viahttp://www.processmining.org/event_logs_and_models_used_in_book .
the second event log, bpi challenge 2012.xes , is a real life event log available via
doi:10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f . for these exper-
iments we used a laptop with a core i5-3570k cpu, 8 gb ram and java se runtime
environment 1.7.0 07-b11 (32 bit).
5.2 performance results
table 2 some key characteristics for both event logs. we examined the eects of the
parameters minfreq ,minactfreq andmaxtracedist on the running time and the
discovered number of episodes. in figure 4 an indication (screenshots) of the prom
plugin output is given.
# traces avg. events/trace min. events/trace max. events/trace
bigger-example.xes 1391 5 5 17
bpi challenge 2012.xes 13087 20 3 175
table 2. metadata for the used event logs
(a) event log: bigger-example.xes { minfreq = 0:05,minactfreq = 0:05,maxtracedist = 3
(b) event log: bpi challenge 2012 { minfreq = 0:55,minactfreq = 0:55,maxtracedist = 5
fig. 4. screenshots of the results in the prom plugin. shown are the transitive reductions
of the discovered episodes. note that in the episodes in figure 4(a), multiple nodes are
allowed to have the same label.
as can be seen in all the experiments in figure 5, we see that the running time is
strongly related to the discovered number of episodes. note that if some parametersare poorly chosen, like high maxtracedist in figure 5(f), then a relatively large class
of episodes seems to become frequent, thus increasing the running time drastically.
for a reasonably low number of frequent episodes ( <500, more will a human not in-
spect), the algorithm turns out to be quite fast (at most a few seconds for the challenge
log). we noted a virtual nonexistent contribution of the parallel episode mining phase
to the total running time. this can be explained by a simple combinatorial argument:
there are far more partial orders to be considered than there are parallel episodes.
an analysis of the eects of changing the minfreq parameter (figure 5(a), 5(b))
shows that a poorly chosen value results in many episodes. in addition, the minfreq
parameter gives us ne-grained control of the number of results. it gradually increases
the total number of episodes for lower values. note that, especially for the challenge
event log, low values for minfreq can dramatically increase the running time. this
is due to the large number of candidate episodes being generated.
secondly, note that for the minactfreq parameter (figure 5(c), 5(d)), there
seems to be a cuto point that separates frequent from infrequent activities. small
changes around this cuto point may have a dramatic eect on the number of episodes
discovered.
finally, for the maxtracedist parameter (figure 5(e), 5(f)), we see that this
parameter seems to have a sweet-spot where a low { but not too low { number of
episodes are discovered. chosen a value for maxtracedist just after this sweet-spot
yields a huge number of episodes.
when comparing the articial and real life event logs, we see a remarkable pattern.
the articial event log ( bigger-example.xes ), shown in figure 5(a) appears to be
far more ne-grained than the real life event log ( bpi challenge 2012.xes ) shown in
figure 5(b). in the real life event log there appears to be a clear distinction between
frequent and infrequent episodes. in the articial event log a more exponential pattern
occurs. most of the increase in frequent episodes, for decreasing minfreq , is again
in the partial order discovery phase.
5.3 comparison to existing discovery algorithms
as noted in the introduction, often the overall end-to-end process models are rather
complicated. therefore, the search for local patterns (i.e., episodes) is interesting. a
good example of a complicated process is the bpi challenge 2012 log. in figure 6 part
of the \spaghetti-like" process models are shown, as an indication of the complexity.
the episodes discovered over same log, depicted in figure 4(b) gives us a simple and
clear insight into important local patterns in the bpi challenge 2012 log. hence,
in these \spaghetti-like" process models, the episode discovery technique allows us
to quickly understand the main patterns.
6 conclusion and future work
in this paper, we considered the problem of discovering frequently occurring episodes
in an event log. an episode is a collection of events that occur in a given partial order.
we presented ecient algorithms for the discovery of frequent episodes and episode
rules occurring in an event log, and presented experimental results.
our experimental evaluation shows that the running time is strongly related to
the discovered number of episodes. for a reasonably low number of frequent episodes0 200 400 600 800 
0 500 1000  1500  2000  
1 
0.95  
0.9 
0.85  
0.8 
0.75  
0.7 
0.65  
0.6 
0.55  
0.5 
0.45  
0.4 
0.35  
0.3 
0.25  
0.2 
0.15  
0.1 
0.05  
runtime (ms) [95% conf. interval]  # episodes  
minfreq  bigger -example.xes -- minfreq  
# episodes  runtime  (a)parameter: minfreq
event log: bigger-example.xes
minactfreq = 0:65,maxtracedist = 4
0 100 200 300 400 500 600 700 
0 10 20 30 40 50 
1 0.95  0.9 0.85  0.8 0.75  0.7 0.65  0.6 0.55  0.5 0.45  0.4 
time (ms) [95% conf. interval]  # episodes  
minfreq  bpi_challenge_2012.xes -- minfreq  
# episodes  runtime  (b)parameter: minfreq
event log: bpi challenge 2012
minactfreq = 0:65,maxtracedist = 4
0 100 200 300 400 
0 200 400 600 800 
1 
0.95  
0.9 
0.85  
0.8 
0.75  
0.7 
0.65  
0.6 
0.55  
0.5 
0.45  
0.4 
0.35  
0.3 
0.25  
0.2 
0.15  
0.1 
0.05  
runtime (ms) [95% conf. interval]  # episodes  
minactfreq (activity frequency)  bigger -example.xes -- minactfreq  
# episodes  runtime  
(c)parameter: minactfreq
event log: bigger-example.xes
minfreq = 0:45,maxtracedist = 4
0 100 200 300 400 500 600 700 
0 10 20 30 40 
1 
0.95  
0.9 
0.85  
0.8 
0.75  
0.7 
0.65  
0.6 
0.55  
0.5 
0.45  
0.4 
0.35  
0.3 
0.25  
0.2 
0.15  
0.1 
0.05  
runtime (ms) [95% conf. interval]  # episodes  
minactfreq (activity frequency)  bpi_challenge_2012.xes -- minactfreq  
# episodes  runtime  (d)parameter: minactfreq
event log: bpi challenge 2012
minfreq = 0:50,maxtracedist = 4
0 100 200 300 400 
0 200 400 600 800 
0 1 2 3 4 5 6 7 8 9 
runtime (ms) [95% conf. interval]  # episodes  
maxtracedist  bigger -example.xes -- maxtracedist  
# episodes  runtime  
(e)parameter: maxtracedist
event log: bigger-example.xes
minfreq = 0:45,minactfreq = 0:65
0 5000  10000  15000  20000  25000  
0 200 400 600 800 1000  1200  
0 1 2 3 4 5 6 7 8 9 
runtime (ms) [95% conf. interval]  # episodes  
maxtracedist  bpi_challenge_2012.xes -- maxtracedist  
# episodes  runtime  (f)parameter: maxtracedist
event log: bpi challenge 2012
minfreq = 0:50,minactfreq = 0:55
fig. 5. eects of the parameter on the performance and number of discovered episodes.
(<500, more will a human not inspect), the algorithm turns out to be quite fast (at
most a few seconds). the main problem is the correct setting of the episode pruning
parameters minfreq ,minactfreq , and maxtracedist .
during the development of the algorithm for prom 6, special attention was paid
to optimizing the occurs () algorithm (algorithm 5) implementation, which proved
to be the main bottleneck. future work could be to prune occurrence checking based
on the parents of an episode, leveraging the fact that an episode cannot occur in
a trace if a parent also did occur in that trace.
another approach to improve the algorithm is to apply the generic divide and
conquer approach for process mining , as dened in [ 22]. this approach splits the set
of activities into a collection of partly overlapping activity sets. for each activity
set, the log is projected onto the relevant events, and the regular episode discovery
algorithm is applied. in essence, the same trick is applied as used by the minactfreq(a) event log: bpi challenge 2012 { discovery algorithm: -algorithm [10].
(b) event log: bpi challenge 2012 { discovery algorithm: [11].
fig. 6. screenshots of results in other prom plugin. shown are parts of the petri-nets mined
with the-algorithm and the heuristics miner.
parameter (using an alphabet subset), which is to create a dierent set of initial
1-node parallel episodes to start discovering with.
the main bottleneck is the frequency computation by checking the occurrence of
each episode in each trace. typically, we have a small amount of episodes to check, but
many traces to check against. using the mapreduce programming model developed by
dean and ghemawat, we can easily parallelize the episode discovery algorithm and ex-
ecute it on a large cluster of commodity machines [ 23]. the mapreduce programming
model requires us to dene map andreduce functions. the map function, in our case,
accepts a trace and produces [episode, trace] pairs for each episode occurring in the
given trace. the reduce function accepts an episode plus a list of traces in which that
episode occurs, and outputs a singleton list if the episode is frequent, and an empty list
otherwise. this way, the main bottleneck of the algorithm is eectively parallelized.
references
[1]van der aalst, w.m.p.: process mining: discovery, conformance and enhancement
of business processes. springer-verlag, berlin (2011)
[2]mannila, h., toivonen, h., verkamo, a.i.: discovery of frequent episodes in event
sequences. data mining and knowledge discovery 1(3) (1997) 259{289
[3]lu, x., fahland, d., van der aalst, w.m.p.: conformance checking based on partially or-
dered event data. to appear in business process intelligence 2014, workshop sbs (2014)
[4] agrawal, r., srikant, r.: fast algorithms for mining association rules in large
databases. in: proceedings of the 20th international conference on very large data
bases. vldb '94, san francisco, ca, usa, morgan kaufmann publishers inc. (1994)
487{499
[5]agrawal, r., srikant, r.: mining sequential patterns. in: proceedings of the eleventh
international conference on data engineering. icde '95, washington, dc, usa,
ieee computer society (1995) 3{14
[6]srikant, r., agrawal, r.: mining sequential patterns: generalization and performance
improvements. in: proceedings of the 5th international conference on extending
database technology: advances in database technology. edbt '96, london, uk,
uk, springer-verlag (1996) 3{17[7]lu, x., mans, r.s., fahland, d., van der aalst, w.m.p.: conformance checking in
healthcare based on partially ordered event data. to appear in emerging technologies
and factory automation 2014, workshop m2h (2014)
[8]fahland, d., van der aalst, w.m.p.: repairing process models to reect reality. in:
proceedings of the 10th international conference on business process management.
bpm'12, berlin, heidelberg, springer-verlag (2012) 229{245
[9]laxman, s., sastry, p.s., unnikrishnan, k.p.: fast algorithms for frequent episode
discovery in event sequences. in: proc. 3rd workshop on mining temporal and
sequential data. (2004)
[10] van der aalst, w.m.p., weijters, a.j.m.m., maruster, l.: workow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering 16(9) (2004) 1128{1142
[11] weijters, a.j.m.m., van der aalst, w.m.p., de medeiros, a.k.a.: process mining with
the heuristics miner-algorithm. beta working paper series, wp 166, eindhoven
university of technology, eindhoven (2006)
[12] de medeiros, a.k.a., van der aalst, w.m.p., weijters, a.j.m.m.: workow mining:
current status and future directions. in meersman, r., tari, z., schmidt, c.d., eds.: on
the move to meaningful internet systems 2003: coopis, doa, and odbase. volume
2888 of lecture notes in computer science. springer berlin heidelberg (2003) 389{406
[13] wen, l., van der aalst, w.m.p., wang, j., sun, j.: mining process models with non-
free-choice constructs. data mining and knowledge discovery 15(2) (2007) 145{180
[14] de medeiros, a.k.a., weijters, a.j.m.m., van der aalst, w.m.p.: genetic process
mining: an experimental evaluation. data mining and knowledge discovery 14(2)
(2007) 245{304
[15] buijs, j.c.a.m., van dongen, b.f., van der aalst, w.m.p.: on the role of fitness,
precision, generalization and simplicity in process discovery. in meersman,
r., rinderle, s., dadam, p., zhou, x., eds.: otm federated conferences, 20th
international conference on cooperative information systems (coopis 2012). volume
7565 of lecture notes in computer science., springer-verlag, berlin (2012) 305{322
[16] sol e, m., carmona, j.: process mining from a basis of state regions. in: applications
and theory of petri nets (petri nets 2010). volume 6128 of lecture notes in
computer science., springer-verlag, berlin (2010) 226{245
[17] van der aalst, w.m.p., rubin, v., verbeek, h.m.w., van dongen, b.f., kindler,
e., g unther, c.w.: process mining: a two-step approach to balance between
undertting and overtting. software and systems modeling 9(1) (2010) 87{111
[18] bergenthum, r., desel, j., lorenz, r., mauser, s.: process mining based on regions of
languages. in alonso, g., dadam, p., rosemann, m., eds.: international conference
on business process management (bpm 2007). volume 4714 of lecture notes in
computer science., springer-verlag, berlin (2007) 375{383
[19] van der werf, j.m.e.m., van dongen, b.f., hurkens, c.a.j., serebrenik, a.: process dis-
covery using integer linear programming. fundamenta informaticae 94(2010) 387{412
[20] leemans, s.j.j., fahland, d., van der aalst, w.m.p.: discovering block-structured
process models from incomplete event logs. in ciardo, g., kindler, e., eds.:
applications and theory of petri nets 2014. volume 8489 of lecture notes in
computer science., springer-verlag, berlin (2014) 91{110
[21] kryszkiewicz, m.: fast discovery of representative association rules. in polkowski,
l., skowron, a., eds.: rough sets and current trends in computing. volume 1424
of lecture notes in computer science. springer berlin heidelberg (1998) 214{222
[22] van der aalst, w.m.p.: decomposing petri nets for process mining: a generic
approach. distributed and parallel databases 31(4) (2013) 471{507
[23] dean, j., ghemawat, s.: mapreduce: simplied data processing on large clusters.
communications of the acm 51(1) (2008) 107{113