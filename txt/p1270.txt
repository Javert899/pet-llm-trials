event log sampling for predictive monitoring
mohammadreza fani sani1, mozhgan vazifehdoostirani2, gyunam park1,
marco pegoraro1, sebastiaan j. van zelst3;1, and wil m.p. van der aalst1;3
1process and data science chair, rwth aachen university, aachen, germany
2industrial engineering and innovation science, eindhoven university of technology,
eindhoven, the netherlands
3fraunhofer fit, birlinghoven castle, sankt augustin, germany
ffanisani, gnpark, pegoraro, s.j.v.zelst,
wvdaalstg@pads.rwth-aachen.de
m.vazifehdoostirani@tue.nl
abstract predictive process monitoring is a subÔ¨Åeld of process mining that aims
to estimate case or event features for running process instances. such predictions
are of signiÔ¨Åcant interest to the process stakeholders. however, state-of-the-art
methods for predictive monitoring require the training of complex machine learn-
ing models, which is often inefÔ¨Åcient. this paper proposes an instance selection
procedure that allows sampling training process instances for prediction models.
we show that our sampling method allows for a signiÔ¨Åcant increase of training
speed for next activity prediction methods while maintaining reliable levels of
prediction accuracy.
keywords: process miningpredictive monitoring samplingmachine
learningdeep learninginstance selection
1 introduction
as the environment surrounding business processes becomes more dynamic and com-
petitive, it becomes imperative to predict process behaviors and take proactive ac-
tions [1]. predictive business process monitoring aims at predicting the behavior of
business processes, to mitigate the risk resulting from undesired behaviors in the pro-
cess. for instance, by predicting the next activities in the process, one can foresee the
undesired execution of activities, thus preventing possible risks resulting from it [12].
moreover, by predicting an expected high service time for an activity, one may bypass or
add more resources for the activity [15]. recent breakthroughs in machine learning have
enabled the development of effective techniques for predictive business process moni-
toring. speciÔ¨Åcally, techniques based on deep neural networks, e.g., long-short term
memory (lstm) networks, have shown high performance in different tasks [8]. addi-
tionally, the emergence of ensemble learning methods leads to improvement in accuracy
in different areas [4]. particularly, for predictive process monitoring, extreme gradient
boosting (xgboost) [6] has shown promising results, often outperforming other en-
semble methods such as random forest or using a single regression tree [25, 28].
indeed, machine learning algorithms suffer from the expensive computational costs
in their training process [34]. in particular, machine learning algorithms based on neuralarxiv:2204.01470v1  [cs.lg]  4 apr 2022networks and ensemble learning might require tuning their hyperparameters to be able
to provide acceptable accuracy. such long training time limits the application of the
techniques considering the limitations in time and hardware [21]. this is particularly
relevant for predictive business process monitoring techniques. business analysts need
to test the efÔ¨Åciency and reliability of their conclusions via repeated training of differ-
ent prediction models with different parameters [15]. moreover, the dynamic nature of
business processes requires new models adapting to new situations in short intervals.
instance selection aims at reducing original datasets to a manageable volume to per-
form machine learning tasks, while the quality of the results (e.g., accuracy) is main-
tained as if the original dataset was used [11]. instance selection techniques are cate-
gorized into two classes based on the way they select instances. first, some techniques
select the instances at the boundaries of classes. for instance, decremental reduction
optimization procedure (drop) [32] selects instances using k-nearest neighbors by
incrementally discarding an instance if its neighbors are correctly classiÔ¨Åed without
the instance. the other techniques preserve the instances residing inside classes, e.g.,
edited nearest neighbor (enn) [33] preserves instances by repeatedly discarding an
instance if it does not belong to the class of the majority of its neighbors.
such techniques assume independence among instances [32]. however, in predic-
tive business process monitoring training, instances may be highly correlated [2], im-
peding the application of techniques for instance selection. such instances are com-
puted from event data that are recorded by the information system supporting business
processes [14]. the event data are correlated by the notion of case, e.g., patients in a
hospital or products in a factory. in this regard, we need new techniques for instance
selection applicable to event data.
in this work, we suggest an instance selection approach for predicting the next ac-
tivity, one of the main applications of predictive business process monitoring. by con-
sidering the characteristics of the event data, the proposed approach samples event data
such that the training speed is improved while the accuracy of the resulting predic-
tion model is maintained. we have evaluated the proposed methods using two real-life
datasets and state-of-the-art techniques for predictive business process monitoring, in-
cluding lstm [13] and xgboost [6].
the remainder is organized as follows. we discuss the related work in section 2.
next, we present the preliminaries in section 3 and proposed methods in section 4.
afterward, section 5 evaluates the proposed methods using real-life event data and
section 6 provides discussions. finally, section 7 concludes the paper.
2 related work
predictive process monitoring is an exceedingly active Ô¨Åeld of research. at its core,
the fundamental component of predictive monitoring is the abstraction technique it
uses to obtain a Ô¨Åxed-length representation of the process component subject to the
prediction (often, but not always, process traces). in the earlier approaches, the need
for such abstraction was overcome through model-aware techniques, employing pro-
cess models and replay techniques on partial traces to abstract a Ô¨Çat representation of
event sequences. such process models are mostly automatically discovered from a setof available complete traces, and require perfect Ô¨Åtness on training instances (and, sel-
domly, also on unseen test instances). for instance, van der aalst et al. [1] proposed a
time prediction framework based on replaying partial traces on a transition system, ef-
fectively clustering training instances by control-Ô¨Çow information. this framework has
later been the basis for a prediction method by polato et al. [20], where the transition
system is annotated with an ensemble of svr and na ¬®ƒ±ve bayes classiÔ¨Åers, to perform a
more accurate time estimation. a related approach, albeit more linked to the simulation
domain and based on a monte carlo method, is the one proposed by rogge-solti and
weske [24], which maps partial process instances in an enriched petri net.
recently, predictive process monitoring started to use a plethora of machine learn-
ing approaches, achieving varying degrees of success. for instance, teinemaa et al. [27]
provided a framework to combine text mining methods with random forest and logis-
tic regression. senderovich et al. [25] studied the effect of using intra-case and inter-
case features in predictive process monitoring and showed a promising result for xg-
boost compared to other ensemble and linear methods. a comprehensive benchmark on
using classical machine learning approaches for outcome-oriented predictive process
monitoring tasks [28] has shown that the xgboost is the best-performing classiÔ¨Åer
among different machine learning approaches such as svm, decision tree, random
forest, and logistic regression.
more recent methods are model-unaware and perform based on a single and more
complex machine learning model instead of an ensemble. the lstm network model
has proven to be particularly effective for predictive monitoring [8,26], since the recur-
rent architecture can natively support sequences of data of arbitrary length. it allows per-
forming trace prediction while employing a Ô¨Åxed-length event abstraction, which can be
based on control-Ô¨Çow alone [8, 26], data-aware [16], time-aware [17], text-aware [19],
or model-aware [18].
a concept similar to the idea proposed in this paper, and of current interest in the
Ô¨Åeld of machine learning, is dataset distillation : utilizing a dataset to obtain a smaller
set of training instances that contain the same information (with respect to training
a machine learning model) [31]. while this is not considered sampling, since some
instances of the distilled dataset are created ex-novo, it is an approach very similar to
the one we illustrate in our paper. moreover, recently some instance selection algorithms
have been proposed to help process mining algorithms. for example, [9, 10] proposed
to use instance selection techniques to improve the performance of process discovery
and conformance checking procedures.
in this paper, we examine the underexplored topic of event data sampling and selec-
tion for predictive process monitoring, with the objective of assessing if and to which
extent prediction quality can be retained when we utilize subsets of the training data.
3 preliminaries
in this section, some process mining concepts such as event log and sampling are dis-
cussed. in process mining, we use events to provide insights into the execution of busi-
ness processes. each event is related to speciÔ¨Åc activities of the underlying process.
furthermore, we refer to a collection of events related to a speciÔ¨Åc process instanceas a case. both cases and events may have different attributes. an event log that is a
collection of events and cases is deÔ¨Åned as follows.
deÔ¨Ånition 1 (event log). letebe the universe of events, cbe the universe of cases,
at be the universe of attributes, and ube the universe of attribute values. more-
over, letccbe a non-empty set of cases, let eebe a non-empty set of events, and
letatat be a set of attributes. we deÔ¨Åne (c;e;c;e)as an event log, where
c:cat 7!u ande:eat 7!u . any event in the event log has a case, therefore,
@e2e(e(e;case )62c)ands
e2e(e(e;case ))=c.
furthermore, letau be the universe of activities and let vabe the uni-
verse of sequences of activities. for any e2e, functione(e;activity )2a, which
means that any event in the event log has an activity. moreover, for any c2cfunction
c(c;variant )2anfhig that means any case in the event log has a variant.
therefore, there are some mandatory attributes that are case andactivity for events and
variants for cases. in some process mining applications, e.g., process discovery and
conformance checking, just variant information is considered. therefore, event logs are
considered as a multiset of sequences of activities. in the following, a simple event log
is deÔ¨Åned.
deÔ¨Ånition 2 (simple event log). letabe the universe of activities and let the universe
of multisets over a set xbe denoted byb(x). a simple event log is l2b(a). more-
over, letelbe the universe of event logs and el=(c;e;c;e)2el be an event log.
we deÔ¨Åne function sl:el!b (fe(e;activity )je2eg)returns the simple event log of
an event log. the set of unique variants in the event log is denoted by sl(el).
therefore,slreturns the multiset of variants in the event logs. note that the size of a
simple event log equals the number of cases in the event logs, i.e., sl(el)=jcj
in this paper, we use sampling techniques to reduce the size of event logs. an event
log sampling method is deÔ¨Åned as follows.
deÔ¨Ånition 3 (event log sampling). letel be the universe of event logs and
abe the universe of activities. moreover, let el=(c;e;c;e)2el be an
event log, we deÔ¨Åne function :el!el that returns the sampled event log where
if(c0;e0;0
c;0
e)=(el), thenc0c,e0e,0
ee,0
cc, and conse-
quently,sl((el))sl(el). we deÔ¨Åne that is a variant-preserving sampling if
sl((el))=sl(el).
in other words, a sampling method is variant-preserving if and only if all the variants of
the original event log are presented in the sampled event log.
to use machine learning methods for prediction, we usually need to transfer each
case to one or more features. the feature is deÔ¨Åned as follows.
deÔ¨Ånition 4 (feature). letat be the universe of attributes, ube the universe of
attribute values, and cbe the universe of cases. moreover, let atat be a set of
attributes. a feature is a relation between a sequence of attributes‚Äô values for atand
the target attribute value, i.e., f2(ujatju). we deÔ¨Åne fe:cel!b (ujatju)is a
function that receives a case and an event log, and returns a multiset of features. 
figure 1: a schematic view of the proposed sampling procedure
for the next activity prediction, i.e., our prediction goal, the target attribute value should
be an activity. moreover, a case in the event log may have different features. for exam-
ple, suppose that we only consider the activities. for the case ha;b;c;di, we may have
(hai;b),(ha;bi;c), and (ha;b;ci;d)as features. furthermore,p
c2cfe(c;el )are the
corresponding features of event log el=(c;e;c;e)that could be given to differ-
ent machine learning algorithms. for more details on how to extract features from event
logs please refer to [23].
4 proposed sampling methods
in this section, we propose an event log preprocessing procedure that helps predic-
tion algorithms to perform faster while maintaining reasonable accuracy. the schematic
view of the proposed sampling approach is presented in fig. 1. we Ô¨Årst need to traverse
the event log and Ô¨Ånd the variants and corresponding traces of each variant in the event
log. moreover, different distributions of data attributes in each variant will be computed.
afterward, using different sorting and instance selection strategies, we are able to select
some of the cases and return the sample event log. in the following, each of these steps
is explained in more detail.
1.traversing the event log : in this step, the unique variants of the event
log and the corresponding traces of each variant are determined. in other
words, consider event log el thatsl(el)=f1;:::;ngwheren=jsl(el)j,
we aim to split el toel 1;::;elnwhereelionly contains all the cases
thatci=fc2cjc(c;variant )=igandei=fe2eje(e;case )2cig. obviously,s
1in(ci)=candt
1in(ci)=?.
2.distribution computation : in this step, for each variant of the event log, we com-
pute the distribution of different data attributes a2at. it would be more practical
if the interesting attributes are chosen by an expert. both event and case attributes
can be considered. a simple approach is to compute the frequency of categorical
data values. for numerical data attributes, it is possible to consider the average or
the median of values for all cases of each variant.
3.sorting the cases of each variant : in this step, we aim to sort the traces of each
variant. we need to sort the traces to give a higher priority to those traces that can
represent the variant better. one way is to sort the traces based on the frequency
of the existence of the most occurred data values of the variant. for example, wecan give a higher priority to the traces that have more frequent resources of each
variant. it is also possible to sort the traces based on their arrival time or randomly.
4.returning sample event logs : finally, depending on the setting of the sampling
function, we return some of the traces with the highest priority for all variants. the
most important point about this step is to know how many traces of each variant
should be selected. in the following, some possibilities will be introduced.
‚Äìunique selection : in this approach, we select only one trace with the highest
priority. in other words, suppose that l0=sl((el)),82l0l0()=1. there-
fore, using this approach we will have jsl((el))j=jsl(el)j. it is expected
that using this approach, the distribution of frequency of variants will be
changed and consequently the resulted prediction model will be less accurate.
‚Äìlogarithmic distribution : in this approach, we reduce the number of traces
in each variant in a logarithmic way. if l=sl(el)andl0=sl((el)),
82l0l0()=[logk(l())]. using this approach, the infrequent variants will
not have any trace in the sampled event log. by using a higher k, the size of the
sampled event log is reduced more.
‚Äìdivision : this approach performs similar to the previous one, however, instead
of using logarithmic scale, we apply the division operator. in this approach,
82l0l0()=d()
ke. a higherkresults in fewer cases in the sample event log.
note that using this approach all the variants have at least one trace in the
sampled event log.
there is also a possibility to consider other selection methods. for example, we can
select the traces completely randomly from the original event log.
by choosing different data attributes in step 2 and different sorting algorithms in
step 3, we are able to lead the sampling of the method on which cases should be chosen.
moreover, by choosing the type of distribution in step 4, we determine how many cases
should be chosen. to compute how sampling method reduces the size of the given
event logel, we use the following equation:
rs=jsl(el)j
jsl((el))j(1)
the higherrsvalue means, the sampling method reduces more the size of the training
log. by choosing different distribution methods and different k-values , we are able to
control the size of the sampled event log. it should be noted that the proposed method
will apply just to the training event log. in other words, we do not sample event logs for
development and test datasets.
5 evaluation
in this section, we aim at designing some experiments to answer our research question,
i.e., ‚Äùcan we improve the computational performance of prediction methods by using
the sampled event logs, while maintaining a similar accuracy?‚Äù. it should be noted that
the focus of the experiments is not on prediction model tuning to have higher accuracy.
conversely, we aim to analyze the effect of using sampled event logs (instead of thetable 1: overview of the event logs that are used in the experiments. the accuracy and
the required times (in seconds) of different prediction methods for these event logs are
also presented.
event log cases activities variants attributes fe time lstm train time lstm acc xgtrain time xg acc
rtfm 150370 11 231 1 73649 3021 0.791 11372 0.814
bpic-2012-w 9658 6 2643 2 1212 3344 0.68 2011 0.685
whole datasets) on the required time and the accuracy of prediction models. in the
following, we Ô¨Årst explain the event logs that are used in the experiments. afterward, we
provide some information about the implementation of sampling methods. moreover,
the experimental setting is discussed and, Ô¨Ånally, we show the experimental results.
5.1 event logs
to evaluate the proposed sampling procedure for prediction, we have used two event
logs widely used in the literature. some information about these event logs is presented
in table 1. in the rtfm event log, which corresponds to a road trafÔ¨Åc management
system, we have some high frequent variants and several infrequent variants. moreover,
the number of activities in this event log is high. some of these activities are infrequent,
which makes this event log imbalanced. in the bpic-2012-w event log, relating to a
process of an insurance company, the average of variant frequencies is lower.
5.2 implementation
we have developed the sampling methods as a plug-in in the prom framework [30],
accessible via https://svn.win.tue.nl/repos/prom/packages/
logfiltering . this plug-in takes an event log and returns k different train and
test event logs in the csv format. moreover, to train the prediction method, we
have used xgboost [6] and lstm [13] methods as they are widely used in the
literature and outperformed their counterparts. our lstm network consisted of an
input layer, two lstm layers with dropout rates of 10%, and a dense output layer
with the softmax activation function. we used ‚Äúcategorical cross-entropy‚Äù to calculate
the loss and adopted adam as an optimizer. we used gbtree with a max depth of
6as a booster in our xgboost model. uniform distribution is used as the sampling
method inside our xgboost model. to avoid overÔ¨Åtting in both models, the training
set is further divided into 90% training set and 10% validation set to stop training
once the model performance on the validation set stops improving. we used the same
setting of both models for original event logs and sampled event logs. to access
our implementations of these methods and the feature generation please refer to
https://github.com/gyunamister/pm-prediction/ . for details of the
feature generation and feature encoding steps, please refer to [18].
5.3 evaluation setting
to sample the event logs, we use three distributions that are logdistribution ,division ,
andunique variants . for thelogdistribution method, we have used 2;3, and 10(i.e.,
log2;log 3, andlog10). for the division method, we have used 2;5, and 10(i.e.,d2;d5,andd10). for each event log and for each sampling method, we have used a 5-fold
cross-validation. moreover, as the results of the experiments are non-deterministic, all
the experiments have been repeated 5times and the average values are represented.
note that, for both training and evaluation phases, we have used the same settings
for extracting features and training prediction models. we used one-hot encoding to
encode the sequence of activities for both lstm and xgboost models. we ran the
experiment on a server with intel xeon cpu e7-4850 2.30ghz, and 512 gb of ram.
in all the steps, one cpu thread has been used. we employed the weighted accuracy
metric [22] to compute how a prediction method performs for test data. to compare
the accuracy of the prediction methods, we use the relative accuracy that is deÔ¨Åned as
follows.
racc=accuracy using the sampled training log
accuracy using the whole training log(2)
ifraccis close to 1, it means that using the sampling event logs, the prediction methods
behave almost similar to the case that the whole data is used for the training. moreover,
values higher than 1indicate the accuracy of prediction methods has improved.
to compute the improvement in the performance of training time, we will use the
following equations.
rt=training time using whole data
training time using the sampled data(3)
rfe=feature extraction time using whole data
feature extraction time using the sampled data(4)
for both equations, the resulting values indicate how many times the sampled log is
faster than using all data.
5.4 experimental results
table 2 presents the reduction rate and the improvement in the feature extraction phase
using different sampling methods. as it is expected, the highest reduction rate is for
log10(as it removes infrequent variants and keeps few traces of frequent variants), and
respectively it has the biggest improvement in rfe. moreover, the lowest reduction
is for d2, especially if there are lots of unique variants in the event log (i.e., for the
rtfm event log). we expected smaller event logs to require less feature extraction
time. however, results indicate that the relationship is not linear, and by having more
reduction in the size of the sampled event log there will be a much higher reduction in
the feature extraction time.
in table 3 and table 4, the results of improvement in rtandraccare shown for
lstm and xg prediction methods. as expected, by using fewer cases in the training,
the performance of training time improvement will be higher. comparing the results in
these two tables and the results in table 2, it is interesting to see that in some cases, even
by having a high reduction rate, the accuracy of the trained prediction model is close
to the case in which whole training log is used. for example, using d10for the rtfm
event log, we will have high accuracy for both prediction methods. in other words, we
are able to improve the performance of the prediction procedure while the accuracy is
still reasonable.table 2: the reduction in the size of training logs (i.e., rs) and the improvement in the
performance of feature extraction part (i.e., rfe) using different sampling methods.
sampling methods d2 d3 d10log2log3log10 unique
event log rsrfersrfersrfersrfersrfersrfersrfe
rtfm [7] 1.99 4.8 3.0 11.1 9.8 106.9 153.5 12527.6 236.3 23699.2 572.3 74912.8 285.1 24841.8
bpic-2012-w [29] 1.22 1.37 1.41 1.80 1.66 2.51 6.06 22.41 9.05 37.67 28.50 208.32 1.73 2.36
table 3: the accuracy and the improvement in the performance of prediction using
different sampling methods for lstm.
sampling methods d2 d3 d10log2log3log10 unique
event log raccrtraccrtraccrtraccrtraccrtraccrtraccrt
rtfm 1.001 2.0 1.004 2.9 0.990 9.0 0.716 26.7 0.724 33.0 0.767 41.8 0.631 29.1
bpic-2012-w 1.000 1.4 0.985 1.3 0.938 1.3 0.977 4.7 0.970 5.8 0.876 11.9 0.996 1.6
table 4: the accuracy and the improvement in the performance of prediction using
different sampling methods for xgboost.
sampling methods d2 d3 d10log2log3log10 unique
event log raccrtraccrtraccrtraccrtraccrtraccrtraccrt
rtfm 1.000 2.4 1.000 1.4 1.000 84.1 0.686 126.4 0.706 191.8 0.772 355.0 0.582 297.7
bpic-2012-w 0.999 2.3 0.998 2.4 0.997 3.4 0.923 10.7 0.970 16.7 0.883 64.8 0.997 2.8
when using the lstm prediction method for the rtfm event log, there are some
cases where we have accuracy improvement. for example, using d3, there is a 0:4%
improvement in the accuracy of the trained model. it is mainly because of the existence
of high frequent variants. these variants lead to having unbiased training logs and con-
sequently, the accuracy of the trained model will be lower for infrequent behaviors.
6 discussion
the results indicate that we do not always have a typical trade-off between the accuracy
of the trained model and the performance of the prediction procedure. in other words,
there are some cases where the training process is much faster than the normal proce-
dure, even though the trained model provides an almost similar accuracy. we did not
provide the results for other metrics; however, there are similar patterns for weighted
recall, precision, and f1-score. thus, the proposed sampling methods can be used when
we aim to apply hyperparameter optimization [3]. in this way, more settings can be an-
alyzed in a limited time. moreover, it is reasonable to use the proposed method when
we aim to train an online prediction method or on naive hardware such as cell phones.
another important outcome of the results is that for different event logs, we should
use different sampling methods to achieve the highest performance. for example, for the
rtfm event log‚Äîas there are some highly frequent variants‚Äîthe division distribution
may be more useful. in other words, independently of the used prediction method, if
we change the distribution of variants (e.g., using unique distribution), it is expected
that the accuracy will sharply decrease. however, for event logs with a more uniform
distribution, we can use logarithmic and unique distributions to sample event logs. the
results indicate that the effect of the chosen distribution (i.e., unique ,division , and
logarithmic ) is more important than the used k-value . therefore, it would be valuable
to investigate more on the characteristics of the given event log and suitable samplingparameters for such distribution. for example, if most variants of a given event log
are unique, the division andunique methods are not able to have remarkable rsand
consequently, rfeandrtwill be close to 1.
moreover, results have shown that by oversampling the event logs, although we will
have a very big improvement in the performance of the prediction procedure, the accu-
racy of the trained model is signiÔ¨Åcantly lower than the accuracy of the model that is
trained by the whole event log. therefore, we suggest gradually increasing (or decreas-
ing) the size of the sampled event log in the hyper-parameter optimization scenarios.
by analysis of the results using common prediction methods, we have found that
the infrequent activities can be ignored using some hyper-parameter settings. this is
mainly because the event logs are unbalanced for these infrequent activities. using the
sampling methods that modify the distribution of the event logs such as the unique
method can help the prediction methods to also consider these activities.
finally, in real scenarios, the process can change because of different reasons [5].
this phenomenon is usually called concept drift . by considering the whole event log for
training the prediction model, it is most probable that these changes are not considered
in the prediction. using the proposed sampling procedure, and giving higher priorities
to newer traces, we are able to adapt to the changes faster, which may be critical for
speciÔ¨Åc applications.
7 conclusion
in this paper, we proposed to use the subset of event logs to train prediction models.
we proposed different sampling methods for next activity prediction. these methods
are implemented in the prom framework. to evaluate the proposed methods, we have
applied them on two real event logs and have used two state-of-the-art prediction meth-
ods: lstm and xgboost. the experimental results have shown that, using the pro-
posed method, we are able to improve the performance of the next activity prediction
procedure while retaining an acceptable accuracy (in some experiments, the accuracy
increased). however, there is a relation between event logs characteristics and suitable
parameters that can be used to sample these event logs. the proposed methods can be
helpful in situations where we aim to train the model fastly or in hyper-parameter opti-
mization scenarios. moreover, in cases where the process can change over time, we are
able to adapt to the modiÔ¨Åed process more quickly using sampling methods.
to continue this research, we aim to extend the experiments to study the relation-
ship between the event log characteristics and the sampling parameters. additionally,
we plan to provide some sampling methods that help prediction methods to predict in-
frequent activities, which could be more critical in the process. finally, it is interesting
to investigate more on using sampling methods for other prediction method applications
such as last activity and remaining time prediction.
acknowledgment
the authors would like to thank the alexander von humboldt (avh) stiftung for fund-
ing this research.references
1. van der aalst, w.m.p., schonenberg, m., song, m.: time prediction based on process mining
36(2), 450‚Äì475. https://doi.org/10.1016/j.is.2010.09.001
2. van der aalst, w.m.p.: process mining - data science in action, second edition. springer
(2016)
3. bergstra, j., bardenet, r., bengio, y ., k ¬¥egl, b.: algorithms for hyper-parameter optimiza-
tion. in: advances in neural information processing systems 24: 25th annual conference
on neural information processing systems 2011. proceedings of a meeting held 12-14 de-
cember 2011, granada, spain. pp. 2546‚Äì2554 (2011)
4. breiman, l.: bagging predictors. machine learning 24(2), 123‚Äì140 (1996)
5. carmona, j., gavald `a, r.: online techniques for dealing with concept drift in process mining.
in: advances in intelligent data analysis xi - 11th international symposium, ida 2012,
helsinki, finland, october 25-27, 2012. proceedings. vol. 7619, pp. 90‚Äì102. springer (2012)
6. chen, t., guestrin, c.: xgboost: a scalable tree boosting system. in: proceedings of the
22nd acm sigkdd international conference on knowledge discovery and data mining,
san francisco, ca, usa, august 13-17, 2016. pp. 785‚Äì794. acm (2016)
7. de leoni, m., mannhardt, f.: road trafÔ¨Åc Ô¨Åne management process. eindhoven university
of technology. dataset (2015)
8. evermann, j., rehse, j., fettke, p.: predicting process behaviour using deep learning. decis.
support syst. 100, 129‚Äì140 (2017)
9. fani sani, m., van zelst, s.j., van der aalst, w.m.p.: conformance checking approximation
using subset selection and edit distance. in: advanced information systems engineering -
32nd international conference, caise 2020, grenoble, france, june 8-12, 2020, proceed-
ings. vol. 12127, pp. 234‚Äì251. springer (2020)
10. fani sani, m., van zelst, s.j., van der aalst, w.m.p.: the impact of biased sampling of event
logs on the performance of process discovery. computing 103(6), 1085‚Äì1104 (2021)
11. garca, s., luengo, j., herrera, f.: data preprocessing in data mining. springer publishing
company, incorporated (2014)
12. hitfox group, breuker, d., matzner, m., university of muenster, delfmann, p., university
of koblenz-landau, becker, j., university of muenster: comprehensible predictive models
for business processes 40(4), 1009‚Äì1034. https://doi.org/10.25300/misq/2016/40.4.10
13. huang, z., xu, w., yu, k.: bidirectional lstm-crf models for sequence tagging. corr
abs/1508.01991 (2015)
14. de leoni, m., van der aalst, w.m.p., dees, m.: a general process mining framework for
correlating, predicting and clustering dynamic behavior based on event logs 56, 235‚Äì257.
https://doi.org/10.1016/j.is.2015.07.003
15. marquez-chamorro, a.e., resinas, m., ruiz-cortes, a.: predictive monitoring of business
processes: a survey 11(6), 962‚Äì977. https://doi.org/10.1109/tsc.2017.2772256
16. navarin, n., vincenzi, b., polato, m., sperduti, a.: lstm networks for data-aware remain-
ing time prediction of business process instances. in: 2017 ieee symposium series on com-
putational intelligence, ssci 2017, honolulu, hi, usa, november 27 - dec. 1, 2017. pp. 1‚Äì
7. ieee (2017)
17. nguyen, a., chatterjee, s., weinzierl, s., schwinn, l., matzner, m., eskoÔ¨Åer, b.m.: time
matters: time-aware lstms for predictive business process monitoring. in: leemans, s.j.j.,
leopold, h. (eds.) process mining workshops - icpm 2020 international workshops, padua,
italy, october 5-8, 2020, revised selected papers. vol. 406, pp. 112‚Äì123. springer (2020)
18. park, g., song, m.: predicting performances in business processes using deep neural net-
works. decis. support syst. 129(2020)19. pegoraro, m., uysal, m.s., georgi, d.b., van der aalst, w.m.p.: text-aware predictive mon-
itoring of business processes. in: abramowicz, w., auer, s., lewanska, e. (eds.) 24th inter-
national conference on business information systems, bis 2021, hannover, germany, june
15-17, 2021. pp. 221‚Äì232 (2021)
20. polato, m., sperduti, a., burattin, a., de leoni, m.: time and activity sequence prediction
of business process instances. computing 100(9), 1005‚Äì1031 (2018)
21. pourghassemi, b., zhang, c., lee, j.h., chandramowlishwaran, a.: on the limits of par-
allelizing convolutional neural networks on gpus. in: spaa ‚Äô20: 32nd acm symposium
on parallelism in algorithms and architectures, virtual event, usa, july 15-17, 2020. pp.
567‚Äì569. acm (2020)
22. powers, d.m.w.: evaluation: from precision, recall and f-measure to roc, informedness,
markedness and correlation. corr abs/2010.16061 (2020)
23. qafari, m.s., van der aalst, w.m.p.: root cause analysis in process mining using structural
equation models. in: business process management workshops - bpm 2020 international
workshops, seville, spain, september 13-18, 2020, revised selected papers. vol. 397, pp.
155‚Äì167. springer (2020)
24. rogge-solti, a., weske, m.: prediction of remaining service execution time using stochastic
petri nets with arbitrary Ô¨Åring delays. in: basu, s., pautasso, c., zhang, l., fu, x. (eds.)
service-oriented computing - 11th international conference, icsoc 2013, berlin, ger-
many, december 2-5, 2013, proceedings. vol. 8274, pp. 389‚Äì403. springer (2013)
25. senderovich, a., di francescomarino, c., ghidini, c., jorbina, k., maggi, f.m.: intra and
inter-case features in predictive process monitoring: a tale of two dimensions. in: interna-
tional conference on business process management. pp. 306‚Äì323. springer (2017)
26. tax, n., verenich, i., rosa, m.l., dumas, m.: predictive business process monitoring with
lstm neural networks. in: dubois, e., pohl, k. (eds.) advanced information systems en-
gineering - 29th international conference, caise 2017, essen, germany, june 12-16, 2017,
proceedings. vol. 10253, pp. 477‚Äì492. springer (2017)
27. teinemaa, i., dumas, m., maggi, f.m., di francescomarino, c.: predictive business process
monitoring with structured and unstructured data. in: international conference on business
process management. pp. 401‚Äì417. springer (2016)
28. teinemaa, i., dumas, m., rosa, m.l., maggi, f.m.: outcome-oriented predictive process
monitoring: review and benchmark. acm transactions on knowledge discovery from data
(tkdd) 13(2), 1‚Äì57 (2019)
29. van dongen, b.f. (boudewijn): bpi challenge 2012 (2012).
https://doi.org/10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f
30. verbeek, e., buijs, j.c.a.m., van dongen, b.f., van der aalst, w.m.p.: prom 6: the process
mining toolkit. in: proceedings of the business process management 2010 demonstration
track, hoboken, nj, usa, september 14-16, 2010. vol. 615. ceur-ws.org (2010)
31. wang, t., zhu, j.y ., torralba, a., efros, a.a.: dataset distillation. arxiv preprint
arxiv:1811.10959 (2020)
32. wilson, d.r., martinez, t.r.: reduction techniques for instance-basedlearning algorithms.
mach. learn. 38(3), 257‚Äì286 (mar 2000). https://doi.org/10.1023/a:1007626913721
33. wilson, d.l.: asymptotic properties of nearest neighbor rules using edited data.
systems, man and cybernetics, ieee transactions on 2(3), 408‚Äì421 (july 1972).
https://doi.org/10.1109/tsmc.1972.4309137
34. zhou, l., pan, s., wang, j., vasilakos, a.v .: machine learning on big
data: opportunities and challenges. neurocomputing 237, 350‚Äì361 (2017).
https://doi.org/https://doi.org/10.1016/j.neucom.2017.01.026