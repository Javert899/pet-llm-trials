business process simulation revisited
wil m.p. van der aalst
department of mathematics and computer science,
eindhoven university of technology,
p.o. box 513, nl-5600 mb, the netherlands
w.m.p.v.d.aalst@tue.nl
abstract. computer simulation attempts to \mimic" real-life or hypo-
thetical behavior on a computer to see how processes or systems can be
improved and to predict their performance under di®erent circumstances.
simulation has been successfully applied in many disciplines and is con-
sidered to be a relevant and highly applicable tool in business process
management (bpm). unfortunately, in reality the use of simulation is
limited. few organizations actively use simulation. even organizations
that purchase simulation software (stand-alone or embedded in some
bpm suite), typically fail to use it continuously over an extended pe-
riod. this keynote paper highlights some of the problems causing the
limited adoption of simulation. for example, simulation models tend to
oversimplify the modeling of people working part-time on a process. also
simulation studies typically focus on the steady-state behavior of busi-
ness processes while managers are more interested in short-term results
(a \fast forward button" into the future) for operational decision making.
this paper will point out innovative simulation approaches leveraging on
recent breakthroughs in process mining.
1 limitations of traditional simulation approaches
simulation was one of the ¯rst applications of computers. the term \monte
carlo simulation" was ¯rst coined in the manhattan project during world war
ii, because of the similarity of statistical simulation to games of chance played
in the monte carlo casino. this illustrates that that already in the 1940s peo-
ple were using computers to simulate processes (in this case to investigate the
e®ects of nuclear explosions). later monte carlo methods were used in all kinds
of other domains ranging from ¯nance and telecommunications to games and
work°ow management. for example, note that the in°uential and well-known
programming language simula, developed in the 1960s, was designed for simula-
tion. simulation has become one of the standard analysis techniques used in the
context of operation research and operations management. simulation is par-
ticularly attractive since it is versatile, imposes few constraints, and produces
results that are relatively easy to interpret. analytical techniques have other
advantages but typically impose additional constraints and are not as easy to
use [9]. therefore, it is no surprise that in the context of business process man-
agement (bpm), simulation is one of the most established analysis techniques
supported by a vast array of tools.informationsystem(s)“world”people
machinesorganizations
business
processes documents
resources/
organizationdata/rulescontrol-flowsimulationmodels
gatherdata
andmodel
byhandgathersimulation
resultstoanswer
“what-if” questionsfig. 1. classical view on simulation: focus is on steady-state and model is made by
hand.
figure 1 positions business process simulation in the context of a \world"
supported by information systems. in the \world" consisting of people, orga-
nizations, products, processes, machines, etc. information systems play an in-
creasingly dominant role. moreover, there is continuous need for process im-
provements resulting in a better performance (e.g., better response times, less
costs, higher service levels, etc.). simulation can assist in this. figure 1 shows
the traditional use of simulation were data is gathered and used to parameter-
ize hand-made models. these models are then used for simulation experiments
answering \what-if" questions. for simulating business processes at least three
perspectives need to be modeled: (a) control-°ow , (b) data/rules , and (c) re-
source/organization . the control-°ow perspective is concerned with the ordering
of activities and uses design artifacts such as sequences, and/xor-splits/joins,
loops, etc. [1]. the data/rules perspective models decisions made within the
process and the role that data plays in these decisions. for simulation it is im-
portant not to model the data in too much detail and select the right abstraction
level. the resource/organization perspective is concerned with the allocation of
activities to resources, availability and speed of resources, and organizational
boundaries [21]. in all of this time (e.g., the duration of an activity) and prob-
abilities (e.g., the likelihood of following a particular path) play an important
role. by answering \what-if" questions, managers and users get more insight
into the e®ects of particular decisions.
although many organizations have tried to use simulation to analyze their
business processes at some stage, few are using simulation in a structured and
e®ective manner . this may be caused by a lack of training and limitations of
existing tools. however, as argued in this paper, there are also several additional
and more fundamental problems. first of all, simulation models tend to over-
2simplify things. in particular the behavior of resources is often modeled in a
rather naive manner. people do not work at constant speeds and need to dis-
tribute their attention over multiple processes. this can have dramatic e®ects
on the performance of a process [2, 15] and, therefore, such aspects should not
be \abstracted away". second, various artifacts available are not used as input
for simulation . modern organizations store events in logs and some may have
accurate process models stored in their bpm/wfm systems. also note that in
may organizations, the state of the information system accurately re°ects the
state of the business processes supported by these systems because of the tight
coupling between both. today such information (i.e., event logs and status data)
is rarely used for simulation or a lot of manual work is needed to feed this infor-
mation into the model. fortunately, process mining can assist in extracting such
information and use this to realize performance improvements [4, 7]. third, the
focus of simulation is mainly on \design" while managers would also like to use
simulation for \operational decision making" (solving the concrete problem at
hand rather than some abstract future problem). fortunately, short-term simu-
lation [16, 20, 24] can provide answers for questions related to \here and now".
the key idea is to start all simulation runs from the current state and focus the
analysis of the transient behavior. this way a \fast forward button" into the
future is provided.
in the remainder, we elaborate on the above three problems and discuss some
solution approaches grounded in process mining.
2 oversimpli¯ed simulation models
everything should be made as simple as possible,
but not one bit simpler.
albert einstein (1879-1955)
simulation can be used to predict the performance under various circumstances,
e.g., di®erent business process re-engineering alternatives can be compared with
the current situation. the value of such predictions stands or falls with the qual-
ity of the simulation model. unfortunately, in many situations the quality of the
simulation model leaves much to be desired. basically, there are three problems:
(a) the process is modeled incorrectly, (b) not enough data was collected to be
able to parameterize the model, and (c) the language does not allow for the
modeling of more subtle behaviors. the ¯rst two problems can be addressed by
training people and a better validation of the model, e.g., by comparing the sim-
ulation results with real data. here process mining can help as will be discussed
in later sections. in this section, we focus on the last problem.
probably the biggest problem of current business simulation approaches is
thathuman resources are modeled in a very naive manner . as a result, it is not
uncommon that the simulated model predicts °ow times of minutes or hours
while in reality °ow times are weeks or even months. therefore, we list some of
the main problems encountered when modeling resources in current simulation
3tools. these problems stem from the fact that resources cannot be modeled
adequately.
people are involved in multiple processes. in practice there are few people
that only perform activities for a single process. often people are involved in
many di®erent processes, e.g., a manager, doctor, or specialist may perform
tasks in a wide range of processes. however, simulation often focuses on a single
process. suppose a manager is involved in 10 di®erent processes and spends
about 20 percent of his time on the process that we want to analyze. in most
simulation tools it is impossible to model that a resource is only available 20
percent of the time. hence, one needs to assume that the manager is there all
the time and has a very low utilization. as a result the simulation results are
too optimistic. in the more advanced simulation tools, one can indicate that
resources are there at certain times in the week (e.g., only on monday). this is
also an incorrect abstraction as the manager distributes his work over the various
processes based on priorities and workload. suppose that there are 5 managers
all working 20 percent of their time on the process of interest. one could think
that these 5 managers could be replaced by a single manager (5*20%=1*100%).
however, from a simulation point of view this is an incorrect abstraction. there
may be times that all 5 managers are available and there may be times that
none of them are available.
people do not work at a constant speed. another problem is that people
work at di®erent speeds based on their workload, i.e., it is not just the distribu-
tion of attention over various processes, but also their absolute working speed
that determines their capacity for a particular process. there are various studies
that suggest a relation between workload and performance of people. a well-
known example is the so-called yerkes-dodson law [23]. the yerkes-dodson law
models the relationship between arousal and performance as a \-shaped curve.
this implies that for a given individual and a given type of tasks, there exists an
optimal arousal level. this is the level where the performance has its maximal
value. thus work pressure is productive, up to a certain point, beyond which per-
formance collapses. although this phenomenon can be easily observed in daily
life, today's business process simulation tools do not support the modeling of
workload dependent processing times.
people tend to work part-time and in batches. as indicated earlier, people
may be involved in di®erent processes. moreover, they may work part-time (e.g.,
only in the morning). in addition to their limited availabilities, people have a
tendency to work in batches (cf. resource pattern 38: piled execution [21]). in
any operational process, the same task typically needs to be executed for many
di®erent cases (process instances). often people prefer to let work-items related
to the same task accumulate, and then process all of these in one batch. in most
simulation tools a resource is either available or not, i.e., it is assumed that a
4resource is eagerly waiting for work and immediately reacts to any work-item
that arrives. clearly, this does not do justice to the way people work in reality.
for example, consider how and when people reply to e-mails. some people handle
e-mails one-by-one when they arrive while others process their e-mail at ¯xed
times in batch. related is the fact that calendars and shifts are typically ignored
in simulation tools. while holidays, lunch breaks, etc. can heavily impact the
performance of a process, they are typically not incorporated in the simulation
model.
priorities are di±cult to model. as indicated above, people are involved in
multiple processes and even within a single process di®erent activities and cases
may compete for resources. one process may be more important than another
and get priority. another phenomenon is that in some processes cases that are
delayed get priority while in other processes late cases are \sacri¯ced" to ¯nish
other cases in time. people need to continuously choose between work-items and
set priorities. although important, this is typically not captured by simulation
models.
process may change depending on context. another problem is that most
simulation tools assume a stable process and organization and that neither of
them change over time. if the °ow times become too long and work is accumulat-
ing, resources may decide to skip certain activities or additional resources may
be mobilized. depending on the context, processes may be con¯gured di®erently
and resources may be deployed di®erently. in [5] it is shown that such \second
order dynamics" heavily in°uence performance.
the problems stem from oversimpli¯ed models. note that although more than
40 resource patterns have been identi¯ed to describe the functionality of resource
allocation mechanisms in the context of work°ow management systems [21], few
of these patterns are supported by today's business process simulation tools.
3 learning from event logs
learning is not compulsory ... neither is survival.
william edwards deming (1900-1993)
as discussed in the previous section, simulation models tend not to capture cer-
tain aspects or stick to an idealized variant of the real process. this can be partly
addressed by better modeling techniques, e.g., additional parameters describing
the resource characteristics. however, to adequately set these parameters and to
make sure that processes are modeled accurately, we propose to also exploit the
information available in event logs .
more and more information about (business) processes is recorded by in-
formation systems in the form of so-called \event logs" (e.g., transaction logs,
5audit trails, databases, message logs). as mentioned earlier, it systems are be-
coming more and more intertwined with the processes they support, resulting in
an \explosion" of available data that can be used for analysis purposes.
informationsystem(s)
current
data“world”people
machinesorganizations
business
processes documents
historic
data
resources/
organizationdata/rulescontrol-flowdejuremodels
resources/
organizationdata/rulescontrol-flowdefactomodelsprovenanceexplore
predict
recommend
detect
check
compare
promote
discover
enhance
diagnosecartography navigation auditingeventlogs
(simulation)models“pre 
mortem”“post 
mortem”
simulationtoanswer
“what if” questions based 
oncurrentstatesimulationtoanswer
“what if” questions in 
steady-state
fig. 2. advanced business process simulation put into the context of process mining.
to illustrate the role that event logs can play, let us ¯rst explain figure 2. we
assume the existence of a collection of information systems that are supporting
a \world" composed of business processes, people, organizations, etc. the event
data extracted from such systems are the starting point for process mining . note
that figure 2 distinguishes between current data andhistoric data . the former
refers to events of cases (i.e., process instances) that are still actively worked
on (\pre mortem"). the latter refers to events of completed cases, i.e., process
instances that cannot be in°uenced anymore (\post mortem"). the historic data
(\post mortem") can be any collection of events where each event refers to an
instance (i.e., case), has a name (e.g., activity name), and has a timestamp. note
that some process mining techniques abstract from time. however, in the context
of business process simulation these timestamps are of the utmost importance.
6the current data (\pre mortem") can be used to construct a well de¯ned starting
point for simulation. this is of particular importance for predictions in the near
future.
the collection of event data is becoming more important. one the one hand,
more and more event data are available. on the other hand, organizations de-
pend on such data; not only for performance measurement, but also for auditing.
we use the term business process provenance [10, 11] to refer to the systematic
collection of the information needed to reconstruct what has actually happened.
the term signi¯es that for most organizations it is vital that \history cannot be
rewritten or obscured". from an auditing point of view the systematic, reliable,
and trustworthy recording of events is essential. therefore, we propose to collect
(whenever possible) provenance data outside of the operational information sys-
tem(s) as shown in figure 2. this means that events need to be collected and
stored persistently. note that semantics play an important role here, i.e., events
need to refer to a commonly agreed-upon ontology [14].
the lower part of figure 2 shows two types of models: de jure models are
normative models that describe a desired or required way of working while de
facto models aim to describe the actual reality with all of its intricacies (policy
violations, ine±ciencies, fraud, etc.). both types of models may cover one or more
perspectives and thus describe control-°ow, time, data, organization, resource,
and/or cost aspects. for process mining one can focus on a particular perspective.
however, when the goal is to build simulation models all factors in°uencing
performance need to be taken into account (e.g., when measuring utilization
and response times, it is not possible to abstract from resources and focus on
control-°ow only). simulation models can be based on a mixture of \de jure"
and \de facto" information. the key idea of process mining is to not simply rely
on de jure models that may have little to do with reality. therefore, the goal is
to shift more to \de facto models for simulation"; this will save time and increase
quality.
in figure 2 three main categories of activities have been identi¯ed: cartog-
raphy ,auditing , and navigation . the individual activities are brie°y described
below.
1.discover. the discovery of good process models from events logs - compa-
rable to geographic maps - remains challenging. process discovery techniques
can be used to discover process models (e.g., petri nets) from event logs [4,
7].
2.enhance. existing process models (either discovered or hand-made) need
to be related to events logs such that these models can be enhanced by
making them more faithful or by adding new perspectives based on event
data. by combining historic data and pre-existing models, these models can
be repaired (e.g., a path that is never taken is removed) or extended (e.g.,
adding time information extracted from logs).
3.diagnose. models (either de jure or de facto) need to be analyzed using
existing model-based analysis techniques, e.g., process models can be checked
7for the absence of deadlocks or simulated to estimate cycle times. probably
the most widely used model-based analysis technique is simulation.
4.detect. for on-line auditing, de jure models need to be compared with
current data (events of running process instances) and deviations of such
partial cases should to be detected at runtime. by replaying the observed
events on a model, it is possible to do conformance checking while the process
is unfolding.
5.check. similarly, historic \post mortem" data can be cross-checked with
de jure models. for this conformance checking techniques are used that can
pinpoint deviations and quantify the level of compliance [18].
6.compare. de facto models can be compared with de jure models to see in
what way reality deviates from what was planned or expected.
7.promote. based on an analysis of the di®erences between a de facto model
and a de jure model, it is possible to promote parts of the de facto model to
a new de jure model. by promoting proven \best practises" to the de jure
model, existing processes can be improved. for example, a simulation model
may be improved and calibrated based on elements of a de facto model.
8.explore. the combination of event data and models can be used to explore
business processes. here new forms of interactive process visualization can
be used (visual analytics).
9.predict. by combining information about running cases with models (dis-
covered or hand-made), it is possible to make predictions about the future,
e.g., the remaining °ow time and the probability of success. here simulation
plays an important role. this will be elaborated in section 4.
10.recommend. the information used for predicting the future can also be
used to recommend suitable actions (e.g. to minimize costs or time). the
goal is to enable functionality similar to the guidance given by navigation
systems like tomtom, but now in the context of bpm.
the ¯rst three activities are grouped under the term \cartography". over time
cartographers have improved their skills and techniques to create maps thereby
addressing problems such as clearly representing desired traits, eliminating ir-
relevant details, reducing complexity, and improving understandability. today,
geographic maps are digital and of high quality. people can seamlessly zoom in
and out using the interactive maps (cf. navigation systems like tomtom and
services linked to google maps). moreover, all kinds of information can be pro-
jected on these interactive maps (e.g., tra±c jams, etc.). process models can be
seen as the \maps" describing the operational processes of organizations. process
mining techniques can be used to generate such maps. these maps can be sim-
ple and without executable semantics. however, as shown in [19] also simulation
models can be discovered.
the next four activities are grouped under the term \auditing" as they com-
pare normative/modeled behavior with real/recorded behavior. this does not
involve simulation; however, these activities may help to increase the quality of
discovered/hand-made simulation models.
the last three activities are grouped under the term \navigation". navigation
systems have proven to be quite useful for many drivers. people increasingly rely
8on the devices of tomtom, garmin and other vendors and ¯nd it useful to get
directions to go from a to b, know the expected arrival time , learn about tra±c
jams on the planned route, and be able to view maps that can be customized
in various ways (zoom-in/zoom-out, show fuel stations, speed limits, etc.). how-
ever, when looking at business processes and their information systems, such
information is typically lacking . fortunately, a combination of process mining
and simulation can help to provide navigation capabilities. the next section
focuses on this.
4 operational support
if you don't know where you are going, any road will get you there.
lewis carroll (1832-1898)
figure 2 illustrated that event logs can be used for all kinds of analysis, e.g., event
logs can be used to discover and improve simulation models. in this section, we
focus on short-term simulation , i.e., a detailed analysis of the near future based
on the current state. traditionally, business process simulation is mainly used
for steady-state analysis and not for operational decision making. to explain the
importance of short-term simulation, we ¯rst elaborate on the di®erence between
transient analysis andsteady-state analysis .
the key idea of simulation is to execute a model repeatedly. the reason for
doing the experiments repeatedly, is to not come up with just a single value (e.g.,
\the average response time is 10.36 minutes") but to provide con¯dence intervals
(e.g., \the average response time is with 90 percent certainty between 10 and
11 minutes"). for transient analysis the focus is on the initial part of future
behavior, i.e., starting from the initial state the \near future" is explored. for
transient analysis the initial state is very important. if the simulation starts in
a state with long queues of work, then in the near future °ow times will be long
and it may take some time to get rid of the backlog. for steady-state analysis
the initial state is irrelevant. typically, the simulation is started \empty" (i.e.,
without any cases in progress) and only when the system is ¯lled with cases the
measurements start.
steady-state analysis is most relevant for answering strategic and tactical
questions. transient analysis is most relevant for operational questions. lion's
share of contemporary simulation support aims at steady-state analysis and
hence at strategic and tactical decision making. we advocate more emphasis on
simulation for operational decision making . therefore, we elaborate on short-
term simulation and relate this to process mining and operational support.
figure 3 shows the input used for operational support. historic data , i.e.,
event logs, can be used to discover new models and to enhance existing models.
this was already discussed in the previous section. the learned models can be
combined with current data (i.e., states of cases and partial execution traces)
todetect deviations, predict performance, and to recommend decisions. predic-
tions may be based on regression models [12]. however, to predict more complex
9current
data
historic
data(simulation)
models
learn
(discoverandenhance)detect
predict
recommendalerts
predictions
recommendationsfig. 3. overview of operational support and the di®erent types of data used.
dynamic behavior, simulation can be used. in this paper, we distinguish be-
tween operational support at the instance level and at the aggregate level . the
instance level focuses on a single case, e.g., a particular loan application that is
being processed. it may be detected that the application is delayed and because
of this an alert is generated. moreover, for the partially executed loan application
it may be predicted that the expected remaining processing time is two weeks
and that therefore it is recommended to bypass an external credit check. unlike
recommendations and predictions at the instance level, operational support at
the aggregate level is concerned with the whole process (or even a set of pro-
cesses). problems are now detected at the aggregate level (\response times are
too long"). moreover, predictions and recommendations are at the process level
and do not refer to particular instances.
table 1 provides examples of operational support questions. both levels (in-
stance level and aggregate level) are discussed in the remainder.
operational support at the instance level. figure 4 illustrates the three
types of operational support. starting point is some model and a partial trace.
note that the model is typically learned using classical process mining tech-
niques. the partial trace refers to a case that is running. the left-hand side of
figure 4 shows a partial trace ha; bi. although figure 4 does not show times-
tamps, resources, data, etc., these may be relevant for operational support.
for the case shown in figure 4, we know that aandboccurred, but we do
not know its future. suppose now that the partial trace ha; biis not possible
according to the model. in this case, the operational support system should gen-
erate an alert. another possibility would be that btook place three weeks after
awhile this should happen within one week. in such a case another noti¯cation
could be sent to the responsible case manager. such scenarios correspond to the
check activity mentioned before. figure 4 also illustrates the goal of predictions .
10table 1. examples of various types of operational support at the instance level and
the aggregate level.
type of opera-
tional supportinstance level aggregate level
detectpartially executed cases are
monitored. as soon as a de-
viation occurs (e.g., a task is
skipped or too late) an alert is
given.processes are monitored as a
whole and as soon as a devia-
tion occurs (e.g., the average re-
sponse times are too high or too
many cases are in the pipeline)
an alert is given.
predictpredictions are made for spe-
ci¯c cases, e.g., after each step
the expected remaining process-
ing time of the case is given.
predictions may also refer to
costs and quality, e.g., the like-
lihood of succes for a particu-
lar instance. short-term simu-
lation can be used to generate
such instance-level predictions.predictions are made for one
process or a collection of pro-
cesses. for example, it is pre-
dicted what the average °ow
time will be in the next two
weeks. predictions at the aggre-
gate level may also refer to uti-
lization (\how busy will people
be next week?"), costs (\will
we reach the break-even point
in this quarter?"), service levels,
etc.
recommendpredictions at the instance level
can be turned into recommenda-
tions by exploring the e®ect of
various decisions. for example,
di®erent routing choices can be
simulated to predict the e®ect
of such choices. similarly, the ef-
fect of various allocation choices
can be compared using simula-
tion.predictions at the aggregate
level can be used to gener-
ate recommendations. the ef-
fect of each decision can be an-
alyzed using short-term simula-
tion. for example, it may be rec-
ommended to temporarily hire
two additional workers to avoid
excessive waiting times.
11given the current state of a case, the model is used to make some kind of pre-
diction [3, 6]. for example, given the ha; bitrace it could be predicted that the
remaining processing time is ten days. this prediction would be based on his-
toric information both in the partial trace and in the event log used to learn the
model. for the actual prediction a simple regression model can be used. how-
ever, for more complex scenarios, short-term simulation is a more likely option.
predictions are not restricted to time, but can also refer to costs, probability
of a particular outcome, resource availability, etc. closely related to predictions
arerecommendations [3, 22]. the main di®erence is that recommendations sug-
gest the next action based on possible continuations of the case. based on the
model, one can try all possible actions and see which one would lead to the
best (predicted) performance. note that recommendations are not only used for
determining the next task, but also for allocating resources to work-items or for
timing a particular action.
abcdknown
pastunknown
futurecurrent
state
abd ab??abc?
check:ddoesnotfitthe
model(notallowed,too
late,etc.)predict:somepredictionis
madeaboutthefuture(e.g.
completiondateoroutcome)t=10
recommend :basedonpast
experiencescisrecommended
(e.g.,tominimizecosts)
fig. 4. operational support at the instance level [3].
operational support at the aggregate level. in figure 4 analysis is done
at the instance level. however, many operational decisions transcend the level of
an individual case. decisions like temporarily adding two workers or stimulate
overwork are made at the level of one or more processes rather than a single
case. short-term simulation is particularly useful for predictions at the aggregate
level. here, simple regression models are unable to capture queueing e®ects,
dependencies, and typical work patterns.
short-term simulation starts from the current state [16, 20, 24]. when a process-
aware information system is present, it is relatively easy to extract the current
state from the system and to upload this into the simulation model. by modi-
fying the simulation model, various \what-if" scenarios can be investigated. for
example, one can add or remove resources, skip activities, etc. and see what the
e®ect is. because the simulation experiments for these scenarios start from the
current state of the actual system, they provide a kind of \fast-forward button"
showing what will happen in the near future, to support operational decision
making. for instance, based on the predicted system behavior, a manager may
decide to hire more personnel or stop accepting new cases.
125 conclusion and further reading
the goal of this keynote paper is to provide a critical analysis of the mainstream
simulation approaches for process management. on the one hand, the paper is
based on practical experiences in numerous simulation projects (cf. [17] for ex-
amples). these experiences showed amongst others that it is almost impossible
to adequately model resources in contemporary simulation tools. on the other
hand, various process mining projects showed that reality rarely matches the ex-
pectations of the modeler. models tend to describe idealized/unrealistic views on
the business processes at hand. these practical experiences with simulation and
process mining resulted in a better understanding of the pitfalls of traditional
business process analysis. some of the lessons learned have been reported. more-
over, as shown, business process simulation can bene¯t from recent breakthroughs
in process mining .
several of the ideas presented in this paper have been realized in the context
ofprom (www.processmining.org, [8]) and yawl (www.yawlfoundation.org,
[13]). to conclude this paper, we provide pointers to papers detailing these re-
sults.
in [3] a concrete approach to operational support is given. this has been im-
plemented in prom and time-based predictions and recommendations are given
by learning a transition system annotated with time information [6]. the focus
in [3] is restricted to individual cases and temporal aspects.
in [15] it is shown how event logs can be used to learn about the behavior
of people. for example, through process mining one can ¯nd empirical evidence
for the yerkes-dodson law [23] and parameterize the corresponding simulation
models.
prom provides comprehensive support for the automated discovery of simu-
lation models based on event logs. in [19] it is shown how di®erent perspectives
can be discovered and merged into one overall simulation model.
while the focus in [19] is on simulation models for steady-state analysis, the
focus of [20] is on short-term simulation, i.e., transient analysis. this is achieved
by an integration of prom and yawl. the work°ow model, event log, and
current state information provided by the work°ow system yawl are used by
prom to generate simulation models. these models are simulated using cpn
tools. key element is that the simulation model is called continuously while
using the latest state information. this way a \fast-forward button" is added to
yawl that allows users and manager explore the near future.
one of the key problems when using business process simulation is the fact
that it is unrealistic to assume that people are continuous available. availabil-
ity and work-speed are °uid. as shown in [2], it is important to capture and
parameterize this \°uidity" as it has a dramatic e®ect on °ow times, etc.
the papers mentioned above present innovations in business process simu-
lation. although quite some work has been done in the context of prom and
yawl, it remains crucial to further improve techniques and tools to better cap-
ture faithful simulation models. hopefully this will stimulate more organizations
to reap the bene¯ts of business process simulation.
13acknowledgments. the author would like to thank all the people that con-
tributed to the development of prom and yawl. this paper refers to simula-
tion techniques developed together with joyce nakatumba, anne rozinat, moe
wynn, ronny mans, minseok song, and several others.
references
1.w.m.p. van der aalst, a.h.m. ter hofstede, b. kiepuszewski, and a.p. barros.
work°ow patterns. distributed and parallel databases , 14(1):5{51, 2003.
2.w.m.p. van der aalst, j. nakatumba, a. rozinat, and n. russell. business pro-
cess simulation: how to get it right? in j. vom brocke and m. rosemann, editors,
handbook on business process management , international handbooks on informa-
tion systems, pages 317{342. springer-verlag, berlin, 2010.
3.w.m.p. van der aalst, m. pesic, and m. song. beyond process mining: from
the past to present and future. in b. pernici, editor, advanced information sys-
tems engineering, proceedings of the 22nd international conference on advanced
information systems engineering (caise'10) , volume 6051 of lecture notes in
computer science , pages 38{52. springer-verlag, berlin, 2010.
4.w.m.p. van der aalst, h.a. reijers, a.j.m.m. weijters, b.f. van dongen, a.k.
alves de medeiros, m. song, and h.m.w. verbeek. business process mining: an
industrial application. information systems , 32(5):713{732, 2007.
5.w.m.p. van der aalst, m. rosemann, and m. dumas. deadline-based escalation
in process-aware information systems. decision support systems , 43(2):492{511,
2007.
6.w.m.p. van der aalst, m.h. schonenberg, and m. song. time prediction based
on process mining. bpm center report bpm-09-04, bpmcenter.org, 2009.
7.w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. work°ow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128{1142, 2004.
8.w.m.p. van der aalst and. b. van dongen, c.w. gä unther, a. rozinat, e. verbeek,
and t. weijters. prom: the process mining toolkit. in a.k.a. de medeiros and
b. weber, editors, business process management demonstration track (bpmde-
mos 2009) , volume 489 of ceur workshop proceedings , pages 1{4. ceur-ws.org,
2009.
9.j.a. buzacott. commonalities in reengineerd business processes: models and
issues. management science , 42(5):768{782, 1996.
10.f. curbera, y. doganata, a. martens, n. mukhi, and a. slominski. business
provenance: a technology to increase traceability of end-to-end operations. in
r. meersman and z. tari, editors, proceedings of the 16th international conference
on cooperative information systems, coopis 2008, otm 2008, part i , volume
5331 of lecture notes in computer science , pages 100{119. springer-verlag, berlin,
2008.
11.s. davidson, s. cohen-boulakia, a. eyal, b. ludaescher, t. mcphillips, s. bow-
ers, m. anand, and j. freire. provenance in scienti¯c work°ow systems. data
engineering bulletin , 30(4):44{50, 2007.
12.b.f. van dongen, r.a. crooy, and w.m.p. van der aalst. cycle time prediction:
when will this case finally be finished? in r. meersman and z. tari, editors,
proceedings of the 16th international conference on cooperative information sys-
tems, coopis 2008, otm 2008, part i , volume 5331 of lecture notes in computer
science , pages 319{336. springer-verlag, berlin, 2008.
1413.a.h.m. ter hofstede, w.m.p. van der aalst, m. adams, and n. russell. modern
business process automation: yawl and its support environment . springer-
verlag, berlin, 2010.
14.a.k. alves de medeiros and w.m.p. van der aalst. process mining towards se-
mantics. in t.s. dillon, editor, advances in web semantics i , volume 4891 of
lecture notes in computer science , pages 35{80. springer-verlag, berlin, 2008.
15.j. nakatumba and w.m.p. van der aalst. analyzing resource behavior using
process mining. in s. rinderle-ma, s. sadiq, and f. leymann, editors, bpm 2009
workshops, proceedings of the fifth workshop on business process intelligence
(bpi'09) , volume 43 of lecture notes in business information processing , pages
69{80. springer-verlag, berlin, 2010.
16.h.a. reijers and w.m.p. van der aalst. short-term simulation: bridging the gap
between operational control and strategic decision making. in m.h. hamza,
editor, proceedings of the iasted international conference on modelling and
simulation , pages 417{421. iasted/acta press, anaheim, usa, 1999.
17.h.a. reijers and w.m.p. van der aalst. the e®ectiveness of work°ow manage-
ment systems: predictions and lessons learned. international journal of infor-
mation management , 25(5):458{472, 2005.
18.a. rozinat and w.m.p. van der aalst. conformance checking of processes based
on monitoring real behavior. information systems , 33(1):64{95, 2008.
19.a. rozinat, r.s. mans, m. song, and w.m.p. van der aalst. discovering simulation
models. information systems , 34(3):305{327, 2009.
20.a. rozinat, m. wynn, w.m.p. van der aalst, a.h.m. ter hofstede, and c. fidge.
work°ow simulation for operational decision support. data and knowledge en-
gineering , 68(9):834{850, 2009.
21.n. russell, w.m.p.van der aalst, a.h.m. ter hofstede, and d. edmond. work°ow
resource patterns: identi¯cation, representation and tool support. in o. pastor
and j. falcao e cunha, editors, proceedings of the 17th conference on advanced
information systems engineering (caise'05) , volume 3520 of lecture notes in
computer science , pages 216{232. springer-verlag, berlin, 2005.
22.h. schonenberg, b. weber, b.f. van dongen, and w.m.p. van der aalst. support-
ing flexible processes through recommendations based on history. in m. dumas,
m. reichert, and m.c. shan, editors, international conference on business pro-
cess management (bpm 2008) , volume 5240 of lecture notes in computer science ,
pages 51{66. springer-verlag, berlin, 2008.
23.c.d. wickens. engineering psychology and human performance . harper, 1992.
24.m. wynn, a. rozinat, w.m.p. van der aalst, a.h.m. ter hofstede, and c. fidge.
chapter 17: process mining and simulation. in modern business process automa-
tion: yawl and its support environment , pages 437{457. springer-verlag, berlin,
2010.
15