responsible data science in a dynamic world
the four essential elements of data science
wil m.p. van der aalst
lehrstuhl f ur informatik 9, process and data science, rwth aachen university,
d-52056 aachen, germany
wvdaalst@pads.rwth-aachen.de
http://vdaalst.com
abstract. data science is changing our world in many dierent ways.
data and the associated data science innovations are changing every-
thing: the way we work, the way we move, the way we interact, the way
we care, the way we learn, and the way we socialize. as a result, many
professions will cease to exist. for example, today's call centers will dis-
appear just like video rental shops disappeared. at the same time, new
jobs, products, services, and opportunities emerge. hence, it is impor-
tant to understand the essence of data science. this extended abstract
discusses the four essential elements of data science: \water" (availabil-
ity, magnitude, and dierent forms of data), \re" (irresponsible uses of
data and threats related to fairness, accuracy, condentiality, and trans-
parency), \wind" (the way data science can be used to improve pro-
cesses), and \earth" (the need for data science research and education).
next to providing an original view on data science, the abstract also
highlights important next steps to ensure that data will not just change,
but also improve our world.
keywords: data science responsible data science process mining 
big data.
1 data science
this extended abstract is based on a keynote given at the ifip world com-
puter congress (wcc 2018) on 18 september 2018, in poznan, poland. the
main theme of wcc 2018 was \information processing in an increasingly con-
nected world: opportunities and threats". data science is the main driver for
the changes that create these opportunities and threats. recent reports [6, 7]
indicate that many jobs will cease to exist because of advances in machine learn-
ing, articial intelligence, robotics, and other forms of smart automation. these
advances are only possible because of both the availability of data and progress
in data science.
it is not easy to dene data science. the data science pipeline shown in
figure 1 illustrates the breadth of the discipline. the \infrastructure" part of the
pipeline is concerned with the huge volume and incredible velocity of data. hence,2 wil van der aalst
infrastructure analysis effect
o big data infrastructures
o distributed systems
o data engineering
o programming 
o security
o ...o statistics
o data/process mining
o machine learning
o artificial intelligence
o visualization
o ...o ethics & privacy
o it law
o operations management
o business models
o entrepreneurship
o ...‚Äúvolume and velocity ‚Äù ‚Äúextracting knowledge ‚Äù ‚Äúpeople , organizations , society‚Äù
mechanical 
engineeringmedicine
social sciences
logisticsscientific 
workflows
energy
high -tech 
systems
fig. 1. the data science pipeline showing that dierent capabilities are needed to turn
data into value.
the primary focus is on making things scalable and instant. the \analysis" part
of the pipeline is concerned with extracting knowledge. this is about providing
answers to known and unknown unknowns.1the \eect" part of the pipeline is
concerned the impact of data science on people, organizations, and society. here
legal, ethical, and nancial aspects come into play.
the uptake of the internet of things (iot) illustrates the pivotal role of data
science. more and more devices (light bulbs, clothes, refrigerators, containers,
bicycles, etc.) are connected to the internet and produce data. these devices are
becoming \smart" by learning from the data collected. the internet of things
(iot) depends on the whole data science pipeline shown in figure 1. we are
(or will be) surrounded by smart devices collecting data and the impact of this
cannot be overestimated.
in the remainder, we dene the four essential elements of data science.
as metaphor we use the classical four elements: \water", \re", \wind", and
\earth". according to the empedocles, a greek pre-socratic philosopher who
lived in sicily in the fth century b.c., all matter is comprised of these four
elements. other ancient cultures had similar lists, sometimes also composed of
more elements (e.g., earth, water, air, re, and aether) that tried to explain
nature and complexity of all matter in terms of simpler substances. today, we
know that this is not the case. however, for data science, we are still in the phase
where we are looking for the essential elements. this paper uses \water" as a
placeholder for the availability of dierent forms of data, \re" as a placeholder
for irresponsible uses of data (e.g., threats to fairness, accuracy, condential-
ity, and transparency), \wind" as a placeholder for the way that data science
1\there are known knowns; there are things we know we know. we also know there
are known unknowns; that is to say we know there are some things we do not know.
but there are also unknown unknowns the ones we don't know we don't know."
(donald rumsfeld, february 12, 2002)responsible data science in a dynamic world 3
can be used to improve processes, and \earth" as a placeholder for education
and research (i.e., the base of data science) underpinning all of this. these four
essential elements are discussed in the remaining sections.
2 the \water" of data science
the rst essential element of data science (\water") is the data itself. the ex-
ponential growth of data is evident. figure 2 (inspired by the analysis in [9])
shows the rapid developments in terms of costs (things are getting exponen-
tially cheaper), speed (things are going exponentially faster), and miniaturiza-
tion (things are getting exponentially smaller). this is not limited to processing
(i.e., cpu and gpu processors), but also applies to storage andcommunication .
consider for example the costs of storage. to store one megabyte (mb) of data
in the sixties one would need to pay one million euros. today, one can buy a
10tb harddisk for less than 300 euro, i.e., 0.00003 cents per mb. another exam-
ple is the bandwidth eciency, also called spectral eciency, which refers to the
information rate that can be transmitted over a given bandwidth. it is the net
bitrate (useful information rate excluding error-correcting codes) or maximum
throughput divided by the bandwidth in hertz of a communication channel or a
data link. the spectacular progress of our data handling capabilities illustrated
by figure 2, explains why data science has become on of the key concerns in any
organization. in the sixties, we only had a few \drops of data" whereas today
we are facing a \tsunami of data" ooding our society.
clearly, data science has its roots in statistics, a discipline that developed
over four centuries [1]. john graunt (1620-1674) started to study london's death
records around 1660. based on this he was able to predict the life expectancy
of a person at a particular age. francis galton (1822-1911) introduced statis-
tical concepts like regression and correlation at the end of the 19th century.
although data science can be seen as a continuation of statistics, the major-
ity of statisticians did not contribute much to recent progress in data science.
most statisticians focused on theoretical results rather than real-world analysis
problems. the computational aspects, which are critical for larger data sets, are
typically ignored by statisticians. the focus is on generative modeling rather
than prediction and dealing with practical challenges related to data quality
and size. when the data mining community realized major breakthroughs in the
discovery of patterns and relationships (e.g., eciently learning decision trees
and association rules), most statisticians referred to these discovery practices as
\data shing", \data snooping", and \data dredging" to express their dismay
[1, 4, 10].
put dierently; most statisticians were focused on techniques to make reliable
statements given a few \drops of data". such viewpoints turned out to be less
eective when dealing with \tsunamis of data".4 wil van der aalst
price per gb (‚Ç¨)
1980 1990 2000 2010 2020latency (ms)
1980 1990 2000 2010 2020gb/cm2 
1980 1990 2000 2010 2020
cost of a processing cycle (‚Ç¨)
1980 1990 2000 2010 2020processor speed (mhz )
1980 1990 2000 2010 2020transistors per chip
1980 1990 2000 2010 2020
price per mbps (‚Ç¨)
1980 1990 2000 2010 2020download /upload speed  (mbps )
1980 1990 2000 2010 2020bandwidth efficiency ((bit/s)/hz) 
1980 1990 2000 2010 2020cheaper faster more compactstorage processing communication
fig. 2. moore's law predicts an exponential growth of the number of transistors per
chip. this can be generalized to storage and transition and also applies to costs and
speed.
3 the \fire" of data science
the second essential element of data science (\re") refers to the dangers of
using data in an irresponsible way. data abundance combined with powerful
data science techniques has the potential to dramatically improve our lives by
enabling new services and products, while improving their eciency and quality.
many of today's scientic discoveries (e.g., in health) are already fueled by devel-
opments in statistics, mining, machine learning, articial intelligence, databases,
and visualization. at the same time, there are also great concerns about the use
of data. increasingly, customers, patients, and other stakeholders are concerned
about irresponsible data use. automated data decisions may be unfair or non-
transparent. condential data may be shared unintentionally or abused by third
parties.
from 2015 until 2017, the author led the responsible data science (rds)
initiative where the strongest dutch data science groups joined forces to ad-
dress problems related to fairness ,accuracy ,condentiality , and transparency
(www.responsibledatascience.org). the goal of rds is to show that data science
techniques, infrastructures and approaches can be made responsible by design.
responsible data science (rds) revolves around four main challenges:responsible data science in a dynamic world 5
{data science without prejudice - how to avoid unfair conclusions even if they
are true?
{data science without guesswork - how to answer questions with a guaranteed
level of accuracy?
{data science that ensures condentiality - how to answer questions without
revealing secrets?
{data science that provides transparency - how to clarify answers such that
they become indisputable?
the term green data science was introduced for cutting-edge solutions that
enable individuals, organizations and society to benet from widespread data
availability while ensuring fairness ,accuracy ,condentiality , and transparency
(fact) [2].
na vely one could think that \re" can be controlled by \water", however
this is not the case. when considering rds, it is better to consider data as \oil"
rather than \water". it needs to be controlled and stored carefully.
there is a need for new and positive data science techniques that are respon-
sible (i.e., \green") by design. this cannot be solved by stricter laws. using the
metaphor of \green energy": we should not be against the use of energy (\data"),
but address the pollution caused by traditional engines. fortunately, there are
plenty of ideas to make data science green. for example, discrimination-aware
data mining [8] can be used to ensure fairness and polymorphic encryption can
be used to ensure condentiality.
4 the \wind" of data science
the third essential element of data science (\wind") is concerned with the way
data and processes interact. storing and processing data is not a goal in itself.
data are there to support processes. the campaign \the best run companies
run sap" illustrates that the purpose of information systems is to ensure that
processes run well. data science can help organizations to be more eective, to
provide a better service, to deliver faster, and to do all of this at lower costs.
this applies to logistics, production, transport, healthcare, banking, insurance,
and government. this also applies to individuals. data science will increasingly
support our personal workows and take over tasks, or at least support them.
data (\water") can be used to manage and support processes (\wind") through
the use of data science technologies.
an emerging technology linking \water" and \wind" is process mining [1].
process mining bridges the gap between traditional model-based process analysis
(e.g., simulation and other business process management techniques) and data-
centric analysis techniques such as machine learning and data mining. process
mining seeks the confrontation between event data (i.e., observed behavior) and
process models (hand-made or discovered automatically) [1]. the process-mining
spectrum is broad and includes techniques for process discovery, conformance
checking, prediction, and bottleneck analysis. these techniques tend to be very6 wil van der aalst
dierent from mainstream data mining and machine learning techniques which
are typically not process-centric.
consider for example the topic of robotic process automation (rpa). rpa
is an umbrella term for tools that operate on the user interface of other com-
puter systems in the way a human would do. rpa aims to replace people by
automation done in an \outside-in" manner [3]. this diers from the classical
\inside-out" approach to improve information systems. unlike traditional work-
ow technology, the information system remains unchanged. the robots are re-
placing humans while leaving the back-end systems intact. rpa is a way to
support processes in a more cost-eective manner. however, this requires learn-
ing what humans do by observing them. data science approaches like process
mining can be used to learn the behavior of people doing routine tasks. after
the desired behavior has been \played in", it can be \played out" to handle new
cases in an intelligent manner.
rpa illustrates that data science will lead to new trade-os between what
humans do and what robots do [6, 7]. these trade-os are interesting: how to
distribute work between given breakthroughs in data science? obviously, the
question needs to take the \re" dimension into account.
5 the \earth" of data science
the fourth essential element of data science (\earth") is concerned with the
foundations of a data-driven society: education andresearch . education (in every
sense of the word) is one of the fundamental factors in the development of data
science. data science education is needed at any level. people need to be aware
of the way algorithms make decisions that may inuence their lives. privacy
discussions reveal the ignorance of policy makers and end users. moreover, to
remain competitive, countries should invest in data science capabilities. this can
only be realized through education. data science research plays a similar role.
on the one hand, it is key for our education. on the other hand, research is
needed to address the many technological and societal challenges (e.g., ensuring
fairness, accuracy, condentiality, and transparency).
currently, eight of the world's ten biggest companies, as measured by market
capitalization, are american: apple, alphabet (incl. google), microsoft, ama-
zon, berkshire hathaway, facebook, jpmorgan chase, and bank of america.2
the two remaining companies are chinese: alibaba and tencent holdings. this
shows the dominance of a few countries due to investments in it. most of the
companies are relatively new and emerged through the smart use of data. ama-
zon and alibaba are dominating the way we buy products. google is controlling
the way we search. facebook is controlling the way we socialize. apple, al-
phabet, and microsoft are controlling the platforms we use (ios, android, and
windows). consider for example facebook. on the one hand, many people are
expressing concerns about the use of data. on the other hand, facebook has
2based on market capitalization data by bloomberg on 31 march 2018.responsible data science in a dynamic world 7
fig. 3. the \water", \re", \wind", and \earth" of data science.
over 2 billion monthly active users that provide personal information in order to
use social media. one of the problems of data science is that due to economies
of scale \the winner takes it all". this may also apply to education, e.g., on
coursera a few us universities are dominating data science education.
data science literacy and major public investments are needed to address
these concerns. this cannot be left to \the market" or solved through half-
hearted legislation like the european general data protection regulation (gdpr)
[5].
6 epilogue
this extended abstract aimed to present some of the key messages of the keynote
presentation for the ifip world computer congress (wcc 2018). it stresses
the importance of data science for people, organizations, and society. just like
computer science emerged as a new discipline from mathematics in the early
eighties, we can now witness that the data science discipline is emerging from
computer science, statistics, and social sciences.
in this paper, we discussed the four essential elements of data science: \wa-
ter" (availability, magnitude, and dierent forms of data), \re" (irresponsible
uses of data and threats related to fairness, accuracy, condentiality, and trans-
parency), \wind" (the way data science can be used to improve processes), and
\earth" (the need for data science research and education). by presenting data
science in this manner, we hope to get more attention for process-centric forms of
data science (e.g., process mining), responsible data science, data science educa-
tion, and data science research. the dominance of a few companies and countries8 wil van der aalst
when it comes to data science is undesirable and requires the attention of politi-
cians and policymakers. the ifip could and should play an active role in this
discussion.
references
1. w.m.p. van der aalst. process mining: data science in action . springer-verlag,
berlin, 2016.
2. w.m.p. van der aalst. responsible data science: using big data in a \people
friendly" manner. in s. hammoudi, l. maciaszek, m. missiko, o. camp, and
j. cordiero, editors, enterprise information systems , volume 291 of lecture notes
in business information processing , pages 3{28. springer-verlag, berlin, 2017.
3. w.m.p. van der aalst, m. bichler, and a. heinzl. robotic process automation.
business and information systems engineering , 60(4):269{272, 2018.
4. l. breiman. statistical modeling: the two cultures. statistical science , 16(3):199{
231, 2001.
5. european commission. proposal for a regulation of the european parliament and
of the council on the protection of individuals with regard to the processing of
personal data and on the free movement of such data (general data protection
regulation). 9565/15, 2012/0011 (cod), june 2015.
6. c.b. frey and m.a. osborne. the future of employment: how susceptible are jobs
to computerisation? technological forecasting and social change , 114(c):254{280,
2017.
7. j. hawksworth, r. berriman, and s. goel. will robots really steal our jobs?
an international analysis of the potential long term impact of automation.
technical report, pricewaterhousecoopers, 2018.
8. d. pedreshi, s. ruggieri, and f. turini. discrimination-aware data mining. in
proceedings of the 14th acm sigkdd international conference on knowledge
discovery and data mining , pages 560{568. acm, 2008.
9. r.brennenraedts, a. vankan, r. te velde, b. minne, j. veldkamp, and
b. kaashoek. the impact of ict on the dutch economy. technical report,
dialogic, 2014.
10. j.w. tukey. the future of data analysis. annals of mathematical statistics ,
33(1):1{67, march 1962.