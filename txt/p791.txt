finding suitable activity clusters for
decomposed process discovery
b.f.a. hompes, h.m.w. verbeek, and w.m.p. van der aalst
department of mathematics and computer science
eindhoven university of technology, eindhoven, the netherlands
b.f.a.hompes@student.tue.nl
{h.m.w.verbeek,w.m.p.v.d.aalst}@tue.nl
abstract. event data can be found in any information system and pro-
vide the starting point for a range of process mining techniques. the
widespread availability of large amounts of event data also creates new
challenges. existing process mining techniques are often unable to handle
\big event data" adequately. decomposed process mining aims to solve
this problem by decomposing the process mining problem into many
smaller problems which can be solved in less time, using less resources,
or even in parallel. many decomposed process mining techniques have
been proposed in literature. analysis shows that even though the de-
composition step takes a relatively small amount of time, it is of key
importance in nding a high-quality process model and for the compu-
tation time required to discover the individual parts. currently there is
no way to assess the quality of a decomposition beforehand. we dene
three quality notions that can be used to assess a decomposition, before
using it to discover a model or check conformance with. we then propose
a decomposition approach that uses these notions and is able to nd a
high-quality decomposition in little time.
keywords: decomposed process mining, decomposed process discovery,
distributed computing, event log
1 introduction
process mining aims to discover, monitor and improve real processes by extract-
ing knowledge from event logs readily available in today's information systems
[1]. in recent years, (business) processes have seen an explosive rise in support-
ing infrastructure, information systems and recorded information, as illustrated
by the term big data. as a result, event logs generated by these information
systems grow bigger and bigger as more event (meta-)data is being recorded and
processes grow in complexity. this poses both opportunities and challenges for
the process mining eld, as more knowledge can be extracted from the recorded
data, increasing the practical relevance and potential economic value of process
mining. traditional process mining approaches however have diculties coping
with this sheer amount of data (i.e. the number of events), as most interesting2 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
algorithms are linear in the size of the event log and exponential in the number
of dierent activities [3]. in order to provide a solution to this problem, tech-
niques for decomposed process mining [3{5] have been proposed. decomposed
process mining aims to decompose the process mining problem at hand into
smaller problems that can be handled by existing process discovery and confor-
mance checking techniques. the results for these individual sub-problems can
then be combined into solutions for the original problems. also, these smaller
problems can be solved concurrently with the use of parallel computing. even
sequentially solving many smaller problems can be faster than solving one big
problem, due to the exponential nature of many process mining algorithms. sev-
eral decomposed process mining techniques have been developed in recent years
[2{5, 7, 8, 10, 12, 13]. though existing approaches have their merits, they lack in
generality. in [5], a generic approach to decomposed process mining is proposed.
the proposed approach provides a framework which can be combined with dier-
ent existing process discovery and conformance checking techniques. moreover,
dierent decompositions can be used while still providing formal guarantees,
e.g. the fraction of perfectly tting traces is not inuenced by the decomposi-
tion. when decomposing an event log for (decomposed) process mining, several
problems arise. in terms of decomposed process discovery, these problems lie in
the step where the overall event log is decomposed into sublogs, where submod-
els are discovered from these sublogs, and/or where submodels are merged to
form the nal model. even though creating a decomposition is computationally
undemanding, it is of key importance for the remainder of the decomposed pro-
cess discovery process in terms of the overall required processing time and the
quality of the resulting process model.
the problem is that there is currently no clear way of determining the quality
of a given decomposition of the events in an event log, before using that decompo-
sition to either discover a process model or check conformance with. the current
decomposition approaches do not use any quality notions to create a decompo-
sition. thus, potential improvements lie in nding such quality notions and a
decomposition approach that uses those notions to create a decomposition with.
the remainder of this paper is organized as follows. in section 2 related work
is discussed briey. section 3 introduces necessary preliminary denitions for
decomposed process mining and the generic decomposition approach. section 4
introduces decomposition quality notions to grade a decomposition upon, and
two approaches that create a high quality decomposition according to those
notions. section 5 shows a (small) use case. the paper is concluded with views
on future work in section 6.
2 related work
little work has been done on the decomposition and distribution of process
mining problems [3{5]. in [14] mapreduce is used to scale event correlation
as a preprocessing step for process mining. in [6] an approach is described to
distribute genetic process mining over multiple computers. in this approach can-finding suitable activity clusters for decomposed process discovery 3
didate models are distributed and in a similar fashion the log can be distributed
as well. however, individual models are not partitioned over multiple nodes.
more related are the divide-and-conquer techniques presented in [9], where it is
shown that region-based synthesis can be done at the level of synchronized state
machine components (smcs). also a heuristic is given to partition the causal
dependency graph into overlapping sets of events that are used to construct sets
of smcs. in [4] a dierent (more local) partitioning of the problem is given which,
unlike [9], decouples the decomposition approach from the actual conformance
checking and process discovery approaches. the approach presented in this pa-
per is an extension of the approach presented in [4]. where [4] splits the process
mining problem at hand into subproblems using a maximal decomposition, our
approach rst aims to recombine the many created activity clusters into better
and fewer clusters, and only then splits the process mining problem into subprob-
lems. as a result, fewer subproblems remain to be solved. the techniques used
to recombine clusters are inspired by existing software quality metrics and the
business process metrics listed in [15]. more information on the use of software
engineering metrics in a process mining context is described there as well.
3 preliminaries
this section introduces the notations needed to dene a better decomposition
approach. a basic understanding of process mining is assumed [1].
3.1 multisets, functions, and sequences
denition 1 (multisets).
multisets are dened as sets where elements may appear multiple times. b(a)
is the set of all multisets over some set a. for some multiset b2b(a), and
elementa2a,b(a)denotes the number of times aappears inb.
for example, take a=fa;b;c;dg:b1= [] denotes the empty multiset, b2= [a;b]
denotes the multiset over awhereb2(c) =b2(d) = 0 andb2(a) =b2(b) = 1,b3=
[a;b;c;d ] denotes the multiset over awhereb3(a) =b3(b) =b3(c) =b3(d) = 1,
b4= [a;b;b;d;a;c ] denotes the multiset over awhereb4(a) =b4(b) = 2 and
b4(c) =b4(d) = 1, andb5= [a2;b2;c;d] =b4. the standard set operators can be
extended to multisets, e.g. a2b2,b5nb2=b3,b2]b3=b4=b5,jb5j= 6
denition 2 (sequences).
a sequence is dened as an ordering of elements of some set. sequences are used
to represent paths in a graph and traces in an event log. s(a)is the set of all
sequences over some set a.s=ha1;a2;:::;a ni2s (a)denotes a sequence s
overaof lengthn. furthermore: s1=hiis the empty sequence and s1s2is
the concatenation of two sequences.
for example, take a=fa;b;c;dg:s1=ha;b;bi,s2=hb;b;c;di,s1s2=
ha;b;b;b;b;c;di4 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
denition 3 (function projection).
letf2x6!ybe a (partial) function and qx.fqdenotes the projection
offonq:dom(fq) =dom(f)\qandfq(x) =f(x)forx2dom(fq).
the projection can be used for multisets. for example, b5fa;bg= [a2;b2].
denition 4 (sequence projection).
letabe a set and qaa subset. q2s(a)!s(q)is a projection function
and is dened recursively: (1) hiq=hiand (2) for s2s(a)anda2a:
(hais)q=(
sq ifa =2q
haisqifa2q
soha;a;b;b;c;d;difa;bg=ha;a;b;bi.
3.2 event logs
event logs are the starting point for process mining. they contain information
recorded by the information systems and resources supporting a process. typ-
ically, the executed activities of multiple cases of a process are recorded. note
that only example behavior is recorded, i.e. event logs only contain information
that has been seen. an event log often contains only a fraction of the possible be-
havior [1]. a trace describes one specic instance (i.e. one \run") of the process
at hand, in terms of the executed activities. an event log is a multiset of traces,
since there can be multiple cases having the same trace. for the remainder of
this paper, we let uabe some universe of activities.
denition 5 (trace).
letauabe a set of activities. a trace s2s(a)is a sequence of activities.
denition 6 (event log).
letauabe a set of activities. let l2b(s(a))be a multiset of traces over
a.lis an event log over a.
an example event log is l1= [ha;b;c;di5;ha;b;b;c;di2;ha;c;di3]. there are three
unique traces in l1, and it contains information about a total of 10 cases. there
are 45+52+33 = 39 events in total. the projection can be used for event logs
as well. that is, for some log l2b(s(a)) and setqa:lq= [sqjs2l].
for example l1fa;b;cg= [ha;b;ci5;ha;b;b;ci2;ha;ci3]. we will refer to these
projected event logs as sublogs .
3.3 activity matrices, graphs, and clusters
in [5] dierent steps for a generic decomposed process mining approach have been
outlined. in [16], an implementation of the generic approach has been created
which decomposes the overal event log based on a causal graph of activities. this
section describes the necessary denitions for this decomposition method.finding suitable activity clusters for decomposed process discovery 5
denition 7 (causal activity matrix).
letauabe a set of activities. m(a) = (aa)![ 1:0;1:0]denotes the
set of causal activity matrices over a. fora;a02aandm2m (a),m(a;a0)
denotes the \directly follows strength" from atoa0.
am(a;a0) value close to 1 :0 signies that we are quite condent there exists
a directly follows relation between two activities while a value close to  1:0
signies that we are quite sure there is no relation. a value close to 0 :0 indicates
uncertainty, i.e., there may be a relation, but there is no strong evidence for it.
denition 8 (causal activity graph).
letauabe a set of activities. g(a)denotes the set of causal activity graphs
overa. a causal activity graph g2g(a)is a 2-tuple g= (v;e)whereva
is the set of nodes and e(vv)is the set of edges. g= (v;e)2g(a)is
the causal activity graph based on m2m (a)and a specic causality threshold
2[ 1:0;1:0]ie=f(a;a0)2aajm(a;a0)>gandv=s
(a;a0)2efa;a0g.
that is, for every pair of activities (a;a0)2a, there's an edge from atoa0ing
i the value for atoa0in the causal activity matrix mexceeds some threshold
. note that vasince some activities in amight not be represented in g.
denition 9 (activity cluster).
letauabe a set of activities. c(a)denotes the set of activity clusters over
a. an activity cluster c2c(a)is a subset of a, that is,ca.
denition 10 (activity clustering).
letauabe a set of activities. bc(a)denotes the set of activity clusterings
overa. an activity clustering bc2bc(a)is a set of activity clusters, that is,
bcp (a). ak-clusteringbc2bc(a)is a clustering with size k, i.e.jbcj=k.
letbc2bc(a)be a clustering over a, the number of activities in bcis denoted by
jjbcjj=js
c2bccj, i.e.jjbcjjsignies the number of unique activities in bc.
3.4 process models and process discovery
process discovery aims at discovering a model from an event log while confor-
mance checking aims at diagnosing the dierences between observed and modeled
behavior (resp. the event log and the model). literature suggests many dierent
notations for models. we abstract from any specic model notation, but will
dene the set of algorithms that discover a model from an event log. various
discovery algorithms have been proposed in literature. these discovery algo-
rithms are often called mining algorithms , orminers in short. for an overview
of dierent algorithms we refer to [1].
denition 11 (process model).
letauabe a set of activities. n(a)denotes the set of process models over
a, irrespective of the specic notation (petri nets, transition systems, bpmn,
uml asds, etc.) used.6 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
denition 12 (discovery algorithm).
letauabe a set of activities. d(a) =b(s(a))!n (a)denotes the set
of discovery algorithms over a. a discovery algorithm d2d(a)discovers a
process model over afrom an event log over a.
3.5 decomposed process discovery
as discussed, in [5], a generic approach to decomposed process mining is pro-
posed. in terms of decomposed process discovery, this approach can be explained
as follows: let auabe a set of activities, and let l2b(s(a))) be an event
log overa. in order to decompose the activities in l, rst a causal activity
matrixm2m (a) is discovered . any causal activity matrix discovery algo-
rithmdca2b(s(a)))!m (a) can be used. from ma causal activity graph
g2g(a) isltered (using a specic causality threshold). by choosing the value
of the causality threshold carefully, we can lter out uncommon causal relations
between activities or relations of which we are unsure, for example those rela-
tions introduced by noise in the event log. once the causal activity graph g
has been constructed, an activity clustering bc2bc(a) is created. any activity
clustering algorithm ac2g(a)!bc(a) can be used to create the clusters.
for example, the maximal decomposition can be used where the causal activity
graph is cut across its vertices and each edge ends up in precisely one submodel.
this leads to the smallest possible submodels [5]. for every cluster in the clus-
tering,lisltered to a corresponding sublog by projecting the cluster to l, i.e.,
for allc2bca subloglcis created. a process model is discovered for each
subloglc. these are the submodels. any discovery algorithm d2d(a) can
be used to discover the submodels. finally, the submodels are merged into an
overal model. any merging algorithm in b(n(a))!n (a) can be used for this
step. currently, submodels are merged based on activity labels. note that we
havejbcjclusters, sublogs and submodels, and jjbcjjactivities in the nal, merged
model.
4 a better decomposition
it is apparent that the manner in which activities are clustered has a substantial
eect on required processing time, and it is possible for similarly sized clusterings
(in the average cluster size) to lead to very dierent total processing times. as
a result of the vertex-cut ( maximal ) decomposition approach [5], most activities
will be in two (or more) activity clusters, leading to double (or more) work, as the
clusters have a lot of overlap and causal relations between them, which might not
be desirable. from the analysis results in [11] we can see that this introduces a
lot of unwanted overhead, and generally reduces model quality. also, sequences
or sets of activities with high causal relations are generally easily (and thus
quickly) discovered by process discovery algorithms, yet the approach will often
split up these activities over dierent clusters. model quality can potentially
suer from a decomposition that is too ne-grained. it might be that the sublogsfinding suitable activity clusters for decomposed process discovery 7
created by the approach contain too little information for the process discovery
algorithm to discover a good, high quality submodel from, or that a process is
split up where it shouldn't be. merging these low-quality submodels introduces
additional problems.
hence, a good decomposition should (1) maximize the causal relations be-
tween the activities within each cluster in the activity clustering, (2) minimize
the causal relations and overlap across the clusters and (3) have approximately
equally sized clusters. the challenge lies in nding a good balance between these
three properties.
a clustering where one cluster is a subset of another cluster is not valid as
it would lead to double work, and would thus result in an increase in required
processing time without increasing (or even decreasing) model quality. note
that this denition of a valid clustering allows for disconnected clusters, and
that some activities might not be in any cluster. this is acceptable as processes
might consist of disconnected parts and event logs may contain noise. however,
if activities are left out some special processing might be required.
denition 13 (valid clustering).
letauabe a set of activities. let bc2bc(a)be a clustering over a.bcis a
valid clustering i: bc6=;^8c1;c22bc^c16=c2c16c2.bcv(a)denotes the set of
valid clusterings over a.
4.1 clustering properties
we dene decomposition quality notions in terms of clustering properties. the
rst clustering property we dene is cohesion . the cohesion of an activity cluster-
ing is dened as the average cohesion of each activity cluster in that clustering.
a clustering with good cohesion (cohesion 1) signies that causal relations
between activities in the same cluster are optimized, whereas bad cohesion (co-
hesion0) signies that activities with few causal relations are clustered to-
gether.
denition 14 (cohesion).
letauabe a set of activities. let m2m (a)be a causal activity matrix over
a, and letbc2bcv(a)be a valid clustering over a. the cohesion of clustering bc
in matrixm, denotedcohesion (bc;m )is dened as follows:
cohesion (bc;m ) =p
c2bccohesion (c;m )
jbcj
cohesion (c;m ) =p
c1;c22cmax(m(c1;c2);0)
jccj
the second clustering property is called coupling , and is also represented
by a number between 0 and 1. good coupling (coupling 1) signies that
causal relations between activities across clusters are minimized. bad coupling
(coupling0) signies that there are a lot of causal relations between activities
in dierent clusters.8 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
denition 15 (coupling).
letauabe a set of activities. let m2m (a)be a causal activity matrix over
a, and letbc2bcv(a)be a valid clustering over a. the coupling of clustering bc
in matrixm, denotedcoupling (bc;m )is dened as follows:
coupling (bc;m ) =8
<
:1 ifjbcj1
1 p
c1;c22bc^c16=c2coupling (c1;c2;m)
jbcj(jbcj 1)ifjbcj>1
coupling (c1;c2;m) =p
c12c1;c22c2h
max(m(c1;c2);0) +max(m(c2;c1);0)i
2jc1c2j
note that the weights of the causal relations are used in the calculation of
cohesion and coupling. relations of which we are not completely sure of (or that
are weak) therefore have less eect on these properties than stronger ones.
the balance of an activity clustering is the third property. a clustering with
good balance has clusters of (about) the same size. decomposing the activities
into clusters with low balance (e.g. a k-clustering with one big cluster holding
almost all of the activities and ( k 1) clusters with only a few activities) will not
speed up discovery or conformance checking, rendering the whole decomposition
approach useless. at the same time nding a clustering with perfect balance (all
clusters have the same size) will most likely split up the process / log in places
that \shouldn't be split up", as processes generally consist out of dierent-sized
natural parts. balance is also represented by a number between 0 and 1, where
a good balance (balance 1) signies that all clusters are about the same size
and a bad balance (balance 0) signies that the cluster sizes dier quite a lot.
this balance formula utilizes the standard deviation of the sizes of the clusters
in a clustering to include the magnitude of the dierences in cluster sizes. a
variation of this formula using squared errors or deviations could also be used
as a clustering balance measure.
denition 16 (balance).
letauabe a set of activities. let bc2bcv(a)be a valid clustering over a.
the balance of clustering bcdenotedbalance (bc)is dened as follows:
balance (bc) = 1 2(bc)
jjbcjj
where(bc)signies the standard deviation of the sizes of the clusters in the
clusteringbc.
in order to assess a certain decomposition based on the clustering properties,
we introduce a weighted scoring function, which grades an activity clustering
with a score between 0 (bad clustering) and 1 (good clustering). a weight can
be set for each clustering property, depending on their relative importance. a
clustering with clustering score 1 has perfect cohesion, coupling and balance
scores, on the set weighing of properties.finding suitable activity clusters for decomposed process discovery 9
denition 17 (clustering score).
letauabe a set of activities. let m2m (a)be a causal activity matrix
overa, and letbc2bcv(a)be a valid clustering over a. the clustering score
(score) of clustering bcin matrixm, denotedscore (bc;m )is dened as follows:
score (bc;m ) =cohesion (bc;m )coh w
coh w+cou w+balw
+coupling (bc;m )cou w
coh w+cou w+balw
+balance (bc)balw
coh w+cou w+balw
wherecoh w,cou w, andbalware the weights for cohesion, coupling, and
balance.
4.2 recomposition of activity clusters
creating a good activity clustering is essentially a graph partitioning problem.
the causal activity graph needs to be partitioned in parts that have (1) good
cohesion, (2) good coupling and (3) good balance. the existing maximal decom-
position approach [5] often leads to a decomposition that is too decomposed,
i.e. too ne-grained. cohesion and balance of clusterings found by this approach
are usually quite good, since all clusters consist of only a few related activities.
however, coupling is inherently bad, since there's a lot of overlap in the activity
clusters and there are many causal relations across clusters. this decomposition
approach leads to unnecessary and unwanted overhead and potential decreased
model quality. we thus want to nd a possibly non-maximal decomposition
which optimizes the three clustering properties.
instead of applying or creating a dierent graph partitioning algorithm, we
recompose the activity clusters obtained by the vertex-cut decomposition. the
idea is that it is possible to create a clustering that has fewer larger clusters,
requiring less processing time to discover the nal model, because overhead as
well as cluster overlap are reduced. additionally, model quality is likely to in-
crease because of the higher number of activities in the clusters and the lower
coupling between clusters.
there are often many ways in which a clustering can be recomposed to the
desired amount of clusters, as shown in figure 1. we are interested in the highest
quality clustering of the desired size, i.e. the clustering that has the best cohesion,
coupling and balance properties. a clustering that has a high clustering score
will very likely lead to such a decomposition.
in order to nd a good decomposition in the form of a high-scoring clustering
quickly, we propose two agglomerative hierarchical recomposition approaches,
which iteratively merge clusters, reducing the size of the clustering by one each
iteration.10 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
fig. 1. 3 possible recompositions from 16 to 4 clusters. creating a coarser clustering
could potentially decrease processing time and increase model quality.
proximity-based approach we propose an hierarchical recomposition ap-
proach based on proximity between activity clusters, where cluster coupling is
used as the proximity measure. the starting point is the clustering as created
by the vertex-cut approach. we repeatedly merge the clusters closest to one an-
other (i.e. the pair of clusters with the highest coupling) until we end up with
the desired amount of clusters ( k). after the k-clustering is found, it is made
valid by removing any clusters that are a subcluster of another cluster, if such
clusters exist. it is therefore possible that the algorithm returns a clustering with
size smaller than k. by merging clusters we are likely to lower the overal cohe-
sion of the clustering. this drawback is minimized, as coupling is used as the
distance measure. coupling is also minimized. the proximity-based hierarchical
recomposition approach however is less favored towards the balance property, as
it is possible that -because of high coupling between clusters- two of the larger
clusters are merged. in most processes however, coupling between two \origi-
nal" clusters will be higher than coupling between \merged" clusters. if not,
the two clusters correspond to parts of the process which are more dicult to
split up (e.g. a loop, a subprocess with many interactions and/or possible paths
between activities, etc.). model quality is therefore also likely to increase by
merging these clusters, as process discovery algorithms don't have to deal with
missing activities, or incorrect causal relations introduced in the corresponding
sublogs. a possible downside is that as the clustering might be less balanced,
processing time can be slightly higher in comparison with a perfectly-balanced
decomposition.finding suitable activity clusters for decomposed process discovery 11
score-based approach we propose a second hierarchical recomposition algo-
rithm that uses the scoring function in a look-ahead fashion. in essence, this
algorithm, like the proximity-based variant, iteratively merges two clusters into
one. for each combination of clusters, the score of the clustering that results
from merging those clusters is calculated. the clustering with the highest score
is used for the next step. the algorithm is nished when a k-clustering is reached.
like in the proximity-based approach, after the k-clustering is found, it is made
valid by removing any clusters that are a subcluster of another cluster, if such
clusters exist. the advantage of this approach is that specic (combinations of)
clustering properties can be given priority, by setting their scoring weight(s) ac-
cordingly. for example, it is possible to distribute the activities over the clusters
near perfectly, by choosing a high relative weight for balance. this would likely
lead to a lower overall processing time. however, it might lead to natural parts
of the process being split over multiple clusters, which could negatively aect
model quality. a downside of this algorithm is that, as the algorithm only looks
ahead one step, it is possible that a choice is made that ultimately leads to a
lower clustering score, as that choice cannot be undone in following steps.
4.3 implementation
all concepts and algorithms introduced in this paper are implemented in the
process mining toolkit prom1, developed at the eindhoven university of tech-
nology. all work can be found in the barthompes package2. for more elaborate
explanations, pseudo-code of the algorithms, and analysis results we refer to [11].
5 use case
the proposed recomposition techniques are tested using event logs of dierent
sizes and properties. results for an event log consisting of 33 unique activities,
and 1000 traces are shown in this section. for this test the ilp miner process
discovery algorithm was used [17]. discovering a model directly for this log will
lead to a high quality model, but takes 25 minutes on a modern quad-core sys-
tem [11]. the vertex-cut decomposed process mining approach is able to discover
a model in roughly 90 seconds, however the resulting model suers from discon-
nected activities (i.e. a partitioned model). the goal is thus to nd a balance
between processing times and model quality.
we are interested in the clustering scores of each algorithm when recompos-
ing the clustering created by the vertex-cut approach to a certain smaller size.
exhaustively nding the best possible clustering proved to be too time- and
resource-consuming, therefore, besides the two hierarchical approaches listed
here, a random recomposition approach was used which recomposes clusters
randomly one million times, as to give an idea of what the best possible cluster-
ing might be. the highest found clustering score is shown on the graph. equal
1seehttp://www.processmining.org
2seehttps://svn.win.tue.nl/repos/prom/packages/barthompes/12 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
weights were used for the three clustering properties in order to compute the
clustering scores. as can be seen in figure 2, the vertex-cut approach creates
22 clusters. we can see that all algorithms perform very similarly in terms of
clustering score. only for very small clustering sizes the proximity-based ap-
proach performs worse than the other approaches, due to its tendency to create
unbalanced clusters.
0,50,550,60,650,70,75
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22clustering score
# clustersclustering scores
agglomerative hierarchical w/ proximity
 agglomerative hierarchical w/ scoring
 random (max)
fig. 2. clustering score per recomposition algorithm.
besides clustering scores, we are even more interested in how each decom-
position method performs in terms of required processing time and quality of
the resulting process model. in figure 3 we can see that decomposing the event
log drastically reduces processing times. for an event log this size, the decom-
position steps relatively takes up negligible time (see base of bars in gure), as
most time is spent discovering the submodels (light blue bars). processing times
are reduced exponentially (as expected), until a certain optimum decomposition
(in terms of speed) is reached, after which overhead starts to increase time lin-
early again. we have included two process models (petri nets) discovered from
the event log. figure 4 shows the model discovered when using the vertex-cut
decomposition. figure 5 shows the model discovered when using the clustering
recomposed to 11 clusters with the proximity-based agglomerative hierarchical
approach. we can see that in figure 4, activity \10" is disconnected (marked
blue). in figure 5, this activity is connected, and a structure (loop) is discov-
ered. we can also see that activity \12" now is connected to more activities. this
shows that the vertex-cut decomposition sometimes splits up related activities,
which leads to a lower quality model. by recomposing the clusters we rediscover
these relations, leading to a higher quality model. processing times for these two
models are comparable, as can be seen in figure 3.finding suitable activity clusters for decomposed process discovery 13
01234
0100200300400500600700800900
2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 2 3 4 5 6 7 8 910 11 12 13 14 15 16 17 18 19 20 21 22
partitions in modeltime in seconds
recomp osition method -# clusterstime per step & p artitions in mode l -ilp m iner discov ery algorithm
matrix graph clustering sublogs submodels merge partition s in modelagglomerative hierarchical w/ proximity max. decomp. agglomerative hierarchical w/ scoring
fig. 3. time per step & partitions in model using the agglomerative hierarchical
recomposition approaches and the ilp miner process discovery algorithm.
fig. 4. process model discovered using the vertex-cut decomposition. some activities
are disconnected in the nal model.
fig. 5. process model discovered using the vertex-cut clustering recomposed to 11 clus-
ters. previously disconnected activities are connected again, improving model quality.
6 conclusions and future work
in decomposed process discovery, large event logs are decomposed by somehow
clustering their events (activities), and there are many ways these activity clus-
terings can be made. hence, good quality notions are necessary to be able to
assess the quality of a decomposition before starting the time-consuming actual
discovery algorithm. being able to nd a high-quality decomposition plays a key
role in the success of decomposed process mining, even though the decomposition
step takes relatively very little time.14 b.f.a. hompes, h.m.w. verbeek, w.m.p. van der aalst
by using a better decomposition, less problems arise when discovering sub-
models for sublogs and when merging submodels into the overal process model.
we introduced three quality notions in the form of clustering properties: cohe-
sion,coupling and balance . it was shown that nding a non-maximal decom-
position can potentially lead to a decrease in required processing time while
maintaining or even improving model quality, compared to the existing vertex-
cutmaximal decomposition approach. we have proposed two variants of an
agglomerative hierarchical recomposition technique, which are able to create a
high-quality decomposition for any given size, in very little time.
even though the scope was limited to decomposed process discovery, the
introduced quality notions and decomposition approaches can be applied to
decomposed conformance checking as well. however, more work is needed to
incorporate them in a conformance checking environment.
besides nding a better decomposition, we believe improvements can be
gained in nding a better, more elaborate algorithm to merge submodels into the
overal process model. by simply merging submodels based on activity labels it is
likely that implicit paths are introduced. model quality in terms of tness, sim-
plicity, generality or precision could suer. an additional post-processing step
(potentially using causal relations) could also solve this issue.
even though most interesting process discovery algorithms are exponential
in the number of dierent activities, adding an infrequent or almost unrelated
activity to a cluster might not increase computation time for that cluster as much
as adding a frequent or highly related one. therefore, besides weighing causal
relations between activities in the causal activity matrix, activities themselves
might be weighted as well. frequency and connectedness are some of the many
possible properties that can be used as weights. it might be possible that one
part of a process can be discovered easily by a simple algorithm whereas another,
more complex part of the process needs a more involved discovery algorithm to
be modeled correctly. further improvements in terms of processing time can be
gained by somehow detecting the complexity of a single submodel in a sublog,
and choosing an adequate discovery algorithm.
finally, as discussed, the proposed recomposition algorithms expect the de-
sired amount of clusters to be given. even though the algorithms were shown to
provide good results for any chosen number, the approach would benet from
some method that determines a tting clustering size for a given event log. this
would also mean one less potentially uncertain step for the end-user.
references
[1] van der aalst, w.m.p.: process mining: discovery, conformance and en-
hancement of business processes. springer, berlin (2011) 1, 3, 4, 5
[2] van der aalst, w.m.p.: decomposing process mining problems using pas-
sages. in: application and theory of petri nets, pp. 72{91. springer (2012)
2finding suitable activity clusters for decomposed process discovery 15
[3] van der aalst, w.m.p.: distributed process discovery and conformance
checking. in: de lara, j., zisman, a. (eds.) fase. lecture notes in com-
puter science, vol. 7212, pp. 1{25. springer (2012) 2
[4] van der aalst, w.m.p.: a general divide and conquer approach for process
mining. in: computer science and information systems (fedcsis), 2013
federated conference on. pp. 1{10. ieee (2013) 3
[5] van der aalst, w.m.p.: decomposing petri nets for process mining: a
generic approach. distributed and parallel databases 31(4), 471{507 (2013)
2, 4, 6, 9
[6] bratosin, c.c., sidorova, n., van der aalst, w.m.p.: distributed genetic
process mining. in: evolutionary computation (cec), 2010 ieee congress
on. pp. 1{8. ieee (2010) 2
[7] carmona, j.: projection approaches to process mining using region-based
techniques. data min. knowl. discov. 24(1), 218{246 (2012), http://dblp.
uni-trier.de/db/journals/datamine/datamine24.html 2
[8] carmona, j., cortadella, j., kishinevsky, m.: a region-based algorithm for
discovering petri nets from event logs. in: business process management
(bpm2008). pp. 358{373 (2008) 2
[9] carmona, j., cortadella, j., kishinevsky, m.: divide-and-conquer strategies
for process mining. in: business process management, pp. 327{343. springer
(2009) 3
[10] goedertier, s., martens, d., vanthienen, j., baesens, b.: robust process
discovery with articial negative events. journal of machine learning
research 10, 1305{1340 (2009) 2
[11] hompes, b.f.a.: on decomposed process mining: how to solve a jigsaw
puzzle with friends. master's thesis, eindhoven university of technology,
eindhoven, the netherlands (2014), http://repository.tue.nl/776743
6, 11
[12] mu~ noz-gama, j., carmona, j., van der aalst, w.m.p.: conformance check-
ing in the large: partitioning and topology. in: daniel, f., wang, j., weber,
b. (eds.) bpm. lecture notes in computer science, vol. 8094, pp. 130{145.
springer (2013) 2
[13] mu~ noz-gama, j., carmona, j., van der aalst, w.m.p.: hierarchical confor-
mance checking of process models based on event logs. in: colom, j.m.,
desel, j. (eds.) petri nets. lecture notes in computer science, vol. 7927,
pp. 291{310. springer (2013) 2
[14] reguieg, h., toumani, f., motahari-nezhad, h.r., benatallah, b.: using
mapreduce to scale events correlation discovery for business processes min-
ing. in: business process management, pp. 279{284. springer (2012) 2
[15] vanderfeesten, i.t.p.: product-based design and support of workow pro-
cesses (2009) 3
[16] verbeek, h.m.w., van der aalst, w.m.p.: decomposed process mining:
the ilp case. in: bpi 2014 workshop (2014), accepted for publication 4
[17] van der werf, j.m.e.m., van dongen, b.f., hurkens, c.a.j., serebrenik,
a.: process discovery using integer linear programming. in: applications
and theory of petri nets, pp. 368{387. springer (2008) 11