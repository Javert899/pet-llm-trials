formal modeling and analysis by simulation of
data paths in digital document printers?
venkatesh kannan, wil m.p. van der aalst, and marc voorhoeve
department of mathematics and computer science,
eindhoven university of technology, eindhoven, the netherlands.
fv.kannan,w.m.p.v.d.aalst,m.voorhoeve g@tue.nl
abstract. this paper reports on a challenging case study conducted in
the context of the octopus project where cpn tools is used to model
and analyze the embedded system of digital document printer. modeling
the dynamic behavior of such systems in a predictable way is a major
challenge. in this paper, we present the approach where colored petri nets
are used to model the system. simulation is used to analyze the behavior
and performance. the challenge in modeling is to create building blocks
that enable exibility in reconguration of architecture and design space
exploration. cpn tools and prom (a process mining tool) are used to
collect and analyze the simulation results. we present the pros and cons
of both the conventional presentation of simulation results and using
prom. using prom it is possible to monitor the simulation is a rened
and exible manner. moreover, the same tool can be used to monitor the
real system and the simulated system making comparison easier.
1 introduction
the octopus project is a co-operation between oc e technologies, the embedded
systems institute (esi), and several research groups in the netherlands. the aim
of the project is to dene new methods and tools to model and design embedded
systems like printers, which interact in an adaptive way to changes during their
functioning. one of the branches of the octopus project is the study of design
of data paths in printers and copiers. a data path encompasses the trajectory
of image data from the source (for instance the network to which a printer is
connected) to the target (the imaging unit). runtime changes in the environment
may require use of dierent algorithms in the data path, deadlines for completion
of processing may change, new jobs arrive randomly, and availability of resources
also changes. to realize such dynamic behavior in a predictable way is a major
challenge. the octopus project is exploring dierent approaches to model and
analyze such systems. this paper focuses on the use of colored petri nets to
model and study such systems. we report on the rst phase of the project,
in which we studied a slightly simplied version of an existing state-of-the-art
image processing pipeline at oc e implemented as an embedded system.
?research carried out in the context of the octopus project, with partial support of
the netherlands ministry of economic aairs under the senter ts program.1.1 the case study
the industrial partner in the octopus project, oc e technologies, is a designer
and manufacturer of systems that perform a variety of image processing functions
on digital documents in addition to scanning, copying and printing. in addition
to locally using the system for scanning and copying, users can also remotely
use the system for image processing and printing. a generic architecture of an
oc e system used in this project is shown in figure 1. [2]
fig. 1: architecture of oc e system.
as shown in figure 1, the system has two input ports: scanner and controller.
users locally come to the system to submit jobs at the scanner and remote jobs
enter the system via the controller. these jobs use the image processing (ip)
components (scanip, ip1, ip2, printip), system resources such as the memory,
and usb bandwidth for the executing the jobs. finally, there are two output
ports where the jobs leave the system: printer and controller. jobs that require
printed outputs use the printer and those that are to be stored in a storage
device or sent to a remote user are sent via the controller.
all the components mentioned above (scanner, scanip, ip1, ip2, printip)
can be used in dierent combinations depending on how a document of a certain
job is requested to be processed by the user. hence this gives rise to dierent
use-cases of the system i.e. each job could use the system in a dierent way.
the list of components used by a job denes the data path for that job. some
possible data paths for jobs are listed and explained below:
{ directcopy : scanner ;scanip;ip1;ip2;usbclient, printip
{ scantostore : scanner ;scanip;ip1;usbclient
{ scantoemail : scanner ;scanip;ip1;ip2;usbclient
{ processfromstore : usbclient ;ip1;ip2;usbclient
{ simpleprint : usbclient ;printip
{ printwithprocessing : usbclient ;ip2;printip
the data path listed for directcopy means that the job is processed in order
by the components scanner, scanip, ip1, ip2 and then simultaneously sent tothe controller via the usbclient and also for printing through printip. in the
case of the processfromstore data path, a job is remotely sent via the controller
and usbclient for processing by ip1 and ip2 after which the result is sent back
to the remote user via the usbclient and the controller. the interpretation for
the rest of the data paths is similar.
furthermore, there are additional constraints possible on the dependency of
the processing of a job by dierent components in the data path. it is not manda-
tory that the components in the data path should process the job sequentially,
as the design of the oc e system allows for a certain degree of parallelism. some
instances of this are shown in figure 2.
fig. 2: dependency between components processing a job.
according to the oc e system design, ip2 can start processing a page in a
document only after ip1 has completed processing that page. this is due to the
nature of the image processing function that ip1 performs. hence as shown in
figure 2(a) ip1 and ip2 process a page in a document in sequence. considering
scanner and scanip, they can process a page in parallel as shown in figure 2(b).
this is because scanip works full streaming and has the same throughput as
the scanner. the dependency between scanip and ip1 is shown in figure 2(c)
and in this case ip1 works streaming and has a higher throughput than scanip.
hence ip1 can start processing the page as scanip is processing it, with a certain
delay due to the higher throughput of ip1.
in addition to using the dierent components of the system for executing
jobs, there are other system resources that are needed to process jobs. the two
key system resources addressed currently in this project are the memory and the
usb bandwidth. regarding the memory, a job is allowed to enter the system
only if the entire memory required for completion of the job is available before
its execution commences. if the memory is available, then it is allocated and
the job is available for execution. each component requires a certain amount of
memory for its processing and releases this memory once it completes processing.
hence utilization of memory is a critical factor in determining the throughput
and eciency of the system. another critical resource is the usb. the usb has
a limited bandwidth and it serves as the bridge between the usbclient and the
memory. whenever the usbclient writes/reads data to/from the memory, it has
to be transmitted via the available usb. since this bandwidth is limited, it canbe allocated only to a limited number of jobs at a time. this determines how
fast the jobs can be transferred from the memory to the controller or vice versa.
the overview of the system just given illustrates the complexity of the oc
system. the characteristics of critical system resources such as memory and usb
bandwidth, and the components determine the overall performance. moreover,
resource conicts need to be resolved to ensure a high performance and through-
put. the resource conicts include competition for system components, memory
availability, and usb bandwidth.
1.2 the approach
in our approach, colored petri nets (cpn) are used to model the oc system.
the cpn modeling strategy [3] is aimed at providing exibility for design space
exploration of the system using the model. hence, design of reusable building
blocks is vital during the modeling process. simulation of the model is used for
performance analysis to identify bottleneck resources, utilization of components,
decisions during design space exploration and design of heuristic scheduling rules
(in the future). cpn tools is used for modeling, simulation and performance
analysis of the system. additionally, prom , a versatile process mining tool, is
used to provide further insights into the simulation results and also present
these results to the domain user in dierent forms. interestingly, prom can be
used to monitor both the simulated and the real system, thus facilitating easy
comparison.
2 modeling using cpn
the modeling approach takes an architecture oriented perspective to model the
oc e system. the model, in addition to the system characteristics, includes the
scheduling rules (first come first served) and is used to study the performance
of the system through simulation. each component in the system is modeled as
a subnet. since the processing time for all the components, except the usb, can
be calculated before they start processing a job, the subnet for these compo-
nents looks like the one shown in figure 3. the transitions start andendmodel
the beginning and completion of processing a job, while the places freeanddo
reect the occupancy of the component. in addition, there are two places that
characterize the subnet to each component: compinfo andpaperinfo . the place
compinfo contains a token with information about the component, namely the
component id, processing speed and the recovery time required by the compo-
nent before starting the next job. the place paperinfo contains information on
the number of bytes the particular component processes for a specic paper size.
the values of the tokens at places compinfo andpaperinfo remain constant af-
ter initialization and govern the behavior of the component. since the behavior
of the usb is dierent from the other components, its model is dierent from
the other components and is shown separately. the color sets for paperinfo and
compinfo used in the cpn tools model are listed below.colset paperinfo=record paper:string*inputsize:int;
colset compinfo=record compid:string*speed:int*recovery:int;
in the color set paperinfo , the record-element paper contains the infor-
mation on the size of the paper, such as a4 or a3, and element inputsize denotes
the memory required for this size of paper. in the color set compinfo , the
element compid is used to name the component (scanner, scanip, etc.), speed
denotes the processing speed of the component and recovery contains the infor-
mation about the recovery time needed by the component between processing
two jobs.
fig. 3: hierarchical subnet for each component
in figure 3, the place jobq contains tokens for the jobs that are available for
the components to process at any instance of time. the color of a token of type
jobcontains information about the job id, the use case and paper size of the
job. hence, the component can calculate the time required to process this job
from the information available in the jobtoken, and the tokens at the places
compinfo andpaperinfo . once the processing is completed, transition endplaces
a token in place freewith a certain delay, governed by the recovery time specic
to each component, thus determining when the component can begin processing
the next available job. the color set for the type jobis as follows,
colset job=record
jobid:string*
jobtype:string*
inputpaper:string*
from:string*
to:string*
starttime:int*
endtime:int timed;
the record element jobid is used to store a unique identier for each job,
jobtype contains the use-case of the job (directcopy or scantoemail, etc.), andthe element inputpaper species what paper size is used in this job. the elements
from andtoare used for the source and destination component ids respectively,
as the job is being processed by one component after another according to the
data path. the starttime andendtime are used by each component to contain
the timestamps of start and estimated end of processing the job.
fig. 4: architectural view of the cpn model.
figure 4 shows an abstract view of the model. new jobs for the system can
be created using the job generator subnet, which are placed as input to the
scheduler subnet at the place newjob . the scheduler subnet is the heart of
the system that models the concepts including the scheduling rules, memory
management rules and routing each job step-by-step from one component to the
next depending on the data path of the use-case to which the job belongs. from
this it can be observed that the scheduling rules are modeled as being global to
system and not local to any of the components or distributed.
vital to the scheduler's task of routing jobs from one component to the next
is the information about the use-cases and the data paths. from the information
on data paths in section 1.1, it can be inferred that each data path is a partial
order. hence, a list of list (of color string) is used to represent the partial
order. an example of a data path represented in the scheduler component is
shown here.
ucid="directcopy",
datapath= [ ["scanner","scanip"],
["scanip","ip1"],
["ip1","ip2"],
["ip2","printip","usbup"],
["usbup"],["printip"]
]the data path of the use-case directcopy is explained in section 1.1. in this
example, each sublist inside the data path list contains two parts: the rst ele-
ment being the source component and the remaining being the destination(s).
hence, ["scanip","ip1"] indicates that in the directcopy use-case, a job pro-
cessed by scanip will be processed by ip1next. similarly, ["ip2","printip","usbup"]
denotes that a job processed by ip2will be processed simultaneously by printip
andusbupload in the next step.
thescheduler picks a new job that enters the system from the place newjob
and estimates the amount of total memory required for executing this job. if
enough memory is available, the memory is allocated (the memory resource is
modeled as an integer token in the place memory) and the job is scheduled for
the rst component in the data path of this job by placing a token of type job
in the place jobq, which will be consumed by the corresponding component for
processing. when a component starts processing a job, it immediately places a
token in the startedjob place indicating this event. the scheduler consumes this
token to schedule the job to the next component in its data path, adding a delay
that depends on the component that just started, the next component in the data
path, and the dependency explained and shown in figure 2 (a), (b) and (c). thus
the logic in the scheduler includes scheduling new jobs entering the system (from
place newjob ) and routing the existing jobs through the components according
to the corresponding data paths.
as mentioned above, the scheduler subnet also handles the memory manage-
ment. this includes memory allocation and release for jobs that are executed.
when a new job enters the system, the scheduler schedules it only if the com-
plete memory required for the job is available (checked against the token in the
place memory). during execution, part of the memory allocated may be released
when a component completes processing a job. this memory release operation
is also performed by the scheduler subnet.
modeling the usb component is dierent from the other components and
cannot be models using the "pattern" shown in figure 5. as described earlier,
for the usb, the time required to transmit a job (upstream or downstream) is
not constant and is governed by other jobs that might be transmitted at the same
time. this necessitates making the real-time behavior of the usb bus dependent
of multiple jobs at the same time. it is to be noted that if only one job is being
transmitted over the usb then a high mbps transmission rate is used, and when
more than one job is being transmitted at the same time then a lower lowmbps
transmission rate is used.
the model of the usb as shown in figure 5 works primarily by monitoring
two events observable in the usb when one or more jobs are being transmit-
ted: (1) the event of a new job joining the transmission, and (2) the event of
completion of transmission of a job. both these events govern the transmission
rates for the other jobs on the usb and hence determine the transmission timesfig. 5: cpn model for the usb.
for the jobs. in the model shown in figure 5, there are two transitions join and
update , and two places trigger andusbjoblist . the place usbjoblist contains
the list of jobs that are currently being transmitted over the usb. apart from
containing information about each job, it also contains the transmission rate
assigned, the number of bytes remaining to be transmitted and the last time
of update for each job. transition join adds a new job waiting at place in that
requests use of the usb (if it can be accommodated) to the usbjoblist , and
places a token in place trigger . this enables transition update that checks the
list of jobs at place usbjoblist and reassigns the transmission rates for all the
jobs according to the number of jobs transmitted over the usb. the update
transition also recalculates the number of bytes remaining to be transmitted for
each job since the last update time, estimates the job that will nish next and
places a timed token at trigger , so that the transition update can remove the
jobs whose transmissions have completed. the jobs whose transmission over the
usb is complete are placed in place out. thus transition join catches the event
of new jobs joining the usb and the transition update catches the event of jobs
leaving the usb, which are critical in determining the transmission time for a
single job.
3 simulation and analysis
this section presents some analysis methods used to study the results from the
simulation of the model. section 3.1 presents the information collected in cpn
tools through monitors and how it is used to measure relevant performance
metrics. section 3.2 presents the use of the process mining tool prom for an
alternative presentation and analysis of the simulation results. prom uses event
logs, which are recorded by cpn tools. the event log contains details about the
events (i.e., transition rings) that take place in the simulation.
we are unable to share detailed data about the oc e system because this
information is highly condential. hence, the actual parameters and simulation
results should be seen as potential settings and outcomes.
for the simulation experiment to illustrate possible results obtained by cpn
tools and prom, 150 jobs are generated by the job generator component of
the model in figure 4 in each run. these jobs are created by picking a random
number of jobs from the six use-cases listed in section 1.1. the arrival times
of jobs are distributed negative exponentially with an inter-arrival time of 2
seconds.3.1 simulation results
when performing simulation in cpn tools, the dierent categories of moni-
tors available can be used to collect the simulation results in dierent ways [1].
here, two examples of how dierent types of monitors are used to aggregate the
simulation results to performance analysis metrics are presented.
table 1 presents the statistics produced by the data collection monitor that
was used to aggregate the waiting times of jobs before their execution starts
at each component. the averages provided by cpn tools in the performance
report can be obtained by replicating the simulation for multiple runs. the
waiting times of jobs thus obtained through monitors during simulations can
be used to identify the components that are probable bottleneck resources in
the system. similarly, using the data collection monitor, the utilization times
for each component can be obtained to determine the under- and over-utilized
components in the system.
name avrg 90% half length 95% half length 99% half length
ip1
count iid 100.119400 0.134347 0.160568 0.212527
max iid 3007.696600 4.862893 5.812036 7.692745
miniid 0.000000 0.000000 0.000000 0.000000
avrg iid 34.302562 1.301284 1.555269 2.058537
ip2
count iid 100.048200 0.133754 0.159861 0.211590
max iid 2860.038400 37.247604 44.517618 58.923016
miniid 0.000000 0.000000 0.000000 0.000000
avrg iid 48.990676 0.935130 1.117649 1.479308
usb
count iid 174.983400 0.105168 0.125695 0.166368
max iid 242724.770400 535.206794 639.668843 846.658458
miniid 0.000000 0.000000 0.000000 0.000000
avrg iid 23679.481434 143.889599 171.974075 227.622944
printip
count iid 74.900800 0.144126 0.172257 0.227998
max iid 96590.504600 524.005807 626.281639 828.939306
miniid 0.000000 0.000000 0.000000 0.000000
avrg iid 13155.451373 126.373949 151.039708 199.914452
scanner
count iid 75.136000 0.141720 0.169381 0.224191
max iid 735681.475800 532.367990 636.275959 842.167675
miniid 5406.491400 866.457382 1035.573160 1370.672942
avrg iid 341606.033984 696.226511 832.116504 1101.380010
table 1: waiting times of jobs at the dierent componentsfrom table 1, it can be observed that the average waiting time for jobs
in front of components scanner and usb is higher than for the rest of the
components. for example, with 90condence, the usb is seen to have an average
waiting time of 23680 seconds, with a half length of 144 seconds, for jobs in the
queue in front of it. this is attributed to the scheduling rule that jobs have to
wait for memory allocation before entering the system for processing through the
scanner or the usbdown . the simulation experiment here was conducted with
minimal memory availability, and hence the longer queues. also, the average
waiting time in front of the printip is also higher as it is the slowest component
in the system according to the design specications.
the second example presented here uses the write-in-le monitor to log the
events when memory is allocated or released by the scheduler component. using
this log of the time stamps and the amount of memory available, a simple tool
can be used to plot the chart shown in figure 6. the chart depicts the amount of
memory available in the system at each instant of time. information about the
utilization characteristics of the memory resource is a key input in designing the
memory architecture, designing scheduling rules for optimal memory utilization
with high system throughput and analyzing the waiting times in front of each
component in the system.
fig. 6: memory utilization chart
the above simulation results are typical for simulation tools, i.e., like most
tools, cpn tools focuses on measuring key performance indicators such as uti-
lization, throughput times, service levels, etc. note that the britney suite an-
imation tool [5] can be used to add animations to cpn simulations. moreover, it
allows for dedicated interactive simulations. this facilitates the interaction with
end users and domain experts (i.e., non-it specialists).
3.2 using prom
prom is a process mining tool, i.e., it is used to investigate real-life processes by
analyzing footprints of processes in the form of event logs, audit trails, databaseentries, message exchanges, translation logs, etc. prom oers a wide variety of
analysis techniques. because simulation can be seen as imitating real-life, it is
interesting to see what additional insights process mining techniques can provide.
this section presents some of the plug-ins of prom that have been explored in
the context of oc e's systems. the plug-ins of prom use event logs, which is list
of events recording when each component starts and completes processing a job.
these event logs have been generated using the approach described in [6].
fuzzy miner the fuzzy miner plug-in along with the animation part of it
provides a visualization of the simulation. the event log is used to replay the
simulation experiment on the fuzzy model of the system. figure 7 shows a snap-
shot during the animation. during the animation, jobs ow between components
in the fuzzy model in accordance with the events during simulation. it provides
a view of the queues in front of each component, which serves as an easy means
to identify key components, bottleneck resources and the utilization of compo-
nents in the system. for example, from figure 7 it can be observed that during
this simulation run, the queue in front of printip was longer, which can be at-
tributed to it being the slowest component in the system. more importantly, the
fuzzy miner animation provides live insight into the simulation run and is an
easier means of communication with the domain users, which is signicant in
the context of the octopus project.
fig. 7: fuzzy miner animationdotted chart analysis this plug-in uses the event log to create a dotted
chart with each dot referring to an event in the log. the chart can be viewed
using dierent perspectives. the x-axis always shows the time (can be absolute
or relative) and the y-axis shows a particular perspective. if the "instance per-
spective" is selected, then each job is represented by a horizontal dotted line
showing the events that took place for this job. if the "originator perspective"
is selected, each use-case is represented by a horizontal dotted line. figure 8
shows the dotted chart from the "task perspective" (i.e., the components in the
system). hence, each pair of dots represents the start and end of processing a
job by that component. the plug-in can provide an overview of the dynamics of
the execution of jobs and also the system load.
fig. 8: dotted chart analysis
for instance, the distribution of the dots along the timeline for each compo-
nent gives an insight into the utilization characteristics of the component, which
helps to identify the under- and overutilized components. for example, from
this chart, it was observed that ip2 is a component with high utilization rate
throughout this simulation experiment. also, the dotted chart provides details
about the distribution of the types of jobs (use-cases) over the simulation. in this
case, it can be observed from figure 8 that the remote jobs (use-cases that orig-
inate at the usbdown) are generated in a burst at the start of the simulation,
whereas the number of local jobs submitted at the scanner is fewer during the
same interval. thus this chart gives detailed insight into the steps of simulation
and hence can provide input for a better design of the simulation environment
setup.performance sequence diagram analysis the performance sequence di-
agram plug-in provides a means to assess the performance of the system. the
plug-in can provide information about behaviors that are common from the
event log. these patterns can be visualized from dierent perspectives such as
the components of the system and the data paths in the system. figure 9 shows a
screenshot of the pattern diagram generated from the view of the components. in
this context, the patterns depicted correspond to the dierent data paths listed
in section 1.1. also, statistics about the throughput times for each pattern are
presented, which can be used to determine the patterns that seem to be common
behavior, those that are rare and those that result in high throughput times.
on the other hand, this plug-in can be used to analyze an event log from the
oc e system to identify the data paths available thus assisting in identifying the
architecture and behavior of the system and also in the modeling process.
fig. 9: pattern diagram - performance sequence diagram analysis
trace clustering figure 9 shows the frequent patterns in the event log as
sequence diagrams. in the context of process and data mining many clustering
algorithms are available. prom supports various types of trace clustering. in fig-
ure 10 the result of applying the k-means clustering algorithm with a particular
distance metric is shown, where six clusters are identied. these correspond tothe dierent usecases or datapaths. for each cluster, the corresponding process
model can be derived. figure 10 shows two petri nets. these nets have been
discovered by applying the alpha algorithm [7] to two of the cluster. these dia-
grams nicely show how the dependencies depicted in figure 2 can be discovered.
for this particular setting of the clustering algorithm, the basic use-cases are
discovered. however, other types of clustering and distance metrics can be used
to provide insights into the dierent data-paths.
fig. 10: using trace clustering the dierent use cases can be identied and the cor-
responding detailed process models can be discovered
performance analysis figure 11 shows a detailed performance analysis of one
of the use-cases using the performance analysis with petri net plug-in. the focus
of the plug-in is to provide key performance indicators, which can be summoned
in an intuitive way. for this, the event logs of the selected cluster are replayed in
the petri net model of the use-case generated using the alpha algorithm. from
this simulation of a single use-case, performance indicators including average
throughput time, minimum and maximum values, and standard deviation for
the use-case throughput are derived. these provide a detailed insight into parts
of the system during the simulation experiment, in this case the six use-cases of
the system.additionally, the color of the places in the petri net indicates where in the
process (datapath in this case) the jobs of this use-case spend most time. for
example, we can observe and verify, based on the prior system knowledge, that
since the printip is the slowest component, jobs spend most time waiting in its
queue.
fig. 11: a detailed performance analysis is performed for one of the clusters dis-
covered
social network analysis figure 12 shows the result of using social net-
work analysis (sna) on the event log. this plug-in is typically used to quantify
and analyze social interaction between people in business process environment.
however, by mapping the roles played by people to components in this con-
text, the analysis provides information about interaction statistics among the
components.
the analysis plug-in uses the sna matrix generated by the social network
miner plug-in, which uses the data on causal dependency in hand over of work
among components, derived from the event log. as a result it is possible to show
the ow of work between the various components. the shape and size of the
nodes give a direct indication of the utilization of the component. the height
of the node is directly proportional to the amount of work owing into the
component and the width to the amount owing out. the arc weights are anindicator of the amount of work owing between the components. this provides
a quantication to analyze the interaction among the components.
fig. 12: social network analysis applied to the components of oc e's system
3.3 comparison and discussion
section 3.1 showed the classical simulation results obtained from monitors in
cpn tools. parameters such as waiting times of jobs and utilization rates help
in identifying the critical resources and to study the system performance and
behavior. the averages and standard deviations of such parameters are helpful
in analyzing the overall performance of the system over the entire simulation.
however, such classical simulation results typically do not present the dynamics
and detailed behavior of the system during the simulation.
on the other hand, section 3.2 looks into some of the plug-ins available in
the process mining tool prom and illustrates their application to event logs of a
cpn simulation. they provide the advantage to observe the dynamics and de-
tails of the system behavior and performance during the simulation experiment.for instance, the fuzzy miner and the dotted chart plug-ins can show views of
utilization characteristics of components in the system from dierent perspec-
tives. also, the performance sequence diagram analysis presents patterns and
their statistics (such as throughput times) helping in studying their occurrences
and impact on the system performance. clustering techniques can be used to
group jobs and analyze each group individually. hence, even though the clas-
sical simulation results provide an overall view of the system performance and
characteristics, prom provides some added advantages in presenting the detailed
view of the simulation process with insights into the dynamics of the system's
behavior and simulation.
another important observation is that process mining tools prom can be
used to observe and analyze real-world process and simulated processes. cur-
rently, system analysts tend to use dierent tools for monitoring real systems
and simulated systems. this is odd, since often the ultimate goal is to compare
the real system with the simulated system. (recall that simulation is used to
understand and improve real systems!)
4 related work
the use of cpn tools as a simulation tool is described in [1]. in this paper, the
monitor concept is described in detail. the britney suite animation tool [5]
extends the visualization and interaction functionality of cpn tools. the anima-
tion tool can be connected to the running simulation engine and used to present
the simulated behavior in a domain specic and interactive manner. prom is
described in [8]. the current release of prom contains more than 230 plug-ins.
in the paper, we could only show a few and we refer to www.processmining.org
for details.
in [2] we modeled the basic components of oc e's copiers using dierent
formalisms. in [9] the authors present the modeling of the features of a mo-
bile phone. the work also involves identication and analysis of the interaction
among features, helping in identifying faults in specication and improvement of
the architecture. in [10] the practical use of colored petri nets is demonstrated by
an industrial case study involving a owmeter instrument that consists of hard-
ware components performing estimation and calculation functions by interacting
with each other.
5 conclusions and future work
in this paper, initial experiences with using colored petri nets in octopus project
have been presented. petri nets allow for modeling all the details and dynamics
of the embedded system used in this case study. this permits providing practical
inputs and solutions to real-world problems. a slightly simplied version of a
currently existing oc e system was used as the case study. in the modeling process
the goal was to identify building blocks to allow re-use of components in the
model. also modeling the dynamic behavior of the usb is a signicant solutionto future problems such as modeling memory bus and processors. cpn tools
and prom prove to be eective tools in analyzing and studying the performance
of the system. they provided insights into identifying the bottleneck resources,
utilization of resources and system dynamics during execution of jobs. the pros
and cons of the classical presentation of simulation results and the application
of prom in analyzing the results are also studied.
from the modeling perspective, the next steps are to model the complete
copier systems at oc e, as opposed to the slightly simplied case studied here.
hence, it is essential to identify patterns and design re-usable building blocks
in the cpn model. this will allow exibility in exploring dierent system ar-
chitectures and design decisions through the model. in addition, the analysis of
simulation results using cpn tools and prom will be used to further explore the
design space and build heuristic scheduling rules in the next steps of the project.
we feel that it is important to use the same tools to monitor and analyze the real
system and its simulated counterparts. this will allow for a better comparison
and a more detailed analysis as shown in this paper.
references
1. k. jensen, l.m. kristensen, and l. wells. coloured petri nets and cpn tools for
modeling and validation of concurrent systems. international journal on software
tools for technology transfer (sttt). , volume 9, numbers 3-4, june 2007.
2. g. igna, v. kannan, y. yang, t. basten, m. geilen, f. vandraager, m. voorho-
eve, s. de smet, and l. somers. formal modeling and scheduling of data paths of
digital document printers. 6th international conference formats 2008 , pro-
ceesings, september 15-17 2008.
3. k. jensen. coloured petri nets. basic concepts, analysis methods and practical
use. eatcs monographs on theoretical computer science , springer-verlag, 1992.
4. w.m.p. van der aalst, j. nakatumba, a. rozinat, and n. russell. business process
simulation: how to get it right? bpm-08-07, eindhoven , bpmcenter.org, 25pp.
5. m. westergaard, k.b. lassen. the britney suite animation tool. proceedings of
the 27th international conference on application theory of petri nets and other
models of concurrency (icatpn 2006) , lecture notes in computer science 4024,
springer, pages 431-440, 2006.
6. a.k. alves de medeiros, and c.w. g unther. scheduling with timed automata.
theor. comput. sci. , 354(2):272{300, 2006.
7. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workow mining:
discovering process models from event logs. ieee transactions on knowledge
and data engineering , 16(9):1128-1142, 2004.
8. w.m.p. van der aalst, b.f. van dongen, c.w. gnther, r.s. mans, a.k. alves
de medeiros, a. rozinat, v. rubin, m. song, h.m.w. verbeek, and a.j.m.m.
weijters. prom 4.0: comprehensive support for real process analysis. in j. kleijn
and a. yakovlev, editors, application and theory of petri nets and other models of
concurrency (icatpn 2007) , volume 4546 of lecture notes in computer science,
pages 484-494. springer-verlag, berlin, 2007.
9. l. lorenstsen, a.-p. touvinene, j. xu. modelling feature interaction patterns in
nokia mobile phones using coloured petri nets and design/cpn. in k. jensen(ed.) proceedings of the third workshop and tutorial on practical use of coloured
petri nets and cpn tools , 2001.
10. l. lorentsen. modelling and analysis of a flowmeter system. proceedings of
workshop and tutorial on practical use of coloured petri nets and design/cpn ,
1999.