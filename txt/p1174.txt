case level counterfactual reasoning in process mining
mahnaz sadat qafari wil van der aalst
rheinisch-westf ¬®alische technische hochschule aachen (rwth), aachen, germany
m.s.qafari@pads.rwth-aachen.de,wvdaalst@pads.rwth-aachen.de
abstract. process mining is widely used to diagnose processes and uncover per-
formance and compliance problems. it is also possible to see relations between
dierent behavioral aspects, e.g., cases that deviate more at the beginning of the
process tend to get delayed in the later part of the process. however, correlations
do not necessarily reveal causalities. moreover, standard process mining diagnos-
tics do not indicate how to improve the process. this is the reason we advocate
the use of structural equation models andcounterfactual reasoning . we use re-
sults from causal inference and adapt these to be able to reason over event logs
and process interventions. we have implemented the approach as a prom plug-in
and have evaluated it on several data sets.
keywords: process mining ¬∑ counterfactual statement ¬∑ structural equation model.
1 introduction
humans tend to learn from the past (their experiences) by analyzing possible alterna-
tives of what happened in the reality and reÔ¨Çecting on their Ô¨Åndings aiming for better
results in future similar cases (e.g., not doing the same mistakes). thinking about pos-
sible alternatives to what happened in the past is called counterfactual thinking .
the information systems of companies save data about the process instances (cases)
in their event logs. process mining extracts knowledge from the event logs for discov-
ering the process model, monitoring process kpis, and improving processes. process
improvement requires a deep comprehension of the process behavior and its cases. in
this paper, we tailor the concept of counterfactual thinking to process mining and ex-
plain why a speciÔ¨Åc situation has a special outcome. given an instance with an unde-
sirable outcome, we aim at providing a set of counterfactual statements (we call them
explanations ) to explain such an outcome.
companies can boost customer satisfaction and build trust by providing explana-
tions for their speciÔ¨Åc cases without putting other people‚Äôs rights and privacy in danger
[8]. case level explanation can be used to explain why a customer has received a partic-
ular result, was it fair, or how to approach to get a better result. moreover, the process
manager can beneÔ¨Åt from this method as it can be used to explain why something hap-
pens in a speciÔ¨Åc case and how to act di erently to get di erent results in the future.
two important aspects of an explanation are accuracy and applicability. both of
them can be amended by distinguishing between correlation and causation among the
process features, which prevents misleading explanations that recommend altering fea-
tures with non-causal relationships with the result. for this matter, we propose usingarxiv:2102.13490v2  [cs.ai]  22 dec 20212 m. s. qafari et al.
fig. 1. the general overview of the proposed method.
thestructural equation model (sem) of the features in the procedure of generating ex-
planations.
case level explanations are case dependent, which means, an explanation that is
useful for a customer may not be favored by another customer with the same unde-
sirable outcome. to overcome this issue, in the proposed method, we present a set of
diverse explanations (i.e. explanations that di er from the given instance in di erent
features) to the user such that the user can decide which one to apply. moreover, as the
explanations are meant to be used by the human, the readability and understandability
of the explanations are important. therefore those explanations with a smaller number
of features with di erent values from the given instance, are preferred [11].
the rest of the paper is organized as follows. in section 2, a brief overview of
the related work is presented. in section 3, the method is presented. the experimental
results are presented in section 4. finally, in section 5 the conclusion is presented.
2 related work
there are already several approaches in the domain of process mining that deal with
root cause analysis using the Ô¨Åndings of a classiÔ¨Åcation techniques [10,2]. the draw-
back of these methods is that the classiÔ¨Åcation techniques are based on correlation and
not causal relationships. also, there are several works considering causal relationships
among di erent process features at the process level [3,4,7]. moreover, in [1] a method
for generating case-level recommendations of treatments that maximize the probability
of a given outcome is proposed. in this method a subset of candidate treatments that are
most correlated with the outcome is extracted by applying an association rule mining
technique. then the subgroups with causal relation between treatment and outcome are
identiÔ¨Åed using uplift tree. finally, the subgroups are sorted by the ratio of the score
associated to them by the uplift trees and their cost.
it is worth noting that counterfactual reasoning for explainability has been studied
extensively in the Ô¨Åeld of data mining and machine learning (e.g., [9,11]).
3 method
the general overview of the proposed method is presented in figure. 1. first, we en-
rich the event log. then, several random counterfactual instances similar to the currentcase level counterfactual reasoning in process mining 3
fig. 2. the process of the repair company.
instance are generated. among them, those that have a desirable outcome regarding a
given threshold are selected and optimization techniques are used to make them as close
as possible to the current instance. the resulting desirable counterfactual instances are
ordered according to their distance with the current instance, and Ô¨Ånally, converted into
a set of explanations and presented to the people involved.
in the following, we Ô¨Årst explain how we extract the data from the event log and
then we describe the explanation generation method.
3.1 situation feature table extraction
here, we mention how to extract features from the event logs of processes. an event log
is a collection of traces where each trace is a collection of events. each event indicates
that speciÔ¨Åc activity has happened at a speciÔ¨Åc time for a speciÔ¨Åc case. a trace is a
sequence of chronologically ordered events that belong to a speciÔ¨Åc case. both traces
and events may have several attributes. here we assume the uniqueness of the events in
the event log. we deÔ¨Åne an event log as follows:
deÔ¨Ånition 1 (event log). an event log is a set of traces where each trace is com-
posed of a chronologically ordered sequence of events and trace attributes (if applies).
moreover, each event refers to a case, an activity, a timestamp, and event attributes (if
applicable).
through this paper, we consider a company that repairs a speciÔ¨Åc product as the
running example. the petri-net model of this repair company is shown in figure 2.
each trace in the event log is corresponding to the process of repairing one product.
in the ‚Äúinspection‚Äù activity, several tests are done to determine the defects of a prod-
uct. then it is repaired, and afterward ‚ÄúÔ¨Ånal check‚Äù is done. we know that the newer
products are more complicated and harder to deal with. in this example, ‚Äúmodel‚Äù is a
trace level attribute where newer products have higher model numbers. ‚Äúteam size‚Äù is
another trace-level attribute that indicates the number of resources involved in repairing
the product. ‚Äúnum test‚Äù is an event-level attribute indicating the number of tests that
have been done in the ‚Äúinspection‚Äù activity. a snapshot of an event log is shown in ta-
ble 1. the manager of the repair company believes that in the trace with ‚Äúcase id‚Äù =c1
the ‚Äúrepair‚Äù activity (event e2) was too long and should have taken at most 500 hours.
he/she needs to know if it was the case, and if so, how they could had prevented it.
when we are interested in features not existing in the event log, we need to en-
rich the event log by adding new derived features from the event log or possibly other
sources to its traces and events. for example, we can enrich the repair company event
log by adding an attribute named ‚Äúduration‚Äù to its events indicating the duration of that
event in hours. in the repair company example, the value of the ‚Äúduration‚Äù attribute can
be computed by subtracting the timestamp of two consecutive events in each trace.
with respect to the time precedence of the cause and e ect, we consider just the
features that have been extracted from that part of a trace that has been recorded before4 m. s. qafari et al.
a speciÔ¨Åc feature as possible causes of it. for example, in the repair company, extracting
the data from the ‚ÄúÔ¨Ånal check‚Äù activity is meaningless when we want to Ô¨Ånd the features
that causally inÔ¨Çuence the ‚Äúduration‚Äù of the ‚Äúrepair‚Äù activity. so we need to extract the
data from a proper preÔ¨Åx of a trace, which we call a situation . also, we deÔ¨Åne the
situation set of an event log as the set of all situations generated using its traces. some
of the subsets of the situation set of a given event log are more meaningful. for example
the set of all the situations that end with an event with a speciÔ¨Åc activity name or the
set of all traces. in the repair company example, we can have a subset of situations
that end with an event whose activity name is ‚Äúrepair‚Äù. in this case, the situation subset
would include all the preÔ¨Åxes of traces which include the events with the activity name
‚Äúinspection‚Äù and ‚Äúrepair‚Äù. the situation extracted from the Ô¨Årst trace would include the
two events with ‚Äúevent id‚Äù e1 and e2. let‚Äôs call this situation s1.
an event log may have both event and trace level attributes. moreover, it is possible
to have the same attribute name in both levels. to concretely determine the attributes
that we are interested in their values, we use situation feature notion. a situation feature
refers to an attribute and an activity name (or possibly ‚Äútrace‚Äù). for example in the
repair company, sfteamsize andsfmodel are two situation features indicating ‚Äúteam size‚Äù
and ‚Äúmodel‚Äù attributes in the trace level. while, sfinspduration andsfinspnumtest are the
situation features referring to the ‚Äúduration‚Äù and ‚Äúnum test‚Äù in the ‚Äúinspection‚Äù activity.
also, sfrepairduration refers to the ‚Äúduration‚Äù of the ‚Äúrepair‚Äù activity. the situation feature
value extraction mechanism from a given situation is as follows:
‚Äìif the situation feature refers to an attribute name and ‚Äútrace‚Äù, then the value of that
attribute in the trace-level is assigned to the situation feature.
‚Äìif the situation feature refers to an attribute name and an activity name, then the
value of that attribute from an event with the given activity name, with the maxi-
mum timestamp is assigned to the situation feature.
for example, for the situation s1, the value assigned to sfinspduration is 71 (computed
using timestamps) and the value assigned to sfmodel is 7.
to generate explanations, we need to know the situation feature that identiÔ¨Åes the
problem (we call it target situation feature ) and a set of descriptive situation features
that are those features that may have causal e ect on the problem. we call the set in-
cluding the descriptive situation features and the target situation feature a situation fea-
ture extraction plan and denote it by sf. we can look at the sfas the schema in
a tabular data. for example in the repair company, as the manager believes that the
duration of ‚Äúrepair‚Äù activity for some cases should have been shorter, the target situa-
tion feature is sfrepairduration . also he has considered sfmodel,sfteamsize ,sfinspnumtest , and
event id case id activity name timestamp team size num test model
e1 c1 inspection 01-04-2020t08:00:00 2 42 7
e2 c1 repair 04-04-2020t07:00:00 2 42 7
e3 c1 Ô¨Ånal test 28-04-2020t08:00:00 2 42 7
e4 c2 inspection 01-05-2020t08:00:00 3 26 5
e5 c2 repair 03-05-2020t11:00:00 3 26 5
e6 c2 Ô¨Ånal test 19-05-2020t20:00:00 3 26 5
:::::::::::::::::::::
table 1. a snapshot of the event log of the repair company.case level counterfactual reasoning in process mining 5
sfinspduration as descriptive situation features. so, in this example we have sfrepair =
fsfmodel ;sfteamsize ;sfinspnumtest ;sfinspduration ;sfrepairdurationg.
given a situation feature extraction plan, sfwe can map each situation to a data
point by simply extracting the values of situation features in sfusing the proper mecha-
nism. we call such a data point an instance . moreover, we can deÔ¨Åne a target-dependent
tabular data, called situation feature table , extracted from a given situation subset, as
the bag of the instances extracted from the situations in a given situation subset. as an
example, using sfrepair instance irepair =f(sfmodel ;7);(sfteamsize ;2);(sfinspnumtest ;42) ;
(sfinspduration ;71) ;(sfrepairduration ;577)gis generated from s1.
3.2 explanation generation method
consider an instance iin the situation feature table with an undesirable target situation
feature value regarding a threshold t. for example, in the repair company the threshold
is 500. w.l.o.g., in this paper, we always assume that the values lower than the given
threshold are desirable. explanations are diverse instances which are close to iand
have a desirable target situation feature value. as it is vain to investigate the e ect of
intervention on those situation features that their value can not be altered by the user,
we study the e ect of changing the value of those situation features that are modiÔ¨Åable
by the user. we call the set of modiÔ¨Åable situation features actionable situation features
and denote it with asf . we deÔ¨Åne a set of counterfactual explanations for a given
instances as follows.
deÔ¨Ånition 2 (a set of counterfactual explanation). let i be an instance for which
the target situation feature value is undesirable. a set of explanations for i is a set of
diverse instances that are close to i and yet di er from i in a subset of asf and have a
desirable result for the target situation feature.
to generate the set of counterfactual explanations, we take the following three steps:
1. generating candidates. we generate several candidates for the values that could
had been assigned to the actionable situation features. each candidate is a value assign-
ment to a subset of situation features in asf . we generate candidates such that for half
of them the situation feature values are selected from their distribution in the situation
feature table and for the other half, they are selected randomly from their domain.
2. predicting the value of the target situation feature. in the second step, we
compute the e ect of replacing the values of the situation features in the given instance
with those in the generated candidates on the value of target situation feature using the
sem of the situation features. the sem of the situation features of a situation feature
table can be provided by a customer who possesses the process domain knowledge or
can be inferred in a data-driven manner using several methods that already exist in the
literature (e.g., [7,4]). loosely speaking, a sem is a set of equations that determine how
to generate the observational and interventional distributions. more formally:
deÔ¨Ånition 3 (structural equation model (sem)). letsfbe a situation feature ex-
traction plan, the sem of sfis deÔ¨Åned aseq2 sf!expr (sf)where for each
sf2s f, expr (sf)is an expression over the situation features in sfand possibly
some noise n sf. moreover, the noise distributions of n sffor all sf2sfhave to be
mutually independent.6 m. s. qafari et al.
we assume that sfincludes all relevant situation features and there is no common
hidden confounder for the situation features in sf. also, we assume that the sem does
not include any loop. in table 2, a possible sem for the repair company is presented.
using semeq, prediction of the class situation feature value for each candidate
involves three steps abduction ,action , and prediction [5]. we explain these steps using
the repair company example.
‚Äì abduction. first we need to incorporate the observed data, instance i, into the
model,eq, and generate a counterfactual sem that explains the conditions and
the behavior of the system and the environment when iwas happening. a counter-
factual sem ,eq0, is obtained by replacing the distribution of noise terms in eq
with the corresponding noise distributions condition on sf=i. considering the
sem in table 2 and irepair , the equations of the counterfactual sem eq0
repair are:
sfmodel=7,sfinspnumtest =2,sfinspduration =10sfmodel+1,sfinspnumtest =5sfmodel+
3sfteams ize +1, and sfrepairduration =50sfmodel+5sfinspnumtest +17.
‚Äì action. the second step is taking action toward enforcing changes in the counter-
factual semeq0, regarding candidate c. the result is a sem eq00where sf=csf
where csfis the value assigned to sfbycifsf2dom(c) and sf=eq0(sf) where sf
is not in the domain of c. as an example, suppose that we are interested in predict-
ing the value of sfrepairduration for the candidatef(sfteams ize ;3)g. intervention on the
counterfactual sem eq0
repair , results in replacing sfteams ize =2 with sfteams ize =3.
‚Äì prediction. the third step involves using the modiÔ¨Åed sem to predict the counter-
factual value of the target situation feature by simply computing the value of targer
situation feature (or its distribution) in the counterfactual sem under the interven-
tion. in this step, we remove those situation features from the domain of cthat do
not aect the target situation feature value. in the above example, computing the
values of the situation features we have: f((sfmodel ;?);7);(sfteams ize ;3);(sfinspnumtest ;
45) ;(sfinspduration ;71) ;(sfrepairduration ;592)g. we call such an instance a counterfac-
tual instance .
3. selecting a subset of candidates. we want explanations to be a set of diverse
candidates with a small domain and a desirable predicted target situation feature value.
also we want them to be close to the given instance. to compute the distance between
instances, we use l1metric on the normalized situation features. as mentioned in [11],
using l1metric, more sparse explanations would be generated. for the diversity, we
partition candidates with desirable predicted outcome based on their domain and then
sort them in each partition according to their distance from the given instance. a set of
these candidates are selected one by one from di erent partitions, with the priority of
those partitions that have a smaller domain.
sfmodel=nsfmodelnsfmodeluni f orm (1;10)
sfteamsize =nsfteamsizensfteamsizeuni f orm (1;3)
sfrepairduration =10sfmodel+nsfrepairdurationnsfrepairdurationuni f orm ( 2;4)
sfinspnumtest =5sfmodel+3sfteamsize +nsfinspnumtestnsfinspnumtestuni f orm ( 1;2)
sfrepairduration =50sfmodel+5sfinspnumtest +nsfrepairdurationnsfrepairdurationuni f orm (10 ;20)
table 2. a possible sem for the repair company.case level counterfactual reasoning in process mining 7
fig. 3. the result of applying the implemented method on the synthetic event logs.
4 experimental results
the implemented plugin is available in prom nightly build under the name counterfac-
tual explanation . in the implemented plugin, we can apply several classiÔ¨Åers (includ-
ing regression tree (rt), locally weighted learning (lwl), multi-layer perceptron
(nn)), as well as sem, to predict the target situation feature value of candidates.
we applied the implemented plugin on a synthetic event log to see how di erent
might be the explanations generated by the sem and by a machine learning technique
with the highest accuracy in terms of predicted target situation feature values and the
number of situation features with di erent values in the given instance and the explana-
tions. so, we did not use optimization on the selected desirable counterfactual instances.
for the synthetic event log, we have used the repair company example and irepair as
the instance with the undesirable target situation feature. also, the values lower than the
given threshold 500 were desirable. we considered all the descriptive situation features
as actionable. we have generated 1000 traces such that the sem of its situation feature
values is the one in table 2. then, we generate a set of 8 explanations by generating
several candidates and using the sem in tabble 2 to evaluate them.
we have used the classiÔ¨Åer with the highest accuracy for predicting the value of
sfrapairduration on the selected candidates in the previous step. the accuracy of rt, lwl,
and nn on the data were 0.818, 0.990, and 0.984, respectively. but their accuracy re-
duced on the counterfactual instances to 0.74, 0.77, and 0.76, respectively.
the results of applying the proposed method using sem and three mentioned ma-
chine learning techniques are presented in figure 3. in left part of figure 3, the predicted
sfrapairduration of the selected desirable candidates using sem (red line), rt (blue line),
lwl (green line), and nn (light green line) are presented. in the right side of figure 3,
the size of the domain of the selected candidates is demonstrated.
discussion. as demonstrated in figures 3, there is a gap between the values predicted
by the machine learning techniques and by sem. also, the accuracy of the classi-
Ô¨Åers predicting the value of the counterfactual instances drops dramatically. this phe-
nomenon can be explained by the di erence in their mechanism of predicting counter-
factual values. using a machine learning technique, neither the behavior of the envi-
ronment nor the e ect of an intervention is considered; but, the generated instance is
regarded as a new instance, which may result in wrong predictions.
the di erence in the number of e ective situation features with di erent values be-
tween the given and explanations comes from the fact that machine learning techniques8 m. s. qafari et al.
do not distinguish among the situation features with causal and mere correlation rela-
tionship with the target situation feature. on the other hand, using sem the changes
in the values of the situation features that have no causal relationships with the target
situation feature in the counterfactual instances are simply ignored.
5 conclusion
we have presented a method that can be used by companies to explain to their cus-
tomers why they have received a speciÔ¨Åc outcome in a case-speciÔ¨Åc manner and help
them to prevent the same outcome in the future. as a result, the interpretability and
accountability of the companies would be boosted.
the results of the evaluations have shown that ignoring the causal relationships
among the situation features may end up in explanations that suggest changing situ-
ation features with no causal e ect on the class situation feature. moreover, using a
machine learning technique, regardless of its accuracy, for predicting the value of the
target situation feature may result in wrong explanations or missing some of the good
explanations.
references
1. bozorgi, z.d., teinemaa, i., dumas, m., rosa, m.l., polyvyanyy, a.: process mining meets
causal machine learning:discovering causal rules from event logs. in: icpm (2020)
2. ferreira, d.r., vasilyev, e.: using logical decision trees to discover the cause of process
delays from event logs. computers in industry 70, 194‚Äì207 (2015)
3. hompes, b.f., maaradji, a., la rosa, m., dumas, m., buijs, j.c., van der aalst, w.m.: dis-
covering causal factors explaining business process performance variation. in: international
conference on advanced information systems engineering. pp. 177‚Äì192. springer (2017)
4. narendra, t., agarwal, p., gupta, m., dechu, s.: counterfactual reasoning for process opti-
mization using structural causal models. in: proceedings of business process management
forum. vol. 360, pp. 91‚Äì106. springer (2019). https: //doi.org /10.1007 /978-3-030-26643-1 6
5. pearl, j., et al.: models, reasoning and inference. cambridge, uk: cambridgeuniversity-
press (2000)
6. peters, j., janzing, d., sch ¬®olkopf, b.: elements of causal inference: foundations and learning
algorithms. mit press (2017)
7. qafari, m.s., van der aalst, w.: root cause analysis in process mining using structural equa-
tion models. in: bpi (2020)
8. reddix-smalls, b.: credit scoring and trade secrecy: an algorithmic quagmire or how the
lack of transparency in complex Ô¨Ånancial models scuttled the Ô¨Ånance market. uc davis bus.
lj12, 87 (2011)
9. russell, c.: e cient search for diverse coherent explanations. in: proceedings of the con-
ference on fairness, accountability, and transparency. pp. 20‚Äì28 (2019)
10. suriadi, s., ouyang, c., van der aalst, w.m., ter hofstede, a.h.: root cause analysis with
enriched process logs. in: international conference on business process management. pp.
174‚Äì186. springer (2012)
11. wachter, s., mittelstadt, b., russell, c.: counterfactual explanations without opening the
black box: automated decisions and the gdpr. harv. jl & tech. 31, 841 (2017)