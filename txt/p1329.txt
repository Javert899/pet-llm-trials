process mining: a 360 degree overview
wil m.p. van der aalst[0000−0002−0955−6940]
process and data science (pads), rwth aachen university, germany
wvdaalst@pads.rwth-aachen.de www.vdaalst.com
abstract. process mining enables organizations to uncover their actual processes,
provide insights, diagnose problems, and automatically trigger corrective actions.
process mining is an emerging scientific discipline positioned at the intersection
between process science and data science. the combination of process modeling
and analysis with the event data present in today’s information systems provides
new means to tackle compliance and performance problems. this chapter pro-
vides an overview of the field of process mining introducing the different types
of process mining (e.g., process discovery and conformance checking) and the
basic ingredients, i.e., process models and event data. to prepare for later chap-
ters, event logs are introduced in detail (including pointers to standards for event
data such as xes and ocel). moreover, a brief overview of process mining
applications and software is given.
keywords: process mining · event data · process modeling · process discovery
1 introduction
process mining can be defined as follows: process mining aims to improve operational
processes through the systematic use of event data [1, 2]. by using a combination of
event data and process models, process mining techniques provide insights, identify
bottlenecks and deviations, anticipate and diagnose performance and compliance prob-
lems, and support the automation or removal of repetitive work. process mining tech-
niques can be backward-looking (e.g., finding the root causes of a bottleneck in a pro-
duction process) or forward-looking (e.g., predicting the remaining processing time of a
running case or providing recommendations to lower the failure rate). both backward-
looking and forward-looking analyses can trigger actions (e.g., countermeasures to ad-
dress a performance or compliance problem). the focus of process mining is on opera-
tional processes , i.e., processes requiring the repeated execution of activities to deliver
products or services. these can be found in all organizations and industries, includ-
ing production, logistics, finance, sales, procurement, education, consulting, healthcare,
maintenance, and government. this chapter provides a 360 degree overview of process
mining, introducing basic concepts and positioning process mining with respect to other
technologies.
the idea of using detailed data about operational processes is not new. for example,
frederick winslow taylor (1856-1915) collected data on specific tasks to improve labor
productivity [35]. with the increasing availability of computers, spreadsheets and other
business intelligence tools were used to monitor and analyze operational processes.2 wil van der aalst
however, in most cases, the focus was on a single task in the process, or behavior was
reduced to aggregated key performance indicators (kpis) such as flow time, utilization,
and costs. process mining aims to analyze end-to-end processes at the level of events ,
i.e., detailed behavior is considered in order to explain and improve performance and
compliance problems.
process mining research started in the late 1990s [23]. in 2004 the first version of
the open-source platform prom was released with 29 plug-ins. over time the prom plat-
form was extended and now includes over 1500 plug-ins. the first commercial process
mining tools appeared around 15 years ago. today, there are over 40 commercial pro-
cess mining tools and process mining is used by thousands of organizations all over the
globe. however, only a small fraction of its potential has been realized. process mining
is generic and can be applied in any organization.
process 
mining
process 
sciencedata 
scienceprocess discovery
conformance checkingoperations research
workflow managementstatistics
operations management
machine learningartificial intelligencedata mining
business intelligencesupervised learningunsupervised learningconcurrency theorysimulation
industrial engineering
planning and controlprocess modelingbusiness process management
data  managementdata warehousing
fig. 1. process mining = data science ∩process science.
figure 1 shows that process mining can be seen as the intersection of data science
and process science. in [2], the following definition is proposed: “data science is an
interdisciplinary field aiming to turn data into real value. data may be structured or
unstructured, big or small, static or streaming. value may be provided in the form of
predictions, automated decisions, models learned from data, or any type of data visual-
ization delivering insights. data science includes data extraction, data preparation, data
exploration, data transformation, storage and retrieval, computing infrastructures, vari-
ous types of mining and learning, presentation of explanations and predictions, and the
exploitation of results taking into account ethical, social, legal, and business aspects.”
in [2], process science is used as an umbrella term to refer to the broader discipline that
combines knowledge from information technology and knowledge from management
sciences to improve and run operational processes. in the more recent [12], the follow-
ing definition is proposed: “process science is the interdisciplinary study of continuous
change. by process, we mean a coherent series of changes that unfold over time andprocess mining: a 360 degree overview 3
occur at multiple levels.” in [12], we emphasize the following key characteristics of
process science: (1) processes are in focus, (2) processes are investigated using scien-
tific methods, (3) an interdisciplinary lens is used, and (4) the goal of process science
is to influence and change processes to realize measurable improvements. as stated in
[2] and visualized in figure 1; process mining can be viewed as the link between data
science and process science. process mining seeks the confrontation between event data
(i.e., observed behavior) and process models (hand-made models or automatically dis-
covered models), and aims to exploit event data in a meaningful way, for example,
to provide insights, identify bottlenecks, anticipate problems, record policy violations,
recommend countermeasures, and streamline processes.
discover
align
replay
enrich
apply
compare
information 
systems
extract
process 
models
explore
 select
filter
clean
conformance
performance 
diagnostics
predictions
improvements
transform
act
 show
model
adapt
show
interpret
drill down
ml
+
 +
event 
data
fig. 2. 360 degrees overview of process mining.
figure 2 shows a high-level view of process mining. event data need to be ex-
tracted from information systems used to support the processes that need to be analyzed.
customer relationship management (crm), enterprise resource planning (erp), and
supply chain management (scm) systems store events. examples are sap s/4hana,
oracle e-business suite, microsoft dynamics 365, and salesforce crm. next to these
sector-agnostic software systems, there are more specialized systems such as health
information systems (his). all of these systems have in common that they are loaded
with event data. however, these are scattered over many database tables and need to be
converted into a format that can be used for process mining. as a consequence, data
extraction is an integral part of any process mining effort, and may be time-consuming.
events are often represented by a case identifier, an activity name, a timestamp, and
optional attributes such as resource, location, cost, etc. object-centric event data allow
events to point to any number of objects rather than a single case (see section 3).
once extracted, event data can be explored ,selected ,filtered , and cleaned (see fig-
ure 2). data visualization techniques such as dotted charts and sequence diagrams can
be used to understand the data. often, the data need to be scoped to the process of4 wil van der aalst
interest. one can use generic query languages like sql, sparql, and xquery or a
dedicated process query language (pql). data may be incomplete, duplicated, or in-
consistent. for example, month and day may be swapped during manual data entry.
there is a variety of techniques and approaches to address such data quality problems
[34].
the resulting dataset is often referred to as an event log , i.e., a collection of events
corresponding to the selected process. process discovery techniques are used to au-
tomatically create process models. commercial tools typically still resort to learning
the so-called directly-follows graph (dfg) which typically leads to underfitting pro-
cess models [3]. if two activities do not occur in a fixed order, then loops are cre-
ated. this leads to spaghetti-like diagrams suggesting repetitions that are not supported
by the data. however, there are numerous approaches to learning higher-level models
represented using business process model and notation (bpmn), petri nets, or uni-
fied modeling language (uml) activity diagrams. in contrast to dfgs, such models
are able to express concurrency. example techniques to discover such models are the
alpha algorithm [8], region-based approaches [13, 11, 33, 36], inductive mining tech-
niques [28, 29], and the split miner [9]. the process model returned may aim to de-
scribe all behavior observed or just the dominant behavior. note that the event log only
contains example behavior, is likely to be incomplete, and at the same time may contain
infrequent behavior.
the combination of a process model and event data can be used to conduct con-
formance checking and performance analysis (figure 2). the process model may have
been discovered or made by hand. discovered process models are descriptive and hand-
crafted models are often normative. conformance checking relates events in the event
log to activities in the process model and compares both. the goal is to find commonal-
ities and discrepancies between the modeled behavior and the observed behavior. if the
process model is normative, deviations correspond to undesired behavior (e.g., fraud or
inefficiencies). if the model was discovered automatically with the goal of showing the
dominant behavior, then deviations correspond to exceptional behavior (i.e., outliers).
note that most processes have a pareto distribution, e.g., 80% of the cases can be de-
scribed by only 20% of the process variants. it is often easy and desirable to create
a process model describing these 80%. however, the remaining 20% cannot be dis-
carded since these cases cover the remaining 80% of the process variants and often also
the majority of performance and compliance problems. sometimes event logs are even
more unbalanced, e.g., it is not uncommon to find logs where 95% of the cases can
be described by less than 5% of the process variants. in the latter case, it may be that
the remaining 5% of cases (covering 95% of the process variants) consume most of the
resources due to rework and exception handling.
since events have timestamps, it is easy to overlay the process model with perfor-
mance diagnostics (service times, waiting times, etc.). after discovering the control-
flow, the process model can be turned into a stochastic model that includes probabilities
and delay distributions.
after applying conformance checking and performance analysis techniques, users
can see performance and compliance problems. it is possible to perform root-cause
analysis for such problems. one may find out that critical deviations are often causedprocess mining: a 360 degree overview 5
by a particular machine or supplier, or that the main bottleneck is caused by poor re-
source planning or excessive rework for some product types. in a procurement process,
price changes by a particular supplier may explain an increase in rework. if “receive
invoice” often occurs before “create purchase requisition”, then this signals a com-
pliance problem in the same process. these are just a few examples. in principle, any
process-related problem can be diagnosed as long as event data are available.
the right-hand side of figure 2 shows that process mining can be used to (1) trans-
form and improve the process and (2) automatically address observed and predicted
problems. the stochastic process models discovered from event data can be used to
conduct “what-if” analysis using simulation or other techniques from operations re-
search (e.g., planning). the combination of event data and process models can be used
to generate machine learning (ml) problems. ml techniques can be used to predict
outcomes without being explicitly programmed to do so. the uptake of ml in recent
years can be attributed to progress in deep learning, where artificial neural networks
having multiple layers progressively extract higher-level features from the raw input.
ml techniques cannot be applied directly to event data. however, by replaying event
data on discovered process models, it is possible to create a range of supervised learning
problems. examples include:
–what is the remaining processing time of a particular insurance claim?
–are we able to handle 95% of the cases within one week?
–is this application going to deviate from the normative process?
–will this patient be moved to the intensive care unit?
–will we have enough free beds in the intensive care unit tomorrow?
it is important to note that the right-hand side of figure 2 (i.e., extraction, discov-
ery, conformance checking, and performance analysis) cannot be supported using main-
stream artificial intelligence (ai) and machine learning (ml) technologies (e.g., neu-
ral networks). one first needs to discover an explicit process model tightly connected
to the event data, to pose the right questions. however, process mining can be used to
create ai/ml problems. the combination can be used to trigger corrective actions or
even complete workflows addressing the problem observed. this way, event data can
be turned into actions that actively address performance and compliance problems.
2 process models
there are many notations to describe processes, ranging from directly-follows graphs
(dfgs) and transition systems, to bpmn and petri nets. we will use an example to
gently introduce these notations. consider a process involving the following activities:
buy ingredients (bi) ,create base (cb) ,add tomato (at) ,add cheese (ac) ,add salami (as) ,
bake in oven (bo) ,eat pizza (ep) , and clean kitchen (ck) . we will call this fictive process
the “pizza process” and use this to illustrate the key concepts and notations.
figure 3 shows a process model using business process model and notation (bpmn)
[17]. the process starts with activity buy ingredients (bi) followed by activity create
base (cb) . then three activities are executed in any order: add tomato (at) ,add cheese
(ac), and add salami (as) . after all three toppings (tomato, cheese, and salami) have6 wil van der aalst
buy 
ingredients 
(bi)startcreate base 
(cb)add tomato
(at)bake in oven 
(bo)eat pizza
(ep)
add salami
(as)clean kitchen 
(ck)
endadd cheese
(ac)
fig. 3. bpmn model of the “pizza process”. the three toppings (tomato, cheese, and salami) can
be added in any order.
been added, the activities bake in oven (bo) ,eat pizza (ep) , and clean kitchen (ck) are
performed in sequence. assuming that the three concurrent activities are performed in
some order (i.e., interleaved), there are 3! = 6 ways to execute the “pizza process”. the
two diamond-shaped symbols with a +inside denote parallel gateways . the first one
is a so-called and-split starting the three concurrent branches and the second one is a
so-called and-join . the bpmn process starts with a start event (shown as a circle) and
ends with an end event (shown as a thick circle).
bi cbac
at
asbo ep ck
fig. 4. petri net modeling the “pizza process” with activities buy ingredients (bi) ,create base
(cb),add cheese (ac) ,add tomato (at) ,add salami (as) ,bake in oven (bo) ,eat pizza (ep) , and
clean kitchen (ck) .
figure 4 models the same process in terms of a petri net. this model also allows for
3! = 6 ways to execute the “pizza process”. the circles correspond to places (to model
states) and the squares correspond to transitions (to model activities). places may hold
tokens. a place is called marked if it contains a token. a marking is a distribution of
tokens over places. in figure 4, the source place (i.e., the input place of transition bi) is
marked, as is indicated by the token (the black dot). a transition is enabled if all input
places are marked. in the initial marking shown in figure 4, transition bi(corresponding
to activity buy ingredients ) is enabled. a transition that is enabled may fire (i.e., it may
occur). this means that a token is removed from each of the input places and a token
is produced for each of the output places. note that transition cbconsumes one token
and produces three tokens (one for each output place) and transition boconsumes three
tokens (one for each input place) and produces one token. the process ends when a
token is put on the sink place, i.e., the output place of ck. in total there are 2 + 23+ 3 =
13reachable markings. although the behavior of the petri net in figure 4 is the same
as the bpmn model in figure 3, it is easier to refer to the states of the process model.
figure 5 models the “pizza process” using a process tree . this representation is
rarely presented to end-users, but several mining algorithms use this internally. processprocess mining: a 360 degree overview 7
cb bo bi ep ck
ac at as
fig. 5. process tree of the “pizza process”: →(bi,cb,∧(ac,at,as),bo,ep,ck).
trees are closer to programming constructs, process algebras, and regular expressions.
the graphical representation can be converted to a compact textual format: →(bi,cb,
∧(ac,at,as),bo,ep,ck). a sequence operator →executes its children in sequential
order. the root node in figure 5 denotes such a sequence, i.e., the six child nodes are
executed in sequence. the third child node models the parallel execution of its three
children. this subtree can be denoted by ∧(ac,at,as). later we will see that there are
four types of operators that can be used in a process tree: →(sequential composition),
×(exclusive choice), ∧(parallel composition), and ⟲(redo loop). the semantics of a
process tree can be expressed in terms of petri nets, e.g., figure 5 and figure 4 represent
the same process.
bi at cbac
asbo ep ck
fig. 6. dfg of the “pizza process”. note that the behavior is different, e.g., one may add 10
toppings to the pizza.
most of the process mining tools directly show a directly-follows graph (dfg)
when loading an event log. this helps get a first impression of the behavior recorded.
figure 6 shows a dfg for our running example. there are two special nodes to model
start (▶) and end ( ■). the other nodes represent activities. the arcs in a dfg denote
the “directly-follows relation”, e.g., the arc connecting cbtoatshows that immediately
after creating the pizza base cbone can add tomato paste at. activity cbhas three
outgoing arcs denoting a choice, i.e., cbis directly followed by at,ac, oras. activity
atalso has three outgoing arcs denoting that one can add another topping ( acoras)
or bake the pizza ( bo). note that the behavior of the dfg in figure 6 is different from
the three models shown before (i.e., the bpmn model, the petri net, and the process
tree). the dfg allows for infinitely many ways to execute the “pizza process” (instead
of3! = 6 ). for example, it is possible to create a pizza where each of the toppings8 wil van der aalst
was added 10 times. the problem is that whenever two activities can occur in any order
(e.g., atandac), there is immediately a loop in the dfg (even when both happen only
once).
buy 
ingredients 
(bi)startcreate base 
(cb)add tomato
(at)bake in oven 
(bo)
eat pizza
(ep)add salami
(as)clean kitchen 
(ck)
endadd cheese
(ac)
add 
mushrooms
(am)
fig. 7. bpmn model of the extended “pizza process”.
to explain other process constructs such as choice, skipping, and looping we extend
the “pizza process”. first of all, we allow for adding multiple servings of cheese, i.e.,
activity accan be executed multiple times after creating the pizza base and before
putting the pizza in the oven. second, instead of adding salami as a topping one can add
mushrooms, i.e., there is a choice between as(add salami) and am(add mushrooms).
third, the eating of the pizza may be skipped (i.e., activity epis optional).
figure 7 shows the bpmn model with these three extensions. in total six exclusive
gateways were added: three xor-splits and three xor-joins (see the diamond-shaped
symbols with a ×inside). after adding cheese, one can loop back. there is a choice
between adding salami and adding mushrooms. also the eating of the pizza can be
skipped.
bi cbac
at
asbo ep ck
am
fig. 8. petri net modeling the extended “pizza process” with two silent transitions (to skip eating
the pizza and to add more cheese), and a transition amcorresponding to activity add mushrooms .
figure 8 shows a petri net modeling the extended process. a new transition am(add
mushrooms) has been added. transitions asand amshare an input place. if the input
place is marked, then both transitions are enabled, but only one of them can occur.process mining: a 360 degree overview 9
ifasconsumes the token from the shared input place, then amgets disabled. if am
consumes the token from the shared input place, then asgets disabled. this way, we
model the choice between two toppings: salami and mushrooms. figure 8 also has two
new so-called silent transitions denoted by the two black rectangles. sometimes such
silent transitions are denoted as a normal transition with a τlabel. silent transitions
do not correspond to activities and are used for routing only, e.g., skipping activities.
in figure 8, there is one silent transition to repeatedly execute ac(to model adding
multiple servings of cheese) and one silent transition to skip ep.
cb bo bi
epck
acat
asτ amτ 
fig. 9. process tree of the extended “pizza process”: →(bi,cb,∧(⟲(ac, τ),at,×(as,am)),bo,
×(ep, τ),ck).
the process tree in figure 9 has the same behavior as the bpmn model and petri
net just shown. the process tree uses all four operators: →(sequential composition),
×(exclusive choice), ∧(parallel composition), and ⟲(redo loop). a silent activity is
denoted by τand cannot be observed. the process tree in figure 9 can also be visualized
in textual form: →(bi,cb,∧(⟲(ac, τ),at,×(as,am)),bo,×(ep, τ),ck).
to understand the notation, we first look at a few smaller examples. process tree
×(a, b)models a choice between activities aandb. process tree ×(a, τ)can be used to
model an activity athat can be skipped. process tree ⟲(a, τ)can be used to model the
process that executes aat least once. the “redo” part is silent, so the process can loop
back without executing any activity. process tree ⟲(τ, a)models a process that executes
aany number of times. the “do” part is now silent and activity ais in the “redo” part.
this way it is also possible to not execute aat all.
now let us take a look at the three modifications of our extended “pizza process”:
⟲(ac, τ)models that multiple servings of cheese can be added, ×(as,am)models the
choice between salami and mushrooms, and ×(ep, τ)models the ability to skip eating
the pizza.
the dfg shown in figure 10 incorporates the three extensions. again, the behavior
is different from figures 7, 8, and 9. unlike the other models, the dfg allows for
adding multiple servings of salami, mushrooms, and tomato paste. it is impossible to
model concurrency properly, because loops are added the moment the order is not fixed.
therefore, dfgs are suitable for a quick first view of the process, but for more advanced
process analytics, higher-level notations such as bpmn, petri nets, and process trees are
needed.10 wil van der aalst
bi at cbac
asbo ep ck
am
fig. 10. dfg of the extended “pizza process”. note that the process becomes increasingly
spaghetti-like, allowing for process executions different from the bpmn model, the petri net,
and the process tree.
note that, in this section, we focused on control-flow. however, process models can
be extended with frequencies, probabilities, decision rules, roles, costs, and time delays
(e.g., mean waiting times). after discovering the control-flow and replaying the event
data on the model, it is easy to extend process models with data, resource, cost, and
time perspectives.
3 event data
using process mining, we would like to analyze and improve processes using event
data. table 1 shows a fragment of an event log in tabular form. one can think of this as
a table in a relational database, a csv (comma separated value) file, or excel spread-
sheet. each row in the table corresponds to an event . an event can have many different
attributes. in this simple example, each event has five attributes: case,activity ,time-
stamp ,resource , and customer . most process mining tools and approaches require at
least three attributes: case (refers to a process instance), activity (refers to the opera-
tion, action, or task), and timestamp (when did the event happen). these three attributes
are enough to discover and check the control-flow perspective. a case may refer to an
order, a patient, an application, a student, a loan, a car, a suitcase, a speeding ticket, etc.
in table 1, each case refers to a pizza being produced and consumed. in section 2 we
showed process models describing this process. however, now we start from the ob-
served behavior recorded in the event log. we can witness the same activities as before:
buy ingredients ( bi), create base ( cb), add cheese ( ac), add tomato ( at), add salami ( as),
add mushrooms ( am), bake in oven ( bo), eat pizza ( ep), and clean kitchen ( ck). table 1
uses a simple time format (e.g., 18:10 ) to simplify the presentation (i.e., we skipped the
date). systems often use the iso 8601 standard (or similar) to exchange date- and time-
related data, e.g., 2021-09-21t18:10:00+00:00 . in the remainder, we formalize event
data and provide useful notions to reason about both observed and modeled behavior.
we start with some basic mathematical notations.process mining: a 360 degree overview 11
table 1. fragment of a larger event log with 6400 events, i.e., the whole table has 6400 rows.
these events describe the production of 800 pizzas. each row refers to an event having five
attributes, including the three mandatory ones: case, activity, and timestamp.
case activity timestamp resource customer
. . . . . . . . . . . . . . .
pizza-56 buy ingredients ( bi) 18:10 stefano valentina
pizza-57 buy ingredients ( bi) 18:12 stefano giulia
pizza-57 create base ( cb) 18:16 mario giulia
pizza-56 create base ( cb) 18:19 mario valentina
pizza-57 add tomato ( at) 18:21 mario giulia
pizza-57 add cheese ( ac) 18:27 mario giulia
pizza-56 add cheese ( ac) 18:34 mario valentina
pizza-56 add tomato ( at) 18:44 mario valentina
pizza-56 add salami ( as) 18:45 mario valentina
pizza-56 bake in oven ( bo) 18:48 stefano valentina
pizza-57 add salami ( as) 18:50 mario giulia
pizza-56 eat pizza ( ep) 19:10 valentina valentina
pizza-58 buy ingredients ( bi) 19:17 stefano laura
pizza-57 bake in oven ( bo) 19:23 stefano giulia
pizza-57 eat pizza ( ep) 19:27 giulia giulia
pizza-57 clean kitchen ( ck) 19:44 mario giulia
pizza-58 create base ( cb) 19:48 mario laura
pizza-58 add salami ( as) 19:49 mario laura
pizza-58 add tomato ( at) 19:55 mario laura
pizza-56 clean kitchen ( ck) 20:08 mario valentina
pizza-58 add cheese ( ac) 20:13 mario laura
pizza-58 bake in oven ( bo) 20:29 stefano laura
pizza-58 eat pizza ( ep) 20:48 laura laura
pizza-58 clean kitchen ( ck) 20:51 mario laura
. . . . . . . . . . . . . . .12 wil van der aalst
3.1 notations
b(a)is the set of all multisets over some set a. for some multiset b∈ b(a),b(a)
denotes the number of times element a∈aappears in b. some examples: b1= [ ] ,
b2= [x, x, y ],b3= [x, y, z ],b4= [x, x, y, x, y, z ], and b5= [x3, y2, z]are multisets
overa={x, y, z}.b1is the empty multiset, b2andb3both consist of three elements,
andb4=b5, i.e., the ordering of elements is irrelevant and a more compact notation
may be used for repeating elements. the standard set operators can be extended to
multisets, e.g., x∈b2,b2⊎b3=b4,b5\b2=b3,|b5|= 6, etc.{a∈b}denotes the set
with all elements afor which b(a)≥1.b(x) =p
a∈xb(x)is the number of elements
inbbelonging to set x, e.g., b5({x, y}) = 3 + 2 = 5 .b≤b′ifb(a)≤b′(a)for all
a∈a. hence, b3≤b4andb2̸≤b3(because b2has two x’s).b < b′ifb≤b′and
b̸=b′. hence, b3< b4andb4̸< b5(because b4=b5).
σ=⟨a1, a2, . . . , a n⟩ ∈x∗denotes a sequence overxof length |σ|=n.σi=ai
for1≤i≤ |σ|.⟨ ⟩is the empty sequence. σ1·σ2is the concatenation of two sequences,
e.g.,⟨x, x, y⟩ · ⟨x, y, z⟩=⟨x, x, y, x, y, z ⟩. the notation [a∈σ]can be used to convert
a sequence into a multiset. [a∈ ⟨x, x, y, x, y, z ⟩] = [x3, y2, z].
f∈x→yis a total function, i.e., f(x)∈yfor any x∈x.f∈x̸→yis a
partial function with domain dom(f)⊆x. ifx̸∈dom(f), then we write f(x) =⊥,
i.e., the function is not defined for x.
3.2 standard event log
an event log is a collection of events. an event ecan have any number of attributes,
and often we require the following three attributes to be present: case #case(e), activity
#act(e), and timestamp #time(e). table 1 shows example events. if eis the first visi-
ble event, then #case(e) =pizza-56, #act(e) = bi(buy ingredients), and #time(e) =
18:10 . for simplicity, we write 18:10 , but the full timestamp includes a date and possi-
bly also seconds and milliseconds.
to formalize event logs, we introduce some basic notations.
definition 1 (universes). uevis the universe of events, uactis the universe of ac-
tivities, ucase is the universe of cases, utime is the universe of timestamps, uatt=
{act,case,time, . . .}is the universe of attributes, uvalis the universe of values, and
umap =uatt̸→ u valis the universe of attribute-value mappings. we assume that
uact∪ ucase∪ utime⊆ uval,⊥ ̸∈ u val, and for any f∈ umap:f(act)∈ uact∪ {⊥} ,
f(case)∈ ucase∪ {⊥} , and f(time)∈ utime∪ {⊥} .
note that standard attributes of an event (activity, case, timestamp, etc.) are treated
as any other attribute. f∈ umapis a function mapping any subset of attributes onto val-
ues. for example, fcould be such that dom(f) ={case,act,time,resource ,customer ,
cost,size},f(case) =pizza-56, f(act) =bi,f(time) =2021-09-21t18:10:00+00:00 ,
f(resource ) =stefano, f(customer ) =valentina, f(size) = 33 cm, and f(cost) =
e9.99. note that the last two attributes are not shown in table 1. and that 2021-09-
21t18:10:00+00:00 is abbreviated to 18:10 .
to be general, we assume that events are partially ordered. recall that a strict partial
order is irreflexive (e̸≺e),transitive (e1≺e2ande2≺e3implies e1≺e3), and
asymmetric (ife1≺e2, then e2̸≺e1).process mining: a 360 degree overview 13
definition 2 (event log). an event log is a tuple l= (e,#,≺)consisting of a set of
events e⊆ uev, a mapping #∈e→ u map, and a strict partial ordering ≺⊆e×e
on events.
for any e∈eandatt∈dom(#(e)):#att(e) = #( e)(att)is the value of attribute
attfor event e. for example, #act(e),#case(e), and #time(e)are the activity, case,
and timestamp of an event e.
the ordering of events respects time, i.e., if e1, e2∈e,#time(e1)̸=⊥,#time(e2)
̸=⊥, and #time(e1)<#time(e2), then e2̸≺e1.
to be general, events can have any number of attributes and no attribute is manda-
tory. however, when using simplified event logs, we only consider events having a case
and activity (with an order derived using timestamps).
assume l= (e,#,≺)is the event log in table 1. the whole event log has 6400
events, i.e., the table has many more rows. let e={e1, e2, . . . , e 6400}be the whole
set of events and assume the first event shown in table 1 is e433.#(e433)is a mapping
with dom(#(e433)) ={case,act,time,resource ,customer }(the columns shown in
the table). #case(e433) =pizza-56, #act(e433) =bi(buy ingredients), #time(e433) =
18:10, #resource (e433) = stefano, and #customer (e433) = valentina. assuming that
the event identifiers follow the order shown in table 1, the last event visible in the table
ise456, and #case(e456) =pizza-58, #act(e456) =ck(clean kitchen), #time(e456) =
20:51, #resource (e456) =mario, and #customer (e456) =laura. assuming a total order
as shown in the table, e433≺e434,e434≺e435,e455≺e456,e433≺e456, etc.
as stated in definition 2, ≺is a strict partial order and it is not allowed that time-
stamps (when present) and the partial order disagree. using table 1 and the event identi-
fierse433ande456. it cannot be that e456≺e433, because #time(e456)>#time(e433).
for two arbitrary events e1ande2it cannot be that both #time(e1)<#time(e2)and
e2≺e1. however, it can be that #time(e1)<#time(e2)ande1̸≺e2(the time per-
spective is more fine grained) or that #time(e1) = # time(e2)ande1≺e2(the partial
order is more fine grained). optionally, the partial order can be derived from the time-
stamps (when present): ≺={(e1, e2)∈e×e|#time(e1)<#time(e2)}. in this
case, the event log is fully defined by l= (e,#)(no explicit ordering relation is
needed).
it should be noted that in the often used bpi challenge 2011 log provided by a dutch
academic hospital [16], 85% of the events have the same timestamp as the previous
one. this is because, for many events, only dates are available. many publicly available
event logs have similar issues, for example, in the so-called sepsis log [30], 30% of
the events have the same timestamp as the previous one. in this event log, activities
for the same case are sometimes batched, leading to events with the same timestamp.
these examples illustrate that one should inspect timestamps and not take the order in
the event log for granted. it may be beneficial to use partially ordered event data in case
of data quality problems or when there is explicit causal information.
3.3 simplified event log
for process mining techniques focusing on control-flow, it often suffices to focus only
on the activity attribute and the ordering within a case. this leads to a much simpler
event log notion.14 wil van der aalst
definition 3 (simplified event log). a simplified event log l∈ b(uact∗)is a multiset
of traces. a trace σ=⟨a1, a2, . . . a n⟩ ∈ u act∗is a sequence of activities. l(σ)is the
number of times trace σappears in event log l.
consider case pizza-56 in table 1. there are eight events having this case attribute.
by ordering these events based on their timestamps we get the trace σpizza-56 =⟨bi,cb,
ac,at,as,bo,ep,ck⟩. we can do the same for the other two cases shown in table 1:
σpizza-57 =⟨bi,cb,at,ac,as,bo,ep,ck⟩andσpizza-58 =⟨bi,cb,as,at,ac,bo,ep,ck⟩. we
are using the same shorthands as before, i.e., buy ingredients ( bi), create base ( cb), add
cheese ( ac), add tomato ( at), add salami ( as), add mushrooms ( am), bake in oven ( bo),
eat pizza ( ep), and clean kitchen ( ck).
the same trace may appear multiple times in a log. for example, l= [⟨a, b, c, e ⟩10,
⟨a, c, b, e ⟩5,⟨a, d, e⟩]is a simple event log with 10+5+1 = 16 cases and 40+20+3 =
63events.
an event log with events having any number of attributes (definition 2) can be
transformed into a simplified event log by ignoring the additional attributes and se-
quentializing the events belonging to the same case. events without a case or activity
attribute are ignored in the transformation process.
definition 4 (conversion). an event log l= (e,#,≺)defines a simplified event log
˜l∈ b(uact∗)that is constructed as follows:
–e′={e∈e|#case(e)̸=⊥ ∧#act(e)̸=⊥}are all events having an activity
and a case attribute.
–c={#case(e)|e∈e′}anda={#act(e)|e∈e′}are the cases and
activities in l.
–for any case c∈c:
•ec={e∈e′|#case(e) =c}are the events in c,
•σc=⟨e1, e2, . . . , e n⟩is a (deterministically chosen) sequentialization of the
events in c, i.e.,σcis such that {e1, e2, . . . , e n}=ec,|ec|=|σc|, and for any
1≤i < j≤n:ej̸≺ei.
•˜σc=⟨#act(e1),#act(e2), . . . , #act(en)⟩ ∈a∗is the trace corresponding to
c(i.e., the events in σcare replaced by the corresponding activities).
–˜l= [˜σc|c∈c]∈ b(a∗)is the simplified event log derived from l.
letl= (e,#,≺)be the event log corresponding to the events visible in table 1
(assuming the order in the table). then: ˜l= [⟨bi,cb,ac,at,as,bo,ep,ck⟩,⟨bi,cb,at,
ac,as,bo,ep,ck⟩,⟨bi,cb,as,at,ac,bo,ep,ck⟩]. table 1 only shows a fragment of the
whole event log. for the whole event log l= (e,#,≺), we have ˜l= [⟨bi,cb,ac,at,
as,bo,ep,ck⟩400,⟨bi,cb,at,ac,as,bo,ep,ck⟩200,⟨bi,cb,as,at,ac,bo,ep,ck⟩100,
⟨bi,cb,ac,as,at,bo,ep,ck⟩50,⟨bi,cb,at,as,ac,bo,ep,ck⟩25,⟨bi,cb,as,ac,at,bo,
ep,ck⟩25]. this event log has 800 cases and 6400 events. using process discovery tech-
niques we can automatically discover the models in figures 3–6 from such an event log.
if the event log also has cases where cheese is added multiple times (e.g., ⟨bi,cb,ac,
at,ac,ac,as,bo,ep,ck⟩), mushrooms are added instead of salami (e.g., ⟨bi,cb,ac,
at,am,bo,ep,ck⟩), and the eating activity is skipped (e.g., ⟨bi,cb,ac,at,as,bo,ck⟩),
then we can automatically discover the models in figures 7–10 using suitable process
mining techniques.process mining: a 360 degree overview 15
3.4 object-centric event logs
table 1 corresponds to a conventional “flat” event log where each event (i.e., row) refers
to a case, activity, and timestamp. it is very natural to assume that an event has indeed a
timestamp and refers to an activity. however, the assumption that it refers to precisely
one case may cause problems [4]. object-centric event logs (ocel) aim to overcome
this limitation [22]. in ocel, an event may refer to any number of objects (of different
types) rather than a single case. object-centric process mining techniques may produce
petri nets with different types of objects [7] or artifact-centric process models [18, 19].
table 2. fragment of a larger object-centric event log (ocel) with four types of objects: pizza,
resource, customer, and location. one event may refer to a set of objects, e.g., three pizzas, three
customer, and a location.
activity timestamp pizza resource customer location
. . . . . . . . . . . . . . . . . .
buy ingredients ( bi) 18:10{pizza-56,
pizza-57,
pizza-58 }{stefano }{valentina,
giulia,
laura}{supermarket }
create base ( cb) 18.16 {pizza-57 }{mario,
stefano }{giulia}{kitchen-1 }
create base ( cb) 18.19 {pizza-56 }{mario,
stefano }{valentina }{kitchen-1 }
add tomato ( at) 18.21 {pizza-57 } {mario} {giulia}{kitchen-1 }
add cheese ( ac) 18.27 {pizza-57 } {mario} {giulia}{kitchen-1 }
add cheese ( ac) 18.34 {pizza-56 } {mario}{valentina }{kitchen-1 }
add tomato ( at) 18.44 {pizza-56 } {mario}{valentina }{kitchen-1 }
add salami ( as) 18.45 {pizza-56 } {mario}{valentina }{kitchen-1 }
bake in oven ( bo) 18.48 {pizza-56 }{stefano }{valentina }{kitchen-1 }
add salami ( as) 18.50 {pizza-57 } {mario} {giulia}{kitchen-1 }
eat pizza ( ep) 19.10 {pizza-56 }{valentina }{valentina }{restaurant }
bake in oven ( bo) 19.23 {pizza-57 }{stefano }{giulia}{kitchen-1 }
eat pizza ( ep) 19.27 {pizza-57 } {giulia} {giulia}{restaurant }
create base ( cb) 19.48 {pizza-58 }{mario,
stefano }{laura}{kitchen-2 }
add salami ( as) 19.49 {pizza-58 } {mario} {laura}{kitchen-2 }
add tomato ( at) 19.55 {pizza-58 } {mario} {laura}{kitchen-2 }
clean kitchen ( ck) 20.08 ∅ {mario} ∅ {kitchen-1 }
add cheese ( ac) 20.13 {pizza-58 } {mario} {laura}{kitchen-2 }
bake in oven ( bo) 20.29 {pizza-58 }{stefano } {laura}{kitchen-2 }
eat pizza ( ep) 20.48 {pizza-58 } {laura} {laura}{restaurant }
clean kitchen ( ck) 20.51 ∅ {mario} ∅ {kitchen-2 }
. . . . . . . . . . . . . . . . . .
to understand the problem, we use table 2, which shows ocel data in tabular
form. compared to table 1, we do not assume a single case notion. instead, an event16 wil van der aalst
may refer to any number of objects. in this toy example, we assume four types of ob-
jects: pizza, resource, customer, and location. assume that eis the first event listed in
table 2. #act(e) = bi(buy ingredients), #time(e) =18:10 ,#pizza(e) ={pizza-56 ,
pizza-57 ,pizza-58 },#resource (e) ={stefano },#customer (e) ={valentina ,giulia ,
laura}, and #location (e) ={supermarket }. note that in table 1 there were three
bi(buy ingredients) events, one for each pizza. hence, table 2 is closer to reality if
the ingredients were indeed bought in the same visit to the supermarket. in a classi-
cal event log with a single case identifier, we need to artificially replicate events (one
bievent per pizza). this may lead to misleading statistics, i.e., there was just one trip
to the supermarket and not three. the three pizzas were created on demand, so the
bievent also refers to the three customers. table 2 also shows that creating the pizza
base is team work, i.e., all cbevents are done by both mario and stefano. if we as-
sume that eis the last event visible in table 2, then #act(e) = ck(clean kitchen),
#time(e) = 20 .51,#pizza(e) =∅,#resource (e) ={mario},#customer (e) =∅, and
#location (e) ={kitchen-2 }. this expresses that, according to this event log, cleaning
the second kitchen is unrelated to the pizza prepared in it.
definition 2 can be easily extended to allow for object-centric event logs (ocel).
we just need to assume that event attributes include object types and that attribute-value
mappings may yield sets of values (e.g., objects) rather than individual values. without
fully formalizing this, we simply assume that uobjtyp⊆ u attis the universe of object
types ,uobjsis the universe of objects , andp(uobjs)⊆ uval(i.e., values can be sets of
objects). moreover, for any f∈ umap and ot∈ uobjtyp∩dom(f):f(ot)⊆ u objs.
hence, attribute value mappings can be used to also map object types onto sets of
objects .
to apply classical process mining techniques, we need to convert the object-centric
event data to traditional event data. for example, we need to convert table 2 into table 1
if we pick object type pizza as a case notion. this is called “flattening the event log”
and always requires picking an object type as a case notion. this can be formalized in
a rather straightforward manner.
definition 5 (ocel conversion). letl= (e,#,≺)be an event log having an ob-
ject type ot∈ uobjtyp such that for any e∈e:#ot(e)⊆ uobjsis the set of objects of
type otinvolved in event e. based on this assumption, we can create a “flattened event
log” ˜lot∈ b(uact∗)that is constructed as follows:
–e′={e∈e|#ot(e)̸=∅ ∧#act(e)̸=⊥}are all events having an activity and
referring to at least one object of type ot.
–o=s
e∈e′#ot(e)anda={#act(e)|e∈e′}are the objects of type otand
activities in l.
–for any object o∈o:
•eo={e∈e′|o∈#ot(e)}are the events involving object o,
•σo=⟨e1, e2, . . . , e n⟩is a (deterministically chosen) sequentialization of the
events involving o, i.e.,σois such that {e1, e2, . . . , e n}=eo,|eo|=|σo|, and
for any 1≤i < j≤n:ej̸≺ei.
•˜σo=⟨#act(e1),#act(e2), . . . , #act(en)⟩ ∈a∗is the trace corresponding to
o(i.e., the events in σoare replaced by the corresponding activities).process mining: a 360 degree overview 17
–˜l= [˜σo|o∈o]∈ b(a∗)is the simplified event log derived from l.
definition 5 shows that any ocel can be transformed into a simplified event log.
the simplified event log is a multiset of traces where each trace refers to the “lifecy-
cle” of an object. consider for example ˜σpizza-56 =⟨bi,cb,ac,at,as,bo,ep⟩showing
the lifecycle of pizza-56 in table 2. ˜σstefano=⟨bi,cb,cb,bo,bo,cb,bo⟩is the trace cor-
responding to resource stefano. ˜σvalentina =⟨bi,cb,ac,at,as,bo,ep⟩is the trace corre-
sponding to customer valentina. this trace is now the same as σpizza-56, but this would not
be the case if valentina eats multiple pizzas (e.g., in subsequent visits to the restaurant).
˜σsupermarket =⟨bi⟩is the trace corresponding to the location “supermarket” (assuming there
was just one visit to the supermarket). ˜σrestaurant =⟨ep,ep,ep⟩is the trace corresponding
to the location “restaurant” (again considering only the events visible in table 2). these
traces are rather short because we only consider the events shown in table 2.
by converting an ocel to a conventional event log, we can apply all existing pro-
cess mining techniques. for each object type, we can create a process model showing
the “flow of objects” of that type. however, flattening the event log using otas a case
notion potentially leads to the following problems.
–deficiency : events in the original event log that have nocorresponding events in
the flattened event log disappear from the data set (i.e., #ot(e) =∅). for example,
when selecting object type pizza as a case notion, the clean kitchen events disappear
from the event log.
–convergence : events referring to multiple objects of the selected type are repli-
cated, possibly leading to unintentional duplication (i.e., |#ot(e))| ≥2). for ex-
ample, when selecting object type pizza as a case notion, the first event in table 2
will be mapped onto three events in the flattened event log. when selecting object
type resource as a case notion, all create pizza base events are duplicated in the
flattened event log. the replication of events can lead to misleading diagnostics.
–divergence : events referring to different objects of a type notselected as the case
notion may still be considered causally related because a more coarse-grained ob-
ject is shared. for example, when selecting object type location as a case notion,
events corresponding to different pizzas are interleaved and one can no longer see
the causal dependencies.
the first two problems are easy to understand: events disappear completely (de-
ficiency) or are replicated leading to potentially misleading management information
(convergence). the problem of divergence is more subtle. to understand this better,
consider ˜σkitchen-1 =⟨cb,cb,at,ac,ac,at,as,bo,as,bo,ck⟩describing the “lifecycle”
of the first kitchen. in this trace one can see cbfollowed by cb(two subsequent create
pizza base events) and acfollowed by ac(two subsequent add cheese events). how-
ever, these events refer to different pizzas and are not causally related. the discovered
process model is likely to show loops involving cbandac, although these events occur
precisely once per pizza.
in summary, one can create different views on the process by flattening the event
data for selected object types, but one should be careful to interpret these correctly (e.g.,
be aware of data duplication and the blurring of causalities).18 wil van der aalst
the running “pizza process” example is not very realistic, and is only used to in-
troduce the basic concepts in a clear manner. earlier, we mentioned crm systems like
salesforce and erp systems like sap s/4hana, oracle e-business suite, and mi-
crosoft dynamics 365. these systems are loaded with event data scattered over many
database tables. erp and crm systems are widely used, broad in scope, and sector-
agnostic. also, more sector-specific systems used in banking, insurance, and healthcare
have event data distributed over numerous tables. these tables refer to different types
of objects that are often in a one-to-many or many-to-many relation. this immediately
leads to the challenges described before.
let us consider two of the processes almost any organization has: purchase-to-pay
(p2p) and order-to-cash (o2c). the p2p process is concerned with the buy-side of
an organization. the o2c process is concerned with the sell-side of a company. in the
p2p process the organization is dealing with purchasing documents, items, suppliers,
purchase requisitions, contracts, receipts, etc. note that there may be many purchase
orders per supplier and an order may consist of multiple items. hence, events may refer
to different objects and also multiple objects of the same time. in the o2c process, we
can witness similar phenomena. a customer may place three orders on the same day
and each order may have several items. items from different orders may end up in the
same delivery. moreover, items in the same order may end up in different deliveries.
p2p and o2c processes are considered simple and there is a lot of experience with
extracting such data from systems such as sap. still, these processes are more compli-
cated than what many people think. it is not uncommon to find thousands of process
variants. this offers great opportunities for process mining, because unexpected vari-
ants provide hints on how to improve the process. however, one should not underes-
timate the efforts needed for data extraction. therefore, we discussed ocel as it sits
in-between the real database tables in systems such as sap, oracle, and salesforce, and
the flattened event logs assumed by most systems.
3.5 xes standard
the initial version of the xes (extensible event stream) format was defined by the
ieee task force on process mining in september 2010. after several iterations, xes
became the official ieee standard for storing event data in 2016 [24]. xes is supported
by most of the open-source process mining tools and many of the leading commercial
tools. the goal is to facilitate the seamless exchange of event data between different
systems. of course, it is also possible to do this using relational databases or simple file
formats. however, xes adds semantics to the data exchanged. therefore, we focus on
the concepts and refer to [24] for the syntax.
figure 11 shows the xes meta model expressed in terms of a uml class diagram. a
xes document (e.g., an xml file) contains one log consisting of any number of traces.
each trace describes a sequential list of events corresponding to a particular case. the
log, its traces, and its events may have any number of attributes. attributes may be
nested. there are five core types: string ,date ,int,float , and boolean . xes does not
prescribe a fixed set of mandatory attributes for each element (log, trace, and event),
e.g., an event can have any number of attributes. however, to provide semantics for such
attributes, the log refers to so-called xes extensions . an extension gives semantics toprocess mining: a 360 degree overview 19
log
trace
event
attribute
extension
key
string
date
int
float
boolean
value<contains >
<contains ><contains >
<contains ><trace-global >
<event-global ><deﬁnes><declares >
name
preﬁx
uri
classi ﬁer<deﬁnes> <deﬁnes>
fig. 11. meta model of xes [24]. a log contains traces and each trace contains events [24, 2].
log, traces, and events have attributes. extensions may define new attributes and a log should
declare the extensions used in it. global attributes are attributes that are declared to be mandatory.
such attributes reside at the trace or event level. attributes may be nested. event classifiers are
defined for the log and assign a “label” (e.g., activity name) to each event. there may be multiple
classifiers.
particular attributes. for example, the time extension defines a timestamp attribute of
type xs:datetime . this corresponds to the #time(e)attribute used before. the orga-
nizational extension defines a resource attribute of type xs:string , i.e., the #resource (e)
attribute. users can define their own extensions. for example, it is possible to develop
domain-specific or even organization-specific extensions.
xes also supports three concepts that are of general interest and important for pro-
cess mining: classifiers ,lifecycle information, and activity instances . these concepts
are interrelated as is discussed next.
classifiers are used to attach labels to events. there is always at least one classifier
and by default; this is the activity name. when turning an event log linto a simplified
event log ˜l∈ b(uact∗)in definition 4, we are using this default classifier: each event
eis mapped onto #act(e). however, it is also possible to project events onto resources,
locations, departments, etc., or combinations of attributes. an event classifier assigns
to each event an identity, which makes it comparable to other events (via their assigned
identity). event classifiers are defined for the whole log, and there may be an arbitrary
number of classifiers.20 wil van der aalst
thus far, we implicitly assumed that events are atomic. therefore, an event has a
timestamp. to handle activities that take time, xes provides the possibility to repre-
sent lifecycle information and to connect events through activity instances . an activity
instance is a collection of related events that together represent the execution of an ac-
tivity for a case. for example, an activity instance may be composed of a start event and
a complete event. this way, we can derive information about the duration of an activity
instance. the xes lifecycle model distinguishes between the following types of events:
schedule ,assign ,withdraw ,reassign ,start,suspend ,resume ,abort ,complete ,autoskip ,
andmanualskip . using this xes extension, an event ehas an attribute #type(e). for ex-
ample, assume that e1ande2are two events that belong to the same activity instance and
#type(e1) =start and #type(e2) =complete. #time(e2)−#time(e1)is the duration of
the activity. similarly, we can measure waiting times, etc. note that classifiers can also
use lifecycle information, e.g., an event eis identified by the pair (#act(e),#type(e)).
this implies that when we discover process models, there may be activities (a,start)
and(a,complete ).
many xes logs contain lifecycle information, but few contain explicit activity in-
stances. this implies that heuristics are needed to link events. for example, (a,start)is
coupled to the first (a,complete )following it. however, in the trace ⟨. . . ,(a,start), . . . ,
(a,start), . . . , (a,complete ), . . . , (a,complete ), . . .⟩, there are two possible ways to
match starts and ends. fortunately, it is often possible to extract activity instances from
the original data source.
4 different types of process mining
after introducing multiple ways to represent process models (bpmn, petri nets, pro-
cess trees, and dfgs) and different types of events logs (e.g., xes and ocel), we
now briefly introduce some of the standard process mining tasks (see figure 12). as a
starting point, we assume that high-quality event data are available. in practice, it is of-
ten time-consuming to extract event data from existing systems. as mentioned before,
events may be scattered over multiple database tables or even multiple information sys-
tems using different identifiers. when starting with process mining, data extraction and
data cleaning may take 80% of the time. of course, the exact percentage depends on
the type of process and information system. also if the data pipeline is set up properly,
this is a one-time effort that can be reused continuously.
4.1 process discovery
event logs contain example behavior. the challenge is to discover a process model
based on such example behavior. the model should not be “overfitting” (i.e., simply
enumerating the observed example traces) and not “underfitting” (i.e., allow for behav-
ior unrelated to what was observed). this is a difficult task and numerous algorithms
have been proposed in literature, including the alpha algorithm [8], region-based ap-
proaches [13, 11, 33, 36], inductive mining techniques [28, 29], and the split miner [9].
a baseline approach is the creation of a dfg, where the observed activities are addedprocess mining: a 360 degree overview 21
discover
align
replay
enrich
apply
compare
information 
systems
extract
process 
models
explore
 select
filter
clean
conformance
performance 
diagnostics
predictions
improvements
transform
act
 show
model
adapt
show
interpret
drill down
ml
+
 +
event 
data
6: action -oriented 
process mining
5: predictive process 
mining
3: performance 
analysis
2: conformance 
checking
1: process
discovery
4: comparative 
process mining
fig. 12. six frequently used types of process mining.
as nodes and two nodes aandbare connected through a directed arc if activity ais di-
rectly followed by activity bat least once. obviously, such an approach is too simplistic
and leads to underfitting process models. if activity ais directly followed by activity
bin one case and activity bis directly followed by activity ain another case, then a
loop is introduced. the techniques mentioned above address this problem and are able
to uncover concurrency. however, there are many other challenges. the event log may
contain infrequent behavior , i.e., traces or patterns which are less frequent compared to
the mainstream behavior. should this infrequent behavior be included or not? hence,
most approaches are parameterized to discard rare behavior. on the one hand, we often
want to leave out infrequent behavior to simplify models. on the other hand, one cannot
assume to have seen all behavior. concurrency leads to an exponential number of states
and a factorial number of possible traces. an unbounded loop leads to infinitely many
possible traces. process discovery is further complicated by the fact that event logs do
notcontain negative examples (i.e., traces that cannot happen) and are often incomplete
(i.e., only a small fraction of all possible behavior is observed).
it is important to focus on a particular process orproblem , having a particular goal
in mind. one needs to select and filter the data based on a well-defined goal. randomly
using sliders to simplify process models may be useful for a first exploration, but will
rarely lead to the desired insights.
to introduce process discovery, we focus on the control-flow , i.e., the ordering of
activities. however, process models may include other perspectives , including time,
data, resources, costs, etc. for example, a choice may be based on the attributes of the
case or preceding event, and we may attach resource allocation rules to activities (e.g.,
role information and authorizations). process discovery may add such perspectives, but
we typically try to get clarity on the control-flow first. if no reasonable control-flow
can be established, one should not try to add additional perspectives. several process
discovery techniques are explained in detail in [5, 10].22 wil van der aalst
4.2 conformance checking
conformance checking requires both an event log and a process model as input. the
goal is to indicate where log and model disagree. to illustrate this consider figures 7,
8, and 9. these three models describe exactly the same behavior of the extended “pizza
process” that can be compactly described as →(bi,cb,∧(⟲(ac, τ),at,×(as,am)),bo,
×(ep, τ),ck). letm={⟨bi,cb,ac,at,as,bo,ep,ck,⟩, . . .⟨bi,cb,am,at,ac,ac,ac,
bo,ep,ck,⟩, . . .⟨bi,cb,at,ac,am,bo,ck⟩}be the infinite set of all traces allowed by
the bpmn model, petri net, and process tree depicted in the three figures. let l∈
b(uact∗)be an event log containing 800 traces. assume σ1=⟨bi,cb,ac,at,as,bo,
ep,ck⟩ ∈l,σ2=⟨bi,cb,ac,ac,at,am,ep,ck⟩ ∈l, and σ3=⟨bi,cb,at,ac,at,
as,bo,ck⟩ ∈l. hence, l= [σ1, σ2, σ3, . . .]and|l|= 800 .σ1∈m, i.e., this is a per-
fectly fitting trace. σ2̸∈mbecause activity bo(bake in oven) is missing, i.e., someone
was eating an uncooked pizza. σ3̸∈mbecause activity at(add tomato) occurs twice.
the goal of conformance checking is to detect such deviations.
lfit= [σ∈l|σ∈m]is the multiset of fitting traces and ldev= [σ∈l|σ̸∈
m]is the multiset of deviating traces. hence, fitness at the trace level can be defined as
|lfit|/|l|. the fraction is 1 if all traces are fitting and 0 if none of the traces is fitting.
there are many measures for fitness. for example, the above fraction does not take
into account to what degree a trace is fitting or not. trace σ4=⟨bo,bo,bo,at,at,at,
at,at⟩ ∈lis obviously more deviating than σ2andσ3. moreover, it is not enough to
produce a number. in practice, good diagnostics are much more important than a single
quality measure.
there are many techniques for conformance checking. the two most frequently
used approaches are token-based replay [32] and alignments [6, 14]. for token-based
replay, the process model is represented as a petri net and traces in the event log are
replayed on the model. if the trace indicates that an activity needs to take place, the
corresponding transition is executed. if this is not possible because an input place is
empty, a so-called missing token is added. tokens that are never consumed are called re-
maining tokens . the numbers of missing and remaining tokens relative to the numbers
of consumed and produced tokens indicate the severity of the conformance problem.
token-based replay can be extended to petri nets with silent and duplicate activities
using heuristics. for example, if there are two activities with the same label, pick the
one that is enabled. if both are enabled, pick one of them. similarly, silent transitions
(i.e., transitions not corresponding to recorded activities) are executed when they en-
able a transition corresponding to the next activity in the event log. this requires an
exploration of the states reachable from the current state and may lead to inconclusive
results.
compared to computing alignments, token-based replay is fairly efficient, but does
not always produce valid paths through the process model. alignments are often seen
as the gold standard for conformance checking because they provide paths through the
process model that are as close to the observed behavior as possible. we would like
to map observed behavior onto modeled behavior to provide better diagnostics and to
relate also non-fitting cases to the model. alignments were introduced to overcome
the limitations of token-based replay. the diagnostics are more detailed and more pre-
cise, because each observed trace is mapped onto a model behavior that is as closeprocess mining: a 360 degree overview 23
to what was observed as possible. the alignment shows common behavior, but also
skipped and inserted events signaling deviations. such skipped and inserted events are
easier to interpret than missing and remaining tokens. however, for large event logs and
processes, alignment computations may be intractable. moreover, there may be many
optimal alignments, making the diagnostics non-deterministic.
several conformance checking techniques are explained in detail in [15]. when
comparing observed and modeled behavior, we typically consider four main quality
dimensions [1, 2, 6]:
–recall (also called replay fitness ): the discovered model should allow for the be-
havior seen in the event log. this can be quantified by the minimal number of edit
operations needed to make all traces in the event log fitting into the model (or sim-
ply the fraction of perfectly fitting traces).
–precision : the discovered model should not allow for behavior completely unrelated
to what was seen in the event log. this can be quantified by the number of possible
continuations in the model never observed in the event log.
–generalization : the discovered model should generalize the example behavior seen
in the event log. it is easy to create a process model that only allows for the behavior
observed and nothing more. however, such a model is likely to overfit. to avoid
overfitting, the model should generalize. this can only be tested on “fresh unseen”
event data. to evaluate a process discovery algorithm, standard cross-validation can
be used to detect overfitting problems. this is less clear when evaluating a process
model rather than a discovery algorithm [6].
–simplicity : the discovered model should be as simple as possible. this fourth qual-
ity criterion is related to occam’s razor, which states that “one should not increase,
beyond what is necessary, the number of entities required to explain anything”.
4.3 performance analysis
the goal of process mining is to improve processes by uncovering problems. these may
be the conformance problems just described, but (of course) also include performance
problems such as untimely completion of a case, limited production, missed deadlines,
tardiness, excessive rework, and recurring quality problems. using token-based replay
[32] and alignments [6, 14] it is possible to relate event data to a process model. as
a result, it is fairly straightforward to annotate the process model with frequency and
time information. frequencies of undesired activities and loops can be used to identify
quality and efficiency problems. since events have timestamps, it is possible to measure
times in-between activities, including statistics such as mean, median, standard devia-
tion, minimum, and maximum. this allows for analyzing performance indicators, e.g.,
waiting times, response times, and service times.
aservice level agreement (sla) is an agreement between a service provider and
a client. process mining can be used to analyze slas, e.g., when is a particular sla
not met. some well-known slas are churn/abandonment rate (number of cases lost),
average speed to answer (response time seen by customer), percentage of cases handled
within a predefined timeframe, first-call resolution (cases successfully handled with-
out rerouting), percentage of duplicated cases (e.g., multiple procurement documents24 wil van der aalst
corresponding to the same order), mean time between failures, mean time to recovery,
etc.
4.4 comparative process mining
comparative process mining uses as input multiple event logs, e.g., l1, l2, . . . , l n∈
b(uact∗). these event logs may refer to different locations, periods, or categories of
cases. for example, we may have the event logs laachen andlmunich referring to the
same processes performed at two locations. we may have the event logs ljan, lfeb,
lmar, . . . , l decreferring to different periods or lgold andlsilver referring to gold
and silver customers.
having multiple event logs allows for comparison and highly relevant questions.
what are the striking differences and commonalities? what factors lead to these differ-
ences? root cause analysis can be used to explain the observed differences. for exam-
ple, in lfebwaiting times may be much longer than in ljandue to limited resource
availability. comparative process mining may focus on frequently occurring problems,
sometimes referred to as execution gaps . such execution gaps include lost customers,
additional work due to price changes, the merging of duplicate orders, and rework due
to quality problems.
comparative process mining is also a great tool for inter- or intra-organizational
benchmarking . for example, an insurance company may have different regional offices.
using comparative process mining, these offices can learn from each other and increase
the overall performance.
4.5 predictive process mining
process discovery, conformance checking, performance analysis, and comparative pro-
cess mining are backward-looking . although the value of such techniques is obvious,
the actual goal is to continuously improve processes and respond to changes. opera-
tional processes are subject to many changes, e.g., a sudden increase in the number of
orders or disruptions in the supply chain. moreover, many compliance and performance
problems can be foreseen and addressed proactively. fortunately, process models dis-
covered and enriched using process mining can be used in a forward-looking manner.
process mining can be used to create a range of ml questions that can be answered
using standard software libraries. for example, when detecting a recurring bottleneck
or deviation, it is possible to extract features from the event log and create a predictive
model. this leads to a so-called situation-feature table with several descriptive features
(e.g., people involved, path taken, and time of day) and one target feature (e.g., waiting
time or decision). then standard ml techniques ranging from regression and decision
trees to neural networks can be applied to explain the target feature in terms of descrip-
tive features. this leads to better diagnostics and explanations. moreover, the models
can be used in a predictive manner.
predictive process mining questions also create specific ml challenges. most ml
techniques assume a fixed number of features as input (i.e., a fixed-length feature vec-
tor) and assume inputs to be independent. artificial recurrent neural network archi-
tectures such as long short-term memory (lstm) can be used to handle traces ofprocess mining: a 360 degree overview 25
variable length. contextual features can be added to include information about the uti-
lization of resources. however, this requires fine-tuning and domain knowledge.
a discovered process model can be viewed as a description of the as-is situation.
using simulation and model adaptation, it is possible to explore possible to-be situa-
tions. simulation enables forward-looking forms of process mining. comparative pro-
cess mining can be used to compare the different alternatives.
4.6 action-oriented process mining
process mining can be used to show (1) what has happened, (2) what is happening
now, and (3) what will happen next in the process. hence, it covers the full spectrum
from backward-looking to forward-looking types of analysis. backward-looking forms
of process mining can lead to process redesigns and organizational changes. forward-
looking forms of process mining and diagnostics of the current state of a process can
trigger improvement actions. action-oriented process mining aims to turn diagnostics
into actions. assisted by low-code automation platforms, process mining software can
trigger workflows. some examples:
–the moment the average waiting time exceeds 2 hours, additional resources are
added and no new orders are accepted.
–if a supplier changes prices repeatedly for a longer period, then the supplier is
blacklisted.
–if a check is repeatedly skipped by an employee, the manager is notified.
next to triggering improvement actions, process mining can also detect repetitive work
that may be automated using robotic process automation (rpa). rpa can be used to
automate repetitive tasks done by humans without changing the underlying systems.
typical examples include copying information from one system into another system.
process mining can be used to discover such repetitive tasks. the term task mining is
often used to refer to the discovery of processes based on user-interface interactions
(filling out a form, pushing a button, copying text, etc.). task mining can be used to
uncover repetitive processes that can be automated. there is also a connection to online
scheduling and other operations research (or) techniques. for example, based on
historical information, it is possible to create a robust schedule with events taking place
in the future. differences between scheduled events and the actual events may trigger
improvement actions.
5 applications and software
process mining started as an exercise in the late 1990s trying to automatically create a
petri net from example traces [2]. according to gartner there are now over 40 process
mining vendors [26]. some of them are listed in table 3. note that the list is very
dynamic with new vendors emerging and large it companies acquiring smaller process
mining vendors. for an up-to-date overview, see the website www.processmining.
org which lists all process mining tools.26 wil van der aalst
table 3. some of the process mining tools available at the end of 2021. for each tool the vendor
and website are listed. the last column indicates whether an academic version is available.
vendor tool website acad.
ver.
abbyy abbyy timeline www.abbyy.com no
appian (lana labs) lana process mining lanalabs.com no
apromore apromore enterprise edition apromore.org yes
bupar bupar bupar.net yes
businessoptix businessoptix businessoptix.com yes
celonis celonis ems celonis.com yes
datricks datricks datricks.com yes
dcr dcr portal www.dcrsolutions.net yes
deloitte process x-ray processxray.deloitte.com no
everflow everflow everflow.ai no
fluxicon disco fluxicon.com yes
fortressiq fortressiq fortressiq.com no
fraunhofer fit pm4py pm4py.fit.fraunhofer.de yes
hyland onbase www.hyland.com no
ibm (myinvenio) myinvenio my-invenio.com no
integris explora process integris.it no
kofax kofax insight www.kofax.com no
livejourney livejourney www.livejourney.com no
logpickr logpickr process explorer 360 www.logpickr.com no
mavim mavim www.mavim.co no
mehrwerk gmbh mpm mpm-processmining.com no
mindzie mindzie mindzie.com yes
minit (microsoft) minit www.minit.io yes
nintex uk ltd nintex www.nintex.com no
oniq iq/a www.oniq.com no
pafnow (celonis) pafnow pafnow.com no
process.science process.science www.process.science no
processdiamond processdiamond processdiamond.com yes
processm pmbi processm.com yes
puzzle data prodiscovery www.puzzledata.com no
qpr software qpr processanalyzer www.qpr.com no
sap (signavio) sap signavio www.signavio.com yes
skan ai skan www.skan.ai no
software ag aris aris-process-mining.com yes
soroco scout platform soroco.com no
stereologic stereologic process mining www.stereologic.com no
tu/e prom www.promtools.org yes
tu/e rapidprom www.rapidprom.org yes
ui path ui path process mining www.uipath.com yes
ultimatesuite ultimatesuite tm/rpa www.ultimatesuite.com no
upflux upflux upflux.net no
worksoft worksoft www.worksoft.com noprocess mining: a 360 degree overview 27
all of the tools in table 3 support the discovery of directly-follows graphs (dfgs)
with frequencies and times. most of them (but not all) support some form of confor-
mance checking and bpmn visualization. some of the tools target process or data
analysts rather than people managing or executing processes. these tools are typi-
cally lightweight and can be deployed quickly. enterprise-level process mining tools
are more difficult to deploy, but aim to be used by many stakeholders within an or-
ganization. for example, within siemens, over 6000 employees are using the celonis
software to improve a range of processes. enterprise-level process mining tools have
automated connections to existing information systems (e.g., sap, salesforce, oracle,
servicenow, and workday) to allow for the continuous ingestion of data. these tools
also allow for customized dashboards to lower the threshold to use process mining. in
2020, gartner estimated the process mining software market revenue to be $550 mil-
lion, which was over 70% market size growth from the previous year [26]. the process
mining market is forecast to keep growing 50% per year (compound annual growth
rate) in the coming years. note that this does not include consultancy based on process
mining. the big four (i.e., deloitte, ernst & young, kpmg, and pwc) all have process
mining competence centers providing process mining services all over the globe.
the technology is generic and can be used in any domain. for example, process
mining is used in
–finance and insurance (rabobank, wells fargo, hypovereinsbank, caixa general,
adac, apg, suncorp, vtb, etc.),
–logistics and transport (uber, deutsche bahn, lufthansa, airbus, schukat, vander-
lande, etc.),
–production (abb, siemens, bmw, fiat, bosch, akzonobel, bayer, neste, etc.),
–healthcare, biomedicine, and pharmacy (uniklinik rwth aachen, charite univer-
sity hospital, ge healthcare, philips, medtronic, pfizer, bayer, astrazeneca, etc.),
–telecom (deutsche telekom, v odafone, a1 telekom austria, telekom italia, etc.),
–food and retail (edeka, mediamarkt, globus, zalando, ab inbev, etc.),
–energy (uniper, chevron, shell, bp, e.on, etc.), and
–it services (dell, xerox, ibm, nokia, servicenow, etc.).
in [31], several use cases are described in detail. in [26, 27], typical applications are
described, and in [21] the results of a global process mining survey are presented. these
show that the adoption is increasing, e.g., according to the global survey, 83% of com-
panies already using process mining on a global scale plan to expand their initiatives
[21]. process mining helps organizations to improve processes, provide transparency,
reduce costs, ensure compliance, avoid risks, eliminate waste, and redesign problem-
atic processes [21]. to get a glimpse of the possible applications, the reader can take
a look at the use cases collected by the ieee task force on process mining [25] and
hspi management consulting [20]. note that these cover just a fraction of the actual
applications of process mining. it has become fairly standard to apply process mining
to standard processes such as purchase-to-pay (p2p) and order-to-cash (o2c).28 wil van der aalst
6 summary and outlook
this chapter aimed to provide a 360 degree overview of the field of process mining.
we showed that process mining connects data science and process science leading to
data-driven process-centric techniques and approaches. event data and process models
were introduced. events can be grouped in event logs, but also stored in databases. in
the standard setting an event has a few mandatory attributes such as case, activity, and
timestamp. this can be further reduced to representing an event log by a multiset of
traces where each trace is a sequence of activities. this format is often used for control-
flow discovery. however, in real-life settings it is not so easy to find a single case notion.
often events may refer to multiple objects of different types. there may also be data
quality problems and data may be scattered over multiple source systems. moreover, ad-
ditional attributes such as costs, time, and resources need to be incorporated in models.
we introduced directly-follows graphs (dfg), petri nets, bpmn models, and process
trees as basic control-flow representations. these will be used in the remainder.
we informally described six common types of process mining: (1) process discov-
ery, (2) conformance checking, (3) performance analysis, (4) comparative process min-
ing, (5) predictive process mining, and (6) action-oriented process mining. these char-
acterize the scope of process mining and challenges. the chapter also provided pointers
to the over 40 process mining tools and case studies.
although process mining is already used by many of the larger organizations, it is
a relatively new technology and only a fraction of its potential is realized today. three
important trends can be witnessed that together lead to a wider adoption.
–supporting data extraction and analysis through process-specific and domain-specific
adapters and applications (“process mining apps”). this reduces the effort to get
started with process mining and leverages past experiences in other organizations.
–initially, process mining software aimed at experts involved in process improve-
ment projects. however, process mining should be done continuously and at a large
scale . it is a generic technology that should be accessible for many users every day.
by scaling (both in terms of processes and users) and continuous use, the return on
investment is the highest.
–increasingly, process mining and automation are combined . process mining diag-
nostics trigger corrective actions through low-code automation platforms. this is
the only way to ensure that improvements are realized. without some form of au-
tomation, workers may slip back into the old ineffective ways of working that were
exposed using process mining.
process mining can also play a role in realizing sustainability goals and help to address
environmental, social and economic challenges. process mining can help to quantify
and steer sustainability efforts, e.g., by removing waste and quantifying emissions. pro-
cess mining can easily handle multiple dimensions, such as time, cash flow, resource
usage, and co 2emissions, during analysis. sustainability is just one of many topics
where process mining can play a role. moreover, these applications also pose interest-
ing research questions leading to new concepts and techniques.process mining: a 360 degree overview 29
acknowledgments
funded by the alexander von humboldt (avh) stiftung and the deutsche forschungs-
gemeinschaft (dfg, german research foundation) under germany’s excellence strat-
egy – exc 2023 internet of production – 390621612.
references
1. w.m.p. van der aalst. process mining: discovery, conformance and enhancement of busi-
ness processes . springer-verlag, berlin, 2011.
2. w.m.p. van der aalst. process mining: data science in action . springer-verlag, berlin,
2016.
3. w.m.p. van der aalst. a practitioner’s guide to process mining: limitations of the directly-
follows graph. in international conference on enterprise information systems (centeris
2019) , volume 164 of procedia computer science , pages 321–328. elsevier, 2019.
4. w.m.p. van der aalst. object-centric process mining: dealing with divergence and con-
vergence in event data. in p.c. ¨olveczky and g. sala ¨un, editors, software engineering and
formal methods (sefm 2019) , volume 11724 of lecture notes in computer science , pages
3–25. springer-verlag, berlin, 2019.
5. w.m.p. van der aalst. chapter 2 - foundations of process discovery. in w.m.p. van der
aalst and j. carmona, editors, process mining handbook , volume 448 of lecture notes in
business information processing , pages 1–39. springer-verlag, berlin, 2022.
6. w.m.p. van der aalst, a. adriansyah, and b. van dongen. replaying history on process
models for conformance checking and performance analysis. wires data mining and
knowledge discovery , 2(2):182–192, 2012.
7. w.m.p. van der aalst and a. berti. discovering object-centric petri nets. fundamenta
informaticae , 175(1-4):1–40, 2020.
8. w.m.p. van der aalst, a.j.m.m. weijters, and l. maruster. workflow mining: discovering
process models from event logs. ieee transactions on knowledge and data engineering ,
16(9):1128–1142, 2004.
9. a. augusto, r. conforti, m. marlon, m. la rosa, and a. polyvyanyy. split miner: auto-
mated discovery of accurate and simple business process models from event logs. knowl-
edge information systems , 59(2):251–284, 2019.
10. a. augusto, j.carmona, and e. verbeek. chapter 3 - advanced process discovery tech-
niques. in w.m.p. van der aalst and j. carmona, editors, process mining handbook , volume
448 of lecture notes in business information processing . springer-verlag, berlin, 2022.
11. r. bergenthum, j. desel, r. lorenz, and s. mauser. process mining based on regions of
languages. in g. alonso, p. dadam, and m. rosemann, editors, international conference
on business process management (bpm 2007) , volume 4714 of lecture notes in computer
science , pages 375–383. springer-verlag, berlin, 2007.
12. j. vom brocke, w.m.p van der aalst, t. grisold, w. kremser, j. mendling, b. pent-
land, j. recker, m. roeglinger, m. rosemann, and b. weber. process sci-
ence: the interdisciplinary study of continuous change. available via ssrn:
http://ssrn.com/abstract=3916817, 2021.
13. j. carmona, j. cortadella, and m. kishinevsky. a region-based algorithm for discovering
petri nets from event logs. in business process management (bpm 2008) , pages 358–373,
2008.
14. j. carmona, b. van dongen, a. solti, and m. weidlich. conformance checking: relating
processes and models . springer-verlag, berlin, 2018.30 wil van der aalst
15. j. carmona, b. van dongen, and m. weidlich. chapter 5 - conformance checking: foun-
dations, milestones and challenges. in w.m.p. van der aalst and j. carmona, editors, pro-
cess mining handbook , volume 448 of lecture notes in business information processing .
springer-verlag, berlin, 2022.
16. b.f. van dongen. real-life event logs: hospital log (4tu.researchdata). https:
//doi.org/10.4121/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54 ,
2011.
17. m. dumas, m. la rosa, j. mendling, and h. reijers. fundamentals of business process
management . springer-verlag, berlin, 2018.
18. m.l. van eck, n. sidorova, and w.m.p. van der aalst. guided interaction exploration and
performance analysis in artifact-centric process models. business and information systems
engineering , 61(6):649–663, 2019.
19. d. fahland. describing behavior of processes with many-to-many interactions. in s. do-
natelli and s. haar, editors, applications and theory of petri nets 2019 , volume 11522 of
lecture notes in computer science , pages 3–24. springer-verlag, berlin, 2019.
20. g. cotroneo and r. carbone and s. boggini and m. cerini. process mining: a database of
applications 2021. www.hspi.it, hspi management consulting, 2021.
21. g. galic and m. wolf. global process mining survey 2021: delivering value with
process analytics - adoption and success factors of process mining . deloitte,
2021. https://www2.deloitte.com/de/de/pages/finance/articles/
global-process-mining-survey-2021.html .
22. a.f. ghahfarokhi, g. park, a. berti, and w.m.p. van der aalst. ocel standard. www.ocel-
standard.org, 2021.
23. ieee task force on process mining. process mining manifesto. in f. daniel, k. barkaoui,
and s. dustdar, editors, business process management workshops , volume 99 of lecture
notes in business information processing , pages 169–194. springer-verlag, berlin, 2012.
24. ieee task force on process mining. xes standard definition. www.xes-standard.org,
2016.
25. ieee task force on process mining. case studies. www.tf-pm.org, 2022.
26. m. kerremans, t. srivastava, and f.choudhary. gartner market guide for process mining,
research note g00737056. www.gartner.com , 2021.
27. r. koplowitz, c. mines, a. vizgaitis, and a. reese. process mining: your compass for
digital transformation: the customer journey is the destination. www.forrester.
com, 2019.
28. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. discovering block-structured pro-
cess models from event logs containing infrequent behaviour. in n. lohmann, m. song,
and p. wohed, editors, business process management workshops, international workshop
on business process intelligence (bpi 2013) , volume 171 of lecture notes in business in-
formation processing , pages 66–78. springer-verlag, berlin, 2014.
29. s.j.j. leemans, d. fahland, and w.m.p. van der aalst. scalable process discovery with
guarantees. in k. gaaloul, r. schmidt, s. nurcan, s. guerreiro, and q. ma, editors, enter-
prise, business-process and information systems modeling (bpmds 2015) , volume 214 of
lecture notes in business information processing , pages 85–101. springer-verlag, berlin,
2015.
30. f. mannhardt. road traffic fine management process (4tu.researchdata). https:
//doi.org/10.4121/uuid:915d2bfb-7e84-49ad-a286-dc35f063a460 ,
2016.
31. l. reinkemeyer. process mining in action: principles, use cases and outlook . springer-
verlag, berlin, 2020.
32. a. rozinat and w.m.p. van der aalst. conformance checking of processes based on moni-
toring real behavior. information systems , 33(1):64–95, 2008.process mining: a 360 degree overview 31
33. m. sol ´e and j. carmona. process mining from a basis of state regions. in j. lilius and
w. penczek, editors, applications and theory of petri nets 2010 , volume 6128 of lecture
notes in computer science , pages 226–245. springer-verlag, berlin, 2010.
34. s. suriadi, r. andrews, a.h.m. ter hofstede, and m.t. wynn. event log imperfection
patterns for process mining: towards a systematic approach to cleaning event logs. in-
formation systems , 64:132–150, 2017.
35. f.w. taylor. the principles of scientific management . harper and brothers publishers, new
york, 1919.
36. s.j. van zelst, b.f. van dongen, w.m.p. van der aalst, and h.m.w verbeek. discovering
workflow nets using integer linear programming. computing , 100(5):529–556, 2018.