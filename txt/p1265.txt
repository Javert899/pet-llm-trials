a scalable database for the storage of object-centric event logs (extended
abstract)
alessandro berti, anahita farhang ghahfarokhi, gyunam park, wil m.p. van der aalst
process and data science department, rwth aachen university
process and data science department, lehrstuhl fur informatik 9 52074 aachen, germany
emails: a.berti@pads.rwth-aachen.de, wvdaalst@pads.rwth-aachen.de
abstract —object-centric process mining provides a set of
techniques for the analysis of event data where events are
associated to several objects. to store object-centric event logs
(ocels), the json-ocel and json-xml formats have been
recently proposed. however, the proposed implementations of
the ocel are ﬁle-based. this means that the entire ﬁle needs
to be parsed in order to apply process mining techniques, such
as the discovery of object-centric process models. in this paper,
we propose a database storage for the ocel format using the
mongodb document database. since documents in mongodb are
equivalent to json objects, the current json implementation
of the standard could be translated straightforwardly in a series
of mongodb collections.
index terms —object-centric process mining; object-centric
event log; database support; mongodb
i. s ignificance of the tool
ocel http://www.ocel-standard.org/1.0/speciﬁcation.pdf
has been proposed to model the structure of object-centric
event logs [1]. implementations of the format have been made
available for json and xml ﬁle formats, and tool support is
proposed for the python and java languages. for all these, the
event log is stored in a json/xml ﬁle that can be ingested
in-memory by the tools/libraries. the necessity to load the
log in-memory makes it difﬁcult to manage a huge amount of
object-centric event data since memory is a limited asset. with
this paper, a novel implementation of the format is proposed
based on the mongodb document database. documents can
be imported in mongodb starting from json objects. hence,
the json-ocel implementation could be translated easily
to mongodb. moreover, mongodb can mix in-memory
and on-disk computations to provide efﬁcient data science
pipelines. other advantages of mongodb that we exploit
are: the ﬁne-grained support for indexes (i.e., multikey),
which makes ad-hoc querying faster; the ﬁne-grained support
for aggregations (i.e., grouping) that permits to move some
of the computations at the database level; the support to
replication, which provides redundancy and increases data
availability https://docs.mongodb.com/manual/replication/.
graph databases have been assessed previously for the
storage of object-centric event data [2], [3], [4], but the direct
translation of the speciﬁcation of ocel in a graph database
is more challenging1. also, columnar storages have been used
[5], [6], with the limitations that they work for basic column
1even if object-centric event logs can, in general, be uploaded to graph
databases as shown in https://doi.org/10.5281/zenodo.3865221types but do not provide comprehensive support to json and
advanced data types.
ii. m ainfeatures of the tool
the implementation of the schema to host the elements of
the ocel standard follows from the implementation of json-
ocel http://www.ocel-standard.org/1.0/speciﬁcation.pdf. fig.
1 shows how the translation of the different entities is possible.
“ocel:events”: {…
“ev1”: {“ocel:activity”: “a”,
“ocel:timestamp”:
 “2020-01-01t00:00:00”,
“ocel:omap”:[…],
“ocel:vmap”:{...} }
...}
“ocel:objects”: {…
“obj1”: {“ocel:type”: “order”,
“ocel:ovmap”:{...} }
...}
“ocel:global-event”: {…}
“ocel:global-object”: {…}
“ocel:global-log”: {…}…{ “ocel:id”: “ev1”,
“ocel:activity”:”a”,
“ocel:timestamp”:date(1970,01,01,00,00,00), 
“ocel:omap”:[…],
“ocel:vmap”:{...} } ...
…{ “ocel:id”: “obj1”,
“ocel:type”:”order”,
“ocel:vmap”:{...} } ...
...
ocel:events collection
ocel:objects collection
ocel:others collection
fig. 1. json-ocel implementation (left) and its equivalence to the mon-
godb ocel schema (right).
some ﬁelds are colored in red, meaning that an index
has been applied to the ﬁelds to optimize the execution of
some queries. in particular, the identiﬁer, the activity, and the
object map (multikey index) have been set as an index for the
events. in contrast, the identiﬁer and the type have been set as
identiﬁers for the objects. the tool permits ingestion of logs in
the json/xml-ocel formats or exporting of the mongodb
implementation’s contents to json/xml-ocel. moreover,
some essential object-centric process mining operations have
been implemented at the mongodb level (retrieving the lifecy-
cle of the objects, providing statistics on the number of events,
unique and total objects, counting the events per activity and
the objects per type . . . ) to reduce the data exchange with the
database and use the aggregation features of mongodb. these
are illustrated later in this extended abstract.
iii. u sage of the tool
the provided tool is based on the python language and
supports all existing ocel implementations (json, xml,
and mongodb). the tool is available at the address https:
//github.com/ocel-standard/ocel-support. in particular, ex-
ample scripts for the usage of the mongodb interface arearxiv:2202.05639v1  [cs.db]  11 feb 2022table i
assessment of the scalability of mongo db asocel storage ,for logs of different size .
insertion + indexing mdfg calculation
size time(s) json(mb) bson(mb) index(mb) ex.time(s) cpu(%) ram(mb) disk usage(mb)
1 m 79.64 610 210 182 25.50 64.9 883 109
5 m 353.61 2805 1094 875 117.65 99.9 3414 501
20 m 1520.34 10899 4244 3367 590.30 99.9 6705 6499
100 m 7983.87 54642 21304 16852 3657.00 99.9 6709 36478
available in the folder examples/mongodb . first, the con-
nection string and the database name could be set in the
script commons.py . the script exporting.py permits to load
an existing json/xml-ocel ﬁle in the mongodb database,
while the script importing.py permits to save the object-centric
event log to a json/xml-ocel ﬁle. other scripts perform
computations on object-centric event logs:
objcentr dfg.py provides routine for the computations of
the directly-follows graph for each object type of the log.
activities stats.py and otstats.py provide some basic
statistics for the activities (number of events and objects)
and the object types (number of objects per type) of the
event log.
times between activities.py provides some statistics of
the time passed between a couple of the activities of the
log (regardless of the object type).
mongodb offers a powerful aggregation package that per-
mits performing signiﬁcant object-centric process mining op-
erations directly at the database level. as an example of a
crucial object-centric process mining operation, we show an
aggregation that is useful for the computation of the multi-
directly follows graph (ﬁnding the events that belong to the
lifecycle of an object). first, the ocel:omap attribute (list of
related objects) is unrolled, so the same event is replicated for
all the related objects. then, a grouping operation based on
the unrolled ocel:omap attribute is performed to collect the
activities of the events related to the same object.
e v e n t s c o l l e c t i o n . a g g r e g a t e (
[f” $unwind ” : ” $ o c e l : omap” g,
f” $group ” : f’i d ’ : ’ $ o c e l : omap ’ ,
’ l i f e c y c l e ’ : f” $push ” : ’ $ o c e l : a c t i v i t y ’ ggg ] ,
allowdiskuse =true )
the output of the aggregation can be used to calculate the
directly-follows graph for the objects of a speciﬁc type, and
looks like:
[f”i d ” : ” o1 ” , ” l i f e c y c l e ” : [ ” c r e a t e order ” ,
” payment ” ]g,
f”i d ” : ” o2 ” , ” l i f e c y c l e ” : [ ” c r e a t e order ” ,
” change order ” , ” cancel order ” ] g,
f”i d ” : ” i 1 ” , ” l i f e c y c l e ” : [ ” emit i n v o i c e ” ,
” record payment ”] g. . .
]
iv. m aturity of the tool
the prototypal tool available at the address https://github.
com/ocel-standard/ocel-support has not been used in anyreal-life case study. we analyzed the scalability of the mon-
godb implementation. all the experiments have been con-
ducted with a notebook having an i7-7500u cpu, 16 gb of
ram, and an ssd hard drive. table i reports on the results
attained from logs of different size. the binary compression
used to store the documents by mongodb permits to save
a signiﬁcant amount of disk space in the storage of the
log. we can also see that the index, which is necessary to
increase the speed of the computations, occupies a signiﬁcant
amount of space compared to the size of the collection.
in the computation of mdfgs, we can see that mongodb
mixes in-memory calculations with on-disk ones, especially
if the amount of memory needed is higher than the amount
of memory available. compared to an in-memory approach,
where the entire json object is imported into the memory,
the computation of the object-centric directly-follows graph
takes signiﬁcantly more time. however, the amount of memory
required to store the json is also considerably higher than the
memory requirements of mongodb. our workstation went
out of memory trying to ingest an event log having 6.8
m events, while mongodb can manage bigger logs, as our
experiments show. a video displaying the ingestion of an
object-centric event log in mongodb, and the execution of
some computations, is available at the address https://www.
youtube.com/watch?v=vdd5casy1y0.
v. a cknowledgments
we thank the alexander von humboldt (avh) stiftung for
supporting our research. funded by the deutsche forschungs-
gemeinschaft (dfg, german research foundation) under ger-
many’s excellence strategy–exc-2023 internet of production
– 390621612.references
[1] a. f. ghahfarokhi, g. park, a. berti, and w. m. van der aalst, “ocel:
a standard for object-centric event logs,” in european conference on
advances in databases and information systems . springer, 2021, pp.
169–175.
[2] s. esser and d. fahland, “multi-dimensional event data in graph
databases,” journal on data semantics , pp. 1–33, 2021.
[3] a. jalali, “graph-based process mining,” arxiv preprint
arxiv:2007.09352 , 2020.
[4] s. esser and d. fahland, “storing and querying multi-dimensional pro-
cess event logs using graph databases,” in international conference on
business process management . springer, 2019, pp. 632–644.
[5] y . wang and a. kogan, “cloud-based in-memory columnar database ar-
chitecture for continuous audit analytics,” journal of information systems ,
vol. 34, no. 2, pp. 87–107, 2020.
[6] a. berti and w. m. van der aalst, “extracting multiple viewpoint
models from relational databases,” in 8th international symposium on
data-driven process discovery and analysis (simpda) . springer
international publishing, 2018, pp. 24–51.