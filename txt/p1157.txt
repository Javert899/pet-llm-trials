the data science revolution
how learning machines changed
the way we work and do business
wil m.p. van der aalst
1process and data science, rwth aachen university, aachen, germany
2fraunhofer fit, sankt augustin, germany
wvdaalst@pads.rwth-aachen.de
www.vdaalst.com
abstract. data science technology is rapidly changing the role of infor-
mation technology in society and all economic sectors. articial intelli-
gence (ai) and machine learning (ml) are at the forefront of attention.
however, data science is much broader and also includes data extrac-
tion, data preparation, data exploration, data transformation, storage
and retrieval, computing infrastructures, other types of mining and learn-
ing, presentation of explanations and predictions, and the exploitation
of results taking into account ethical, social, legal, and business aspects.
this paper provides an overview of the eld of data science also showing
the main developments, thereby focusing on (1) the growing importance
of learning from data (rather than modeling or programming), (2) the
transfer of tasks from humans to (software) robots, and (3) the risks
associated with data science (e.g., privacy problems, unfair or nontrans-
parent decision making, and the market dominance of a few platform
providers).
keywords: data science ·machine learning ·articial intelligence ·
responsible data science ·big data
1 introduction
the international federation for information processing (ifip) was established
in 1960 under the auspices of unesco as a result of the rst world computer
congress held in paris in 1959. this year we celebrate the 60th anniversary of
ifip. ifip was created in 1960 because of the anticipated impact and trans-
formative power of information technology. however, the impact of information
technology over the past 60 years has been even larger than foreseen. infor-
mation technology has dramatically transformed the lives of individuals and
businesses. over the last 60 years, data science , i.e., extracting knowledge and
insights from structured and unstructured data, has become the main driver of
such transformations. in this paper, we reect on the impact of data science and
key developments.
in [?], data science is dened as follows: \ data science is an interdisciplinary
eld aiming to turn data into real value. data may be structured or unstructured,2 wil van der aalst
infrastructure analysis effect
o big data infrastructures
o distributed systems
o data engineering
o programming 
o security
o ...o statistics
o data/process mining
o machine learning
o artificial intelligence
o visualization
o ...o ethics & privacy
o it law
o operations management
o business models
o entrepreneurship
o ...“volume and velocity ” “extracting knowledge ” “people , organizations , society”
mechanical 
engineeringmedicine
social sciences
logisticsscientific 
workflows
energy
high -tech 
systems
fig. 1. overview of the key ingredients of data science [ ?].
big or small, static or streaming. value may be provided in the form of predic-
tions, automated decisions, models learned from data, or any type of data visu-
alization delivering insights. data science includes data extraction, data prepa-
ration, data exploration, data transformation, storage and retrieval, computing
infrastructures, various types of mining and learning, presentation of explana-
tions and predictions, and the exploitation of results taking into account ethical,
social, legal, and business aspects. " data science can be seen as an umbrella term
for machine learning, articial intelligence, mining, big data, visual analytics,
etc. the term is not new. turing award winner peter naur (1928-2016) rst used
the term `data science' long before it was in vogue. in 1974, naur wrote [ ?]: \a
basic principle of data science, perhaps the most fundamental that may be for-
mulated, can now be stated: the data representation must be chosen with due
regard to the transformation to be achieved and the data processing tools avail-
able" . in [ ?], naur discusses `large data systems' referring to data sets stored
on magnetic disks having a maximum capacity of a few megabytes. clearly, the
notion of what is large has changed dramatically since the early seventies, and
will continue to change.
figure ??visualizes the above denition. the top part shows that data sci-
ence can be applied in many dierent areas, i.e., most data science approaches
are generic. the middle part shows that there are three main ingredients: in-
frastructure (concerned with the huge volume and incredible velocity of data),
analysis (concerned with extracting knowledge using a variety of techniques),
and eect (concerned the impact of data science on people, organizations, and
society). the diagram shows the interdisciplinary nature of data science. as an
example, take a self-driving car. to build a self-driving car one needs an infras-
tructure composed of sensors (camera, lidar, radar, etc.), hardware controllers,
networking capabilities, powerful processing units, and analysis techniques for
perception (e.g., convolutional neural networks), localization, prediction, plan-
ning, and control using this infrastructure. however, one also needs to considerthe data science revolution 3
theeect . the self-driving car has to be economically feasible and may require
new business models. while creating such a car, one also needs to consider legal
and ethical implications. in july 2016, tesla reported the rst fatality of a driver
in a self-driving car triggering heated debates. who is responsible when the car
crashes? what decisions should be taken when a crash is unavoidable (e.g. pro-
tect passengers or pedestrians)? who owns the data collected by the self-driving
car? due to the huge impact of data science on people, organizations, and society,
many legal, ethical, and nancial aspects come into play.
as figure ??shows, the eld of data science is broad, building on multiple
scientic disciplines. how does data science relate to hyped terms such as big
data, articial intelligence (ai), and machine learning (ml)? big data seems
to be out of fashion and ai and ml are the new buzzwords used in the media.
ai may mean everything and nothing. on the one hand, the term ai roughly
translates to \using data in an intelligent manner" looking at its use in the
media. this means that everything in figure ??is ai. on the other hand,
the term is also used to refer to very specic approaches, such as deep neural
networks [ ?]. the same applies to the term ml. all subelds of data mining
(classication, clustering, patterns mining, regression, logistic regression, etc.)
can be seen as forms of machine learning. however, ml is also used to refer to
only deep learning.
john mccarthy coined the term ai in 1955 as \the science and engineering
of making intelligent machines". today, the eld of ai is typically split in sym-
bolic ai and non-symbolic ai. symbolic ai , also known as good old fashioned
ai (gofai), uses high-level symbolic (i.e., human-readable) representations of
problems, logic, and rules. experts systems tend to use symbolic ai to make
deductions and to determine what additional information it needs (i.e., what
questions to ask) using human-readable symbols. the main disadvantage of sym-
bolic ai is that the rules and knowledge have to be hand-coded. non-symbolic
aidoes not aim for human-readable representations and explicit reasoning, and
uses techniques imitating evolution and human learning. example techniques
include genetic algorithms, neural networks and deep learning. the two main
disadvantages of non-symbolic ai are the need for a lot of training data and the
problem of understanding why a particular result is returned. symbolic ai is still
not widely adopted in industry. although non-symbolic ai performed worse than
symbolic ai for many decades, by using back-propagation in multi-layer neural
networks, non-symbolic ai started to outperform conventional approaches [ ?,?].
as a result, these techniques are now also used in industry for tasks such as
speech recognition, automated translation, fraud detection, image recognition,
etc.
the successes of non-symbolic ai are amazing. however, ai is only a small
part of data science, often tailored towards specic tasks (e.g., speech recogni-
tion) and using specic models (e.g., deep convolutional neural networks). the
same applies to ml (which is often considered to be a subeld of ai). when
the term ai or ml is used in the media, this often refers to data mining, pat-4 wil van der aalst
tern/process mining, statistics, information retrieval, optimization, regression,
etc.
this paper aims to `demystify' data science, present key concepts, discuss
important developments. we also reect on the impact of data science on the
way we work and do business. the paper is partly based on a keynote given
at the ifip world computer congress (wcc 2018) on 18 september 2018, in
poznan, poland. it extends the corresponding keynote paper [ ?].
the remainder is organized as follows. section ??metaphorically discusses
the four essential elements of data science: \water" (availability, magnitude, and
dierent forms of data), \re" (irresponsible uses of data and threats related
to fairness, accuracy, condentiality, and transparency), \wind" (the way data
science can be used to improve processes), and \earth" (the need for data sci-
ence research and education). section ??discusses the shift from modeling and
programming to data-driven learning enabled by the abundance of data. due to
the uptake of data science, traditional jobs and business models will disappear
and new ones will emerge. section ??reects on these changes. data science
can make things cheaper, faster, and better. however, also negative side-eects
are possible (see the \re" element mentioned before). therefore, section ??
discusses the topic of responsible data science in the context of the growing
dominance of digital platforms. section ??concludes this paper.
2 the four essential ingredients of data science
in this section, we dene the four essential elements of data science [ ?]. as
metaphors, we use the classical four elements: \water", \re", \wind", and
\earth" (see figure ??). according to empedocles, all matter is comprised of
these four elements. other ancient cultures had similar lists, sometimes also
composed of more elements (e.g., earth, water, air, re, and aether) that tried to
explain the nature and complexity of all matter in terms of simpler substances.
to explain the essence of data science, we use \water" as a placeholder for the
availability of dierent forms of data, \re" as a placeholder for irresponsible uses
of data (e.g., threats to fairness, accuracy, condentiality, and transparency),
\wind" as a placeholder for the way that data science can be used to improve
processes, and \earth" as a placeholder for education and research (i.e., the
base of data science) underpinning all of this. note that figure ??complements
figure ??, allowing us to emphasize specic aspects.
2.1 the \water" of data science
the rst essential element of data science (\water") is the data itself [ ?]. the ex-
ponential growth of data and data processing capabilities since the establishment
of ifip in 1960 is evident:
{things are getting exponentially cheaper , e.g., the price of storage dropped
from one million euros per mb in the 1960-ties to 0.00002 cents per mb
today.the data science revolution 5
fig. 2. the \water", \re", \wind", and \earth" of data science [ ?].
{things are getting exponentially faster , e.g., the number of oating-point
operations per second increased from a few kflops (103oating-point op-
erations per second) to hundreds of pflops (1015oating-point operations
per second).
{things are getting exponentially smaller , e.g., the size of a transistor de-
creased from a few centimeters (10 2meter) to a few nanometers (10 9
meter).
the reductions in price and size and the increase in speed apply to processing
(i.e., cpu and gpu processors), storage , and communication . a gpu may have
thousands of processors and a company like google has over one million servers.
the above numbers illustrate our increased capabilities to process large
amounts of data. these are used to align the digital world and the physical world.
for example, organizations are collecting 'events' at a large scale [ ?]. events may
take place inside a machine (e.g., an x-ray machine, an atm, or baggage han-
dling system), inside an enterprise information system (e.g., an order placed by
a customer or the submission of a tax declaration), inside a hospital (e.g., the
analysis of a blood sample), inside a social network (e.g., exchanging e-mails or
twitter messages), inside a transportation system (e.g., checking in, buying a
ticket, or passing through a toll booth), etc. the uptake of the so-called internet
of things (iot) resulted in many connected devices ranging from light bulbs to
wearable heart monitors [ ?]. events may be `life events', `machine events', or
`organization events'. these may be stored in traditional relational databases
(e.g., microsoft sql server, oracle database, mysql, and ibm db2), nosql
databases (e.g., couchdb, mongodb, cassandra, and hbase), or distributed
ledgers using blockchain technology (e.g., ethereum, neo, cardano). the term
internet of events (ioe), coined in [ ?], refers to the omnipresence of event data
in all application domains.6 wil van der aalst
in 2001, doug laney introduced the rst three v's describing challenges
related to big data: volume, velocity, and variety [ ?]. later, additional v's
were added: veracity, variability, visualization, value, venue, validity, etc. the
above refers to the rst `v' (volume) describing the incredible scale of some data
sources. the second `v' (velocity) refers to the speed of the incoming data that
need to be processed. in many applications, it has become impossible to store all
data and process it later. such streaming data needs to be handled immediately.
the third `v' (variety) refers to the dierent types of data coming from multiple
sources. structured data may be augmented by unstructured data (e.g. free text,
audio, and video).
2.2 the \fire" of data science
the second essential element of data science (\re") refers to the dangers of
using data in an irresponsible way [ ?]. data abundance combined with powerful
data science techniques has the potential to dramatically improve our lives by
enabling new services and products, while improving their eciency and quality.
many of today's scientic discoveries (e.g., in health) are already fueled by devel-
opments in statistics, mining, machine learning, articial intelligence, databases,
and visualization. at the same time, there are also great concerns about the use
of data. increasingly, customers, patients, and other stakeholders are concerned
about irresponsible data use. automated data-based decisions may be unfair or
non-transparent. condential data may be shared unintentionally or abused by
third parties.
when ifip was created sixty years ago, one could not foresee the possible
risks related to data science. the facebook-cambridge analytica scandal in 2018
and many other scandals involving unacceptable uses of data heavily inuenced
public opinion. also books such as \weapons of math destruction: how big data
increases inequality and threatens democracy" [ ?], created increased awareness
of the risks associated with data science. the european general data protection
regulation (gdpr) [ ?] is a response to such risks. however, legislation such
as gdpr may also inhibit certain applications. hence, technological solutions
involving distribution and encryption are needed. in section ??, we elaborate
further on the topic of responsible data science.
2.3 the \wind" of data science
the third essential element of data science (\wind") is concerned with the way
data and processes interact. storing and processing data is typically not a goal in
itself. data are there to support processes. data science can help organizations
to be more eective, to provide a better service, to deliver faster, and to do all of
this at lower costs. this applies to logistics, production, transport, healthcare,
banking, insurance, and government.
data science can be used to improve tasks within the process ; e.g., certain
checks can be automated using a neural network trained on many examples.
data science can also be used to improve the design or management of thethe data science revolution 7
whole process , e.g., using process mining, one can identify root causes for specic
bottlenecks and deviations. data science can also be used to create new products
and services , e.g., spotify is able to recommend music based on the listener's
preferences.
clearly, there is a tradeo between \water" and \wind". giving up privacy
concerns may lead to better processes, services, and products.
2.4 the \earth" of data science
the fourth essential element of data science (\earth") is concerned with the
foundations of a data-driven society: education and research [?]. education (in
every sense of the word) is one of the fundamental factors in the development
of data science. data science education is needed at all levels. people need to be
aware of the way algorithms make decisions that may inuence their lives. hyped
terms such as big data, articial intelligence (ai), and machine learning (ml)
are not well understood and may mean anything and nothing (see section ??).
some examples of phenomena that illustrate the need for education.
{correlation is not the same as causality. it may be that ice cream sales and
crime rates strongly correlate. however, this does not imply that one causes
the other. the so-called hidden variable is the weather. higher temperatures
lead to higher ice cream sales and higher crime rates. it makes no sense to
try to reduce crime by putting a ban on ice cream sales.
{simpson's paradox. it may be that within dierent subsets of data a variable
has a positive inuence, whereas it as a negative inuence when considering
all the data. for example, in each study program, females are more likely to
pass. however, when considering all students, males are more likely to pass.
another example is that for both young patients and old patients, exercising
has a positive eect on one health. however, when looking at all patients,
there is a negative correlation between exercising and health.
{hacking deep neural networks. given a well-trained neural network that is
able to separate cars and horses, it is possible to add a bit of `noise' to the
images (invisible to the human eye) such that horses are classied as cars
and cars are classied as horses. the same applies to speech recognition.
{homomorphic encryption. it is possible to do computations on encrypted
ciphertexts such that the encrypted result, when decrypted, matches the
result of the operations as if they had been performed on the non-encrypted
data.
{secure multi-party computation. it is possible to jointly compute a function
over multiple parties that all keep their data private. hence, one can apply
data science techniques without sharing the actual data.
the above example phenomena and the oversimplied coverage of ai in the
media illustrate that policy and decision makers need to know more about data
science. this cannot be left to \the market" or solved through half-hearted
legislation like the gdpr [ ?]. to remain competitive, countries should invest
in data science capabilities. this can only be realized through education and
research.8 wil van der aalst
3 learning versus modeling and programming
currently, many elds of science are undergoing a paradigm shift. a new genera-
tion of scientists emerges that focuses on the analysis and interpretation of data
rather than models detached from data. this shift is caused by the availability
of data and easy-to-use data-science tooling.
the elds of science can be roughly split into:
{formal sciences (logic, mathematics, statistics, theoretical computer science,
etc.) that are based on a priori knowledge or assumptions that are indepen-
dent of real/life observations.
{natural sciences (physics, chemistry, biology, etc.) that study natural phe-
nomena (atoms, molecules, gravity, magnetism, cells, planets, etc.).
{social sciences (psychology, sociology, economics, literature, etc.) that study
human and societal behavior.
{applied sciences (engineering, medicine, software engineering, etc.) that ap-
ply scientic knowledge to practical applications (e.g., creating systems).
natural, social, and applied sciences heavily depend on observations of natural
phenomena, people, and systems. in inductive research , the goal of a researcher
is to infer models and theories (i.e., theoretical concepts and patterns) from
observed data. in deductive research , the goal of the researcher is to test models
and theories using new empirical data. the importance of data has grown in all
of these elds. this is a direct result of our ability to observe natural phenomena,
people, and systems much more directly.
consider, for example, the social sciences with research methods such as
surveys (i.e., questionnaires), laboratory experiments, eld experiments, inter-
views, and case studies. traditional surveys may have low response rates and
a sample bias (the set of participants that was invited and accepted may not
be representative). laboratory experiments are often too small and also have
a sample bias. interviews and case studies tend to be subjective. therefore,
most scientic results cannot be reproduced. this is commonly referred to as
the \replication crisis" [ ?]. therefore, younger social science researchers prefer
to use research methods that use objective larger-scale observations. for ex-
ample, directly recording the activities of participants rather than relying on
self-reporting or more eld experiments with many subjects rather than lab ex-
periments with a only few subjects.
another example is the uptake of computational biology and bioinformatics
where large collections of biological data, such as genetic sequences, cell popu-
lations or protein samples are used to make predictions or discover new models
and theories.
also the eld of computer science is changing markedly. there seems to be
less emphasis on theoretical computer science due to the desire to relate mod-
els and theories to real-world phenomena. it is no longer fashionable to create
new modeling languages and to prove properties in self-created articial set-
tings. instead, sub-disciplines related to data science are rapidly growing in thethe data science revolution 9
number of students and researchers. automated learning (e.g., machine learn-
ing, dierent forms of mining, and articial intelligence) are replacing parts of
programming. rules are no longer programmed but learned from data. this is
changing computer science. for example, how to verify the correctness a system
that uses neural networks?
the shift from modeling and programming to automated learning is aecting
science and also the economies that build upon it. consider for example the way
that marketing changed. today's marketeer is expected to have data science
skills. in fact, many professions have become much more data-driven or are
about disappear (see next section).
4 machines versus people
the uptake of data science will continue to change the way we work, the way
we move, the way we interact, the way we care, the way we learn, and the way
we socialize [ ?]. as a result, many professions will cease to exist [ ?,?,?]. at the
same time, new jobs, products, services, and opportunities emerge.
the frontier between the tasks performed by humans and those performed
by machines and algorithms is continuously moving and changing global labor
markets. in [?], frey and osborne provide predictions for the computerization
of 702 occupations. they estimate that 47 percent of jobs in the us will be
replaced by (software) robots.
in [?] three types of roles are identied: stable roles (work that remains), new
roles (new types of work that did not exist before), and redundant roles (work
that is taken over by e.g. robots). examples of redundant roles are clerical work
(e.g., data entry), factory work (e.g., assembly), postal service, and cashiers. of
the new roles mentioned in [ ?], most are related to data science.
in [?] three waves of automation are predicted: (1) algorithmic wave (replac-
ing simple computational tasks in data-driven sectors such as banking), (2) aug-
mentation wave (replacing more complex clerical work and materials handling
closed environments such as warehouses), and (3) autonomous wave (replacing
physical work in transport, construction, and healthcare). the algorithmic wave
is currently in full swing. the augmentation wave has started with the uptake
ofrobotic process automation (rpa) and robots in production and warehouse.
this wave is likely to come to full maturity in the next decade. the autonomous
wave uses technologies that are already under development, but, according to
[?], will only come to full maturity on an economy-wide scale in the 2030s.
as a concrete example, consider the uptake of robotic process automation
(rpa) [ ?]. rpa software provided by vendors such as uipath, automation any-
where, and blue prism provides software robots (bots) replacing humans. in
the 1990-ties workow management (wfm) software already aimed to realize
straight through processing (stp), i.e., handling cases with no or little human
involvement. however, in many cases, this was not cost-eective because exist-
ing systems needed to be changed. moreover, wfm often failed because of a
limited understanding of the complexity of the actual processes performed by10 wil van der aalst
different types of clerical work 
sorted based on frequencyfrequencytraditional 
automation
candidates for rpa 
(traditional automation 
is not cost effective )low-frequent process 
variants that cannot be 
automated and still require 
human involvementrpa shifts the boundary of 
cost-effective automation
fig. 3. robotic process automation (rpa) has lowered the threshold for process au-
tomation [ ?].
people. the key dierence between rpa and traditional wfm is that rpa does
not aim to replace existing (back-end) information systems. instead, software
robots interact with the existing information systems in the same way as hu-
mans do. rpa software interacts with the pre-existing applications through the
user interface directly replacing humans, i.e., automation is realized by taking
over tasks from workers without the back-end systems knowing. a typical rpa
scenario is a sequence of copy-and-paste actions normally performed by a hu-
man. compared to wfm, rpa lowers the threshold for automation. as shown
in figure ??, the focus of traditional automation is on high-frequent clerical
work. for less frequent work, automation is not cost-eective. rpa shifts the
boundary where automation is still cost-eective. machine learning, articial in-
telligence, and process mining are enabling technologies making rpa possible.
the software robots need to learn from humans and need to be monitored con-
tinuously. process mining [ ?] can be used to identify repeating work. moreover,
if a software robot malfunctions due to technical glitches, exceptions, changing
user interfaces, or changing contextual factors, then this can be detected using
process mining. machine learning and articial intelligence can be used to learn
specic tasks. in more advanced applications of rpa, work is exibly distributed
over workers and software robots. for example, tasks are initially performed by
robots and are escalated to workers the moment there is a complication or ex-
ception. similarly, workers can hand o work to robots using an `auto-complete'
option. moreover, the rpa solution may adapt due to changes in the underlying
process (e.g., concept drift). this illustrates that the border between humans
and (software) robots will continue to shift.
reports such as [ ?,?,?] analyze the impact for specic groups (e.g., based
on gender, education, or nationality). although it is dicult to predict suchthe data science revolution 11
phenomena accurately, it is clear that the impact of data science on the work of
people is accelerating and will be larger than ever before.
5 responsible data science in a platform economy
the distribution of work between humans, machines, and algorithms is changing
due to the uptake of data science. moreover, the growing importance of data is
also changing the economy and leads to new concerns related to privacy and
fairness.
in recent years we have witnessed the rise of the platform economy [ ?]. the
world's most valuable public companies are ve american technology rms: mi-
crosoft, amazon, apple, alphabet (google), and facebook. these companies
are closely followed by chinese tech giants such as alibaba, tencent, baidu,
and xiaomi, and many more us-based internet companies such as netix, ebay,
uber, salesforce, and airbnb. these organizations were able to grow extremely
fast due to the digital platforms they provide. some (e.g., amazon, alibaba,
airbnb, uber, and baidu) provide a transaction platform that matches supply
and demand, others provide a technical infrastructure that other people can
build upon (e.g., the app stores of google and apple), and some provide both
(e.g., amazon also oers cloud services). successful digital platforms have the
characteristic that they tend to grow very fast and, in the end, often one win-
ner remains. for example, amazon and alibaba are dominating the way we
buy products, google is controlling the way we search, and facebook is control-
ling the way we socialize. apple, alphabet, and microsoft are controlling the
platforms we use (ios, android, and windows). after a platform becomes the
market leader, it is very dicult to compete for organizations that started later,
e.g., for a new company it is dicult (if not impossible) to compete with google's
search engine or with amazon's marketplace. large tech companies use prots
generated with one platform to create other platforms. see, for example, the cur-
rent competition to become the leading digital platform for smart homes (e.g.,
amazon alexa, apple homekit, google assistant, philips hue, and samsung
smartthings). often \the winner takes it all" due to low marginal transaction
costs and so-called network eects [ ?]. the resulting monopoly may stie inno-
vation and makes society dependent on a few technology providers. moreover,
the platform providers may use their prots to extend their platforms in other
directions. for example, google is using its prots from search engine marketing
to invest in many other services and products (e.g., autonomous driving) and
amazon is using its marketplace to promote its own products (smartphones,
televisions, speakers, tv-series, diapers, etc.).
next to the large-scale economic concerns, there are also smaller-scale con-
cerns impacting individuals. the responsible data science (rds) initiative ini-
tiated by the author in 2015, aims to address problems related to fairness ,ac-
curacy ,condentiality , and transparency [?]. figure ??shows the key challenges
of rds:12 wil van der aalst
data in a 
variety of 
systems
data used as 
input for 
analytics
   
  
 
results
modelsextract , load , 
transform , clean , 
anonymize , de-
identify , etc.
report , discover , 
mine , learn , check , 
visualize , etc.interpretation by 
analysts , scientists , 
doctors , managers , etc.
information systems , devices , etc.
automated 
decisions , 
predictions , and 
recommendationsfairness  - data science 
without prejudice : how to 
avoid unfair conclusions 
even if they are true ?confidentiality  - data science 
that ensures confidentiality : how 
to answer questions without 
revealing secrets ?
accuracy  - data science 
without guesswork : how to 
answer questions with a 
guaranteed level of accuracy ?transparency  - data science 
that provides transparency : 
how to clarify answers such 
that they become indisputable ?
fig. 4. fairness ,accuracy ,condentiality , and transparency (fact) are the main
concerns of responsible data science (rds).
{data science without prejudice - how to avoid unfair conclusions even if they
are true?
{data science without guesswork - how to answer questions with a guaranteed
level of accuracy?
{data science that ensures condentiality - how to answer questions without
revealing secrets?
{data science that provides transparency - how to clarify answers such that
they become indisputable?
to sustain the use of data science, it will become increasingly important to
address concerns related to fairness, accuracy, condentiality, and transparency.
data science techniques need to ensure fairness : automated decisions and
insights should not be used to discriminate in ways that are unacceptable from
a legal or ethical point of view. this may conict with the goal of applying
data science, e.g., discriminating between students that will be successful in
their studies or not, discriminating between customers that will be able to pay
back the loan or not, or discriminating between patients that will benet from
treatment or not. these decisions can all be seen as classication problems: the
goal is to explain a response variable (e.g., the person will pay back the loan) in
terms of predictor variables (e.g., credit history, employment status, age, etc.).
ideally, the learned model explains the response variable in terms of predictor
variables. however, these may correlate with sensitive attributes such as gender,
nationality, age, etc. as a result, the learned classier may eectively reject cases
from certain groups of persons. this explains the question \how to avoid unfair
conclusions even if they are true?".the data science revolution 13
data science techniques also need to ensure accuracy : most data science tech-
niques return an answer even when there is not enough evidence in the data.
when using many variables relative to the number of instances, classication
may result in complex rules overtting the data [ ?]. this is often referred to as
the curse of dimensionality: as dimensionality increases, the number of combina-
tions grows so fast that the available data become sparse. with a xed number
of instances, the predictive power reduces as the dimensionality increases. using
cross-validation most ndings (e.g., classication rules) will get rejected. how-
ever, if there are many ndings, some may survive cross-validation by sheer luck.
data science techniques also need to ensure condentiality : the results should
not reveal certain types of personal or otherwise sensitive information. the im-
portance of protecting personal data is widely acknowledged and supported by
legislation such as the general data protection regulation (gdpr) [ ?] which
states that \the principles of data protection should apply to any information
concerning an identied or identiable natural person. personal data which have
undergone pseudonymisation, which could be attributed to a natural person by
the use of additional information should be considered to be information on an
identiable natural person. to determine whether a natural person is identiable,
account should be taken of all the means reasonably likely to be used, such as sin-
gling out, either by the controller or by another person to identify the natural
person directly or indirectly. to ascertain whether means are reasonably likely
to be used to identify the natural person, account should be taken of all objective
factors, such as the costs of and the amount of time required for identication,
taking into consideration the available technology at the time of the processing
and technological developments. the principles of data protection should there-
fore not apply to anonymous information, namely information which does not
relate to an identied or identiable natural person or to personal data rendered
anonymous in such a manner that the data subject is not or no longer identi-
able." organizations that violate the the gdpr can be ned to up to 20 million
euro or up to 4 percent of the annual worldwide turnover. the regulations have
been criticized for the administrative overhead they generate and the unclear
compliance requirements.
there are many techniques to anonymize data, e.g., data masking, generaliza-
tion, pseudonymization, swapping, perturbation, and sampling. the problem is
that also anonymized data may unintentionally reveal sensitive information, e.g.,
by combining results and background information. there are also more advanced
approaches using (homomorphic) encryption or secure multi-party computation.
unfortunately, legislation and policy makers are lagging behind. therefore, con-
dentiality concerns may lead to inaccurate, non-optimal, and even unfair decision
making [ ?].
finally, data science techniques need to ensure transparency : it should be
clear how the data were processed and that the stakeholders correctly under-
stand the results. often results highly depend on data cleaning, selection, and
parameters of the techniques used. it is easy to change the outcome by altering
the analysis-pipeline. moreover, users that do not understand the phenomena14 wil van der aalst
described in section ??will have diculties interpreting data science results
accurately. black-box machine learning techniques such as deep neural nets pro-
vide results (e.g., decisions) without explanation. in many application domains
this is unacceptable (e.g., sending people to prison, rejecting job applications,
or medical decisions). explainable ai refers to methods and techniques enhanc-
ing articial intelligence such that the results of the solution can be understood
by human experts. however, explanations may be very complex and full trans-
parency may not always be possible or required.
the above challenges show that there is a need for data science techniques
that are responsible (i.e., \green") by design. this will be one of the main data
science challenges of the coming years.
6 conclusion
data science has become one of the main ingredients of information processing
and is changing the way we use and think about it. data science includes data
extraction, data preparation, data exploration, data transformation, storage and
retrieval, computing infrastructures, various types of mining and learning, and
the presentation of explanations and predictions. moreover, given the impact of
data science, also ethical, social, legal, and business aspects play a major role.
the multidisciplinary nature makes data science particularly challenging.
in this paper, we used the classical four elements (\water", \re", \wind",
and \earth") as metaphors for the essential ingredients of data science. more-
over, we zoomed in on the growing importance of learning from data (rather
than modeling or programming) and the transfer of tasks from humans to (soft-
ware) robots. we also elaborated on the risks associated with data science, e.g.,
the dominance of digital platforms and concerns related to fairness, accuracy,
condentiality, and transparency.
in 1943, ibm's president, thomas j watson, apparently said: \i think there
is a world market for about ve computers." up until the 1950-ties many people
thought that a handful of computers per country would suce. this illustrates
that, in 1960, the year that international federation for information process-
ing (ifip) was established, it was impossible to foresee the importance of data
science. however, data science has become the main driver of transformation in
society and business, and is likely to remain a key topic for ifip in the next 60
years.
acknowledgments : we thank the alexander von humboldt (avh) stiftung
for supporting our research.
references
1. w.m.p. van der aalst. data scientist: the engineer of the future. in k. mertins,
f. benaben, r. poler, and j. bourrieres, editors, proceedings of the i-esa confer-
ence, volume 7 of enterprise interoperability , pages 13{28. springer-verlag, berlin,
2014.the data science revolution 15
2. w.m.p. van der aalst. process mining: data science in action . springer-verlag,
berlin, 2016.
3. w.m.p. van der aalst. responsible data science: using big data in a \people
friendly" manner. in s. hammoudi, l. maciaszek, m. missiko, o. camp, and
j. cordiero, editors, enterprise information systems , volume 291 of lecture notes
in business information processing , pages 3{28. springer-verlag, berlin, 2017.
4. w.m.p. van der aalst. responsible data science in a dynamic world: the four
essential elements of data science. in l. strous and v.g. cerf, editors, internet
of things - information processing in an increasingly connected world , volume
548 of ifip advances in information and communication technology , pages 3{10.
springer-verlag, berlin, 2018.
5. w.m.p. van der aalst, m. bichler, and a. heinzl. robotic process automation.
business and information systems engineering , 60(4):269{272, 2018.
6. w.m.p. van der aalst, o. hinz, and c. weinhardt. big digital platforms -
growth, impact, and challenges. business and information systems engineer-
ing, 61(6):645{648, 2019.
7. council of the european union. general data protection regulation (gdpr).
regulation (eu) 2016/679 of the european parliament and of the council of 27
april 2016 on the protection of natural persons with regard to the processing of
personal data and on the free movement of such data, and repealing directive
95/46/ec, april 2016.
8. european commission. proposal for a regulation of the european parliament and
of the council on the protection of individuals with regard to the processing of
personal data and on the free movement of such data (general data protection
regulation). 9565/15, 2012/0011 (cod), june 2015.
9. c.b. frey and m.a. osborne. the future of employment: how susceptible are jobs
to computerisation? technological forecasting and social change , 114(c):254{280,
2017.
10. j. hawksworth, r. berriman, and s. goel. will robots really steal our jobs?
an international analysis of the potential long term impact of automation.
technical report, pricewaterhousecoopers, 2018.
11. j.p.a. ioannidis. why most published research findings are false. plos
medicine , 2:e124, 2005.
12. d. laney. 3d data management: controlling data volume, velocity, and variety
(research note 949). technical report, meta group, february 2001.
13. y. lecun, y. bengio, and g. hinton. deep learning. nature , 521:436{444, 2015.
14. t.a. leopold, v. ratcheva, and s. zahidi. the future of jobs report. technical
report, centre for the new economy and society, world economic forum, 2018.
15. p. naur. concise survey of computer methods . studentlitteratur lund, akademisk
forlag, kobenhaven, 1974.
16. c. o'neil. weapons of math destruction: how big data increases inequality and
threatens democracy . crown publishing group, new york, 2016.
17. d.e. rumelhart, g. hinton, and r.j. williams. learning representations by back-
propagating errors. nature , 323:533{536, 1986.
18. l. strous and v.g. cerf, editors. internet of things - information processing in
an increasingly connected world , volume 548 of ifip advances in information
and communication technology . springer-verlag, berlin, 2018.