1
process mining manifesto
wil van der aalst1;2?, arya adriansyah1, ana karla alves de medeiros50,
franco arcieri26, thomas baier11;53, tobias blickle6, jagadeesh chandra
bose1, peter van den brand4, ronald brandtjen7, joos buijs1, andrea
burattin28, josep carmona29, malu castellanos8, jan claes45, jonathan
cook30, nicola costantini21, francisco curbera9, ernesto damiani27,
massimiliano de leoni1, pavlos delias51, boudewijn van dongen1, marlon
dumas44, schahram dustdar46, dirk fahland1, diogo r. ferreira31, walid
gaaloul49, frank van geen24, sukriti goel12, christian g unther5, antonella
guzzo32, paul harmon17, arthur ter hofstede2;1, john hoogland3, jon espen
ingvaldsen14, koki kato10, rudolf kuhn7, akhil kumar33, marcello la rosa2,
fabrizio maggi1, donato malerba34, ronny mans1, alberto manuel20, martin
mccreesh15, paola mello38, jan mendling35, marco montali52, hamid
motahari nezhad8, michael zur muehlen36, jorge munoz-gama29, luigi
pontieri25, joel ribeiro1, anne rozinat5, hugo seguel p erez23, ricardo seguel
p erez22, marcos sep ulveda47, jim sinur18, pnina soer37, minseok song39,
alessandro sperduti28, giovanni stilo26, casper stoel3, keith swenson13,
maurizio talamo26, wei tan9, chris turner40, jan vanthienen41, george
varvaressos16, eric verbeek1, marc verdonk19, roberto vigo21, jianmin
wang42, barbara weber43, matthias weidlich48, ton weijters1, lijie wen42,
michael westergaard1, and moe wynn2
1eindhoven university of technology, the netherlands
2queensland university of technology, australia
3pallas athena, the netherlands
4futura process intelligence, the netherlands
5fluxicon, the netherlands
6software ag, germany
7processgold ag, germany
8hp laboratories, usa
9ibm t.j. watson research center, usa
10fujitsu laboratories ltd., japan
11bwi systeme gmbh, germany
12infosys technologies ltd, india
13fujitsu america inc., usa
14fourspark, norway
15iontas/verint, usa
16business process mining, australia
17business process trends, usa
18gartner, usa
19deloitte innovation, the netherlands
20process sphere, portugal
21siav spa, italy
22bpm chile, chile
23excellentia bpm, chile
24rabobank, the netherlands25icar-cnr, italy
26university of rome \tor vergata", italy
27universit a degli studi di milano, italy
28university of padua, italy
29universitat polit ecnica de catalunya, spain
30new mexico state university, usa
31ist - technical university of lisbon, portugal
32university of calabria, italy
33penn state university, usa
34university of bari, italy
35vienna university of economics and business, austria
36stevens institute of technology, usa
37university of haifa, israel
38university of bologna, italy
39ulsan national institute of science and technology, korea
40craneld university, uk
41k.u. leuven, belgium
42tsinghua university, china
43university of innsbruck, austria
44university of tartu, estonia
45ghent university, belgium
46technical university of vienna, austria
47ponticia universidad cat olica de chile, chile
48hasso plattner institute, germany
49telecom sudparis, france
50capgemini consulting, the netherlands
51kavala institute of technology, greece
52free university of bozen-bolzano, italy
53humboldt-universit at zu berlin
summary. process mining techniques are able to extract knowledge
from event logs commonly available in today's information systems.
these techniques provide new means to discover, monitor, and improve
processes in a variety of application domains. there are two main drivers
for the growing interest in process mining. on the one hand, more and
more events are being recorded, thus, providing detailed information
about the history of processes. on the other hand, there is a need to im-
prove and support business processes in competitive and rapidly chang-
ing environments. this manifesto is created by the ieee task force
on process mining and aims to promote the topic of process mining.
moreover, by dening a set of guiding principles and listing important
challenges, this manifesto hopes to serve as a guide for software develop-
ers,scientists ,consultants ,business managers , and end-users . the goal
is to increase the maturity of process mining as a new tool to improve
the (re)design, control, and support of operational business processes.
?corresponding author, e-mail: w.m.p.v.d.aalst@tue.nl .process mining manifesto 3
1 ieee task force on process mining
amanifesto is a \public declaration of principles and intentions" by a group
of people. this manifesto is written by members and supporters of the ieee
task force on process mining . the goal of this task force is to promote the
research, development, education, implementation, evolution, and understanding
of process mining.
register 
requestexamine 
casuallyexamine 
thoroughly
check ticketdecidepay 
compensation
reject 
request
reinitiate 
requeststartend
performance information (e.g., the average time 
between two subsequent activities) can be extracted 
from the event log and visualized on top of the model.a
aaa
am
mpete
mike
ellenrole a:
assistant
sue
seanrole e:
expert
sararole m:
manager
decision rules (e.g., a decision 
tree based on data known at 
the time a particular choice 
was made) can be learned 
from the event log and used to 
annotate decisions.the event log can be 
used to discover roles in 
the organization (e.g., 
groups of people with 
similar work patterns). 
these roles can be used 
to relate individuals and 
activities.
e
discovery techniques can be used to find a control-
flow model (in this case in terms of a bpmn model) 
that describes the observed behavior best.
starting point is an event log. each event refers to a 
process instance (case) and an activity. events are 
ordered and additional properties (e.g. timestamp or 
resource data) may be present.
fig. 1. process mining techniques extract knowledge from event logs in order to dis-
cover, monitor and improve processes [1].4 wil van der aalst et al.
process mining is a relatively young research discipline that sits between
computational intelligence and data mining on the one hand, and process mod-
eling and analysis on the other hand. the idea of process mining is to discover,
monitor and improve real processes (i.e., not assumed processes) by extracting
knowledge from event logs readily available in today's (information) systems (see
fig. 1). process mining includes (automated) process discovery (i.e., extracting
process models from an event log), conformance checking (i.e., monitoring de-
viations by comparing model and log), social network/organizational mining,
automated construction of simulation models, model extension, model repair,
case prediction, and history-based recommendations.
process mining provides an important bridge between data mining and busi-
ness process modeling and analysis. under the business intelligence (bi) um-
brella many buzzwords have been introduced to refer to rather simple reporting
and dashboard tools. business activity monitoring (bam) refers to technologies
enabling the real-time monitoring of business processes. complex event process-
ing(cep) refers to technologies to process large amounts of events, utilizing
them to monitor, steer and optimize the business in real time. corporate perfor-
mance management (cpm) is another buzzword for measuring the performance
of a process or organization. also related are management approaches such as
continuous process improvement (cpi), business process improvement (bpi),
total quality management (tqm), and six sigma . these approaches have in
common that processes are \put under a microscope" to see whether further
improvements are possible. process mining is an enabling technology for cpm,
bpi, tqm, six sigma, and the like.
whereas bi tools and management approaches such as six sigma and tqm
aim to improve operational performance, e.g., reducing ow time and defects,
organizations are also putting more emphasis on corporate governance ,risks, and
compliance . legislations such as the sarbanes-oxley act (sox) and the basel
ii accord illustrate the focus on compliance issues. process mining techniques
oer a means to more rigorously check compliance and ascertain the validity and
reliability of information about an organization's core processes.
over the last decade, event data have become readily available and pro-
cess mining techniques have matured. moreover, as just mentioned, manage-
ment trends related to process improvement (e.g., six sigma, tqm, cpi, and
cpm) and compliance (sox, bam, etc.) can benet from process mining. for-
tunately, process mining algorithms have been implemented in various academic
and commercial systems. today, there is an active group of researchers working
on process mining and it has become one of the \hot topics" in business process
management (bpm) research. moreover, there is a huge interest from industry in
process mining. more and more software vendors are adding process mining func-
tionality to their tools. examples of software products with process mining ca-
pabilities are: aris process performance manager (software ag), comprehend
(open connect), discovery analyst (stereologic), flow (fourspark), futura
reect (futura process intelligence), interstage automated process discovery
(fujitsu), okt process mining suite (exeura), process discovery focus (ion-process mining manifesto 5
tas/verint), processanalyzer (qpr), prom (tu/e), rbminer/dbminer (upc),
and reectjone (pallas athena). the growing interest in log-based process anal-
ysis motivated the establishment of a task force on process mining.
the task force was established in 2009 in the context of the data mining
technical committee (dmtc) of the computational intelligence society (cis)
of the institute of electrical and electronic engineers (ieee). the current task
force has members representing software vendors (e.g., pallas athena, software
ag, futura process intelligence, hp, ibm, infosys, fluxicon, businesscape,
iontas/verint, fujitsu, fujitsu laboratories, business process mining, stereo-
logic), consultancy rms/end users (e.g., processgold, business process trends,
gartner, deloitte, process sphere, siav spa, bpm chili, bwi systeme gmbh,
excellentia bpm, rabobank), and research institutes (e.g., tu/e, university
of padua, universitat polit ecnica de catalunya, new mexico state university,
technical university of lisbon, university of calabria, penn state university,
university of bari, humboldt-universit at zu berlin, queensland university of
technology, vienna university of economics and business, stevens institute of
technology, university of haifa, university of bologna, ulsan national insti-
tute of science and technology, craneld university, k.u. leuven, tsinghua
university, university of innsbruck, university of tartu).
concrete objectives of the task force are:
{ to make end-users, developers, consultants, business managers, and researchers
aware of the state-of-the-art in process mining,
{ to promote the use of process mining techniques and tools and stimulate new
applications,
{ to play a role in standardization eorts for logging event data,
{ to organize tutorials, special sessions, workshops, panels, and
{ to publish articles, books, videos, and special issues of journals.
since its establishment in 2009 there have been various activities related to the
above objectives. for example, several workshops and special tracks were (co-)
organized by the task force, e.g., the workshops on business process intelli-
gence (bpi'09, bpi'10, and bpi'11) and special tracks at main ieee confer-
ences (e.g. cidm'11). knowledge was disseminated via tutorials (e.g. wcci'10
and pmpm'09), summer schools (esscass'09, acpn'10, cich'10, etc.), videos
(cf.www.processmining.org ), and several publications including the rst book
on process mining recently published by springer [1]. the task force also
(co-)organized the rst business process intelligence challenge (bpic'11): a
competition where participants had to extract meaningful knowledge from a
large and complex event log. in 2010, the task force also standardized xes
(www.xes-standard.org ), a standard logging format that is extensible and sup-
ported by the openxes library (www.openxes.org ) and by tools such as prom,
xesame, nitro, etc.
the reader is invited to visit http://www.win.tue.nl/ieeetfpm/ for more
information about the activities of the task force.6 wil van der aalst et al.
2 process mining: state of the art
the expanding capabilities of information systems and other systems that de-
pend on computing, are well characterized by moore's law. gordon moore, the
co-founder of intel, predicted in 1965 that the number of components in inte-
grated circuits would double every year. during the last fty years the growth has
indeed been exponential, albeit at a slightly slower pace. these advancements
resulted in a spectacular growth of the \digital universe" (i.e., all data stored
and/or exchanged electronically). moreover, the digital and the real universe
continue to become more and more aligned.
the growth of a digital universe that is well-aligned with processes in orga-
nizations makes it possible to record and analyze events . events may range from
the withdrawal of cash from an atm, a doctor adjusting an x-ray machine, a
citizen applying for a driver license, the submission of a tax declaration, and
the receipt of an e-ticket number by a traveler. the challenge is to exploit event
data in a meaningful way, for example, to provide insights, identify bottlenecks,
anticipate problems, record policy violations, recommend countermeasures, and
streamline processes. process mining aims to do exactly that.
starting point for process mining is an event log . all process mining tech-
niques assume that it is possible to sequentially record events such that each
event refers to an activity (i.e., a well-dened step in some process) and is related
to a particular case (i.e., a process instance). event logs may store additional
information about events. in fact, whenever possible, process mining techniques
use extra information such as the resource (i.e., person or device) executing or
initiating the activity, the timestamp of the event, or data elements recorded
with the event (e.g., the size of an order).
software 
system
(process)
modelevent
logsmodels
analyzes
discoveryrecords 
events, e.g., 
messages, 
transactions, 
etc.specifies 
configures 
implements
analyzessupports/
controls
enhancementconformance“world”
peoplemachines
organizationscomponentsbusiness
processes
fig. 2. positioning of the three main types of process mining: (a) discovery , (b) con-
formance checking, and (c) enhancement [1].process mining manifesto 7
as shown in fig. 2, event logs can be used to conduct three types of pro-
cess mining. the rst type of process mining is discovery . a discovery technique
takes an event log and produces a model without using any a-priori information.
process discovery is the most prominent process mining technique. for many
organizations it is surprising to see that existing techniques are indeed able to
discover real processes merely based on example executions in event logs. the
second type of process mining is conformance . here, an existing process model
is compared with an event log of the same process. conformance checking can
be used to check if reality, as recorded in the log, conforms to the model and
vice versa. note that dierent types of models can be considered: conformance
checking can be applied to procedural models, organizational models, declarative
process models, business rules/policies, laws, etc. the third type of process min-
ing is enhancement . here, the idea is to extend or improve an existing process
model using information about the actual process recorded in some event log.
whereas conformance checking measures the alignment between model and real-
ity, this third type of process mining aims at changing or extending the a-priori
model. for instance, by using timestamps in the event log one can extend the
model to show bottlenecks, service levels, throughput times, and frequencies.
event log
modelconformance
checkingdiagnosticsevent log discovery model
event log
modelenhancement new model(a)
(b)
(c)
fig. 3. the three basic types of process mining explained in terms of input and output:
(a) discovery, (b) conformance checking, and (c) enhancement.
figure 3 describes the three types of process mining in terms of input and
output. techniques for discovery take an event log and produce a model. the
discovered model is typically a process model (e.g., a petri net, bpmn, epc, or
uml activity diagram), however, the model may also describe other perspectives
(e.g., a social network). conformance checking techniques need an event log and a
model as input. the output consists of diagnostic information showing dierences
and commonalities between model and log. techniques for model enhancement
(repair or extension) also need an event log and a model as input. the output
is an improved or extended model.
process mining may cover dierent perspectives. the control-ow perspective
focuses on the control-ow, i.e., the ordering of activities. the goal of mining this
perspective is to nd a good characterization of all possible paths. the result
is typically expressed in terms of a petri net or some other process notation8 wil van der aalst et al.
(e.g., epcs, bpmn, or uml activity diagrams). the organizational perspective
focuses on information about resources hidden in the log, i.e., which actors (e.g.,
people, systems, roles, or departments) are involved and how are they related.
the goal is to either structure the organization by classifying people in terms of
roles and organizational units or to show the social network. the case perspective
focuses on properties of cases. obviously, a case can be characterized by its
path in the process or by the actors working on it. however, cases can also be
characterized by the values of the corresponding data elements. for example, if a
case represents a replenishment order, it may be interesting to know the supplier
or the number of products ordered. the time perspective is concerned with the
timing and frequency of events. when events bear timestamps it is possible to
discover bottlenecks, measure service levels, monitor the utilization of resources,
and predict the remaining processing time of running cases.
there are some common misconceptions related to process mining. some ven-
dors, analysts, and researchers limit the scope of process mining to a special data
mining technique for process discovery that can only be used for oine analysis.
this is notthe case, therefore, we emphasize the following three characteristics.
{process mining is not limited to control-ow discovery. the discovery of pro-
cess models from event logs fuels the imagination of both practitioners and
academics. therefore, control-ow discovery is often seen as the most exciting
part of process mining. however, process mining is not limited to control-ow
discovery. on the one hand, discovery is just one of the three basic forms
of process mining (discovery, conformance, and enhancement). on the other
hand, the scope is not limited to control-ow; the organizational, case and
time perspectives also play an important role.
{process mining is not just a specic type of data mining. process mining can be
seen as the \missing link" between data mining and traditional model-driven
bpm. most data mining techniques are not process-centric at all. process mod-
els potentially exhibiting concurrency are incomparable to simple data mining
structures such as decision trees and association rules. therefore, completely
new types of representations and algorithms are needed.
{process mining is not limited to oine analysis. process mining techniques
extract knowledge from historical event data. although \post mortem" data is
used, the results can be applied to running cases. for example, the completion
time of a partially handled customer order can be predicted using a discovered
process model.
to position process mining, we use the business process management (bpm)
life-cycle shown in fig. 4. the bpm life-cycle shows seven phases of a business
process and its corresponding information system(s). in the (re)design phase a
new process model is created or an existing process model is adapted. in the
analysis phase a candidate model and its alternatives are analyzed. after the
(re)design phase, the model is implemented ( implementation phase ) or an exist-
ing system is (re)congured ( reconguration phase ). in the execution phase the
designed model is enacted. during the execution phase the process is monitored .
moreover, smaller adjustments may be made without redesigning the processprocess mining manifesto 9
(re)design
implementation (re)configurationexecutionadjustmentdiagnosis
analysis
fig. 4. the bpm life-cycle identifying the various phases of a business process and its
corresponding information system(s); process mining (potentially) plays a role in all
phases (except for the implementation phase).
(adjustment phase ). in the diagnosis phase the enacted process is analyzed and
the output of this phase may trigger a new process redesign phase. process min-
ing is a valuable tool for most of the phases shown in fig. 4. obviously, the
diagnosis phase can benet from process mining. however, process mining is
not limited to the diagnosis phase. for example, in the execution phase, process
mining techniques can be used for operational support . predictions and recom-
mendations based on models learned using historic information can be used to
inuence running cases. similar forms of decision support can be used to adjust
processes and to guide process (re)conguration.
whereas fig. 4 shows the overall bpm life-cycle, fig. 5 focuses on the con-
crete process mining activities and artifacts. figure 5 describes the possible
stages in a process mining project. any process mining project starts with a plan-
ning and a justication for this planning (stage 0). after initiating the project,
event data, models, objectives, and questions need to be extracted from systems,
domain experts, and management (stage 1). this requires an understanding of
the available data (\what can be used for analysis?") and an understanding
of the domain (\what are the important questions?") and results in the arti-
facts shown in fig. 5 (i.e., historical data, handmade models, objectives, and
questions). in stage 2 the control-ow model is constructed and linked to the
event log. here automated process discovery techniques can be used. the discov-
ered process model may already provide answers to some of the questions and
trigger redesign or adjustment actions. moreover, the event log may be ltered
or adapted using the model (e.g., removing rare activities or outlier cases, and
inserting missing events). sometimes signicant eorts are needed to correlate
events belonging to the same process instance. the remaining events are related
to entities of the process model. when the process is relatively structured, the
control-ow model may be extended with other perspectives (e.g., data, time,
and resources) during stage 3. the relation between the event log and the model
established in stage 2 is used to extend the model (e.g., timestamps of associated
events are used to estimate waiting times for activities). this may be used to10 wil van der aalst et al.
stage 0: plan and justify
stage 2: create control-flow model 
and connect event logstage 1: extract
historical 
datahandmade 
modelsobjectives 
(kpis)questions
event log control-flow model
stage 3: create integrated process 
model
event log process modelunderstanding of the 
available dataunderstanding of the 
domain
stage 4: operational support
interpret
current dataredesign
adjust
intervene
support
fig. 5. thellife-cycle model describing a process mining project consisting of ve
stages: plan and justify (stage 0), extract (stage 1), create a control-ow model and
connect it to the event log (stage 2), create an integrated process model (stage 3), and
provide operational support (stage 4) [1].
answer additional questions and may trigger additional actions. ultimately, the
models constructed in stage 3 may be used for operational support (stage 4).
knowledge extracted from historical event data is combined with information
about running cases. this may be used to intervene, predict, and recommend.
stages 3 and 4 can only be reached if the process is suciently stable and struc-
tured.
currently, there are techniques and tools that can support all stages shown
in fig. 5. however, process mining is a relatively new paradigm and most of
the currently available tools are still rather immature. moreover, prospective
users are often not aware of the potential and the limitations of process min-
ing. therefore, this manifesto catalogs some guiding principles (cf. section 3)process mining manifesto 11
and challenges (cf. section 4) for users of process mining techniques as well as
researchers and developers that are interested in advancing the state-of-the-art.
3 guiding principles
as with any new technology, there are obvious mistakes that can be made when
applying process mining in real-life settings. therefore, we list six guiding prin-
ciples to prevent users/analysts from making such mistakes.
3.1 gp1: event data should be treated as first-class citizens
starting point for any process mining activity are the events recorded. we refer
to collections of events as event logs , however, this does not imply that events
need to be stored in dedicated log les. events may be stored in database ta-
bles, message logs, mail archives, transaction logs, and other data sources. more
important than the storage format, is the quality of such event logs. the qual-
ity of a process mining result heavily depends on the input. therefore, event
logs should be treated as rst-class citizens in the information systems sup-
porting the processes to be analyzed. unfortunately, event logs are often merely
a \by-product" used for debugging or proling. for example, the medical de-
vices of philips healthcare record events simply because software developers
have inserted \print statements" in the code. although there are some informal
guidelines for adding such statements to the code, a more systematic approach
is needed to improve the quality of event logs. event data should be viewed as
rst-class citizens (rather than second-class citizens).
there are several criteria to judge the quality of event data. events should be
trustworthy , i.e., it should be safe to assume that the recorded events actually
happened and that the attributes of events are correct. event logs should be
complete , i.e., given a particular scope, no events may be missing. any recorded
event should have well-dened semantics . moreover, the event data should be
safein the sense that privacy and security concerns are addressed when recording
the events. for example, actors should be aware of the kind of events being
recorded and the way they are used.
table 1 denes ve event log maturity levels ranging from excellent quality
(????? ) to poor quality ( ?). for example, the event logs of philips healthcare
reside at level ??? , i.e., events are recorded automatically and the recorded
behavior matches reality, but no systematic approach is used to assign semantics
to events and to ensure coverage at a particular level. process mining techniques
can be applied to logs at levels ????? ,???? and???. in principle, it is also
possible to apply process mining using event logs at level ??or?. however, the
analysis of such logs is typically problematic and the results are not trustworthy.
in fact, it does not make much sense to apply process mining to logs at level ?.
in order to benet from process mining, organizations should aim at event
logs at the highest possible quality level.12 wil van der aalst et al.
table 1. maturity levels for event logs.
level characterization
? ? ? ? ? highest level: the event log is of excellent quality (i.e., trustworthy
and complete) and events are well-dened. events are recorded in an
automatic, systematic, reliable, and safe manner. privacy and security
considerations are addressed adequately. moreover, the events recorded
(and all of their attributes) have clear semantics. this implies the ex-
istence of one or more ontologies. events and their attributes point to
this ontology.
example: semantically annotated logs of bpm systems.
? ? ?? events are recorded automatically and in a systematic and reliable
manner, i.e., logs are trustworthy and complete. unlike the systems
operating at level ? ? ? , notions such as process instance (case) and
activity are supported in an explicit manner.
example: the events logs of traditional bpm/workow systems.
? ? ? events are recorded automatically, but no systematic approach is fol-
lowed to record events. however, unlike logs at level ??, there is some
level of guarantee that the events recorded match reality (i.e., the event
log is trustworthy but not necessarily complete). consider, for exam-
ple, the events recorded by an erp system. although events need to
be extracted from a variety of tables, the information can be assumed
to be correct (e.g., it is safe to assume that a payment recorded by the
erp actually exists and vice versa).
examples: tables in erp systems, events logs of crm systems, trans-
action logs of messaging systems, event logs of high-tech systems, etc.
?? events are recorded automatically, i.e., as a by-product of some infor-
mation system. coverage varies, i.e., no systematic approach is followed
to decide which events are recorded. moreover, it is possible to bypass
the information system. hence, events may be missing or not recorded
properly.
examples: event logs of document and product management systems,
error logs of embedded systems, worksheets of service engineers, etc.
? lowest level: event logs are of poor quality. recorded events may not
correspond to reality and events may be missing. event logs for which
events are recorded by hand typically have such characteristics.
examples: trails left in paper documents routed through the organiza-
tion (\yellow notes"), paper-based medical records, etc.
3.2 gp2: log extraction should be driven by questions
as shown in fig. 5, process mining activities need to be driven by questions.
without concrete questions it is very dicult to extract meaningful event data.
consider, for example, the thousands of tables in the database of an erp system
like sap. without concrete questions it is impossible to select the tables relevant
for data extraction.
a process model such as the one shown in fig. 1 describes the life-cycle of
cases (i.e., process instances) of a particular type. hence, before applying any
process mining technique one needs to choose the type of cases to be analyzed.process mining manifesto 13
this choice should be driven by the questions that need to be answered and
this may be non-trivial. consider, for example, the handling of customer orders.
each customer order may consist of multiple order lines as the customer may
order multiple products in one order. one customer order may result in multiple
deliveries. one delivery may refer to order lines of multiple orders. hence, there
is a many-to-many relationship between orders and deliveries and a one-to-many
relationship between orders and order lines. given a database with event data
related to orders, order lines, and deliveries, there are dierent process models
that can be discovered. one can extract data with the goal to describe the
life-cycle of individual orders. however, it is also possible to extract data with
the goal to discover the life-cycle of individual order lines or the life-cycle of
individual deliveries.
3.3 gp3: concurrency, choice and other basic control-flow
constructs should be supported
a plethora of process modeling languages exists (e.g., bpmn, epcs, petri nets,
bpel, and uml activity diagrams). some of these languages provide many
modeling elements (e.g., bpmn oers more than 50 distinct graphical elements)
whereas others are very basic (e.g., petri nets are composed of only three dierent
elements: places, transitions, and arcs). the control-ow description is the back-
bone of any process model. basic workow constructs (also known as patterns )
supported by all mainstream languages are sequence, parallel routing (and-
splits/joins), choice (xor-splits/joins), and loops. obviously, these patterns
should be supported by process mining techniques. however, some techniques
are not able to deal with concurrency and support only markov chains/transition
systems.
figure 6 shows the eect of using process mining techniques unable to dis-
cover concurrency (no and-split/joins). consider an event log l=fha;b;
c;d;ei;ha;b;d;c;ei;ha;c;b;d;ei;ha;c;d;b;ei;ha;d;b;c;ei;ha;d;
c;b;eig.lcontains cases that start with aand end with e. activities b,c,
anddoccur in any order in-between aande. the bpmn model in fig. 6(a)
shows a compact representation of the underlying process using two and gate-
ways. suppose that the process mining technique does not support and gate-
ways. in this case, the other two bpmn models in fig. 6 are obvious candidates.
the bpmn model in fig. 6(b) is compact but allows for too much behavior
(e.g., cases such as ha;b;b;b;eiare possible according to the model but are
not likely according to the event log). the bpmn model in fig. 6(c) allows for
the cases in l, but encodes all sequences explicitly, so it is not a compact repre-
sentation of the log. the example shows that for real-life models having dozens
of potentially concurrent activities the resulting models are severely undertting
(i.e., allow for too much behavior) and/or extremely complex if concurrency is
not supported.
as is illustrated by fig. 6, it is important to support at least the basic work-
ow patterns. besides the basic patterns mentioned it is also desirable to support14 wil van der aalst et al.
a c
db
e
a c
db
e
a c
dbc
d
b
d
b
cc
bd
e(a)  b, c, and d can be executed in any order
(b)    b, c, and d can be executed in any order but also multiple times
(c)  b, c, and d can be executed in any order, but activities need to be duplicated to model all observed sequences.
fig. 6. example illustrating problems when concurrency (i.e., and-splits/joins) can-
not be expressed directly. in the example just three activities ( b,c, and d) are con-
current. imagine the resulting process models when there are 10 concurrent activities
(210= 1024 states and 10! = 3 ;628;800 possible execution sequences).
or-splits/joins, because these provide a compact representation of inclusive de-
cisions and partial synchronizations.
3.4 gp4: events should be related to model elements
as indicated in section 2, it is a misconception that process mining is limited
to control-ow discovery. as shown in fig. 1, the discovered process model may
cover various perspectives (organizational perspective, time perspective, data
perspective, etc.). moreover, discovery is just one of the three types of process
mining shown in fig. 3. the other two types of process mining (conformance
checking and enhancement) heavily rely on the relationship between elements in
the model and events in the log . this relationship may be used to \replay" the
event log on the model. replay may be used to reveal discrepancies between an
event log and a model, e.g., some events in the log are not possible according
to the model. techniques for conformance checking quantify and diagnose such
discrepancies. timestamps in the event log can be used to analyze the temporalprocess mining manifesto 15
behavior during replay. time dierences between causally related activities can
be used to add expected waiting times to the model. these examples show that
the relation between events in the log and elements in the model serves as a
starting point for dierent types of analysis.
in some cases it may be non-trivial to establish such a relationship. for ex-
ample, an event may refer to two dierent activities or it is unclear to which
activity it refers. such ambiguities need to be removed in order to interpret pro-
cess mining results properly. besides the problem of relating events to activities,
there is the problem of relating events to process instances. this is commonly
referred to as event correlation .
3.5 gp5: models should be treated as purposeful abstractions of
reality
models derived from event data provide views on reality . such a view should
provide a purposeful abstraction of the behavior captured in the event log. given
an event log, there may be multiple views that are useful. moreover, the various
stakeholders may require dierent views. in fact, models discovered from event
logs should be seen as \maps" (like geographic maps). this guiding principle
provides important insights, two of which are described in the remainder.
first of all, it is important to note that there is no such thing as \the map" for
a particular geographic area. depending on the intended use there are dierent
maps: road maps, hiking maps, cycling maps, etc. all of these maps show a view
on the same reality and it would be absurd to assume that there would be such a
thing as \the perfect map". the same holds for process models: the model should
emphasize the things relevant for a particular type of user. discovered models
may focus on dierent perspectives (control-ow, data ow, time, resources,
costs, etc.) and show these at dierent levels of granularity and precision, e.g.,
a manager may want to see a coarse informal process model focusing on costs
whereas a process analyst may want to see a detailed process model focusing
on deviations from the normal ow. also note that dierent stakeholders may
want to view a process at dierent levels: strategic level (decisions at this level
have long-term eects and are based on aggregate event data over a longer
period), tactical level (decisions at this level have medium-term eects and are
mostly based on recent data), and operational level (decisions at this level have
immediate eects and are based on event data related to running cases).
second, it is useful to adopt ideas from cartography when it comes to produc-
ing understandable maps. for example, road maps abstract from less signicant
roads and cities. less signicant things are either left out or dynamically clus-
tered into aggregate shapes (e.g., streets and suburbs amalgamate into cities).
cartographers not only eliminate irrelevant details, but also use colors to high-
light important features. moreover, graphical elements have a particular size to
indicate their signicance (e.g., the sizes of lines and dots may vary). geographi-
cal maps also have a clear interpretation of the x-axis andy-axis, i.e., the layout
of a map is not arbitrary as the coordinates of elements have a meaning. all of
this is in stark contrast with mainstream process models which are typically not16 wil van der aalst et al.
using color, size, and location features to make models more understandable.
however, ideas from cartography can easily be incorporated in the construction
of discovered process maps. for example, the size of an activity can be used to
reect its frequency or some other property indicating its signicance (e.g., costs
or resource use). the width of an arc can reect the importance of the corre-
sponding causal dependency, and the coloring of arcs can be used to highlight
bottlenecks.
the above observations show that it is important to select the right rep-
resentation and ne-tune it for the intended audience. this is important for
visualizing results to end users and for guiding discovery algorithms towards
suitable models (see also challenge c5).
3.6 gp6: process mining should be a continuous process
process mining can help to provide meaningful \maps" that are directly con-
nected to event data. both historical event data and current data can be pro-
jected onto such models. moreover, processes change while they are being ana-
lyzed. given the dynamic nature of processes, it is not advisable to see process
mining as a one-time activity. the goal should not be to create a xed model,
but to breathe life into process models so that users and analysts are encouraged
to look at them on a daily basis.
compare this to the use of mashups using geo-tagging. there are thousands
of mashups using google maps (e.g., applications projecting information about
trac conditions, real estate, fastfood restaurants, or movie showtimes onto
a selected map). people can seamlessly zoom in and out using such maps and
interact with them (e.g., trac jams are projected onto the map and the user can
select a particular problem to see details). it should also be possible to conduct
process mining based on real-time event data. using the \map metaphor", we
can think of events having gps coordinates that can be projected on maps in real
time. analogous to car navigation systems, process mining tools can help end
users (a) by navigating through processes, (b) by projecting dynamic information
onto process maps (e.g., showing \trac jams" in business processes), and (c)
by providing predictions regarding running cases (e.g., estimating the \arrival
time" of a case that is delayed). these examples demonstrate that it is a pity
to not use process models more actively. therefore, process mining should be
viewed as a continuous process providing actionable information according to
various time scales (minutes, hours, days, weeks, and months).
4 challenges
process mining is an important tool for modern organizations that need to man-
age non-trivial operational processes. on the one hand, there is an incredible
growth of event data. on the other hand, processes and information need to be
aligned perfectly in order to meet requirements related to compliance, eciency,process mining manifesto 17
and customer service. despite the applicability of process mining there are still
important challenges that need to be addressed; these illustrate that process
mining is an emerging discipline. in the remainder, we list some of these chal-
lenges. this list is not intended to be complete and, over time, new challenges
may emerge or existing challenges may disappear due to advances in process
mining.
4.1 c1: finding, merging, and cleaning event data
it still takes considerable eorts to extract event data suitable for process mining.
typically, several hurdles need to be overcome:
{ data may be distributed over a variety of sources. this information needs to
be merged. this tends to be problematic when dierent identiers are used in
the dierent data sources. for example, one system uses name and birthdate
to identify a person whereas another system uses the person's social security
number.
{ event data are often \object centric" rather than \process centric". for ex-
ample, individual products, pallets, and containers may have rfid tags and
recorded events refer to these tags. however, to monitor a particular customer
order such object-centric events need to be merged and preprocessed.
{ event data may be incomplete . a common problem is that events do not
explicitly point to process instances. often it is possible to derive this infor-
mation, but this may take considerable eorts. also time information may be
missing for some events. one may need to interpolate timestamps in order to
still use the timing information available.
{ an event log may contain outliers , i.e., exceptional behavior also referred to
asnoise . how to dene outliers? how to detect such outliers? these questions
need to be answered to clean event data.
{ logs may contain events at dierent levels of granularity . in the event log
of a hospital information system events may refer to simple blood tests or
to complex surgical procedures. also timestamps may have dierent levels of
granularity ranging from milliseconds precision (28-9-2011:h11m28s32ms342)
to coarse date information (28-9-2011).
{ events occur in a particular context (weather, workload, day of the week,
etc.). this context may explain certain phenomena, e.g., the response time is
longer than usual because of work-in-progress or holidays. for analysis, it is
desirable to incorporate this context. this implies the merging of event data
with contextual data. here the \curse of dimensionality" kicks in as analysis
becomes intractable when adding too many variables.
better tools and methodologies are needed to address the above problems. more-
over, as indicated earlier, organizations need to treat event logs as rst-class
citizens rather than some by-product. the goal is to obtain ????? event logs
(see table 1). here, the lessons learned in the context of datawarehousing are
useful to ensure high-quality event logs. for example, simple checks during data
entry can help to reduce the proportion of incorrect event data signicantly.18 wil van der aalst et al.
4.2 c2: dealing with complex event logs having diverse
characteristics
event logs may have very dierent characteristics. some event logs may be ex-
tremely large making it dicult to handle them whereas other event logs are so
small that not enough data is available to make reliable conclusions.
in some domains, mind-boggling quantities of events are recorded. therefore,
additional eorts are needed to improve performance and scalability. for exam-
ple, asml is continuously monitoring all of its wafer scanners. these wafer scan-
ners are used by various organizations (e.g., samsung and texas instruments)
to produce chips (approx. 70% of chips are produced using asml's wafer scan-
ners). existing tools have diculties dealing with the petabytes of data collected
in such domains. besides the number of events recorded there are other charac-
teristics such as the average number of events per case, similarity among cases,
the number of unique events, and the number of unique paths. consider an
event logl1 with the following characteristics: 1000 cases, on average 10 events
per case, and little variation (e.g., several cases follow the same or very similar
paths). event log l2 contains just 100 cases, but on average there are 100 events
per case and all cases follow a unique path. clearly, l2 is much more dicult
to analyze than l1 even though the two logs have similar sizes (approximately
10,000 events).
as event logs contain only sample behavior, they should not be assumed to
be complete. process mining techniques need to deal with incompleteness by
using an \open world assumption": the fact that something did not happen does
not mean that it cannot happen. this makes it challenging to deal with small
event logs with a lot of variability.
as mentioned before, some logs contain events at a very low abstraction level.
these logs tend to be extremely large and the individual low-level events are of
little interest to the stakeholders. therefore, one would like to aggregate low-
level events into high-level events. for example, when analyzing the diagnostic
and treatment processes of a particular group of patients one may not be inter-
ested in the individual tests recorded in the information system of the hospital's
laboratory.
at this point in time, organizations need to use a trial-and-error approach to
see whether an event log is suitable for process mining. therefore, tools should
allow for a quick feasibility test given a particular data set. such a test should
indicate potential performance problems and warn for logs that are far from
complete or too detailed.
4.3 c3: creating representative benchmarks
process mining is an emerging technology. this explains why good benchmarks
are still missing. for example, dozens of process discovery techniques are avail-
able and dierent vendors oer dierent products, but there is no consensus on
the quality of these techniques. although there are huge dierences in functional-
ity and performance, it is dicult to compare the dierent techniques and tools.process mining manifesto 19
therefore, good benchmarks consisting of example data sets and representative
quality criteria need to be developed.
for classical data mining techniques, many good benchmarks are available.
these benchmarks have stimulated tool providers and researchers to improve
the performance of their techniques. in the case of process mining this is more
challenging. for example, the relational model introduced by codd in 1969 is
simple and widely supported. as a result it takes little eort to convert data
from one database to another and there are no interpretation problems. for pro-
cesses such a simple model is missing. standards proposed for process modeling
are much more complicated and few vendors support exactly the same set of
concepts. processes are simply more complex than tabular data.
nevertheless, it is important to create representative benchmarks for process
mining. some initial work is already available. for example, there are various
metrics for measuring the quality of process mining results (tness, simplicity,
precision, and generalization). moreover, several event logs are publicly available
(cf.www.processmining.org ). see for example the event log used for the rst
business process intelligence challenge (bpic'11) organized by the task force
(cf.doi:10.4121/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54 ).
on the one hand, there should be benchmarks based on real-life data sets.
on the other hand, there is the need to create synthetic datasets capturing
particular characteristics. such synthetic datasets help to develop process mining
techniques that are tailored towards incomplete event logs, noisy event logs, or
specic populations of processes.
besides the creation of representative benchmarks, there also needs to be
more consensus on the criteria used to judge the quality of process mining results
(also see challenge c6). moreover, cross-validation techniques from data mining
can be adapted to judge the result. consider for example k-fold checking. one
can split the event log in kparts.k 1 parts can be used to learn a process
model and conformance checking techniques can be used to judge the result with
respect to the remaining part. this can be repeated ktimes, thus providing some
insights into the quality of the model.
4.4 c4: dealing with concept drift
the term concept drift refers to the situation in which the process is chang-
ing while being analyzed. for instance, in the beginning of the event log two
activities may be concurrent whereas later in the log these activities become
sequential. processes may change due to periodic/seasonal changes (e.g., \in
december there is more demand" or \on friday afternoon there are fewer em-
ployees available") or due to changing conditions (e.g., \the market is getting
more competitive"). such changes impact processes and it is vital to detect and
analyze them. concept drift in a process can be discovered by splitting the event
log into smaller logs and analyzing the \footprints" of the smaller logs. such \sec-
ond order" analysis requires much more event data. nevertheless, few processes
are in steady state and understanding concept drift is of prime importance for20 wil van der aalst et al.
the management of processes. therefore, additional research and tool support
are needed to adequately analyze concept drift.
4.5 c5: improving the representational bias used for process
discovery
a process discovery technique produces a model using a particular language (e.g.,
bpmn or petri nets). however, it is important to separate the visualization of
the result from the representation used during the actual discovery process. the
selection of a target language often encompasses several implicit assumptions.
it limits the search space; processes that cannot be represented by the target
language cannot be discovered. this so-called \representational bias" used dur-
ing the discovery process should be a conscious choice and should not be (only)
driven by the preferred graphical representation.
consider for example fig. 6: whether the target language allows for concur-
rency or not may have an eect on both the visualization of the discovered model
and the class of models considered by the algorithm. if the representational bias
does not allow for concurrency (fig. 6(a) is not possible) and does not allow for
multiple activities having the same label (fig. 6(c) is not possible), then only
problematic models such as the one shown in fig. 6(b) are possible. this exam-
ple shows that a more careful and rened selection of the representational bias
is needed.
4.6 c6: balancing between quality criteria such as fitness,
simplicity, precision, and generalization
event logs are often far from being complete, i.e., only example behavior is
given. process models typically allow for an exponential or even innite number
of dierent traces (in case of loops). moreover, some traces may have a much
lower probability than others. therefore, it is unrealistic to assume that every
possible trace is present in the event log. to illustrate that it is impractical to
take complete logs for granted, consider a process consisting of 10 activities that
can be executed in parallel and a corresponding log that contains information
about 10,000 cases. the total number of possible interleavings in the model with
10 concurrent activities is 10! = 3,628,800. hence, it is impossible that each
interleaving is present in the log as there are fewer cases (10,000) than potential
traces (3,628,800). even if there are millions of cases in the log, it is extremely
unlikely that all possible variations are present. an additional complication is
that some alternatives are less frequent than others. these may be considered
as \noise". it is impossible to build a reasonable model for such noisy behaviors.
the discovered model needs to abstract from this; it is better to investigate low
frequency behavior using conformance checking.
noise and incompleteness make process discovery a challenging problem. in
fact, there are four competing quality dimensions: (a) tness, (b) simplicity, (c)
precision, and (d) generalization. a model with good tness allows for most ofprocess mining manifesto 21
the behavior seen in the event log. a model has a perfect tness if all traces in the
log can be replayed by the model from beginning to end. the simplest model that
can explain the behavior seen in the log is the best model. this principle is known
as occam's razor. fitness and simplicity alone are not sucient to judge the
quality of a discovered process model. for example, it is very easy to construct
an extremely simple petri net (\ower model") that is able to replay all traces in
an event log (but also any other event log referring to the same set of activities).
similarly, it is undesirable to have a model that only allows for the exact behavior
seen in the event log. remember that the log contains only example behavior
and that many traces that are possible may not have been seen yet. a model is
precise if it does not allow for \too much" behavior. clearly, the \ower model"
lacks precision. a model that is not precise is \undertting". undertting is the
problem that the model over-generalizes the example behavior in the log (i.e.,
the model allows for behaviors very dierent from what was seen in the log).
a model should generalize and not restrict behavior to just the examples seen
in the log. a model that does not generalize is \overtting". overtting is the
problem that a very specic model is generated whereas it is obvious that the log
only holds example behavior (i.e., the model explains the particular sample log,
but a next sample log of the same process may produce a completely dierent
process model).
balancing tness, simplicity, precision and generalization is challenging. this
is the reason that most of the more powerful process discovery techniques provide
various parameters. improved algorithms need to be developed to better balance
the four competing quality dimensions. moreover, any parameters used should
be understandable by end-users.
4.7 c7: cross-organizational mining
traditionally, process mining is applied within a single organization. however, as
service technology, supply-chain integration, and cloud computing become more
widespread, there are scenarios where the event logs of multiple organizations are
available for analysis. in principle, there are two settings for cross-organizational
process mining .
first of all, we may consider the collaborative setting where dierent orga-
nizations work together to handle process instances. one can think of such a
cross-organizational process as a \jigsaw puzzle", i.e., the overall process is cut
into parts and distributed over organizations that need to cooperate to success-
fully complete cases. analyzing the event log within one of these organizations
involved is insucient. to discover end-to-end processes, the event logs of dif-
ferent organizations need to be merged. this is a non-trivial task as events need
to be correlated across organizational boundaries.
second, we may also consider the setting where dierent organizations are
essentially executing the same process while sharing experiences, knowledge,
or a common infrastructure. consider for example salesforce.com. the sales
processes of many organizations are managed and supported by salesforce. on
the one hand, these organizations share an infrastructure (processes, databases,22 wil van der aalst et al.
etc.). on the other hand, they are not forced to follow a strict process model
as the system can be congured to support variants of the same process. as
another example, consider the basic processes executed within any municipality
(e.g., issuing building permits). although all municipalities in a country need
to support the same basic set of processes, there may be also be dierences.
obviously, it is interesting to analyze such variations among dierent organi-
zations. these organizations can learn from one another and service providers
may improve their services and oer value-added services based on the results
of cross-organizational process mining.
new analysis techniques need to be developed for both types of cross-
organizational process mining. these techniques should also consider privacy
and security issues. organizations may not want to share information for com-
petitive reasons or due to a lack of trust. therefore, it is important to develop
privacy-preserving process mining techniques.
4.8 c8: providing operational support
initially, the focus of process mining was on the analysis of historical data. today,
however, many data sources are updated in (near) real-time and sucient com-
puting power is available to analyze events when they occur. therefore, process
mining should not be restricted to o-line analysis and can also be used for on-
line operational support. three operational support activities can be identied:
detect ,predict , and recommend . the moment a case deviates from the predened
process, this can be detected and the system can generate an alert. often one
would like to generate such notications immediately (to still be able to inu-
ence things) and not in an o-line fashion. historical data can be used to build
predictive models. these can be used to guide running process instances. for
example, it is possible to predict the remaining processing time of a case. based
on such predictions, one can also build recommender systems that propose par-
ticular actions to reduce costs or shorten the ow time. applying process mining
techniques in such an online setting creates additional challenges in terms of
computing power and data quality.
4.9 c9: combining process mining with other types of analysis
operations management, and in particular operations research, is a branch of
management science heavily relying on modeling. here a variety of mathemati-
cal models ranging from linear programming and project planning to queueing
models, markov chains, and simulation are used. data mining can be dened as
\the analysis of (often large) data sets to nd unsuspected relationships and to
summarize the data in novel ways that are both understandable and useful to
the data owner". a wide variety of techniques have been developed: classication
(e.g., decision tree learning), regression, clustering (e.g., k-means clustering) and
pattern discovery (e.g., association rule learning).
both elds (operations management and data mining) provide valuable anal-
ysis techniques. the challenge is to combine the techniques in these elds withprocess mining manifesto 23
process mining. consider for example simulation. process mining techniques can
be used to learn a simulation model based on historical data. subsequently, the
simulation model can be used to provide operational support. because of the
close connection between event log and model, the model can be used to replay
history and one can start simulations from the current state thus providing a
\fast forward button" into the future based on live data.
similarly, it is desirable to combine process mining with visual analytics . vi-
sual analytics combines automated analysis with interactive visualizations for a
better understanding of large and complex data sets. visual analytics exploits
the amazing capabilities of humans to see patterns in unstructured data. by com-
bining automated process mining techniques with interactive visual analytics, it
is possible to extract more insights from event data.
4.10 c10: improving usability for non-experts
one of the goals of process mining is to create \living process models", i.e.,
process models that are used on a daily basis rather than static models that end
up in some archive. new event data can be used to discover emerging behavior.
the link between event data and process models allows for the projection of
the current state and recent activities onto up-to-date models. hence, end-users
can interact with the results of process mining on a day-to-day basis. such
interactions are very valuable, but also require intuitive user interfaces. the
challenge is to hide the sophisticated process mining algorithms behind user-
friendly interfaces that automatically set parameters and suggest suitable types
of analysis.
4.11 c11: improving understandability for non-experts
even if it is easy to generate process mining results, this does not mean that
the results are actually useful. the user may have problems understanding the
output or is tempted to infer incorrect conclusions. to avoid such problems, the
results should be presented using a suitable representation (see also gp5). more-
over, the trustworthiness of the results should always be clearly indicated. there
may be too little data to justify particular conclusions. in fact, existing process
discovery techniques typically do not warn for a low tness or for overtting.
they always show a model, even when it is clear that there is too little data to
justify any conclusions.
5 epilogue
the ieee task force on process mining aims to (a) promote the application of
process mining, (b) guide software developers, consultants, business managers,
and end-users when using state-of-the-art techniques, and (c) stimulate research
on process mining. this manifesto states the main principles and intentions24 wil van der aalst et al.
of the task force. after introducing the topic of process mining, the manifesto
catalogs some guiding principles (section 3) and challenges (section 4). the
guiding principles can be used in order to avoid obvious mistakes. the list of
challenges is intended to direct research and development eorts. both aim to
increase the maturity level of process mining.
to conclude, a few words on terminology. the following terms are used in
the process mining space: workow mining, (business) process mining, auto-
mated (business) process discovery, and (business) process intelligence. dierent
organizations seem to use dierent terms for overlapping concepts. for exam-
ple, gartner is promoting the term \automated business process discovery"
(abpd) and software ag is using \process intelligence" to refer to their con-
trolling platform. the term \workow mining" seems less suitable as the creation
of workow models is just one of the many possible applications of process min-
ing. similarly, the addition of the term \business" narrows the scope to certain
applications of process mining. there are numerous applications of process min-
ing (e.g., analyzing the use of high-tech systems or analyzing websites) where
this addition seems to be inappropriate. although process discovery is an im-
portant part of the process mining spectrum, it is only one of the many use
cases. conformance checking, prediction, organizational mining, social network
analysis, etc. are other use cases that extend beyond process discovery.
business intelligence
process intelligence
process mining
(automated business) process discovery
conformance checking
model enhancement 
fig. 7. relating the dierent terms.
figure 7 relates some of the terms just mentioned. all technologies and meth-
ods that aim at providing actionable information that can be used to support
decision making can be positioned under the umbrella of business intelligence
(bi). (business) process intelligence can be seen as the combination of bi and
bpm, i.e., bi techniques are used to analyze and improve processes and their
management. process mining can be seen as a concretization of process intelli-process mining manifesto 25
gence taking event logs as a starting point. (automated business) process dis-
covery is just one of the three basic types of process mining. figure 7 may be
a bit misleading in the sense that most bi tools do not provide process mining
functionality as described in this document. the term bi is often conveniently
skewed towards a particular tool or method covering only a small part of the
broader bi spectrum.
there may be commercial reasons for using alternative terms. some vendors
may also want to emphasize a particular aspect (e.g., discovery or intelligence).
however, to avoid confusion, it is better to use the term \process mining" for
the discipline covered by this manifesto.
references
1. w.m.p. van der aalst. process mining: discovery, conformance and enhancement
of business processes . springer-verlag, berlin, 2011.
glossary
{activity : a well-dened step in the process. events may refer to the start,
completion, cancelation, etc. of an activity for a specic process instance.
{automated business process discovery : seeprocess discovery .
{business intelligence (bi): broad collection of tools and methods that use
data to support decision making.
{business process intelligence : seeprocess intelligence .
{business process management (bpm): the discipline that combines
knowledge from information technology and knowledge from management sci-
ences and applies both to operational business processes.
{case : seeprocess instance .
{concept drift : the phenomenon that processes often change over time. the
observed process may gradually (or suddenly) change due to seasonal changes
or increased competition, thus complicating analysis.
{conformance checking : analyzing whether reality, as recorded in a log,
conforms to the model and vice versa. the goal is to detect discrepancies and
to measure their severity. conformance checking is one of the three basic types
of process mining.
{cross-organizational process mining : the application of process mining
techniques to event logs originating from dierent organizations.
{data mining : the analysis of (often large) data sets to nd unexpected re-
lationships and to summarize the data in ways that provide new insights.
{event : an action recorded in the log, e.g., the start, completion, or cancelation
of an activity for a particular process instance.
{event log : collection of events used as input for process mining. events do
not need to be stored in a separate log le (e.g., events may be scattered over
dierent database tables).26 wil van der aalst et al.
{fitness : a measure determining how well a given model allows for the behavior
seen in the event log. a model has a perfect tness if all traces in the log can
be replayed by the model from beginning to end.
{generalization : a measure determining how well the model is able to allow
for unseen behavior. an \overtting" model is not able to generalize enough.
{model enhancement : one of the three basic types of process mining. a
process model is extended or improved using information extracted from some
log. for example, bottlenecks can be identied by replaying an event log on a
process model while examining the timestamps.
{mxml : an xml-based format for exchanging event logs. xes replaces
mxml as the new tool-independent process mining format.
{operational support : on-line analysis of event data with the aim to monitor
and inuence running process instances. three operational support activities
can be identied: detect (generate an alert if the observed behavior deviates
from the modeled behavior), predict (predict future behavior based on past be-
havior, e.g., predict the remaining processing time), and recommend (suggest
appropriate actions to realize a particular goal, e.g., to minimize costs).
{precision : measure determining whether the model prohibits behavior very
dierent from the behavior seen in the event log. a model with low precision
is \undertting".
{process discovery : one of the three basic types of process mining. based on
an event log a process model is learned. for example, the algorithm is able
to discover a petri net by identifying process patterns in collections of events.
{process instance : the entity being handled by the process that is analyzed.
events refer to process instances. examples of process instances are customer
orders, insurance claims, loan applications, etc.
{process intelligence : a branch of business intelligence focusing on business
process management.
{process mining : techniques, tools, and methods to discover, monitor and
improve real processes (i.e., not assumed processes) by extracting knowledge
from event logs commonly available in today's (information) systems.
{representational bias : the selected target language for presenting and con-
structing process mining results.
{simplicity : a measure operationalizing occam's razor, i.e., the simplest
model that can explain the behavior seen in the log, is the best model. sim-
plicity can be quantied in various ways, e.g., number of nodes and arcs in
the model.
{xes : is an xml-based standard for event logs. the standard has been
adopted by the ieee task force on process mining as the default interchange
format for event logs (cf. www.xes-standard.org ).